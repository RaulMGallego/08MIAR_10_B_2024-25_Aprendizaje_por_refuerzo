{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5010851-cd69-481b-a08e-1f2d94b1f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SpaceInvaders DQN - Entrenamiento en CPU con checkpoints (sin matplotlib)\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import gym\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import ModelIntervalCheckpoint, FileLogger, Callback\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./models\\\\dqn_weights_10000.h5f.index', './models\\\\dqn_weights_100000.h5f.index', './models\\\\dqn_weights_1000000.h5f.index', './models\\\\dqn_weights_110000.h5f.index', './models\\\\dqn_weights_120000.h5f.index', './models\\\\dqn_weights_130000.h5f.index', './models\\\\dqn_weights_140000.h5f.index', './models\\\\dqn_weights_150000.h5f.index', './models\\\\dqn_weights_160000.h5f.index', './models\\\\dqn_weights_170000.h5f.index', './models\\\\dqn_weights_180000.h5f.index', './models\\\\dqn_weights_190000.h5f.index', './models\\\\dqn_weights_20000.h5f.index', './models\\\\dqn_weights_200000.h5f.index', './models\\\\dqn_weights_210000.h5f.index', './models\\\\dqn_weights_220000.h5f.index', './models\\\\dqn_weights_230000.h5f.index', './models\\\\dqn_weights_240000.h5f.index', './models\\\\dqn_weights_250000.h5f.index', './models\\\\dqn_weights_260000.h5f.index', './models\\\\dqn_weights_270000.h5f.index', './models\\\\dqn_weights_280000.h5f.index', './models\\\\dqn_weights_290000.h5f.index', './models\\\\dqn_weights_30000.h5f.index', './models\\\\dqn_weights_300000.h5f.index', './models\\\\dqn_weights_310000.h5f.index', './models\\\\dqn_weights_320000.h5f.index', './models\\\\dqn_weights_330000.h5f.index', './models\\\\dqn_weights_340000.h5f.index', './models\\\\dqn_weights_350000.h5f.index', './models\\\\dqn_weights_360000.h5f.index', './models\\\\dqn_weights_370000.h5f.index', './models\\\\dqn_weights_380000.h5f.index', './models\\\\dqn_weights_390000.h5f.index', './models\\\\dqn_weights_40000.h5f.index', './models\\\\dqn_weights_400000.h5f.index', './models\\\\dqn_weights_410000.h5f.index', './models\\\\dqn_weights_420000.h5f.index', './models\\\\dqn_weights_430000.h5f.index', './models\\\\dqn_weights_440000.h5f.index', './models\\\\dqn_weights_450000.h5f.index', './models\\\\dqn_weights_460000.h5f.index', './models\\\\dqn_weights_470000.h5f.index', './models\\\\dqn_weights_480000.h5f.index', './models\\\\dqn_weights_490000.h5f.index', './models\\\\dqn_weights_50000.h5f.index', './models\\\\dqn_weights_500000.h5f.index', './models\\\\dqn_weights_510000.h5f.index', './models\\\\dqn_weights_520000.h5f.index', './models\\\\dqn_weights_530000.h5f.index', './models\\\\dqn_weights_540000.h5f.index', './models\\\\dqn_weights_550000.h5f.index', './models\\\\dqn_weights_560000.h5f.index', './models\\\\dqn_weights_570000.h5f.index', './models\\\\dqn_weights_580000.h5f.index', './models\\\\dqn_weights_590000.h5f.index', './models\\\\dqn_weights_60000.h5f.index', './models\\\\dqn_weights_600000.h5f.index', './models\\\\dqn_weights_610000.h5f.index', './models\\\\dqn_weights_620000.h5f.index', './models\\\\dqn_weights_630000.h5f.index', './models\\\\dqn_weights_640000.h5f.index', './models\\\\dqn_weights_650000.h5f.index', './models\\\\dqn_weights_660000.h5f.index', './models\\\\dqn_weights_670000.h5f.index', './models\\\\dqn_weights_680000.h5f.index', './models\\\\dqn_weights_690000.h5f.index', './models\\\\dqn_weights_70000.h5f.index', './models\\\\dqn_weights_700000.h5f.index', './models\\\\dqn_weights_710000.h5f.index', './models\\\\dqn_weights_720000.h5f.index', './models\\\\dqn_weights_730000.h5f.index', './models\\\\dqn_weights_740000.h5f.index', './models\\\\dqn_weights_750000.h5f.index', './models\\\\dqn_weights_760000.h5f.index', './models\\\\dqn_weights_770000.h5f.index', './models\\\\dqn_weights_780000.h5f.index', './models\\\\dqn_weights_790000.h5f.index', './models\\\\dqn_weights_80000.h5f.index', './models\\\\dqn_weights_800000.h5f.index', './models\\\\dqn_weights_810000.h5f.index', './models\\\\dqn_weights_820000.h5f.index', './models\\\\dqn_weights_830000.h5f.index', './models\\\\dqn_weights_840000.h5f.index', './models\\\\dqn_weights_850000.h5f.index', './models\\\\dqn_weights_860000.h5f.index', './models\\\\dqn_weights_870000.h5f.index', './models\\\\dqn_weights_880000.h5f.index', './models\\\\dqn_weights_890000.h5f.index', './models\\\\dqn_weights_90000.h5f.index', './models\\\\dqn_weights_900000.h5f.index', './models\\\\dqn_weights_910000.h5f.index', './models\\\\dqn_weights_920000.h5f.index', './models\\\\dqn_weights_930000.h5f.index', './models\\\\dqn_weights_940000.h5f.index', './models\\\\dqn_weights_950000.h5f.index', './models\\\\dqn_weights_960000.h5f.index', './models\\\\dqn_weights_970000.h5f.index', './models\\\\dqn_weights_980000.h5f.index', './models\\\\dqn_weights_990000.h5f.index']\n",
      "Cargando pesos desde: ./models\\dqn_weights_1000000.h5f\n",
      "Hay pesos anteriores y se van a cargar\n",
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rociniel\\Anaconda3\\envs\\miar_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   546/20000: episode: 1, duration: 5.688s, episode steps: 546, steps per second:  96, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1263/20000: episode: 2, duration: 6.758s, episode steps: 717, steps per second: 106, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  2288/20000: episode: 3, duration: 10.946s, episode steps: 1025, steps per second:  94, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  2783/20000: episode: 4, duration: 4.412s, episode steps: 495, steps per second: 112, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3300/20000: episode: 5, duration: 5.496s, episode steps: 517, steps per second:  94, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3680/20000: episode: 6, duration: 3.922s, episode steps: 380, steps per second:  97, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.782 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4476/20000: episode: 7, duration: 8.851s, episode steps: 796, steps per second:  90, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rociniel\\Anaconda3\\envs\\miar_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5191/20000: episode: 8, duration: 23.847s, episode steps: 715, steps per second:  30, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.023540, mae: 2.767672, mean_q: 3.338632, mean_eps: 0.747748\n",
      "  6177/20000: episode: 9, duration: 123.681s, episode steps: 986, steps per second:   8, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.017756, mae: 2.784768, mean_q: 3.358375, mean_eps: 0.718642\n",
      "  6552/20000: episode: 10, duration: 25.832s, episode steps: 375, steps per second:  15, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.017109, mae: 2.770625, mean_q: 3.345133, mean_eps: 0.684982\n",
      "  7734/20000: episode: 11, duration: 106.563s, episode steps: 1182, steps per second:  11, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.015900, mae: 2.763540, mean_q: 3.330317, mean_eps: 0.646471\n",
      "  8863/20000: episode: 12, duration: 87.707s, episode steps: 1129, steps per second:  13, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.015193, mae: 2.761318, mean_q: 3.328075, mean_eps: 0.589249\n",
      "  9755/20000: episode: 13, duration: 63.523s, episode steps: 892, steps per second:  14, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.015301, mae: 2.697782, mean_q: 3.253124, mean_eps: 0.539254\n",
      " 10275/20000: episode: 14, duration: 38.087s, episode steps: 520, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.012981, mae: 2.644216, mean_q: 3.189466, mean_eps: 0.504307\n",
      " 11242/20000: episode: 15, duration: 64.501s, episode steps: 967, steps per second:  15, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.013270, mae: 2.642651, mean_q: 3.188099, mean_eps: 0.467479\n",
      " 11994/20000: episode: 16, duration: 49.164s, episode steps: 752, steps per second:  15, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012324, mae: 2.650836, mean_q: 3.197981, mean_eps: 0.424909\n",
      " 12801/20000: episode: 17, duration: 59.979s, episode steps: 807, steps per second:  13, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.012064, mae: 2.644880, mean_q: 3.190824, mean_eps: 0.386299\n",
      " 13370/20000: episode: 18, duration: 38.366s, episode steps: 569, steps per second:  15, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.015315, mae: 2.621974, mean_q: 3.161616, mean_eps: 0.352243\n"
     ]
    }
   ],
   "source": [
    "# Configuración\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "ENV_NAME = 'SpaceInvaders-v0'\n",
    "MODEL_DIR = './models/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "# class EpisodeRewardLogger(Callback):\n",
    "#     def __init__(self):\n",
    "#         self.episode_rewards = []\n",
    "\n",
    "#     def on_episode_end(self, episode, logs={}):\n",
    "#         self.episode_rewards.append(logs.get('episode_reward'))\n",
    "\n",
    "# Cargar automáticamente el último checkpoint si existe\n",
    "def get_latest_weights(model_dir):\n",
    "    #weights_files = glob.glob(os.path.join(model_dir, 'dqn_weights_*.h5f'))\n",
    "    weights_files = glob.glob(os.path.join(model_dir, 'dqn_weights_*.h5f.index'))\n",
    "    print(weights_files)\n",
    "    if not weights_files:\n",
    "        print(\"No se encontraron checkpoints previos.\")\n",
    "        return None\n",
    "    latest_file = max(weights_files, key=os.path.getctime)\n",
    "    latest_file = latest_file.replace('.index', '')\n",
    "    print(\"Cargando pesos desde:\", latest_file)\n",
    "    return latest_file\n",
    "\n",
    "latest_weights = get_latest_weights(MODEL_DIR)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env = Monitor(env, './video_test_01', force=True, video_callable=lambda episode_id: True)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        return np.array(img).astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch.astype('float32') / 255.\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "# Memoria y política\n",
    "memory = SequentialMemory(limit=1_000_000, window_length=WINDOW_LENGTH)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', \n",
    "                              value_max=1.0,\n",
    "                              value_min=0.01, \n",
    "                              value_test=0.01, \n",
    "                              nb_steps=20_000)\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions, \n",
    "               policy=policy, memory=memory,\n",
    "               processor=AtariProcessor(), \n",
    "               enable_double_dqn=True,\n",
    "               nb_steps_warmup=5_000,\n",
    "               gamma=0.99, \n",
    "               target_model_update=50_000, \n",
    "               train_interval=4,\n",
    "               delta_clip=1.0)\n",
    "dqn.compile(Adam(learning_rate=0.0001), metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelIntervalCheckpoint(MODEL_DIR + 'dqn_weights_{step}.h5f', interval=10000)\n",
    "logger = FileLogger(LOG_DIR + 'dqn_log.json', interval=100)\n",
    "\n",
    "# Si hay pesos anteriores, cargarlos antes de seguir\n",
    "if latest_weights:\n",
    "    dqn.load_weights(latest_weights)\n",
    "    print('Hay pesos anteriores y se van a cargar')\n",
    "\n",
    "# reward_logger = EpisodeRewardLogger()\n",
    "\n",
    "# Entrenamiento (ajusta nb_steps según necesites)\n",
    "dqn.fit(env, nb_steps=20_000, visualize=False, verbose=2, callbacks=[checkpoint, logger])\n",
    "# dqn.fit(env, nb_steps=250_000, visualize=False, verbose=2, callbacks=[checkpoint, logger, reward_logger])\n",
    "\n",
    "# Guardar pesos finales\n",
    "dqn.save_weights(MODEL_DIR + 'dqn_final_weights.h5f', overwrite=True)\n",
    "\n",
    "print('########################################################')\n",
    "print('PROCESO TERMINADO')\n",
    "print('########################################################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d6dd74-d227-4d32-95f3-6238935a4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 15.000, steps: 674\n",
      "Episode 2: reward: 5.000, steps: 453\n",
      "Episode 3: reward: 6.000, steps: 505\n",
      "Episode 4: reward: 5.000, steps: 542\n",
      "Episode 5: reward: 12.000, steps: 1113\n",
      "Episode 6: reward: 21.000, steps: 832\n",
      "Episode 7: reward: 16.000, steps: 914\n",
      "Episode 8: reward: 6.000, steps: 770\n",
      "Episode 9: reward: 8.000, steps: 490\n",
      "Episode 10: reward: 8.000, steps: 654\n",
      "\n",
      "Recompensa media en test: 10.20\n"
     ]
    }
   ],
   "source": [
    "# Mostrar datos de ENTRENAMIENTO\n",
    "def mostrar_entrenamiento(log_path = LOG_DIR + 'dqn_log.json', ancho=800, alto=200):\n",
    "    with open(log_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rewards = data['episode_reward']\n",
    "    if not rewards:\n",
    "        print(\"No hay datos de entrenamiento.\")\n",
    "        return\n",
    "\n",
    "    max_reward = max(rewards)\n",
    "    min_reward = min(rewards)\n",
    "    escalar = alto / (max_reward - min_reward + 1)\n",
    "\n",
    "    img = Image.new('RGB', (ancho + 60, alto + 40), color='white')\n",
    "    dibujar = ImageDraw.Draw(img)\n",
    "\n",
    "    paso = max(1, ancho // len(rewards))\n",
    "    offset_x = 40\n",
    "    offset_y = 20\n",
    "\n",
    "    for i, r in enumerate(rewards):\n",
    "        x = offset_x + i * paso\n",
    "        y = offset_y + int((max_reward - r) * escalar)\n",
    "        dibujar.rectangle([x, y, x + paso - 1, alto + offset_y], fill='green')\n",
    "\n",
    "    # Eje Y - min y max\n",
    "    dibujar.text((5, offset_y), f\"{max_reward:.0f}\", fill='black')\n",
    "    dibujar.text((5, alto + offset_y - 10), f\"{min_reward:.0f}\", fill='black')\n",
    "\n",
    "    # Eje X - episodios\n",
    "    dibujar.text((offset_x, alto + offset_y + 5), \"0\", fill='black')\n",
    "    dibujar.text((offset_x + len(rewards)*paso - 30, alto + offset_y + 5),\n",
    "                 f\"{len(rewards)}\", fill='black')\n",
    "\n",
    "    print(\"Gráfico de entrenamiento con ejes:\")\n",
    "    display(img)\n",
    "\n",
    "# Mostrar datos de TEST\n",
    "def mostrar_test_con_ejes(rewards, ancho=600, alto=200, divisiones_y=4):\n",
    "    if not rewards:\n",
    "        print(\"No hay datos de test.\")\n",
    "        return\n",
    "\n",
    "    max_reward = max(rewards)\n",
    "    min_reward = min(rewards)\n",
    "    rango = max_reward - min_reward if max_reward != min_reward else 1\n",
    "    escalar = alto / (rango + 10)\n",
    "\n",
    "    margen_izquierdo = 50\n",
    "    margen_inferior = 30\n",
    "    img = Image.new('RGB', (ancho + margen_izquierdo, alto + margen_inferior), color='white')\n",
    "    dibujar = ImageDraw.Draw(img)\n",
    "\n",
    "    paso = max(1, ancho // len(rewards))\n",
    "\n",
    "    # Dibujar barras\n",
    "    for i, r in enumerate(rewards):\n",
    "        x = margen_izquierdo + i * paso\n",
    "        y = int((max_reward - r) * escalar)\n",
    "        dibujar.rectangle([x, y, x + paso - 1, alto], fill='blue')\n",
    "\n",
    "    # Líneas y etiquetas en eje Y\n",
    "    for i in range(divisiones_y + 1):\n",
    "        valor = min_reward + i * (rango / divisiones_y)\n",
    "        y = int((max_reward - valor) * escalar)\n",
    "        dibujar.line([(margen_izquierdo - 5, y), (ancho + margen_izquierdo, y)], fill='gray', width=1)\n",
    "        dibujar.text((5, y - 7), f\"{valor:.0f}\", fill='black')\n",
    "\n",
    "    # Etiquetas en eje X\n",
    "    dibujar.text((margen_izquierdo, alto + 5), \"0\", fill='black')\n",
    "    dibujar.text((ancho + margen_izquierdo - 30, alto + 5), f\"{len(rewards)}\", fill='black')\n",
    "\n",
    "    print(\"Gráfico de recompensas de test con ejes:\")\n",
    "    display(img)\n",
    "\n",
    "#  TEST DEL AGENTE\n",
    "history = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "test_rewards = history.history['episode_reward']\n",
    "print(f\"\\nRecompensa media en test: {np.mean(test_rewards):.2f}\")\n",
    "\n",
    "# # VISUALIZACION DE DATOS\n",
    "# print(\"DATOS DE ENTRENAMIENTO\")\n",
    "# print()\n",
    "# mostrar_entrenamiento()\n",
    "# print(\"DATOS DE TEST\")\n",
    "# mostrar_test_con_ejes(test_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miar_rl]",
   "language": "python",
   "name": "conda-env-miar_rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
