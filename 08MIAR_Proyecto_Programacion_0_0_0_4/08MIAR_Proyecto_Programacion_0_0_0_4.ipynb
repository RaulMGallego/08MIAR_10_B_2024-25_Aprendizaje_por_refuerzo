{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5010851-cd69-481b-a08e-1f2d94b1f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SpaceInvaders DQN - Entrenamiento en CPU con checkpoints (sin matplotlib)\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import gym\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import ModelIntervalCheckpoint, FileLogger, Callback\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e38cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./models\\\\dqn_weights_10000.h5f.index', './models\\\\dqn_weights_100000.h5f.index', './models\\\\dqn_weights_1000000.h5f.index', './models\\\\dqn_weights_110000.h5f.index', './models\\\\dqn_weights_120000.h5f.index', './models\\\\dqn_weights_130000.h5f.index', './models\\\\dqn_weights_140000.h5f.index', './models\\\\dqn_weights_150000.h5f.index', './models\\\\dqn_weights_160000.h5f.index', './models\\\\dqn_weights_170000.h5f.index', './models\\\\dqn_weights_180000.h5f.index', './models\\\\dqn_weights_190000.h5f.index', './models\\\\dqn_weights_20000.h5f.index', './models\\\\dqn_weights_200000.h5f.index', './models\\\\dqn_weights_210000.h5f.index', './models\\\\dqn_weights_220000.h5f.index', './models\\\\dqn_weights_230000.h5f.index', './models\\\\dqn_weights_240000.h5f.index', './models\\\\dqn_weights_250000.h5f.index', './models\\\\dqn_weights_260000.h5f.index', './models\\\\dqn_weights_270000.h5f.index', './models\\\\dqn_weights_280000.h5f.index', './models\\\\dqn_weights_290000.h5f.index', './models\\\\dqn_weights_30000.h5f.index', './models\\\\dqn_weights_300000.h5f.index', './models\\\\dqn_weights_310000.h5f.index', './models\\\\dqn_weights_320000.h5f.index', './models\\\\dqn_weights_330000.h5f.index', './models\\\\dqn_weights_340000.h5f.index', './models\\\\dqn_weights_350000.h5f.index', './models\\\\dqn_weights_360000.h5f.index', './models\\\\dqn_weights_370000.h5f.index', './models\\\\dqn_weights_380000.h5f.index', './models\\\\dqn_weights_390000.h5f.index', './models\\\\dqn_weights_40000.h5f.index', './models\\\\dqn_weights_400000.h5f.index', './models\\\\dqn_weights_410000.h5f.index', './models\\\\dqn_weights_420000.h5f.index', './models\\\\dqn_weights_430000.h5f.index', './models\\\\dqn_weights_440000.h5f.index', './models\\\\dqn_weights_450000.h5f.index', './models\\\\dqn_weights_460000.h5f.index', './models\\\\dqn_weights_470000.h5f.index', './models\\\\dqn_weights_480000.h5f.index', './models\\\\dqn_weights_490000.h5f.index', './models\\\\dqn_weights_50000.h5f.index', './models\\\\dqn_weights_500000.h5f.index', './models\\\\dqn_weights_510000.h5f.index', './models\\\\dqn_weights_520000.h5f.index', './models\\\\dqn_weights_530000.h5f.index', './models\\\\dqn_weights_540000.h5f.index', './models\\\\dqn_weights_550000.h5f.index', './models\\\\dqn_weights_560000.h5f.index', './models\\\\dqn_weights_570000.h5f.index', './models\\\\dqn_weights_580000.h5f.index', './models\\\\dqn_weights_590000.h5f.index', './models\\\\dqn_weights_60000.h5f.index', './models\\\\dqn_weights_600000.h5f.index', './models\\\\dqn_weights_610000.h5f.index', './models\\\\dqn_weights_620000.h5f.index', './models\\\\dqn_weights_630000.h5f.index', './models\\\\dqn_weights_640000.h5f.index', './models\\\\dqn_weights_650000.h5f.index', './models\\\\dqn_weights_660000.h5f.index', './models\\\\dqn_weights_670000.h5f.index', './models\\\\dqn_weights_680000.h5f.index', './models\\\\dqn_weights_690000.h5f.index', './models\\\\dqn_weights_70000.h5f.index', './models\\\\dqn_weights_700000.h5f.index', './models\\\\dqn_weights_710000.h5f.index', './models\\\\dqn_weights_720000.h5f.index', './models\\\\dqn_weights_730000.h5f.index', './models\\\\dqn_weights_740000.h5f.index', './models\\\\dqn_weights_750000.h5f.index', './models\\\\dqn_weights_760000.h5f.index', './models\\\\dqn_weights_770000.h5f.index', './models\\\\dqn_weights_780000.h5f.index', './models\\\\dqn_weights_790000.h5f.index', './models\\\\dqn_weights_80000.h5f.index', './models\\\\dqn_weights_800000.h5f.index', './models\\\\dqn_weights_810000.h5f.index', './models\\\\dqn_weights_820000.h5f.index', './models\\\\dqn_weights_830000.h5f.index', './models\\\\dqn_weights_840000.h5f.index', './models\\\\dqn_weights_850000.h5f.index', './models\\\\dqn_weights_860000.h5f.index', './models\\\\dqn_weights_870000.h5f.index', './models\\\\dqn_weights_880000.h5f.index', './models\\\\dqn_weights_890000.h5f.index', './models\\\\dqn_weights_90000.h5f.index', './models\\\\dqn_weights_900000.h5f.index', './models\\\\dqn_weights_910000.h5f.index', './models\\\\dqn_weights_920000.h5f.index', './models\\\\dqn_weights_930000.h5f.index', './models\\\\dqn_weights_940000.h5f.index', './models\\\\dqn_weights_950000.h5f.index', './models\\\\dqn_weights_960000.h5f.index', './models\\\\dqn_weights_970000.h5f.index', './models\\\\dqn_weights_980000.h5f.index', './models\\\\dqn_weights_990000.h5f.index']\n",
      "Cargando pesos desde: ./models\\dqn_weights_1000000.h5f\n",
      "Hay pesos anteriores y se van a cargar\n",
      "Training for 100000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rociniel\\Anaconda3\\envs\\miar_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   420/100000: episode: 1, duration: 4.383s, episode steps: 420, steps per second:  96, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1242/100000: episode: 2, duration: 7.695s, episode steps: 822, steps per second: 107, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1870/100000: episode: 3, duration: 5.536s, episode steps: 628, steps per second: 113, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  2558/100000: episode: 4, duration: 5.043s, episode steps: 688, steps per second: 136, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  2954/100000: episode: 5, duration: 2.281s, episode steps: 396, steps per second: 174, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3560/100000: episode: 6, duration: 5.257s, episode steps: 606, steps per second: 115, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4191/100000: episode: 7, duration: 5.856s, episode steps: 631, steps per second: 108, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4557/100000: episode: 8, duration: 2.452s, episode steps: 366, steps per second: 149, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rociniel\\Anaconda3\\envs\\miar_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5580/100000: episode: 9, duration: 45.479s, episode steps: 1023, steps per second:  22, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.021064, mae: 2.892812, mean_q: 3.482731, mean_eps: 0.952390\n",
      "  6622/100000: episode: 10, duration: 65.970s, episode steps: 1042, steps per second:  16, episode reward:  7.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.016070, mae: 2.846563, mean_q: 3.430616, mean_eps: 0.945100\n",
      "  7280/100000: episode: 11, duration: 42.836s, episode steps: 658, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.014234, mae: 2.866731, mean_q: 3.457438, mean_eps: 0.937450\n",
      "  7776/100000: episode: 12, duration: 32.378s, episode steps: 496, steps per second:  15, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.016337, mae: 2.861814, mean_q: 3.451287, mean_eps: 0.932266\n",
      "  8572/100000: episode: 13, duration: 52.026s, episode steps: 796, steps per second:  15, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.013610, mae: 2.834000, mean_q: 3.418113, mean_eps: 0.926452\n",
      "  9332/100000: episode: 14, duration: 49.148s, episode steps: 760, steps per second:  15, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.013783, mae: 2.834004, mean_q: 3.419742, mean_eps: 0.919450\n",
      "  9807/100000: episode: 15, duration: 33.325s, episode steps: 475, steps per second:  14, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.012725, mae: 2.814185, mean_q: 3.393834, mean_eps: 0.913888\n",
      " 10421/100000: episode: 16, duration: 47.126s, episode steps: 614, steps per second:  13, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012812, mae: 2.793216, mean_q: 3.371191, mean_eps: 0.908974\n",
      " 11008/100000: episode: 17, duration: 37.862s, episode steps: 587, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.013679, mae: 2.784442, mean_q: 3.358614, mean_eps: 0.903574\n",
      " 11929/100000: episode: 18, duration: 60.349s, episode steps: 921, steps per second:  15, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012343, mae: 2.792070, mean_q: 3.367130, mean_eps: 0.896788\n",
      " 12329/100000: episode: 19, duration: 28.710s, episode steps: 400, steps per second:  14, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012982, mae: 2.775917, mean_q: 3.346784, mean_eps: 0.890830\n",
      " 12959/100000: episode: 20, duration: 38.388s, episode steps: 630, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.013310, mae: 2.764067, mean_q: 3.335233, mean_eps: 0.886204\n",
      " 13624/100000: episode: 21, duration: 44.415s, episode steps: 665, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.011286, mae: 2.786705, mean_q: 3.362762, mean_eps: 0.880390\n",
      " 14466/100000: episode: 22, duration: 60.979s, episode steps: 842, steps per second:  14, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.013073, mae: 2.737588, mean_q: 3.302267, mean_eps: 0.873604\n",
      " 15091/100000: episode: 23, duration: 40.051s, episode steps: 625, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.011939, mae: 2.725645, mean_q: 3.288013, mean_eps: 0.866998\n",
      " 16085/100000: episode: 24, duration: 64.988s, episode steps: 994, steps per second:  15, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014633, mae: 2.716699, mean_q: 3.277165, mean_eps: 0.859708\n",
      " 16471/100000: episode: 25, duration: 30.369s, episode steps: 386, steps per second:  13, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.014802, mae: 2.724195, mean_q: 3.285817, mean_eps: 0.853498\n",
      " 16935/100000: episode: 26, duration: 31.637s, episode steps: 464, steps per second:  15, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.011463, mae: 2.702361, mean_q: 3.261097, mean_eps: 0.849682\n",
      " 17993/100000: episode: 27, duration: 79.331s, episode steps: 1058, steps per second:  13, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.013951, mae: 2.706970, mean_q: 3.265862, mean_eps: 0.842824\n",
      " 18649/100000: episode: 28, duration: 42.451s, episode steps: 656, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.010843, mae: 2.705850, mean_q: 3.266735, mean_eps: 0.835102\n",
      " 19305/100000: episode: 29, duration: 42.791s, episode steps: 656, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010639, mae: 2.691842, mean_q: 3.248655, mean_eps: 0.829198\n",
      " 20044/100000: episode: 30, duration: 47.619s, episode steps: 739, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.012864, mae: 2.701199, mean_q: 3.259368, mean_eps: 0.822934\n",
      " 20878/100000: episode: 31, duration: 51.631s, episode steps: 834, steps per second:  16, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012403, mae: 2.715481, mean_q: 3.276940, mean_eps: 0.815860\n",
      " 21629/100000: episode: 32, duration: 50.177s, episode steps: 751, steps per second:  15, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.013993, mae: 2.693481, mean_q: 3.249784, mean_eps: 0.808714\n",
      " 22843/100000: episode: 33, duration: 85.067s, episode steps: 1214, steps per second:  14, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.011671, mae: 2.703664, mean_q: 3.264417, mean_eps: 0.799876\n",
      " 23755/100000: episode: 34, duration: 60.587s, episode steps: 912, steps per second:  15, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.013998, mae: 2.711556, mean_q: 3.271920, mean_eps: 0.790318\n",
      " 24269/100000: episode: 35, duration: 33.717s, episode steps: 514, steps per second:  15, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.011716, mae: 2.680704, mean_q: 3.233610, mean_eps: 0.783892\n",
      " 25021/100000: episode: 36, duration: 50.016s, episode steps: 752, steps per second:  15, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013019, mae: 2.672366, mean_q: 3.224816, mean_eps: 0.778186\n",
      " 25609/100000: episode: 37, duration: 37.868s, episode steps: 588, steps per second:  16, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.014541, mae: 2.691505, mean_q: 3.248800, mean_eps: 0.772156\n",
      " 26328/100000: episode: 38, duration: 48.313s, episode steps: 719, steps per second:  15, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.010607, mae: 2.678826, mean_q: 3.233685, mean_eps: 0.766288\n",
      " 27156/100000: episode: 39, duration: 54.349s, episode steps: 828, steps per second:  15, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010611, mae: 2.675089, mean_q: 3.229552, mean_eps: 0.759340\n",
      " 27645/100000: episode: 40, duration: 30.859s, episode steps: 489, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011378, mae: 2.684937, mean_q: 3.239748, mean_eps: 0.753400\n",
      " 28078/100000: episode: 41, duration: 29.395s, episode steps: 433, steps per second:  15, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.829 [0.000, 5.000],  loss: 0.013163, mae: 2.695318, mean_q: 3.253176, mean_eps: 0.749242\n",
      " 28475/100000: episode: 42, duration: 25.337s, episode steps: 397, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013794, mae: 2.709294, mean_q: 3.269631, mean_eps: 0.745516\n",
      " 28936/100000: episode: 43, duration: 33.957s, episode steps: 461, steps per second:  14, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.014781, mae: 2.691909, mean_q: 3.248302, mean_eps: 0.741664\n",
      " 29464/100000: episode: 44, duration: 34.836s, episode steps: 528, steps per second:  15, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011406, mae: 2.700904, mean_q: 3.262057, mean_eps: 0.737218\n",
      " 30615/100000: episode: 45, duration: 74.150s, episode steps: 1151, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.011747, mae: 2.675974, mean_q: 3.230913, mean_eps: 0.729658\n",
      " 31160/100000: episode: 46, duration: 39.496s, episode steps: 545, steps per second:  14, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.011818, mae: 2.681261, mean_q: 3.237440, mean_eps: 0.722026\n",
      " 31857/100000: episode: 47, duration: 46.474s, episode steps: 697, steps per second:  15, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.011537, mae: 2.714816, mean_q: 3.279431, mean_eps: 0.716428\n",
      " 32503/100000: episode: 48, duration: 40.455s, episode steps: 646, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.011184, mae: 2.693703, mean_q: 3.252031, mean_eps: 0.710380\n",
      " 32899/100000: episode: 49, duration: 28.077s, episode steps: 396, steps per second:  14, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.012705, mae: 2.665158, mean_q: 3.215777, mean_eps: 0.705700\n",
      " 33599/100000: episode: 50, duration: 44.804s, episode steps: 700, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.011986, mae: 2.673771, mean_q: 3.227754, mean_eps: 0.700768\n",
      " 34582/100000: episode: 51, duration: 65.916s, episode steps: 983, steps per second:  15, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.012618, mae: 2.670545, mean_q: 3.225041, mean_eps: 0.693190\n",
      " 34969/100000: episode: 52, duration: 30.296s, episode steps: 387, steps per second:  13, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.011712, mae: 2.643805, mean_q: 3.190430, mean_eps: 0.687016\n",
      " 35826/100000: episode: 53, duration: 58.414s, episode steps: 857, steps per second:  15, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.010836, mae: 2.688035, mean_q: 3.245240, mean_eps: 0.681418\n",
      " 36168/100000: episode: 54, duration: 23.317s, episode steps: 342, steps per second:  15, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.011357, mae: 2.679508, mean_q: 3.234768, mean_eps: 0.676036\n",
      " 37502/100000: episode: 55, duration: 89.279s, episode steps: 1334, steps per second:  15, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.012122, mae: 2.680122, mean_q: 3.235393, mean_eps: 0.668494\n",
      " 37870/100000: episode: 56, duration: 25.821s, episode steps: 368, steps per second:  14, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.011242, mae: 2.680863, mean_q: 3.236834, mean_eps: 0.660826\n",
      " 38766/100000: episode: 57, duration: 60.785s, episode steps: 896, steps per second:  15, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.010721, mae: 2.667749, mean_q: 3.219881, mean_eps: 0.655138\n",
      " 40397/100000: episode: 58, duration: 109.706s, episode steps: 1631, steps per second:  15, episode reward: 34.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012249, mae: 2.684116, mean_q: 3.240781, mean_eps: 0.643762\n",
      " 41053/100000: episode: 59, duration: 43.947s, episode steps: 656, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011739, mae: 2.663657, mean_q: 3.215535, mean_eps: 0.633466\n",
      " 41678/100000: episode: 60, duration: 42.383s, episode steps: 625, steps per second:  15, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.011821, mae: 2.677923, mean_q: 3.231534, mean_eps: 0.627706\n",
      " 42254/100000: episode: 61, duration: 39.007s, episode steps: 576, steps per second:  15, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.011896, mae: 2.691792, mean_q: 3.249864, mean_eps: 0.622306\n",
      " 43459/100000: episode: 62, duration: 79.139s, episode steps: 1205, steps per second:  15, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013541, mae: 2.672147, mean_q: 3.226715, mean_eps: 0.614296\n",
      " 44460/100000: episode: 63, duration: 68.410s, episode steps: 1001, steps per second:  15, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.012069, mae: 2.668959, mean_q: 3.222405, mean_eps: 0.604378\n",
      " 45132/100000: episode: 64, duration: 47.965s, episode steps: 672, steps per second:  14, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.010840, mae: 2.664830, mean_q: 3.216944, mean_eps: 0.596854\n",
      " 46041/100000: episode: 65, duration: 62.637s, episode steps: 909, steps per second:  15, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.010774, mae: 2.663793, mean_q: 3.215966, mean_eps: 0.589726\n",
      " 46411/100000: episode: 66, duration: 24.564s, episode steps: 370, steps per second:  15, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.011828, mae: 2.651796, mean_q: 3.201267, mean_eps: 0.583966\n",
      " 46796/100000: episode: 67, duration: 25.240s, episode steps: 385, steps per second:  15, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.011840, mae: 2.687953, mean_q: 3.244545, mean_eps: 0.580582\n",
      " 47350/100000: episode: 68, duration: 37.495s, episode steps: 554, steps per second:  15, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.012509, mae: 2.645508, mean_q: 3.193134, mean_eps: 0.576352\n",
      " 48290/100000: episode: 69, duration: 62.362s, episode steps: 940, steps per second:  15, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.011975, mae: 2.662900, mean_q: 3.215958, mean_eps: 0.569620\n",
      " 48954/100000: episode: 70, duration: 49.488s, episode steps: 664, steps per second:  13, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.014271, mae: 2.675512, mean_q: 3.227663, mean_eps: 0.562402\n",
      " 49441/100000: episode: 71, duration: 32.654s, episode steps: 487, steps per second:  15, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013364, mae: 2.644578, mean_q: 3.191070, mean_eps: 0.557218\n",
      " 50165/100000: episode: 72, duration: 51.977s, episode steps: 724, steps per second:  14, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.013856, mae: 2.660538, mean_q: 3.212108, mean_eps: 0.551764\n",
      " 50736/100000: episode: 73, duration: 38.715s, episode steps: 571, steps per second:  15, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.013418, mae: 2.645491, mean_q: 3.192621, mean_eps: 0.545950\n",
      " 51309/100000: episode: 74, duration: 38.940s, episode steps: 573, steps per second:  15, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.016124, mae: 2.658050, mean_q: 3.206207, mean_eps: 0.540802\n",
      " 52535/100000: episode: 75, duration: 81.234s, episode steps: 1226, steps per second:  15, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.013056, mae: 2.682375, mean_q: 3.237137, mean_eps: 0.532702\n",
      " 53072/100000: episode: 76, duration: 36.753s, episode steps: 537, steps per second:  15, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.014661, mae: 2.672007, mean_q: 3.223956, mean_eps: 0.524782\n",
      " 53876/100000: episode: 77, duration: 53.457s, episode steps: 804, steps per second:  15, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.014786, mae: 2.656901, mean_q: 3.207581, mean_eps: 0.518752\n",
      " 54538/100000: episode: 78, duration: 45.500s, episode steps: 662, steps per second:  15, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.009987, mae: 2.663981, mean_q: 3.215676, mean_eps: 0.512146\n",
      " 55188/100000: episode: 79, duration: 45.634s, episode steps: 650, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.013746, mae: 2.651944, mean_q: 3.199336, mean_eps: 0.506242\n",
      " 55826/100000: episode: 80, duration: 42.811s, episode steps: 638, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.013085, mae: 2.642078, mean_q: 3.187732, mean_eps: 0.500446\n",
      " 56528/100000: episode: 81, duration: 47.241s, episode steps: 702, steps per second:  15, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011445, mae: 2.647036, mean_q: 3.193298, mean_eps: 0.494416\n",
      " 57252/100000: episode: 82, duration: 50.184s, episode steps: 724, steps per second:  14, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.013152, mae: 2.633666, mean_q: 3.178153, mean_eps: 0.488008\n",
      " 57723/100000: episode: 83, duration: 30.411s, episode steps: 471, steps per second:  15, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.013653, mae: 2.649090, mean_q: 3.196117, mean_eps: 0.482626\n",
      " 58292/100000: episode: 84, duration: 39.198s, episode steps: 569, steps per second:  15, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014527, mae: 2.640804, mean_q: 3.187287, mean_eps: 0.477946\n",
      " 59291/100000: episode: 85, duration: 67.166s, episode steps: 999, steps per second:  15, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011308, mae: 2.653626, mean_q: 3.202803, mean_eps: 0.470890\n",
      " 60234/100000: episode: 86, duration: 63.475s, episode steps: 943, steps per second:  15, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.011910, mae: 2.635687, mean_q: 3.181263, mean_eps: 0.462142\n",
      " 60940/100000: episode: 87, duration: 53.340s, episode steps: 706, steps per second:  13, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.013072, mae: 2.639880, mean_q: 3.187271, mean_eps: 0.454726\n",
      " 61716/100000: episode: 88, duration: 56.372s, episode steps: 776, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.011742, mae: 2.650947, mean_q: 3.199375, mean_eps: 0.448066\n",
      " 62310/100000: episode: 89, duration: 39.460s, episode steps: 594, steps per second:  15, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.012239, mae: 2.662199, mean_q: 3.212746, mean_eps: 0.441892\n",
      " 62824/100000: episode: 90, duration: 39.248s, episode steps: 514, steps per second:  13, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.014548, mae: 2.627644, mean_q: 3.169847, mean_eps: 0.436906\n",
      " 63289/100000: episode: 91, duration: 30.890s, episode steps: 465, steps per second:  15, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.012092, mae: 2.637978, mean_q: 3.183659, mean_eps: 0.432496\n",
      " 64073/100000: episode: 92, duration: 51.888s, episode steps: 784, steps per second:  15, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.013007, mae: 2.640668, mean_q: 3.186888, mean_eps: 0.426862\n",
      " 64773/100000: episode: 93, duration: 48.208s, episode steps: 700, steps per second:  15, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011840, mae: 2.649610, mean_q: 3.197743, mean_eps: 0.420184\n",
      " 65257/100000: episode: 94, duration: 32.316s, episode steps: 484, steps per second:  15, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.010435, mae: 2.635949, mean_q: 3.181825, mean_eps: 0.414856\n",
      " 66069/100000: episode: 95, duration: 58.765s, episode steps: 812, steps per second:  14, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013637, mae: 2.649132, mean_q: 3.196005, mean_eps: 0.409024\n",
      " 67217/100000: episode: 96, duration: 77.359s, episode steps: 1148, steps per second:  15, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.011874, mae: 2.638493, mean_q: 3.183713, mean_eps: 0.400204\n",
      " 67865/100000: episode: 97, duration: 43.448s, episode steps: 648, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.011001, mae: 2.627408, mean_q: 3.169826, mean_eps: 0.392122\n",
      " 68373/100000: episode: 98, duration: 33.327s, episode steps: 508, steps per second:  15, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.015198, mae: 2.649879, mean_q: 3.198182, mean_eps: 0.386920\n",
      " 68930/100000: episode: 99, duration: 39.079s, episode steps: 557, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.013576, mae: 2.642979, mean_q: 3.186668, mean_eps: 0.382132\n",
      " 69605/100000: episode: 100, duration: 46.226s, episode steps: 675, steps per second:  15, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.012651, mae: 2.639997, mean_q: 3.184518, mean_eps: 0.376588\n",
      " 70334/100000: episode: 101, duration: 48.433s, episode steps: 729, steps per second:  15, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012767, mae: 2.660932, mean_q: 3.210530, mean_eps: 0.370270\n",
      " 70699/100000: episode: 102, duration: 24.928s, episode steps: 365, steps per second:  15, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.013180, mae: 2.620437, mean_q: 3.161007, mean_eps: 0.365356\n",
      " 71272/100000: episode: 103, duration: 41.970s, episode steps: 573, steps per second:  14, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.011842, mae: 2.632109, mean_q: 3.175207, mean_eps: 0.361144\n",
      " 71913/100000: episode: 104, duration: 43.266s, episode steps: 641, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.011252, mae: 2.626101, mean_q: 3.167553, mean_eps: 0.355672\n",
      " 72573/100000: episode: 105, duration: 45.731s, episode steps: 660, steps per second:  14, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.014496, mae: 2.630665, mean_q: 3.173400, mean_eps: 0.349804\n",
      " 73354/100000: episode: 106, duration: 53.588s, episode steps: 781, steps per second:  15, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014551, mae: 2.636245, mean_q: 3.179500, mean_eps: 0.343324\n",
      " 73976/100000: episode: 107, duration: 41.586s, episode steps: 622, steps per second:  15, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.009649, mae: 2.615516, mean_q: 3.155361, mean_eps: 0.337024\n",
      " 74830/100000: episode: 108, duration: 67.635s, episode steps: 854, steps per second:  13, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.011744, mae: 2.628043, mean_q: 3.170276, mean_eps: 0.330382\n",
      " 75463/100000: episode: 109, duration: 42.442s, episode steps: 633, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.013036, mae: 2.626977, mean_q: 3.169126, mean_eps: 0.323686\n",
      " 76111/100000: episode: 110, duration: 44.606s, episode steps: 648, steps per second:  15, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.011843, mae: 2.649357, mean_q: 3.197687, mean_eps: 0.317926\n",
      " 76893/100000: episode: 111, duration: 54.827s, episode steps: 782, steps per second:  14, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.013290, mae: 2.631249, mean_q: 3.176778, mean_eps: 0.311482\n",
      " 77541/100000: episode: 112, duration: 42.134s, episode steps: 648, steps per second:  15, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.013055, mae: 2.615572, mean_q: 3.155128, mean_eps: 0.305038\n",
      " 78212/100000: episode: 113, duration: 45.602s, episode steps: 671, steps per second:  15, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.011758, mae: 2.604810, mean_q: 3.142256, mean_eps: 0.299116\n",
      " 79323/100000: episode: 114, duration: 73.757s, episode steps: 1111, steps per second:  15, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.012277, mae: 2.616370, mean_q: 3.156612, mean_eps: 0.291106\n",
      " 79980/100000: episode: 115, duration: 45.266s, episode steps: 657, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.988 [0.000, 5.000],  loss: 0.013428, mae: 2.621151, mean_q: 3.161252, mean_eps: 0.283150\n",
      " 81285/100000: episode: 116, duration: 91.694s, episode steps: 1305, steps per second:  14, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.013416, mae: 2.616633, mean_q: 3.156662, mean_eps: 0.274312\n",
      " 81765/100000: episode: 117, duration: 39.357s, episode steps: 480, steps per second:  12, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.011513, mae: 2.618012, mean_q: 3.160342, mean_eps: 0.266266\n",
      " 82431/100000: episode: 118, duration: 44.474s, episode steps: 666, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.011343, mae: 2.611914, mean_q: 3.154175, mean_eps: 0.261118\n",
      " 82929/100000: episode: 119, duration: 35.501s, episode steps: 498, steps per second:  14, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.013040, mae: 2.635240, mean_q: 3.180074, mean_eps: 0.255880\n",
      " 83756/100000: episode: 120, duration: 55.426s, episode steps: 827, steps per second:  15, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.033 [0.000, 5.000],  loss: 0.011722, mae: 2.605630, mean_q: 3.144572, mean_eps: 0.249922\n",
      " 84309/100000: episode: 121, duration: 37.238s, episode steps: 553, steps per second:  15, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.011921, mae: 2.640911, mean_q: 3.187417, mean_eps: 0.243712\n",
      " 85325/100000: episode: 122, duration: 69.934s, episode steps: 1016, steps per second:  15, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013013, mae: 2.642291, mean_q: 3.187913, mean_eps: 0.236638\n",
      " 86360/100000: episode: 123, duration: 68.822s, episode steps: 1035, steps per second:  15, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.014419, mae: 2.643267, mean_q: 3.188976, mean_eps: 0.227422\n",
      " 87409/100000: episode: 124, duration: 75.903s, episode steps: 1049, steps per second:  14, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.012849, mae: 2.634407, mean_q: 3.177943, mean_eps: 0.218044\n",
      " 88752/100000: episode: 125, duration: 90.741s, episode steps: 1343, steps per second:  15, episode reward: 14.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.012779, mae: 2.631349, mean_q: 3.175567, mean_eps: 0.207280\n",
      " 89151/100000: episode: 126, duration: 25.352s, episode steps: 399, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.010445, mae: 2.627973, mean_q: 3.170834, mean_eps: 0.199450\n",
      " 90344/100000: episode: 127, duration: 84.845s, episode steps: 1193, steps per second:  14, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.886 [0.000, 5.000],  loss: 0.012319, mae: 2.627489, mean_q: 3.169968, mean_eps: 0.192286\n",
      " 90986/100000: episode: 128, duration: 42.442s, episode steps: 642, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013563, mae: 2.630431, mean_q: 3.171894, mean_eps: 0.184024\n",
      " 91678/100000: episode: 129, duration: 46.409s, episode steps: 692, steps per second:  15, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.007 [0.000, 5.000],  loss: 0.013580, mae: 2.640745, mean_q: 3.184748, mean_eps: 0.178012\n",
      " 92428/100000: episode: 130, duration: 54.531s, episode steps: 750, steps per second:  14, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.013828, mae: 2.635032, mean_q: 3.180494, mean_eps: 0.171532\n",
      " 92968/100000: episode: 131, duration: 36.339s, episode steps: 540, steps per second:  15, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.011713, mae: 2.613916, mean_q: 3.153857, mean_eps: 0.165736\n",
      " 93604/100000: episode: 132, duration: 42.234s, episode steps: 636, steps per second:  15, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011101, mae: 2.613614, mean_q: 3.155146, mean_eps: 0.160444\n",
      " 94229/100000: episode: 133, duration: 43.947s, episode steps: 625, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.012797, mae: 2.610514, mean_q: 3.150493, mean_eps: 0.154756\n",
      " 95296/100000: episode: 134, duration: 72.521s, episode steps: 1067, steps per second:  15, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.012220, mae: 2.612091, mean_q: 3.151122, mean_eps: 0.147142\n",
      " 96631/100000: episode: 135, duration: 96.549s, episode steps: 1335, steps per second:  14, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.013944, mae: 2.625758, mean_q: 3.167435, mean_eps: 0.136342\n",
      " 97204/100000: episode: 136, duration: 38.204s, episode steps: 573, steps per second:  15, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.012342, mae: 2.631640, mean_q: 3.174559, mean_eps: 0.127756\n",
      " 97764/100000: episode: 137, duration: 37.072s, episode steps: 560, steps per second:  15, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.011455, mae: 2.636921, mean_q: 3.180691, mean_eps: 0.122662\n",
      " 98403/100000: episode: 138, duration: 45.202s, episode steps: 639, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.013248, mae: 2.621141, mean_q: 3.162000, mean_eps: 0.117262\n",
      " 98917/100000: episode: 139, duration: 36.640s, episode steps: 514, steps per second:  14, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.014100, mae: 2.625363, mean_q: 3.166820, mean_eps: 0.112060\n",
      " 99297/100000: episode: 140, duration: 25.350s, episode steps: 380, steps per second:  15, episode reward: 10.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.014395, mae: 2.615398, mean_q: 3.156851, mean_eps: 0.108028\n",
      " 99944/100000: episode: 141, duration: 50.641s, episode steps: 647, steps per second:  13, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.011477, mae: 2.623233, mean_q: 3.165428, mean_eps: 0.103420\n",
      "done, took 6525.751 seconds\n",
      "########################################################\n",
      "PROCESO TERMINADO\n",
      "########################################################\n"
     ]
    }
   ],
   "source": [
    "# Configuración\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "ENV_NAME = 'SpaceInvaders-v0'\n",
    "MODEL_DIR = './models/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "# class EpisodeRewardLogger(Callback):\n",
    "#     def __init__(self):\n",
    "#         self.episode_rewards = []\n",
    "\n",
    "#     def on_episode_end(self, episode, logs={}):\n",
    "#         self.episode_rewards.append(logs.get('episode_reward'))\n",
    "\n",
    "# Cargar automáticamente el último checkpoint si existe\n",
    "def get_latest_weights(model_dir):\n",
    "    #weights_files = glob.glob(os.path.join(model_dir, 'dqn_weights_*.h5f'))\n",
    "    weights_files = glob.glob(os.path.join(model_dir, 'dqn_weights_*.h5f.index'))\n",
    "    print(weights_files)\n",
    "    if not weights_files:\n",
    "        print(\"No se encontraron checkpoints previos.\")\n",
    "        return None\n",
    "    latest_file = max(weights_files, key=os.path.getctime)\n",
    "    latest_file = latest_file.replace('.index', '')\n",
    "    print(\"Cargando pesos desde:\", latest_file)\n",
    "    return latest_file\n",
    "\n",
    "latest_weights = get_latest_weights(MODEL_DIR)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        return np.array(img).astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch.astype('float32') / 255.\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "# Memoria y política\n",
    "memory = SequentialMemory(limit=1_000_000, window_length=WINDOW_LENGTH)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps', \n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1, \n",
    "                              value_test=0.01, \n",
    "                              nb_steps=100_000)\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions, \n",
    "               policy=policy, memory=memory,\n",
    "               processor=AtariProcessor(), \n",
    "               enable_double_dqn=True,\n",
    "               nb_steps_warmup=5_000,\n",
    "               gamma=0.99, \n",
    "               target_model_update=50_000, \n",
    "               train_interval=4,\n",
    "               delta_clip=1.0)\n",
    "dqn.compile(Adam(learning_rate=0.0001), metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelIntervalCheckpoint(MODEL_DIR + 'dqn_weights_{step}.h5f', interval=10000)\n",
    "logger = FileLogger(LOG_DIR + 'dqn_log.json', interval=100)\n",
    "\n",
    "# Si hay pesos anteriores, cargarlos antes de seguir\n",
    "if latest_weights:\n",
    "    dqn.load_weights(latest_weights)\n",
    "    print('Hay pesos anteriores y se van a cargar')\n",
    "\n",
    "# reward_logger = EpisodeRewardLogger()\n",
    "\n",
    "# Entrenamiento (ajusta nb_steps según necesites)\n",
    "dqn.fit(env, nb_steps=100_000, visualize=False, verbose=2, callbacks=[checkpoint, logger])\n",
    "# dqn.fit(env, nb_steps=250_000, visualize=False, verbose=2, callbacks=[checkpoint, logger, reward_logger])\n",
    "\n",
    "# Guardar pesos finales\n",
    "dqn.save_weights(MODEL_DIR + 'dqn_final_weights.h5f', overwrite=True)\n",
    "\n",
    "print('########################################################')\n",
    "print('PROCESO TERMINADO')\n",
    "print('########################################################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d6dd74-d227-4d32-95f3-6238935a4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: 6.000, steps: 424\n",
      "Episode 2: reward: 9.000, steps: 601\n",
      "Episode 3: reward: 10.000, steps: 661\n",
      "Episode 4: reward: 21.000, steps: 1087\n",
      "Episode 5: reward: 7.000, steps: 547\n",
      "Episode 6: reward: 5.000, steps: 498\n",
      "Episode 7: reward: 5.000, steps: 587\n",
      "Episode 8: reward: 13.000, steps: 884\n",
      "Episode 9: reward: 11.000, steps: 610\n",
      "Episode 10: reward: 6.000, steps: 551\n",
      "Episode 11: reward: 6.000, steps: 622\n",
      "Episode 12: reward: 21.000, steps: 1033\n",
      "Episode 13: reward: 12.000, steps: 797\n",
      "Episode 14: reward: 14.000, steps: 802\n",
      "Episode 15: reward: 6.000, steps: 406\n",
      "Episode 16: reward: 7.000, steps: 706\n",
      "Episode 17: reward: 4.000, steps: 422\n",
      "Episode 18: reward: 4.000, steps: 487\n",
      "Episode 19: reward: 8.000, steps: 693\n",
      "Episode 20: reward: 13.000, steps: 748\n",
      "\n",
      "Recompensa media en test: 9.40\n",
      "DATOS DE ENTRENAMIENTO\n",
      "\n",
      "Gráfico de entrenamiento con ejes:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=860x240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATOS DE TEST\n",
      "Gráfico de recompensas de test con ejes:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=650x230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mostrar datos de ENTRENAMIENTO\n",
    "def mostrar_entrenamiento(log_path = LOG_DIR + 'dqn_log.json', ancho=800, alto=200):\n",
    "    with open(log_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rewards = data['episode_reward']\n",
    "    if not rewards:\n",
    "        print(\"No hay datos de entrenamiento.\")\n",
    "        return\n",
    "\n",
    "    max_reward = max(rewards)\n",
    "    min_reward = min(rewards)\n",
    "    escalar = alto / (max_reward - min_reward + 1)\n",
    "\n",
    "    img = Image.new('RGB', (ancho + 60, alto + 40), color='white')\n",
    "    dibujar = ImageDraw.Draw(img)\n",
    "\n",
    "    paso = max(1, ancho // len(rewards))\n",
    "    offset_x = 40\n",
    "    offset_y = 20\n",
    "\n",
    "    for i, r in enumerate(rewards):\n",
    "        x = offset_x + i * paso\n",
    "        y = offset_y + int((max_reward - r) * escalar)\n",
    "        dibujar.rectangle([x, y, x + paso - 1, alto + offset_y], fill='green')\n",
    "\n",
    "    # Eje Y - min y max\n",
    "    dibujar.text((5, offset_y), f\"{max_reward:.0f}\", fill='black')\n",
    "    dibujar.text((5, alto + offset_y - 10), f\"{min_reward:.0f}\", fill='black')\n",
    "\n",
    "    # Eje X - episodios\n",
    "    dibujar.text((offset_x, alto + offset_y + 5), \"0\", fill='black')\n",
    "    dibujar.text((offset_x + len(rewards)*paso - 30, alto + offset_y + 5),\n",
    "                 f\"{len(rewards)}\", fill='black')\n",
    "\n",
    "    print(\"Gráfico de entrenamiento con ejes:\")\n",
    "    display(img)\n",
    "\n",
    "# Mostrar datos de TEST\n",
    "def mostrar_test_con_ejes(rewards, ancho=600, alto=200, divisiones_y=4):\n",
    "    if not rewards:\n",
    "        print(\"No hay datos de test.\")\n",
    "        return\n",
    "\n",
    "    max_reward = max(rewards)\n",
    "    min_reward = min(rewards)\n",
    "    rango = max_reward - min_reward if max_reward != min_reward else 1\n",
    "    escalar = alto / (rango + 10)\n",
    "\n",
    "    margen_izquierdo = 50\n",
    "    margen_inferior = 30\n",
    "    img = Image.new('RGB', (ancho + margen_izquierdo, alto + margen_inferior), color='white')\n",
    "    dibujar = ImageDraw.Draw(img)\n",
    "\n",
    "    paso = max(1, ancho // len(rewards))\n",
    "\n",
    "    # Dibujar barras\n",
    "    for i, r in enumerate(rewards):\n",
    "        x = margen_izquierdo + i * paso\n",
    "        y = int((max_reward - r) * escalar)\n",
    "        dibujar.rectangle([x, y, x + paso - 1, alto], fill='blue')\n",
    "\n",
    "    # Líneas y etiquetas en eje Y\n",
    "    for i in range(divisiones_y + 1):\n",
    "        valor = min_reward + i * (rango / divisiones_y)\n",
    "        y = int((max_reward - valor) * escalar)\n",
    "        dibujar.line([(margen_izquierdo - 5, y), (ancho + margen_izquierdo, y)], fill='gray', width=1)\n",
    "        dibujar.text((5, y - 7), f\"{valor:.0f}\", fill='black')\n",
    "\n",
    "    # Etiquetas en eje X\n",
    "    dibujar.text((margen_izquierdo, alto + 5), \"0\", fill='black')\n",
    "    dibujar.text((ancho + margen_izquierdo - 30, alto + 5), f\"{len(rewards)}\", fill='black')\n",
    "\n",
    "    print(\"Gráfico de recompensas de test con ejes:\")\n",
    "    display(img)\n",
    "\n",
    "#  TEST DEL AGENTE\n",
    "history = dqn.test(env, nb_episodes=20, visualize=False)\n",
    "test_rewards = history.history['episode_reward']\n",
    "print(f\"\\nRecompensa media en test: {np.mean(test_rewards):.2f}\")\n",
    "\n",
    "# VISUALIZACION DE DATOS\n",
    "print(\"DATOS DE ENTRENAMIENTO\")\n",
    "print()\n",
    "mostrar_entrenamiento()\n",
    "print(\"DATOS DE TEST\")\n",
    "mostrar_test_con_ejes(test_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miar_rl]",
   "language": "python",
   "name": "conda-env-miar_rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
