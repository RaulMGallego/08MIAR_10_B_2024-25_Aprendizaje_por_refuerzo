['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_250000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
    420/250000: episode: 1, duration: 3.360s, episode steps: 420, steps per second: 125, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 5.394s, episode steps: 711, steps per second: 132, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 7.239s, episode steps: 810, steps per second: 112, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2831/250000: episode: 4, duration: 6.752s, episode steps: 890, steps per second: 132, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3206/250000: episode: 5, duration: 2.749s, episode steps: 375, steps per second: 136, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4151/250000: episode: 6, duration: 6.844s, episode steps: 945, steps per second: 138, episode reward:  8.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4943/250000: episode: 7, duration: 5.665s, episode steps: 792, steps per second: 140, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5451/250000: episode: 8, duration: 3.578s, episode steps: 508, steps per second: 142, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6188/250000: episode: 9, duration: 5.190s, episode steps: 737, steps per second: 142, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7270/250000: episode: 10, duration: 8.526s, episode steps: 1082, steps per second: 127, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8025/250000: episode: 11, duration: 6.498s, episode steps: 755, steps per second: 116, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8879/250000: episode: 12, duration: 6.158s, episode steps: 854, steps per second: 139, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9468/250000: episode: 13, duration: 5.023s, episode steps: 589, steps per second: 117, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10333/250000: episode: 14, duration: 26.214s, episode steps: 865, steps per second:  33, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.011761, mae: 1.107588, mean_q: 1.375144, mean_eps: 0.990849
  11279/250000: episode: 15, duration: 54.026s, episode steps: 946, steps per second:  18, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.011437, mae: 1.125097, mean_q: 1.396032, mean_eps: 0.990275
  12193/250000: episode: 16, duration: 55.621s, episode steps: 914, steps per second:  16, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.009869, mae: 1.136997, mean_q: 1.411642, mean_eps: 0.989438
  12705/250000: episode: 17, duration: 27.992s, episode steps: 512, steps per second:  18, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.008932, mae: 1.147375, mean_q: 1.425568, mean_eps: 0.988795
  13235/250000: episode: 18, duration: 28.347s, episode steps: 530, steps per second:  19, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.010514, mae: 1.125625, mean_q: 1.400252, mean_eps: 0.988327
  13779/250000: episode: 19, duration: 31.640s, episode steps: 544, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.008093, mae: 1.131272, mean_q: 1.407138, mean_eps: 0.987845
  14407/250000: episode: 20, duration: 34.772s, episode steps: 628, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.008166, mae: 1.110175, mean_q: 1.381317, mean_eps: 0.987317
  15735/250000: episode: 21, duration: 72.092s, episode steps: 1328, steps per second:  18, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008594, mae: 1.108760, mean_q: 1.377450, mean_eps: 0.986437
  16415/250000: episode: 22, duration: 37.100s, episode steps: 680, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.009318, mae: 1.118883, mean_q: 1.392643, mean_eps: 0.985533
  17087/250000: episode: 23, duration: 37.877s, episode steps: 672, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.008635, mae: 1.117428, mean_q: 1.392494, mean_eps: 0.984925
  17885/250000: episode: 24, duration: 45.301s, episode steps: 798, steps per second:  18, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.007311, mae: 1.111186, mean_q: 1.387222, mean_eps: 0.984263
  18779/250000: episode: 25, duration: 49.526s, episode steps: 894, steps per second:  18, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.007795, mae: 1.099499, mean_q: 1.371754, mean_eps: 0.983501
  19427/250000: episode: 26, duration: 37.483s, episode steps: 648, steps per second:  17, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.008610, mae: 1.123673, mean_q: 1.401397, mean_eps: 0.982808
  19862/250000: episode: 27, duration: 27.802s, episode steps: 435, steps per second:  16, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.007856, mae: 1.104461, mean_q: 1.376355, mean_eps: 0.982320
  20490/250000: episode: 28, duration: 35.004s, episode steps: 628, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.007827, mae: 1.116082, mean_q: 1.393053, mean_eps: 0.981842
  21054/250000: episode: 29, duration: 30.091s, episode steps: 564, steps per second:  19, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.007392, mae: 1.111978, mean_q: 1.385460, mean_eps: 0.981305
  21860/250000: episode: 30, duration: 43.443s, episode steps: 806, steps per second:  19, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.007579, mae: 1.110901, mean_q: 1.385781, mean_eps: 0.980690
  22811/250000: episode: 31, duration: 51.859s, episode steps: 951, steps per second:  18, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.008857, mae: 1.118377, mean_q: 1.396345, mean_eps: 0.979899
  23367/250000: episode: 32, duration: 30.129s, episode steps: 556, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.008018, mae: 1.133996, mean_q: 1.413766, mean_eps: 0.979221
  24090/250000: episode: 33, duration: 38.847s, episode steps: 723, steps per second:  19, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.007109, mae: 1.115843, mean_q: 1.390579, mean_eps: 0.978645
  24727/250000: episode: 34, duration: 34.066s, episode steps: 637, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.007931, mae: 1.126407, mean_q: 1.404255, mean_eps: 0.978033
  25377/250000: episode: 35, duration: 39.215s, episode steps: 650, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.006826, mae: 1.137303, mean_q: 1.420817, mean_eps: 0.977453
  25790/250000: episode: 36, duration: 22.562s, episode steps: 413, steps per second:  18, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.007259, mae: 1.122754, mean_q: 1.400972, mean_eps: 0.976974
  26151/250000: episode: 37, duration: 20.517s, episode steps: 361, steps per second:  18, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.006816, mae: 1.111250, mean_q: 1.387008, mean_eps: 0.976627
  26803/250000: episode: 38, duration: 39.143s, episode steps: 652, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.007808, mae: 1.128619, mean_q: 1.410017, mean_eps: 0.976172
  27410/250000: episode: 39, duration: 33.290s, episode steps: 607, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.007506, mae: 1.128136, mean_q: 1.408689, mean_eps: 0.975605
  28377/250000: episode: 40, duration: 52.481s, episode steps: 967, steps per second:  18, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.008697, mae: 1.123228, mean_q: 1.402128, mean_eps: 0.974895
  29695/250000: episode: 41, duration: 70.911s, episode steps: 1318, steps per second:  19, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.008116, mae: 1.133768, mean_q: 1.414628, mean_eps: 0.973868
  30382/250000: episode: 42, duration: 40.496s, episode steps: 687, steps per second:  17, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.007076, mae: 1.123779, mean_q: 1.400584, mean_eps: 0.972966
  30921/250000: episode: 43, duration: 29.534s, episode steps: 539, steps per second:  18, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.008070, mae: 1.159568, mean_q: 1.447412, mean_eps: 0.972413
  31464/250000: episode: 44, duration: 29.904s, episode steps: 543, steps per second:  18, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.008446, mae: 1.122666, mean_q: 1.401535, mean_eps: 0.971927
  31856/250000: episode: 45, duration: 22.000s, episode steps: 392, steps per second:  18, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.008607, mae: 1.137269, mean_q: 1.422784, mean_eps: 0.971508
  32277/250000: episode: 46, duration: 23.603s, episode steps: 421, steps per second:  18, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.007545, mae: 1.126692, mean_q: 1.408626, mean_eps: 0.971141
  33151/250000: episode: 47, duration: 49.871s, episode steps: 874, steps per second:  18, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.007206, mae: 1.112683, mean_q: 1.390913, mean_eps: 0.970557
  33782/250000: episode: 48, duration: 33.772s, episode steps: 631, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.007036, mae: 1.134117, mean_q: 1.418606, mean_eps: 0.969881
  35054/250000: episode: 49, duration: 68.901s, episode steps: 1272, steps per second:  18, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.007050, mae: 1.115273, mean_q: 1.393527, mean_eps: 0.969024
  35978/250000: episode: 50, duration: 50.459s, episode steps: 924, steps per second:  18, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.008325, mae: 1.119525, mean_q: 1.398817, mean_eps: 0.968036
  36484/250000: episode: 51, duration: 27.559s, episode steps: 506, steps per second:  18, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.008480, mae: 1.129565, mean_q: 1.411485, mean_eps: 0.967393
  36989/250000: episode: 52, duration: 27.854s, episode steps: 505, steps per second:  18, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.008478, mae: 1.136005, mean_q: 1.419700, mean_eps: 0.966938
  37713/250000: episode: 53, duration: 39.059s, episode steps: 724, steps per second:  19, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.009124, mae: 1.136452, mean_q: 1.419726, mean_eps: 0.966383
  38769/250000: episode: 54, duration: 58.340s, episode steps: 1056, steps per second:  18, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.007935, mae: 1.128024, mean_q: 1.410915, mean_eps: 0.965582
  39650/250000: episode: 55, duration: 48.847s, episode steps: 881, steps per second:  18, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.009080, mae: 1.137837, mean_q: 1.423477, mean_eps: 0.964711
  40056/250000: episode: 56, duration: 22.942s, episode steps: 406, steps per second:  18, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.007747, mae: 1.116748, mean_q: 1.398851, mean_eps: 0.964133
  40711/250000: episode: 57, duration: 35.941s, episode steps: 655, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.009916, mae: 1.136913, mean_q: 1.420603, mean_eps: 0.963656
  41306/250000: episode: 58, duration: 33.108s, episode steps: 595, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008261, mae: 1.128899, mean_q: 1.411198, mean_eps: 0.963093
  42103/250000: episode: 59, duration: 42.796s, episode steps: 797, steps per second:  19, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.008807, mae: 1.136521, mean_q: 1.422062, mean_eps: 0.962466
  42742/250000: episode: 60, duration: 35.945s, episode steps: 639, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.008044, mae: 1.131509, mean_q: 1.413970, mean_eps: 0.961820
  43382/250000: episode: 61, duration: 33.938s, episode steps: 640, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.008557, mae: 1.132991, mean_q: 1.415930, mean_eps: 0.961244
  43905/250000: episode: 62, duration: 29.241s, episode steps: 523, steps per second:  18, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.008756, mae: 1.141665, mean_q: 1.428135, mean_eps: 0.960720
  44470/250000: episode: 63, duration: 29.724s, episode steps: 565, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.008251, mae: 1.120789, mean_q: 1.400983, mean_eps: 0.960231
  45083/250000: episode: 64, duration: 34.111s, episode steps: 613, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.007645, mae: 1.126092, mean_q: 1.407490, mean_eps: 0.959702
  45989/250000: episode: 65, duration: 49.227s, episode steps: 906, steps per second:  18, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.008450, mae: 1.141926, mean_q: 1.427105, mean_eps: 0.959018
  46694/250000: episode: 66, duration: 38.790s, episode steps: 705, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.007998, mae: 1.137407, mean_q: 1.423389, mean_eps: 0.958292
  47400/250000: episode: 67, duration: 40.986s, episode steps: 706, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.008724, mae: 1.128775, mean_q: 1.412694, mean_eps: 0.957659
  48127/250000: episode: 68, duration: 43.721s, episode steps: 727, steps per second:  17, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.009271, mae: 1.136123, mean_q: 1.418406, mean_eps: 0.957014
  48705/250000: episode: 69, duration: 31.420s, episode steps: 578, steps per second:  18, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.008070, mae: 1.124149, mean_q: 1.405095, mean_eps: 0.956426
  49457/250000: episode: 70, duration: 40.702s, episode steps: 752, steps per second:  18, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.007462, mae: 1.137440, mean_q: 1.420749, mean_eps: 0.955826
  49989/250000: episode: 71, duration: 31.897s, episode steps: 532, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.007638, mae: 1.109804, mean_q: 1.388711, mean_eps: 0.955248
  50434/250000: episode: 72, duration: 24.071s, episode steps: 445, steps per second:  18, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.010133, mae: 1.168913, mean_q: 1.457272, mean_eps: 0.954809
  51221/250000: episode: 73, duration: 44.620s, episode steps: 787, steps per second:  18, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.009436, mae: 1.183757, mean_q: 1.477029, mean_eps: 0.954255
  52512/250000: episode: 74, duration: 73.440s, episode steps: 1291, steps per second:  18, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009614, mae: 1.165603, mean_q: 1.454452, mean_eps: 0.953321
  53154/250000: episode: 75, duration: 34.435s, episode steps: 642, steps per second:  19, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.009944, mae: 1.179244, mean_q: 1.471062, mean_eps: 0.952451
  53574/250000: episode: 76, duration: 23.177s, episode steps: 420, steps per second:  18, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.009798, mae: 1.182229, mean_q: 1.475350, mean_eps: 0.951972
  54026/250000: episode: 77, duration: 24.709s, episode steps: 452, steps per second:  18, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.009913, mae: 1.168866, mean_q: 1.457035, mean_eps: 0.951580
  55371/250000: episode: 78, duration: 72.934s, episode steps: 1345, steps per second:  18, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.009085, mae: 1.172252, mean_q: 1.461956, mean_eps: 0.950772
  55722/250000: episode: 79, duration: 19.170s, episode steps: 351, steps per second:  18, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.009519, mae: 1.173319, mean_q: 1.464139, mean_eps: 0.950009
  56815/250000: episode: 80, duration: 59.010s, episode steps: 1093, steps per second:  19, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.009599, mae: 1.181231, mean_q: 1.473156, mean_eps: 0.949359
  57330/250000: episode: 81, duration: 28.360s, episode steps: 515, steps per second:  18, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.008396, mae: 1.166980, mean_q: 1.455997, mean_eps: 0.948635
  58318/250000: episode: 82, duration: 54.267s, episode steps: 988, steps per second:  18, episode reward: 10.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.009084, mae: 1.183978, mean_q: 1.476455, mean_eps: 0.947958
  58845/250000: episode: 83, duration: 28.136s, episode steps: 527, steps per second:  19, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.008261, mae: 1.180224, mean_q: 1.472291, mean_eps: 0.947276
  59621/250000: episode: 84, duration: 43.688s, episode steps: 776, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.008427, mae: 1.168359, mean_q: 1.456712, mean_eps: 0.946689
  60291/250000: episode: 85, duration: 35.865s, episode steps: 670, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.008130, mae: 1.173249, mean_q: 1.464073, mean_eps: 0.946040
  60816/250000: episode: 86, duration: 28.689s, episode steps: 525, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.008492, mae: 1.171506, mean_q: 1.464409, mean_eps: 0.945503
  61772/250000: episode: 87, duration: 55.915s, episode steps: 956, steps per second:  17, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009318, mae: 1.188938, mean_q: 1.483264, mean_eps: 0.944837
  62366/250000: episode: 88, duration: 33.584s, episode steps: 594, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.009336, mae: 1.180407, mean_q: 1.474062, mean_eps: 0.944139
  62802/250000: episode: 89, duration: 25.318s, episode steps: 436, steps per second:  17, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.008754, mae: 1.172476, mean_q: 1.461757, mean_eps: 0.943674
  63420/250000: episode: 90, duration: 35.771s, episode steps: 618, steps per second:  17, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.008570, mae: 1.180742, mean_q: 1.472205, mean_eps: 0.943201
  63932/250000: episode: 91, duration: 29.137s, episode steps: 512, steps per second:  18, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.008134, mae: 1.186425, mean_q: 1.480133, mean_eps: 0.942693
  64554/250000: episode: 92, duration: 35.794s, episode steps: 622, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.008737, mae: 1.180188, mean_q: 1.471909, mean_eps: 0.942182
  65118/250000: episode: 93, duration: 32.273s, episode steps: 564, steps per second:  17, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.008189, mae: 1.176288, mean_q: 1.466745, mean_eps: 0.941648
  65771/250000: episode: 94, duration: 38.705s, episode steps: 653, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.008458, mae: 1.151913, mean_q: 1.435419, mean_eps: 0.941100
  66474/250000: episode: 95, duration: 40.170s, episode steps: 703, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.008551, mae: 1.171304, mean_q: 1.457813, mean_eps: 0.940490
  66972/250000: episode: 96, duration: 28.356s, episode steps: 498, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.007929, mae: 1.163626, mean_q: 1.450926, mean_eps: 0.939950
  67735/250000: episode: 97, duration: 44.007s, episode steps: 763, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.007894, mae: 1.176444, mean_q: 1.464887, mean_eps: 0.939383
  68660/250000: episode: 98, duration: 52.284s, episode steps: 925, steps per second:  18, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.009115, mae: 1.178205, mean_q: 1.468306, mean_eps: 0.938624
  69468/250000: episode: 99, duration: 46.412s, episode steps: 808, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.009482, mae: 1.184686, mean_q: 1.476514, mean_eps: 0.937844
  70160/250000: episode: 100, duration: 41.843s, episode steps: 692, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.009292, mae: 1.160290, mean_q: 1.445858, mean_eps: 0.937169
  71291/250000: episode: 101, duration: 64.532s, episode steps: 1131, steps per second:  18, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.009063, mae: 1.178127, mean_q: 1.469250, mean_eps: 0.936348
  71913/250000: episode: 102, duration: 37.348s, episode steps: 622, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.008066, mae: 1.184457, mean_q: 1.477455, mean_eps: 0.935558
  72623/250000: episode: 103, duration: 39.827s, episode steps: 710, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.008500, mae: 1.177730, mean_q: 1.467938, mean_eps: 0.934959
  73213/250000: episode: 104, duration: 34.967s, episode steps: 590, steps per second:  17, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.008781, mae: 1.166922, mean_q: 1.456681, mean_eps: 0.934374
  73863/250000: episode: 105, duration: 35.080s, episode steps: 650, steps per second:  19, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.008531, mae: 1.166611, mean_q: 1.455273, mean_eps: 0.933816
  74807/250000: episode: 106, duration: 52.676s, episode steps: 944, steps per second:  18, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.009378, mae: 1.180218, mean_q: 1.470122, mean_eps: 0.933099
  75390/250000: episode: 107, duration: 32.982s, episode steps: 583, steps per second:  18, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.008383, mae: 1.174532, mean_q: 1.463448, mean_eps: 0.932412
  76151/250000: episode: 108, duration: 42.224s, episode steps: 761, steps per second:  18, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.007919, mae: 1.174142, mean_q: 1.463322, mean_eps: 0.931807
  76748/250000: episode: 109, duration: 34.116s, episode steps: 597, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.009536, mae: 1.174004, mean_q: 1.464870, mean_eps: 0.931197
  77461/250000: episode: 110, duration: 39.665s, episode steps: 713, steps per second:  18, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.008475, mae: 1.169454, mean_q: 1.457472, mean_eps: 0.930606
  78253/250000: episode: 111, duration: 46.879s, episode steps: 792, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.008705, mae: 1.165750, mean_q: 1.453322, mean_eps: 0.929928
  78960/250000: episode: 112, duration: 39.730s, episode steps: 707, steps per second:  18, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.009008, mae: 1.191238, mean_q: 1.485027, mean_eps: 0.929255
  79400/250000: episode: 113, duration: 25.584s, episode steps: 440, steps per second:  17, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.007854, mae: 1.173737, mean_q: 1.461926, mean_eps: 0.928740
  79785/250000: episode: 114, duration: 21.019s, episode steps: 385, steps per second:  18, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.006873, mae: 1.174118, mean_q: 1.463105, mean_eps: 0.928367
  80419/250000: episode: 115, duration: 36.405s, episode steps: 634, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.008743, mae: 1.176365, mean_q: 1.465205, mean_eps: 0.927908
  81165/250000: episode: 116, duration: 42.495s, episode steps: 746, steps per second:  18, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.009346, mae: 1.189810, mean_q: 1.482608, mean_eps: 0.927287
  82080/250000: episode: 117, duration: 51.348s, episode steps: 915, steps per second:  18, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.008998, mae: 1.190140, mean_q: 1.484880, mean_eps: 0.926540
  82751/250000: episode: 118, duration: 37.630s, episode steps: 671, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.009190, mae: 1.176308, mean_q: 1.465517, mean_eps: 0.925827
  83251/250000: episode: 119, duration: 27.645s, episode steps: 500, steps per second:  18, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.010223, mae: 1.180719, mean_q: 1.469405, mean_eps: 0.925300
  84206/250000: episode: 120, duration: 53.735s, episode steps: 955, steps per second:  18, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.008718, mae: 1.182022, mean_q: 1.473071, mean_eps: 0.924645
  84800/250000: episode: 121, duration: 34.514s, episode steps: 594, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.009467, mae: 1.197475, mean_q: 1.494127, mean_eps: 0.923948
  85405/250000: episode: 122, duration: 35.000s, episode steps: 605, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.008875, mae: 1.186550, mean_q: 1.479680, mean_eps: 0.923408
  86100/250000: episode: 123, duration: 39.394s, episode steps: 695, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.007960, mae: 1.175715, mean_q: 1.466573, mean_eps: 0.922823
  86614/250000: episode: 124, duration: 29.147s, episode steps: 514, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.008161, mae: 1.178074, mean_q: 1.467409, mean_eps: 0.922280
  87019/250000: episode: 125, duration: 22.655s, episode steps: 405, steps per second:  18, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.008639, mae: 1.179169, mean_q: 1.469288, mean_eps: 0.921866
  87847/250000: episode: 126, duration: 47.101s, episode steps: 828, steps per second:  18, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.009303, mae: 1.175288, mean_q: 1.464405, mean_eps: 0.921311
  88353/250000: episode: 127, duration: 28.465s, episode steps: 506, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.009045, mae: 1.164554, mean_q: 1.450800, mean_eps: 0.920710
  88882/250000: episode: 128, duration: 29.113s, episode steps: 529, steps per second:  18, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.009375, mae: 1.172675, mean_q: 1.462296, mean_eps: 0.920244
  89425/250000: episode: 129, duration: 29.788s, episode steps: 543, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.009693, mae: 1.190977, mean_q: 1.483465, mean_eps: 0.919761
  90062/250000: episode: 130, duration: 35.483s, episode steps: 637, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.007493, mae: 1.180922, mean_q: 1.471669, mean_eps: 0.919230
  90698/250000: episode: 131, duration: 35.530s, episode steps: 636, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.008779, mae: 1.180740, mean_q: 1.470235, mean_eps: 0.918658
  91196/250000: episode: 132, duration: 28.747s, episode steps: 498, steps per second:  17, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.009168, mae: 1.188963, mean_q: 1.479702, mean_eps: 0.918149
  92031/250000: episode: 133, duration: 46.357s, episode steps: 835, steps per second:  18, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.009294, mae: 1.175579, mean_q: 1.463746, mean_eps: 0.917549
  92560/250000: episode: 134, duration: 29.328s, episode steps: 529, steps per second:  18, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.008857, mae: 1.186654, mean_q: 1.475624, mean_eps: 0.916935
  92942/250000: episode: 135, duration: 21.061s, episode steps: 382, steps per second:  18, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.009728, mae: 1.169779, mean_q: 1.457810, mean_eps: 0.916525
  93341/250000: episode: 136, duration: 23.253s, episode steps: 399, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.008704, mae: 1.187457, mean_q: 1.481581, mean_eps: 0.916172
  94000/250000: episode: 137, duration: 35.757s, episode steps: 659, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009217, mae: 1.192635, mean_q: 1.485350, mean_eps: 0.915697
  94388/250000: episode: 138, duration: 21.999s, episode steps: 388, steps per second:  18, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.008679, mae: 1.187370, mean_q: 1.480451, mean_eps: 0.915227
  94930/250000: episode: 139, duration: 30.381s, episode steps: 542, steps per second:  18, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.007745, mae: 1.186829, mean_q: 1.478560, mean_eps: 0.914808
  95622/250000: episode: 140, duration: 39.122s, episode steps: 692, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.009223, mae: 1.183079, mean_q: 1.472604, mean_eps: 0.914252
  96584/250000: episode: 141, duration: 52.595s, episode steps: 962, steps per second:  18, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.009385, mae: 1.181104, mean_q: 1.470378, mean_eps: 0.913508
  97256/250000: episode: 142, duration: 36.637s, episode steps: 672, steps per second:  18, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.008527, mae: 1.182998, mean_q: 1.473196, mean_eps: 0.912774
  98199/250000: episode: 143, duration: 54.324s, episode steps: 943, steps per second:  17, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.007794, mae: 1.178833, mean_q: 1.469669, mean_eps: 0.912047
  98659/250000: episode: 144, duration: 25.063s, episode steps: 460, steps per second:  18, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.009107, mae: 1.170941, mean_q: 1.457340, mean_eps: 0.911415
  99233/250000: episode: 145, duration: 32.479s, episode steps: 574, steps per second:  18, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.009411, mae: 1.183907, mean_q: 1.474035, mean_eps: 0.910949
  99886/250000: episode: 146, duration: 35.877s, episode steps: 653, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.008757, mae: 1.179015, mean_q: 1.469011, mean_eps: 0.910396
 100758/250000: episode: 147, duration: 49.095s, episode steps: 872, steps per second:  18, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.011146, mae: 1.222864, mean_q: 1.522822, mean_eps: 0.909710
 101575/250000: episode: 148, duration: 45.769s, episode steps: 817, steps per second:  18, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.010027, mae: 1.216250, mean_q: 1.513510, mean_eps: 0.908951
 102344/250000: episode: 149, duration: 41.906s, episode steps: 769, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.010435, mae: 1.225814, mean_q: 1.524103, mean_eps: 0.908238
 102976/250000: episode: 150, duration: 35.803s, episode steps: 632, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.010056, mae: 1.220690, mean_q: 1.518815, mean_eps: 0.907608
 103780/250000: episode: 151, duration: 45.221s, episode steps: 804, steps per second:  18, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.010037, mae: 1.217938, mean_q: 1.515041, mean_eps: 0.906962
 104209/250000: episode: 152, duration: 25.526s, episode steps: 429, steps per second:  17, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.009693, mae: 1.223487, mean_q: 1.521525, mean_eps: 0.906405
 104874/250000: episode: 153, duration: 36.698s, episode steps: 665, steps per second:  18, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.010042, mae: 1.227939, mean_q: 1.527789, mean_eps: 0.905912
 105674/250000: episode: 154, duration: 44.740s, episode steps: 800, steps per second:  18, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.009614, mae: 1.219484, mean_q: 1.519784, mean_eps: 0.905253
 106330/250000: episode: 155, duration: 36.757s, episode steps: 656, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.010418, mae: 1.204053, mean_q: 1.499291, mean_eps: 0.904598
 107257/250000: episode: 156, duration: 51.616s, episode steps: 927, steps per second:  18, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.009314, mae: 1.211657, mean_q: 1.508456, mean_eps: 0.903885
 107952/250000: episode: 157, duration: 39.232s, episode steps: 695, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.009674, mae: 1.227996, mean_q: 1.528798, mean_eps: 0.903156
 108375/250000: episode: 158, duration: 23.556s, episode steps: 423, steps per second:  18, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.009439, mae: 1.220780, mean_q: 1.519290, mean_eps: 0.902654
 109178/250000: episode: 159, duration: 44.511s, episode steps: 803, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.008844, mae: 1.217022, mean_q: 1.515169, mean_eps: 0.902102
 109778/250000: episode: 160, duration: 33.912s, episode steps: 600, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.009829, mae: 1.214715, mean_q: 1.511299, mean_eps: 0.901470
 110521/250000: episode: 161, duration: 42.685s, episode steps: 743, steps per second:  17, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.009063, mae: 1.212508, mean_q: 1.508367, mean_eps: 0.900865
 111099/250000: episode: 162, duration: 32.408s, episode steps: 578, steps per second:  18, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.009792, mae: 1.229364, mean_q: 1.529951, mean_eps: 0.900271
 111486/250000: episode: 163, duration: 21.573s, episode steps: 387, steps per second:  18, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.011269, mae: 1.224237, mean_q: 1.522175, mean_eps: 0.899837
 112160/250000: episode: 164, duration: 38.293s, episode steps: 674, steps per second:  18, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011014, mae: 1.234150, mean_q: 1.533092, mean_eps: 0.899360
 113245/250000: episode: 165, duration: 60.926s, episode steps: 1085, steps per second:  18, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.010104, mae: 1.215504, mean_q: 1.512278, mean_eps: 0.898568
 114811/250000: episode: 166, duration: 87.173s, episode steps: 1566, steps per second:  18, episode reward: 14.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.007857, mae: 1.208147, mean_q: 1.502316, mean_eps: 0.897375
 115582/250000: episode: 167, duration: 43.830s, episode steps: 771, steps per second:  18, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.009570, mae: 1.244295, mean_q: 1.548576, mean_eps: 0.896324
 116099/250000: episode: 168, duration: 28.750s, episode steps: 517, steps per second:  18, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.009517, mae: 1.229969, mean_q: 1.532170, mean_eps: 0.895744
 117338/250000: episode: 169, duration: 71.284s, episode steps: 1239, steps per second:  17, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.009989, mae: 1.214389, mean_q: 1.510081, mean_eps: 0.894954
 118428/250000: episode: 170, duration: 62.059s, episode steps: 1090, steps per second:  18, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.009230, mae: 1.218016, mean_q: 1.515685, mean_eps: 0.893906
 119142/250000: episode: 171, duration: 40.128s, episode steps: 714, steps per second:  18, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.009433, mae: 1.225968, mean_q: 1.524697, mean_eps: 0.893094
 120238/250000: episode: 172, duration: 62.116s, episode steps: 1096, steps per second:  18, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.009685, mae: 1.227581, mean_q: 1.526995, mean_eps: 0.892279
 120753/250000: episode: 173, duration: 28.175s, episode steps: 515, steps per second:  18, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008666, mae: 1.230067, mean_q: 1.530821, mean_eps: 0.891554
 121908/250000: episode: 174, duration: 65.338s, episode steps: 1155, steps per second:  18, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.008958, mae: 1.225003, mean_q: 1.525405, mean_eps: 0.890803
 122607/250000: episode: 175, duration: 39.837s, episode steps: 699, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.009693, mae: 1.232997, mean_q: 1.534335, mean_eps: 0.889970
 123003/250000: episode: 176, duration: 21.695s, episode steps: 396, steps per second:  18, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.010031, mae: 1.219869, mean_q: 1.518507, mean_eps: 0.889476
 123612/250000: episode: 177, duration: 35.530s, episode steps: 609, steps per second:  17, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.009850, mae: 1.218002, mean_q: 1.514199, mean_eps: 0.889025
 124437/250000: episode: 178, duration: 45.829s, episode steps: 825, steps per second:  18, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.010164, mae: 1.237785, mean_q: 1.538990, mean_eps: 0.888378
 124943/250000: episode: 179, duration: 29.131s, episode steps: 506, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.008859, mae: 1.217519, mean_q: 1.516381, mean_eps: 0.887779
 125816/250000: episode: 180, duration: 47.883s, episode steps: 873, steps per second:  18, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.009477, mae: 1.229648, mean_q: 1.527885, mean_eps: 0.887160
 126723/250000: episode: 181, duration: 51.519s, episode steps: 907, steps per second:  18, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.007964, mae: 1.225166, mean_q: 1.523459, mean_eps: 0.886359
 127442/250000: episode: 182, duration: 40.596s, episode steps: 719, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.010020, mae: 1.226119, mean_q: 1.524191, mean_eps: 0.885626
 127840/250000: episode: 183, duration: 25.899s, episode steps: 398, steps per second:  15, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.008745, mae: 1.243454, mean_q: 1.548739, mean_eps: 0.885124
 128312/250000: episode: 184, duration: 27.119s, episode steps: 472, steps per second:  17, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.009252, mae: 1.193591, mean_q: 1.482476, mean_eps: 0.884733
 129304/250000: episode: 185, duration: 55.314s, episode steps: 992, steps per second:  18, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008849, mae: 1.230570, mean_q: 1.529389, mean_eps: 0.884075
 129948/250000: episode: 186, duration: 38.588s, episode steps: 644, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.009218, mae: 1.227487, mean_q: 1.527208, mean_eps: 0.883338
 130587/250000: episode: 187, duration: 36.484s, episode steps: 639, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.009840, mae: 1.224530, mean_q: 1.523858, mean_eps: 0.882761
 131271/250000: episode: 188, duration: 38.481s, episode steps: 684, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.009735, mae: 1.238270, mean_q: 1.540258, mean_eps: 0.882165
 132051/250000: episode: 189, duration: 45.041s, episode steps: 780, steps per second:  17, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.008576, mae: 1.233985, mean_q: 1.534653, mean_eps: 0.881506
 132442/250000: episode: 190, duration: 22.493s, episode steps: 391, steps per second:  17, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.008681, mae: 1.245020, mean_q: 1.548496, mean_eps: 0.880979
 133130/250000: episode: 191, duration: 40.488s, episode steps: 688, steps per second:  17, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.009375, mae: 1.228512, mean_q: 1.526862, mean_eps: 0.880493
 133836/250000: episode: 192, duration: 38.810s, episode steps: 706, steps per second:  18, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.010799, mae: 1.249366, mean_q: 1.554527, mean_eps: 0.879866
 134323/250000: episode: 193, duration: 28.804s, episode steps: 487, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.009815, mae: 1.243194, mean_q: 1.546015, mean_eps: 0.879330
 135380/250000: episode: 194, duration: 60.131s, episode steps: 1057, steps per second:  18, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.009988, mae: 1.229284, mean_q: 1.529178, mean_eps: 0.878635
 136255/250000: episode: 195, duration: 50.087s, episode steps: 875, steps per second:  17, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.009075, mae: 1.238459, mean_q: 1.540807, mean_eps: 0.877766
 136757/250000: episode: 196, duration: 28.845s, episode steps: 502, steps per second:  17, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.009217, mae: 1.223449, mean_q: 1.524486, mean_eps: 0.877145
 137312/250000: episode: 197, duration: 31.717s, episode steps: 555, steps per second:  17, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.009099, mae: 1.235793, mean_q: 1.537621, mean_eps: 0.876669
 137980/250000: episode: 198, duration: 38.276s, episode steps: 668, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.009220, mae: 1.242288, mean_q: 1.545875, mean_eps: 0.876120
 138337/250000: episode: 199, duration: 20.671s, episode steps: 357, steps per second:  17, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.008283, mae: 1.245403, mean_q: 1.549183, mean_eps: 0.875658
 138924/250000: episode: 200, duration: 33.919s, episode steps: 587, steps per second:  17, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.010358, mae: 1.235604, mean_q: 1.536597, mean_eps: 0.875233
 139284/250000: episode: 201, duration: 19.820s, episode steps: 360, steps per second:  18, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.009643, mae: 1.232216, mean_q: 1.530005, mean_eps: 0.874808
 140106/250000: episode: 202, duration: 47.043s, episode steps: 822, steps per second:  17, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.009434, mae: 1.234174, mean_q: 1.536433, mean_eps: 0.874275
 140796/250000: episode: 203, duration: 38.039s, episode steps: 690, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.008316, mae: 1.246632, mean_q: 1.548805, mean_eps: 0.873595
 141352/250000: episode: 204, duration: 32.226s, episode steps: 556, steps per second:  17, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.008753, mae: 1.229628, mean_q: 1.529621, mean_eps: 0.873035
 142126/250000: episode: 205, duration: 44.973s, episode steps: 774, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.009851, mae: 1.217068, mean_q: 1.512833, mean_eps: 0.872436
 142597/250000: episode: 206, duration: 27.052s, episode steps: 471, steps per second:  17, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.009591, mae: 1.229964, mean_q: 1.530174, mean_eps: 0.871874
 143122/250000: episode: 207, duration: 29.551s, episode steps: 525, steps per second:  18, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.009863, mae: 1.226897, mean_q: 1.525977, mean_eps: 0.871426
 143956/250000: episode: 208, duration: 47.542s, episode steps: 834, steps per second:  18, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.008754, mae: 1.231717, mean_q: 1.531411, mean_eps: 0.870816
 144351/250000: episode: 209, duration: 22.004s, episode steps: 395, steps per second:  18, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.007942, mae: 1.227609, mean_q: 1.526064, mean_eps: 0.870263
 144746/250000: episode: 210, duration: 22.894s, episode steps: 395, steps per second:  17, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.008254, mae: 1.234731, mean_q: 1.535068, mean_eps: 0.869907
 145191/250000: episode: 211, duration: 24.666s, episode steps: 445, steps per second:  18, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.008834, mae: 1.219548, mean_q: 1.516086, mean_eps: 0.869529
 145599/250000: episode: 212, duration: 23.558s, episode steps: 408, steps per second:  17, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.009561, mae: 1.244758, mean_q: 1.546583, mean_eps: 0.869145
 146436/250000: episode: 213, duration: 47.368s, episode steps: 837, steps per second:  18, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.008960, mae: 1.231678, mean_q: 1.531641, mean_eps: 0.868586
 147032/250000: episode: 214, duration: 34.677s, episode steps: 596, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.008650, mae: 1.228592, mean_q: 1.527164, mean_eps: 0.867941
 147876/250000: episode: 215, duration: 48.682s, episode steps: 844, steps per second:  17, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.009765, mae: 1.228238, mean_q: 1.525781, mean_eps: 0.867293
 148581/250000: episode: 216, duration: 40.826s, episode steps: 705, steps per second:  17, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.008217, mae: 1.227539, mean_q: 1.527655, mean_eps: 0.866595
 148981/250000: episode: 217, duration: 23.208s, episode steps: 400, steps per second:  17, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.010454, mae: 1.232376, mean_q: 1.533257, mean_eps: 0.866096
 149581/250000: episode: 218, duration: 33.924s, episode steps: 600, steps per second:  18, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.009804, mae: 1.223128, mean_q: 1.519644, mean_eps: 0.865646
 150196/250000: episode: 219, duration: 35.521s, episode steps: 615, steps per second:  17, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.008841, mae: 1.235523, mean_q: 1.535209, mean_eps: 0.865101
 151001/250000: episode: 220, duration: 45.294s, episode steps: 805, steps per second:  18, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010843, mae: 1.274598, mean_q: 1.582218, mean_eps: 0.864462
 151845/250000: episode: 221, duration: 49.891s, episode steps: 844, steps per second:  17, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011972, mae: 1.284336, mean_q: 1.594256, mean_eps: 0.863718
 152448/250000: episode: 222, duration: 33.900s, episode steps: 603, steps per second:  18, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.010712, mae: 1.287465, mean_q: 1.598080, mean_eps: 0.863069
 153326/250000: episode: 223, duration: 49.530s, episode steps: 878, steps per second:  18, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011037, mae: 1.276459, mean_q: 1.584400, mean_eps: 0.862403
 154000/250000: episode: 224, duration: 39.417s, episode steps: 674, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.011297, mae: 1.298733, mean_q: 1.610854, mean_eps: 0.861704
 154693/250000: episode: 225, duration: 38.521s, episode steps: 693, steps per second:  18, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.010297, mae: 1.272136, mean_q: 1.579648, mean_eps: 0.861089
 155255/250000: episode: 226, duration: 31.897s, episode steps: 562, steps per second:  18, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.011521, mae: 1.297597, mean_q: 1.610326, mean_eps: 0.860523
 155712/250000: episode: 227, duration: 26.019s, episode steps: 457, steps per second:  18, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.010643, mae: 1.292452, mean_q: 1.603724, mean_eps: 0.860066
 156350/250000: episode: 228, duration: 36.727s, episode steps: 638, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.010742, mae: 1.271607, mean_q: 1.577449, mean_eps: 0.859573
 156973/250000: episode: 229, duration: 34.636s, episode steps: 623, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.010587, mae: 1.286617, mean_q: 1.596124, mean_eps: 0.859004
 157782/250000: episode: 230, duration: 46.309s, episode steps: 809, steps per second:  17, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.011110, mae: 1.282796, mean_q: 1.590763, mean_eps: 0.858360
 158805/250000: episode: 231, duration: 60.730s, episode steps: 1023, steps per second:  17, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.010703, mae: 1.287522, mean_q: 1.597046, mean_eps: 0.857535
 159510/250000: episode: 232, duration: 39.942s, episode steps: 705, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.010936, mae: 1.276493, mean_q: 1.581836, mean_eps: 0.856758
 160124/250000: episode: 233, duration: 35.338s, episode steps: 614, steps per second:  17, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.009648, mae: 1.271832, mean_q: 1.578845, mean_eps: 0.856166
 161097/250000: episode: 234, duration: 55.602s, episode steps: 973, steps per second:  17, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.010226, mae: 1.282359, mean_q: 1.589987, mean_eps: 0.855451
 161743/250000: episode: 235, duration: 36.977s, episode steps: 646, steps per second:  17, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010475, mae: 1.284993, mean_q: 1.593496, mean_eps: 0.854722
 162250/250000: episode: 236, duration: 28.848s, episode steps: 507, steps per second:  18, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.009701, mae: 1.274606, mean_q: 1.581376, mean_eps: 0.854204
 162882/250000: episode: 237, duration: 36.174s, episode steps: 632, steps per second:  17, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010577, mae: 1.288156, mean_q: 1.597649, mean_eps: 0.853691
 163430/250000: episode: 238, duration: 30.783s, episode steps: 548, steps per second:  18, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.009987, mae: 1.269787, mean_q: 1.574543, mean_eps: 0.853160
 163978/250000: episode: 239, duration: 31.395s, episode steps: 548, steps per second:  17, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.011029, mae: 1.281888, mean_q: 1.590980, mean_eps: 0.852666
 164863/250000: episode: 240, duration: 51.438s, episode steps: 885, steps per second:  17, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.010002, mae: 1.278105, mean_q: 1.585474, mean_eps: 0.852022
 165855/250000: episode: 241, duration: 56.814s, episode steps: 992, steps per second:  17, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.008972, mae: 1.280490, mean_q: 1.588378, mean_eps: 0.851178
 166403/250000: episode: 242, duration: 31.175s, episode steps: 548, steps per second:  18, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.011096, mae: 1.291280, mean_q: 1.600713, mean_eps: 0.850485
 167257/250000: episode: 243, duration: 50.449s, episode steps: 854, steps per second:  17, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.009921, mae: 1.279868, mean_q: 1.587951, mean_eps: 0.849853
 167857/250000: episode: 244, duration: 40.170s, episode steps: 600, steps per second:  15, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.008822, mae: 1.272737, mean_q: 1.579736, mean_eps: 0.849198
 168358/250000: episode: 245, duration: 32.803s, episode steps: 501, steps per second:  15, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.010846, mae: 1.284665, mean_q: 1.592562, mean_eps: 0.848703
 168987/250000: episode: 246, duration: 38.079s, episode steps: 629, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010147, mae: 1.269148, mean_q: 1.574745, mean_eps: 0.848195
 169759/250000: episode: 247, duration: 44.648s, episode steps: 772, steps per second:  17, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.009129, mae: 1.276142, mean_q: 1.584556, mean_eps: 0.847565
 170657/250000: episode: 248, duration: 52.691s, episode steps: 898, steps per second:  17, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.009581, mae: 1.273103, mean_q: 1.577770, mean_eps: 0.846813
 171145/250000: episode: 249, duration: 28.649s, episode steps: 488, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.010821, mae: 1.284598, mean_q: 1.591701, mean_eps: 0.846188
 171750/250000: episode: 250, duration: 34.524s, episode steps: 605, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.009621, mae: 1.271259, mean_q: 1.576784, mean_eps: 0.845697
 172349/250000: episode: 251, duration: 35.089s, episode steps: 599, steps per second:  17, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.011502, mae: 1.283603, mean_q: 1.592165, mean_eps: 0.845155
 172916/250000: episode: 252, duration: 33.344s, episode steps: 567, steps per second:  17, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.010000, mae: 1.280308, mean_q: 1.587729, mean_eps: 0.844631
 173292/250000: episode: 253, duration: 23.536s, episode steps: 376, steps per second:  16, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.010660, mae: 1.276217, mean_q: 1.581489, mean_eps: 0.844208
 173888/250000: episode: 254, duration: 34.802s, episode steps: 596, steps per second:  17, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.009670, mae: 1.288058, mean_q: 1.597453, mean_eps: 0.843771
 174653/250000: episode: 255, duration: 45.579s, episode steps: 765, steps per second:  17, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.009991, mae: 1.268483, mean_q: 1.572772, mean_eps: 0.843157
 175255/250000: episode: 256, duration: 35.271s, episode steps: 602, steps per second:  17, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.010455, mae: 1.298815, mean_q: 1.612286, mean_eps: 0.842541
 176272/250000: episode: 257, duration: 60.289s, episode steps: 1017, steps per second:  17, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.010853, mae: 1.292680, mean_q: 1.604673, mean_eps: 0.841814
 176871/250000: episode: 258, duration: 37.619s, episode steps: 599, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.009376, mae: 1.269933, mean_q: 1.577757, mean_eps: 0.841087
 177390/250000: episode: 259, duration: 32.804s, episode steps: 519, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.009910, mae: 1.278508, mean_q: 1.585994, mean_eps: 0.840583
 178337/250000: episode: 260, duration: 62.137s, episode steps: 947, steps per second:  15, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010202, mae: 1.276539, mean_q: 1.583667, mean_eps: 0.839922
 179001/250000: episode: 261, duration: 38.876s, episode steps: 664, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.009363, mae: 1.277435, mean_q: 1.585524, mean_eps: 0.839197
 180222/250000: episode: 262, duration: 70.729s, episode steps: 1221, steps per second:  17, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.009232, mae: 1.278950, mean_q: 1.585440, mean_eps: 0.838349
 180723/250000: episode: 263, duration: 29.098s, episode steps: 501, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.010114, mae: 1.291815, mean_q: 1.602542, mean_eps: 0.837575
 181938/250000: episode: 264, duration: 69.610s, episode steps: 1215, steps per second:  17, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.011150, mae: 1.295813, mean_q: 1.607698, mean_eps: 0.836803
 182393/250000: episode: 265, duration: 27.962s, episode steps: 455, steps per second:  16, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.010756, mae: 1.285409, mean_q: 1.593028, mean_eps: 0.836051
 182931/250000: episode: 266, duration: 34.222s, episode steps: 538, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.010019, mae: 1.289591, mean_q: 1.599257, mean_eps: 0.835604
 183560/250000: episode: 267, duration: 36.533s, episode steps: 629, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.009879, mae: 1.278673, mean_q: 1.586178, mean_eps: 0.835080
 184152/250000: episode: 268, duration: 38.306s, episode steps: 592, steps per second:  15, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.009564, mae: 1.286436, mean_q: 1.594281, mean_eps: 0.834531
 184802/250000: episode: 269, duration: 42.183s, episode steps: 650, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.009252, mae: 1.281922, mean_q: 1.589772, mean_eps: 0.833972
 185201/250000: episode: 270, duration: 26.454s, episode steps: 399, steps per second:  15, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.010200, mae: 1.275040, mean_q: 1.581556, mean_eps: 0.833498
 185817/250000: episode: 271, duration: 39.790s, episode steps: 616, steps per second:  15, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.010309, mae: 1.283265, mean_q: 1.591836, mean_eps: 0.833041
 186434/250000: episode: 272, duration: 40.993s, episode steps: 617, steps per second:  15, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.010593, mae: 1.291020, mean_q: 1.601018, mean_eps: 0.832487
 187235/250000: episode: 273, duration: 54.141s, episode steps: 801, steps per second:  15, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010298, mae: 1.284919, mean_q: 1.593735, mean_eps: 0.831849
 188185/250000: episode: 274, duration: 73.938s, episode steps: 950, steps per second:  13, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.010409, mae: 1.280172, mean_q: 1.587400, mean_eps: 0.831061
 188984/250000: episode: 275, duration: 71.619s, episode steps: 799, steps per second:  11, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.011020, mae: 1.286977, mean_q: 1.596882, mean_eps: 0.830274
 190000/250000: episode: 276, duration: 54.844s, episode steps: 1016, steps per second:  19, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.009868, mae: 1.271785, mean_q: 1.576244, mean_eps: 0.829459
 191024/250000: episode: 277, duration: 56.277s, episode steps: 1024, steps per second:  18, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.010494, mae: 1.275638, mean_q: 1.580518, mean_eps: 0.828541
 191545/250000: episode: 278, duration: 29.236s, episode steps: 521, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.009681, mae: 1.282816, mean_q: 1.589815, mean_eps: 0.827844
 191926/250000: episode: 279, duration: 21.973s, episode steps: 381, steps per second:  17, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.010915, mae: 1.287985, mean_q: 1.596007, mean_eps: 0.827438
 192553/250000: episode: 280, duration: 35.230s, episode steps: 627, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.010156, mae: 1.277111, mean_q: 1.584327, mean_eps: 0.826984
 193591/250000: episode: 281, duration: 58.609s, episode steps: 1038, steps per second:  18, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.009492, mae: 1.284339, mean_q: 1.591501, mean_eps: 0.826235
 194295/250000: episode: 282, duration: 38.495s, episode steps: 704, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.009538, mae: 1.277040, mean_q: 1.585546, mean_eps: 0.825452
 194886/250000: episode: 283, duration: 33.001s, episode steps: 591, steps per second:  18, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.011120, mae: 1.277299, mean_q: 1.583312, mean_eps: 0.824869
 195376/250000: episode: 284, duration: 27.144s, episode steps: 490, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.009249, mae: 1.277802, mean_q: 1.585861, mean_eps: 0.824383
 195983/250000: episode: 285, duration: 33.564s, episode steps: 607, steps per second:  18, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.009443, mae: 1.285894, mean_q: 1.594882, mean_eps: 0.823890
 196462/250000: episode: 286, duration: 25.670s, episode steps: 479, steps per second:  19, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.010396, mae: 1.268896, mean_q: 1.573918, mean_eps: 0.823400
 197178/250000: episode: 287, duration: 39.983s, episode steps: 716, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.010985, mae: 1.276297, mean_q: 1.582709, mean_eps: 0.822862
 197774/250000: episode: 288, duration: 32.366s, episode steps: 596, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.010449, mae: 1.272822, mean_q: 1.578237, mean_eps: 0.822272
 198266/250000: episode: 289, duration: 28.563s, episode steps: 492, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.010132, mae: 1.278373, mean_q: 1.587182, mean_eps: 0.821782
 198644/250000: episode: 290, duration: 20.337s, episode steps: 378, steps per second:  19, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.008661, mae: 1.280117, mean_q: 1.587625, mean_eps: 0.821391
 199201/250000: episode: 291, duration: 30.723s, episode steps: 557, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.009310, mae: 1.270410, mean_q: 1.574934, mean_eps: 0.820970
 200448/250000: episode: 292, duration: 68.230s, episode steps: 1247, steps per second:  18, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.010488, mae: 1.309434, mean_q: 1.624819, mean_eps: 0.820158
 201106/250000: episode: 293, duration: 36.309s, episode steps: 658, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.012285, mae: 1.333226, mean_q: 1.650632, mean_eps: 0.819302
 201595/250000: episode: 294, duration: 26.890s, episode steps: 489, steps per second:  18, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.012830, mae: 1.324750, mean_q: 1.641400, mean_eps: 0.818785
 202138/250000: episode: 295, duration: 29.934s, episode steps: 543, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.010872, mae: 1.353523, mean_q: 1.674700, mean_eps: 0.818321
 203223/250000: episode: 296, duration: 59.976s, episode steps: 1085, steps per second:  18, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011422, mae: 1.331902, mean_q: 1.650361, mean_eps: 0.817588
 203729/250000: episode: 297, duration: 27.510s, episode steps: 506, steps per second:  18, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.011733, mae: 1.333779, mean_q: 1.652933, mean_eps: 0.816872
 204117/250000: episode: 298, duration: 21.436s, episode steps: 388, steps per second:  18, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.009324, mae: 1.328036, mean_q: 1.645684, mean_eps: 0.816468
 204933/250000: episode: 299, duration: 44.783s, episode steps: 816, steps per second:  18, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.009873, mae: 1.336776, mean_q: 1.656470, mean_eps: 0.815927
 205876/250000: episode: 300, duration: 51.884s, episode steps: 943, steps per second:  18, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.011036, mae: 1.329615, mean_q: 1.647814, mean_eps: 0.815136
 206431/250000: episode: 301, duration: 30.999s, episode steps: 555, steps per second:  18, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011436, mae: 1.339302, mean_q: 1.658588, mean_eps: 0.814463
 207067/250000: episode: 302, duration: 35.275s, episode steps: 636, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.011634, mae: 1.333973, mean_q: 1.650145, mean_eps: 0.813927
 207594/250000: episode: 303, duration: 29.447s, episode steps: 527, steps per second:  18, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.012358, mae: 1.331069, mean_q: 1.647787, mean_eps: 0.813403
 208117/250000: episode: 304, duration: 29.544s, episode steps: 523, steps per second:  18, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.011489, mae: 1.327163, mean_q: 1.643767, mean_eps: 0.812930
 208621/250000: episode: 305, duration: 27.474s, episode steps: 504, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.012300, mae: 1.326729, mean_q: 1.642770, mean_eps: 0.812467
 209348/250000: episode: 306, duration: 40.818s, episode steps: 727, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.010344, mae: 1.337090, mean_q: 1.656546, mean_eps: 0.811914
 209746/250000: episode: 307, duration: 21.465s, episode steps: 398, steps per second:  19, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.011163, mae: 1.352109, mean_q: 1.675755, mean_eps: 0.811409
 210373/250000: episode: 308, duration: 35.254s, episode steps: 627, steps per second:  18, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.011152, mae: 1.323410, mean_q: 1.639354, mean_eps: 0.810946
 211188/250000: episode: 309, duration: 44.946s, episode steps: 815, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.011482, mae: 1.336045, mean_q: 1.650932, mean_eps: 0.810298
 211885/250000: episode: 310, duration: 39.137s, episode steps: 697, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.010940, mae: 1.333495, mean_q: 1.650005, mean_eps: 0.809618
 212464/250000: episode: 311, duration: 32.410s, episode steps: 579, steps per second:  18, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.012297, mae: 1.324966, mean_q: 1.639792, mean_eps: 0.809043
 213014/250000: episode: 312, duration: 30.128s, episode steps: 550, steps per second:  18, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.011047, mae: 1.324850, mean_q: 1.641838, mean_eps: 0.808536
 214064/250000: episode: 313, duration: 58.572s, episode steps: 1050, steps per second:  18, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011431, mae: 1.344178, mean_q: 1.664235, mean_eps: 0.807816
 214570/250000: episode: 314, duration: 28.350s, episode steps: 506, steps per second:  18, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.011656, mae: 1.313084, mean_q: 1.624408, mean_eps: 0.807116
 215248/250000: episode: 315, duration: 37.453s, episode steps: 678, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.009295, mae: 1.327482, mean_q: 1.643791, mean_eps: 0.806583
 215846/250000: episode: 316, duration: 33.721s, episode steps: 598, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010943, mae: 1.339115, mean_q: 1.656857, mean_eps: 0.806009
 217057/250000: episode: 317, duration: 67.089s, episode steps: 1211, steps per second:  18, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.010572, mae: 1.330949, mean_q: 1.646100, mean_eps: 0.805193
 217677/250000: episode: 318, duration: 34.253s, episode steps: 620, steps per second:  18, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010915, mae: 1.319883, mean_q: 1.634394, mean_eps: 0.804369
 218381/250000: episode: 319, duration: 38.304s, episode steps: 704, steps per second:  18, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.011184, mae: 1.330022, mean_q: 1.647431, mean_eps: 0.803773
 219063/250000: episode: 320, duration: 37.832s, episode steps: 682, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.010879, mae: 1.324833, mean_q: 1.639964, mean_eps: 0.803150
 219723/250000: episode: 321, duration: 37.457s, episode steps: 660, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.010999, mae: 1.336779, mean_q: 1.654392, mean_eps: 0.802547
 220727/250000: episode: 322, duration: 55.111s, episode steps: 1004, steps per second:  18, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.010901, mae: 1.322496, mean_q: 1.638544, mean_eps: 0.801798
 221167/250000: episode: 323, duration: 24.447s, episode steps: 440, steps per second:  18, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.010464, mae: 1.336656, mean_q: 1.654919, mean_eps: 0.801149
 221862/250000: episode: 324, duration: 38.239s, episode steps: 695, steps per second:  18, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010513, mae: 1.326111, mean_q: 1.640948, mean_eps: 0.800637
 222806/250000: episode: 325, duration: 54.402s, episode steps: 944, steps per second:  17, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012218, mae: 1.323235, mean_q: 1.637295, mean_eps: 0.799899
 223306/250000: episode: 326, duration: 28.188s, episode steps: 500, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.009348, mae: 1.337421, mean_q: 1.655617, mean_eps: 0.799250
 224035/250000: episode: 327, duration: 40.217s, episode steps: 729, steps per second:  18, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.009685, mae: 1.333714, mean_q: 1.652304, mean_eps: 0.798697
 224628/250000: episode: 328, duration: 33.540s, episode steps: 593, steps per second:  18, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.011089, mae: 1.339603, mean_q: 1.659236, mean_eps: 0.798103
 225090/250000: episode: 329, duration: 25.932s, episode steps: 462, steps per second:  18, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011161, mae: 1.338041, mean_q: 1.658533, mean_eps: 0.797628
 226072/250000: episode: 330, duration: 54.140s, episode steps: 982, steps per second:  18, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.011246, mae: 1.332819, mean_q: 1.649082, mean_eps: 0.796978
 227188/250000: episode: 331, duration: 62.753s, episode steps: 1116, steps per second:  18, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010620, mae: 1.320174, mean_q: 1.633851, mean_eps: 0.796035
 227703/250000: episode: 332, duration: 29.151s, episode steps: 515, steps per second:  18, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011808, mae: 1.346205, mean_q: 1.666276, mean_eps: 0.795300
 228602/250000: episode: 333, duration: 49.211s, episode steps: 899, steps per second:  18, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.010620, mae: 1.334630, mean_q: 1.652759, mean_eps: 0.794663
 229392/250000: episode: 334, duration: 45.464s, episode steps: 790, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.010127, mae: 1.339404, mean_q: 1.659675, mean_eps: 0.793904
 230037/250000: episode: 335, duration: 36.305s, episode steps: 645, steps per second:  18, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.011357, mae: 1.338616, mean_q: 1.656994, mean_eps: 0.793257
 230502/250000: episode: 336, duration: 26.976s, episode steps: 465, steps per second:  17, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.010677, mae: 1.322872, mean_q: 1.639415, mean_eps: 0.792757
 231363/250000: episode: 337, duration: 48.220s, episode steps: 861, steps per second:  18, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.010625, mae: 1.336254, mean_q: 1.654395, mean_eps: 0.792161
 232317/250000: episode: 338, duration: 52.411s, episode steps: 954, steps per second:  18, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.011596, mae: 1.333150, mean_q: 1.648182, mean_eps: 0.791344
 232846/250000: episode: 339, duration: 30.069s, episode steps: 529, steps per second:  18, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.010943, mae: 1.331903, mean_q: 1.648356, mean_eps: 0.790676
 233917/250000: episode: 340, duration: 60.427s, episode steps: 1071, steps per second:  18, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011110, mae: 1.341035, mean_q: 1.659957, mean_eps: 0.789956
 234814/250000: episode: 341, duration: 49.781s, episode steps: 897, steps per second:  18, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.011615, mae: 1.326239, mean_q: 1.640520, mean_eps: 0.789071
 235344/250000: episode: 342, duration: 30.367s, episode steps: 530, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.010681, mae: 1.325924, mean_q: 1.639718, mean_eps: 0.788430
 236253/250000: episode: 343, duration: 50.684s, episode steps: 909, steps per second:  18, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.010722, mae: 1.326546, mean_q: 1.641891, mean_eps: 0.787782
 236887/250000: episode: 344, duration: 34.833s, episode steps: 634, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.010817, mae: 1.330449, mean_q: 1.647782, mean_eps: 0.787087
 237622/250000: episode: 345, duration: 41.802s, episode steps: 735, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.011480, mae: 1.342049, mean_q: 1.661309, mean_eps: 0.786471
 238271/250000: episode: 346, duration: 36.392s, episode steps: 649, steps per second:  18, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011773, mae: 1.340108, mean_q: 1.661168, mean_eps: 0.785849
 239140/250000: episode: 347, duration: 48.327s, episode steps: 869, steps per second:  18, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.011339, mae: 1.337975, mean_q: 1.657559, mean_eps: 0.785166
 239801/250000: episode: 348, duration: 37.545s, episode steps: 661, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.011359, mae: 1.327813, mean_q: 1.644286, mean_eps: 0.784477
 240764/250000: episode: 349, duration: 55.109s, episode steps: 963, steps per second:  17, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.010844, mae: 1.329105, mean_q: 1.644606, mean_eps: 0.783746
 241138/250000: episode: 350, duration: 21.474s, episode steps: 374, steps per second:  17, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.010230, mae: 1.341934, mean_q: 1.661186, mean_eps: 0.783145
 241717/250000: episode: 351, duration: 32.137s, episode steps: 579, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.011454, mae: 1.327437, mean_q: 1.642735, mean_eps: 0.782715
 242226/250000: episode: 352, duration: 29.025s, episode steps: 509, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.011475, mae: 1.329768, mean_q: 1.645958, mean_eps: 0.782225
 243260/250000: episode: 353, duration: 59.631s, episode steps: 1034, steps per second:  17, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.011852, mae: 1.338435, mean_q: 1.657298, mean_eps: 0.781532
 244098/250000: episode: 354, duration: 47.469s, episode steps: 838, steps per second:  18, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.010710, mae: 1.339369, mean_q: 1.659797, mean_eps: 0.780690
 244779/250000: episode: 355, duration: 37.879s, episode steps: 681, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.011032, mae: 1.323675, mean_q: 1.636616, mean_eps: 0.780006
 245333/250000: episode: 356, duration: 31.293s, episode steps: 554, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010527, mae: 1.342095, mean_q: 1.659850, mean_eps: 0.779450
 245975/250000: episode: 357, duration: 35.610s, episode steps: 642, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.012231, mae: 1.333105, mean_q: 1.648984, mean_eps: 0.778911
 246584/250000: episode: 358, duration: 34.623s, episode steps: 609, steps per second:  18, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.011672, mae: 1.341628, mean_q: 1.661183, mean_eps: 0.778350
 247318/250000: episode: 359, duration: 41.098s, episode steps: 734, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.010552, mae: 1.333787, mean_q: 1.651295, mean_eps: 0.777745
 247953/250000: episode: 360, duration: 35.962s, episode steps: 635, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.012401, mae: 1.337557, mean_q: 1.653295, mean_eps: 0.777128
 248799/250000: episode: 361, duration: 46.905s, episode steps: 846, steps per second:  18, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.010560, mae: 1.322794, mean_q: 1.636269, mean_eps: 0.776462
 249453/250000: episode: 362, duration: 36.735s, episode steps: 654, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011632, mae: 1.319595, mean_q: 1.631010, mean_eps: 0.775787
done, took 13720.063 seconds
########################################################
PROCESO TERMINADO
########################################################