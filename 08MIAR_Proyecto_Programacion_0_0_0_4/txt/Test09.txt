['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_250000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
    420/250000: episode: 1, duration: 1.942s, episode steps: 420, steps per second: 216, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 3.411s, episode steps: 711, steps per second: 208, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 4.200s, episode steps: 810, steps per second: 193, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2827/250000: episode: 4, duration: 4.521s, episode steps: 886, steps per second: 196, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3345/250000: episode: 5, duration: 2.318s, episode steps: 518, steps per second: 223, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3990/250000: episode: 6, duration: 2.720s, episode steps: 645, steps per second: 237, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4451/250000: episode: 7, duration: 1.941s, episode steps: 461, steps per second: 237, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5247/250000: episode: 8, duration: 4.152s, episode steps: 796, steps per second: 192, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5629/250000: episode: 9, duration: 1.846s, episode steps: 382, steps per second: 207, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6732/250000: episode: 10, duration: 5.281s, episode steps: 1103, steps per second: 209, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7409/250000: episode: 11, duration: 3.438s, episode steps: 677, steps per second: 197, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8051/250000: episode: 12, duration: 3.046s, episode steps: 642, steps per second: 211, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8656/250000: episode: 13, duration: 2.782s, episode steps: 605, steps per second: 217, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9268/250000: episode: 14, duration: 3.034s, episode steps: 612, steps per second: 202, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9891/250000: episode: 15, duration: 3.058s, episode steps: 623, steps per second: 204, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10287/250000: episode: 16, duration: 12.126s, episode steps: 396, steps per second:  33, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.013199, mae: 1.937303, mean_q: 2.392589, mean_eps: 0.996348
  10670/250000: episode: 17, duration: 15.109s, episode steps: 383, steps per second:  25, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.009374, mae: 1.879750, mean_q: 2.318383, mean_eps: 0.996228
  11394/250000: episode: 18, duration: 28.887s, episode steps: 724, steps per second:  25, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.011148, mae: 1.910910, mean_q: 2.356746, mean_eps: 0.996028
  11927/250000: episode: 19, duration: 24.111s, episode steps: 533, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.009726, mae: 1.922823, mean_q: 2.369464, mean_eps: 0.995802
  12795/250000: episode: 20, duration: 34.341s, episode steps: 868, steps per second:  25, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.009399, mae: 1.930408, mean_q: 2.380877, mean_eps: 0.995550
  13510/250000: episode: 21, duration: 28.952s, episode steps: 715, steps per second:  25, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010059, mae: 1.921572, mean_q: 2.367967, mean_eps: 0.995265
  14079/250000: episode: 22, duration: 21.899s, episode steps: 569, steps per second:  26, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.010193, mae: 1.898106, mean_q: 2.341013, mean_eps: 0.995034
  14965/250000: episode: 23, duration: 35.064s, episode steps: 886, steps per second:  25, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010386, mae: 1.912822, mean_q: 2.358666, mean_eps: 0.994772
  15577/250000: episode: 24, duration: 26.467s, episode steps: 612, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.009675, mae: 1.949431, mean_q: 2.407005, mean_eps: 0.994502
  15975/250000: episode: 25, duration: 16.165s, episode steps: 398, steps per second:  25, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.009383, mae: 1.963208, mean_q: 2.422429, mean_eps: 0.994321
  16371/250000: episode: 26, duration: 15.643s, episode steps: 396, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.009751, mae: 1.947187, mean_q: 2.400053, mean_eps: 0.994178
  16877/250000: episode: 27, duration: 19.191s, episode steps: 506, steps per second:  26, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.010122, mae: 1.941283, mean_q: 2.391930, mean_eps: 0.994015
  18342/250000: episode: 28, duration: 57.596s, episode steps: 1465, steps per second:  25, episode reward: 23.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.009207, mae: 1.931412, mean_q: 2.379158, mean_eps: 0.993660
  18881/250000: episode: 29, duration: 21.257s, episode steps: 539, steps per second:  25, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.010249, mae: 1.955982, mean_q: 2.410887, mean_eps: 0.993300
  19731/250000: episode: 30, duration: 33.269s, episode steps: 850, steps per second:  26, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.010389, mae: 1.927023, mean_q: 2.374873, mean_eps: 0.993050
  20725/250000: episode: 31, duration: 38.528s, episode steps: 994, steps per second:  26, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.009964, mae: 1.913222, mean_q: 2.358391, mean_eps: 0.992718
  21318/250000: episode: 32, duration: 25.183s, episode steps: 593, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.009487, mae: 1.932958, mean_q: 2.385307, mean_eps: 0.992432
  21959/250000: episode: 33, duration: 25.384s, episode steps: 641, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.009997, mae: 1.924563, mean_q: 2.374068, mean_eps: 0.992210
  22902/250000: episode: 34, duration: 37.338s, episode steps: 943, steps per second:  25, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.009727, mae: 1.923785, mean_q: 2.370777, mean_eps: 0.991925
  23464/250000: episode: 35, duration: 21.428s, episode steps: 562, steps per second:  26, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010773, mae: 1.916690, mean_q: 2.363464, mean_eps: 0.991654
  24067/250000: episode: 36, duration: 24.228s, episode steps: 603, steps per second:  25, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.010512, mae: 1.945424, mean_q: 2.401178, mean_eps: 0.991445
  24753/250000: episode: 37, duration: 27.677s, episode steps: 686, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.010333, mae: 1.923978, mean_q: 2.373928, mean_eps: 0.991212
  25393/250000: episode: 38, duration: 24.480s, episode steps: 640, steps per second:  26, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.009082, mae: 1.916257, mean_q: 2.364207, mean_eps: 0.990973
  26860/250000: episode: 39, duration: 58.597s, episode steps: 1467, steps per second:  25, episode reward: 19.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.010783, mae: 1.919308, mean_q: 2.367323, mean_eps: 0.990595
  27231/250000: episode: 40, duration: 14.762s, episode steps: 371, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010496, mae: 1.949967, mean_q: 2.404416, mean_eps: 0.990264
  27935/250000: episode: 41, duration: 27.868s, episode steps: 704, steps per second:  25, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.011450, mae: 1.938239, mean_q: 2.389756, mean_eps: 0.990070
  28639/250000: episode: 42, duration: 28.547s, episode steps: 704, steps per second:  25, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.010706, mae: 1.918255, mean_q: 2.363619, mean_eps: 0.989817
  29175/250000: episode: 43, duration: 21.643s, episode steps: 536, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.009965, mae: 1.935322, mean_q: 2.386586, mean_eps: 0.989594
  29797/250000: episode: 44, duration: 26.092s, episode steps: 622, steps per second:  24, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.010230, mae: 1.938874, mean_q: 2.388211, mean_eps: 0.989385
  30835/250000: episode: 45, duration: 44.034s, episode steps: 1038, steps per second:  24, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.009816, mae: 1.920789, mean_q: 2.365895, mean_eps: 0.989086
  31265/250000: episode: 46, duration: 17.125s, episode steps: 430, steps per second:  25, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.010056, mae: 1.927154, mean_q: 2.376212, mean_eps: 0.988822
  31937/250000: episode: 47, duration: 26.302s, episode steps: 672, steps per second:  26, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.012129, mae: 1.915629, mean_q: 2.363948, mean_eps: 0.988623
  32805/250000: episode: 48, duration: 34.925s, episode steps: 868, steps per second:  25, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.008229, mae: 1.896246, mean_q: 2.338298, mean_eps: 0.988346
  33482/250000: episode: 49, duration: 25.662s, episode steps: 677, steps per second:  26, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010412, mae: 1.903739, mean_q: 2.346825, mean_eps: 0.988068
  33899/250000: episode: 50, duration: 16.865s, episode steps: 417, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.009779, mae: 1.908605, mean_q: 2.352107, mean_eps: 0.987872
  34291/250000: episode: 51, duration: 15.413s, episode steps: 392, steps per second:  25, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.011382, mae: 1.899336, mean_q: 2.342165, mean_eps: 0.987726
  34850/250000: episode: 52, duration: 22.649s, episode steps: 559, steps per second:  25, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.009978, mae: 1.888203, mean_q: 2.325305, mean_eps: 0.987555
  35747/250000: episode: 53, duration: 35.109s, episode steps: 897, steps per second:  26, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010428, mae: 1.894730, mean_q: 2.336826, mean_eps: 0.987293
  36458/250000: episode: 54, duration: 29.208s, episode steps: 711, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.011529, mae: 1.907681, mean_q: 2.351211, mean_eps: 0.987003
  37327/250000: episode: 55, duration: 34.302s, episode steps: 869, steps per second:  25, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010566, mae: 1.901646, mean_q: 2.345058, mean_eps: 0.986719
  37964/250000: episode: 56, duration: 26.161s, episode steps: 637, steps per second:  24, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.008614, mae: 1.885925, mean_q: 2.324889, mean_eps: 0.986448
  38369/250000: episode: 57, duration: 15.910s, episode steps: 405, steps per second:  25, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.010764, mae: 1.887114, mean_q: 2.323907, mean_eps: 0.986260
  38742/250000: episode: 58, duration: 14.648s, episode steps: 373, steps per second:  25, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.009819, mae: 1.898036, mean_q: 2.339689, mean_eps: 0.986120
  40009/250000: episode: 59, duration: 50.786s, episode steps: 1267, steps per second:  25, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.010007, mae: 1.885361, mean_q: 2.322004, mean_eps: 0.985825
  40554/250000: episode: 60, duration: 21.028s, episode steps: 545, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.009500, mae: 1.879582, mean_q: 2.316679, mean_eps: 0.985498
  41157/250000: episode: 61, duration: 24.756s, episode steps: 603, steps per second:  24, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.012239, mae: 1.895991, mean_q: 2.335975, mean_eps: 0.985292
  41961/250000: episode: 62, duration: 31.999s, episode steps: 804, steps per second:  25, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.009393, mae: 1.897089, mean_q: 2.339167, mean_eps: 0.985038
  42352/250000: episode: 63, duration: 15.829s, episode steps: 391, steps per second:  25, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.009557, mae: 1.861898, mean_q: 2.297680, mean_eps: 0.984824
  43223/250000: episode: 64, duration: 34.458s, episode steps: 871, steps per second:  25, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.009460, mae: 1.884672, mean_q: 2.323836, mean_eps: 0.984597
  43696/250000: episode: 65, duration: 18.689s, episode steps: 473, steps per second:  25, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.010529, mae: 1.910572, mean_q: 2.355493, mean_eps: 0.984355
  44406/250000: episode: 66, duration: 29.824s, episode steps: 710, steps per second:  24, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.012276, mae: 1.909661, mean_q: 2.354189, mean_eps: 0.984142
  44977/250000: episode: 67, duration: 24.697s, episode steps: 571, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.010748, mae: 1.893951, mean_q: 2.333779, mean_eps: 0.983911
  45523/250000: episode: 68, duration: 26.927s, episode steps: 546, steps per second:  20, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.009518, mae: 1.879078, mean_q: 2.319316, mean_eps: 0.983710
  46164/250000: episode: 69, duration: 27.115s, episode steps: 641, steps per second:  24, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.009978, mae: 1.876141, mean_q: 2.315198, mean_eps: 0.983497
  47063/250000: episode: 70, duration: 36.445s, episode steps: 899, steps per second:  25, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.011227, mae: 1.889188, mean_q: 2.328417, mean_eps: 0.983220
  48473/250000: episode: 71, duration: 60.650s, episode steps: 1410, steps per second:  23, episode reward: 14.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011274, mae: 1.887954, mean_q: 2.326797, mean_eps: 0.982804
  49000/250000: episode: 72, duration: 23.588s, episode steps: 527, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010424, mae: 1.863722, mean_q: 2.295285, mean_eps: 0.982455
  49630/250000: episode: 73, duration: 25.003s, episode steps: 630, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.008771, mae: 1.868505, mean_q: 2.301390, mean_eps: 0.982247
  50208/250000: episode: 74, duration: 23.344s, episode steps: 578, steps per second:  25, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.011765, mae: 1.918931, mean_q: 2.364578, mean_eps: 0.982030
  50651/250000: episode: 75, duration: 17.582s, episode steps: 443, steps per second:  25, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012632, mae: 1.920943, mean_q: 2.364790, mean_eps: 0.981846
  51949/250000: episode: 76, duration: 52.972s, episode steps: 1298, steps per second:  25, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.011057, mae: 1.922671, mean_q: 2.368251, mean_eps: 0.981532
  52507/250000: episode: 77, duration: 21.530s, episode steps: 558, steps per second:  26, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.012796, mae: 1.922337, mean_q: 2.365266, mean_eps: 0.981198
  53516/250000: episode: 78, duration: 41.048s, episode steps: 1009, steps per second:  25, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.012556, mae: 1.904521, mean_q: 2.342484, mean_eps: 0.980916
  54435/250000: episode: 79, duration: 36.162s, episode steps: 919, steps per second:  25, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.012683, mae: 1.920275, mean_q: 2.363057, mean_eps: 0.980569
  55237/250000: episode: 80, duration: 32.453s, episode steps: 802, steps per second:  25, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011424, mae: 1.923494, mean_q: 2.366457, mean_eps: 0.980259
  56286/250000: episode: 81, duration: 41.338s, episode steps: 1049, steps per second:  25, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.010826, mae: 1.903061, mean_q: 2.341488, mean_eps: 0.979926
  57351/250000: episode: 82, duration: 42.172s, episode steps: 1065, steps per second:  25, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.009822, mae: 1.909864, mean_q: 2.350962, mean_eps: 0.979546
  58008/250000: episode: 83, duration: 26.692s, episode steps: 657, steps per second:  25, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011816, mae: 1.916856, mean_q: 2.359485, mean_eps: 0.979236
  58911/250000: episode: 84, duration: 35.224s, episode steps: 903, steps per second:  26, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.011207, mae: 1.921149, mean_q: 2.362637, mean_eps: 0.978955
  59696/250000: episode: 85, duration: 31.098s, episode steps: 785, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.011290, mae: 1.924513, mean_q: 2.366122, mean_eps: 0.978651
  60370/250000: episode: 86, duration: 27.346s, episode steps: 674, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.011916, mae: 1.915815, mean_q: 2.357252, mean_eps: 0.978388
  60736/250000: episode: 87, duration: 13.780s, episode steps: 366, steps per second:  27, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.010271, mae: 1.917309, mean_q: 2.359466, mean_eps: 0.978201
  61242/250000: episode: 88, duration: 21.119s, episode steps: 506, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.010488, mae: 1.880874, mean_q: 2.311235, mean_eps: 0.978044
  62039/250000: episode: 89, duration: 31.566s, episode steps: 797, steps per second:  25, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010162, mae: 1.900858, mean_q: 2.338800, mean_eps: 0.977810
  62756/250000: episode: 90, duration: 27.548s, episode steps: 717, steps per second:  26, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.008955, mae: 1.906821, mean_q: 2.344846, mean_eps: 0.977537
  63389/250000: episode: 91, duration: 25.737s, episode steps: 633, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.010249, mae: 1.911688, mean_q: 2.351683, mean_eps: 0.977294
  63949/250000: episode: 92, duration: 22.386s, episode steps: 560, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.010987, mae: 1.917543, mean_q: 2.357485, mean_eps: 0.977079
  64283/250000: episode: 93, duration: 12.379s, episode steps: 334, steps per second:  27, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.010727, mae: 1.924573, mean_q: 2.366356, mean_eps: 0.976918
  64633/250000: episode: 94, duration: 14.472s, episode steps: 350, steps per second:  24, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.011760, mae: 1.915556, mean_q: 2.354016, mean_eps: 0.976795
  65714/250000: episode: 95, duration: 43.364s, episode steps: 1081, steps per second:  25, episode reward:  9.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.010665, mae: 1.891578, mean_q: 2.324073, mean_eps: 0.976537
  66419/250000: episode: 96, duration: 27.790s, episode steps: 705, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.011624, mae: 1.897209, mean_q: 2.333198, mean_eps: 0.976216
  67040/250000: episode: 97, duration: 25.205s, episode steps: 621, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.009784, mae: 1.887623, mean_q: 2.320631, mean_eps: 0.975978
  67610/250000: episode: 98, duration: 22.912s, episode steps: 570, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.010223, mae: 1.879910, mean_q: 2.312117, mean_eps: 0.975763
  68027/250000: episode: 99, duration: 16.989s, episode steps: 417, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.008851, mae: 1.873951, mean_q: 2.303797, mean_eps: 0.975586
  68763/250000: episode: 100, duration: 29.043s, episode steps: 736, steps per second:  25, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.011411, mae: 1.882162, mean_q: 2.315074, mean_eps: 0.975378
  69638/250000: episode: 101, duration: 35.449s, episode steps: 875, steps per second:  25, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.011756, mae: 1.888618, mean_q: 2.321033, mean_eps: 0.975088
  70024/250000: episode: 102, duration: 16.068s, episode steps: 386, steps per second:  24, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.012052, mae: 1.849896, mean_q: 2.272956, mean_eps: 0.974861
  70571/250000: episode: 103, duration: 21.320s, episode steps: 547, steps per second:  26, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010866, mae: 1.890932, mean_q: 2.325405, mean_eps: 0.974693
  71292/250000: episode: 104, duration: 29.017s, episode steps: 721, steps per second:  25, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.010231, mae: 1.888984, mean_q: 2.325137, mean_eps: 0.974465
  72058/250000: episode: 105, duration: 31.276s, episode steps: 766, steps per second:  24, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.010679, mae: 1.889085, mean_q: 2.321337, mean_eps: 0.974197
  72673/250000: episode: 106, duration: 23.971s, episode steps: 615, steps per second:  26, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.008402, mae: 1.891357, mean_q: 2.324256, mean_eps: 0.973948
  73278/250000: episode: 107, duration: 26.301s, episode steps: 605, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.010974, mae: 1.882774, mean_q: 2.313901, mean_eps: 0.973729
  73914/250000: episode: 108, duration: 25.622s, episode steps: 636, steps per second:  25, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.010428, mae: 1.893033, mean_q: 2.326811, mean_eps: 0.973505
  74544/250000: episode: 109, duration: 24.941s, episode steps: 630, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.012763, mae: 1.874746, mean_q: 2.301622, mean_eps: 0.973278
  75148/250000: episode: 110, duration: 25.333s, episode steps: 604, steps per second:  24, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.009899, mae: 1.892233, mean_q: 2.325893, mean_eps: 0.973056
  75713/250000: episode: 111, duration: 21.554s, episode steps: 565, steps per second:  26, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011324, mae: 1.883477, mean_q: 2.314965, mean_eps: 0.972845
  76291/250000: episode: 112, duration: 23.685s, episode steps: 578, steps per second:  24, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.011697, mae: 1.910121, mean_q: 2.346812, mean_eps: 0.972639
  76681/250000: episode: 113, duration: 15.787s, episode steps: 390, steps per second:  25, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011045, mae: 1.907101, mean_q: 2.346608, mean_eps: 0.972465
  77705/250000: episode: 114, duration: 40.764s, episode steps: 1024, steps per second:  25, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.010787, mae: 1.900101, mean_q: 2.337355, mean_eps: 0.972210
  78504/250000: episode: 115, duration: 32.030s, episode steps: 799, steps per second:  25, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.011412, mae: 1.910366, mean_q: 2.349068, mean_eps: 0.971883
  79163/250000: episode: 116, duration: 26.664s, episode steps: 659, steps per second:  25, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.008919, mae: 1.889188, mean_q: 2.323597, mean_eps: 0.971620
  79838/250000: episode: 117, duration: 27.406s, episode steps: 675, steps per second:  25, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.010911, mae: 1.881541, mean_q: 2.311847, mean_eps: 0.971380
  80307/250000: episode: 118, duration: 19.253s, episode steps: 469, steps per second:  24, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.009845, mae: 1.885034, mean_q: 2.316557, mean_eps: 0.971174
  80929/250000: episode: 119, duration: 24.627s, episode steps: 622, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.010667, mae: 1.888824, mean_q: 2.320998, mean_eps: 0.970978
  81823/250000: episode: 120, duration: 35.968s, episode steps: 894, steps per second:  25, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.010454, mae: 1.891559, mean_q: 2.327396, mean_eps: 0.970705
  82204/250000: episode: 121, duration: 14.723s, episode steps: 381, steps per second:  26, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.008169, mae: 1.877859, mean_q: 2.306812, mean_eps: 0.970476
  82942/250000: episode: 122, duration: 29.883s, episode steps: 738, steps per second:  25, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.009779, mae: 1.914836, mean_q: 2.351756, mean_eps: 0.970274
  83592/250000: episode: 123, duration: 27.031s, episode steps: 650, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.010341, mae: 1.910993, mean_q: 2.348365, mean_eps: 0.970024
  84245/250000: episode: 124, duration: 26.400s, episode steps: 653, steps per second:  25, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.011067, mae: 1.898831, mean_q: 2.334537, mean_eps: 0.969790
  85098/250000: episode: 125, duration: 34.618s, episode steps: 853, steps per second:  25, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.009409, mae: 1.889484, mean_q: 2.323697, mean_eps: 0.969518
  85818/250000: episode: 126, duration: 28.147s, episode steps: 720, steps per second:  26, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.010426, mae: 1.875678, mean_q: 2.303424, mean_eps: 0.969235
  86505/250000: episode: 127, duration: 27.772s, episode steps: 687, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010886, mae: 1.885112, mean_q: 2.314290, mean_eps: 0.968982
  87890/250000: episode: 128, duration: 55.352s, episode steps: 1385, steps per second:  25, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011173, mae: 1.899196, mean_q: 2.333622, mean_eps: 0.968609
  88578/250000: episode: 129, duration: 26.621s, episode steps: 688, steps per second:  26, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.011026, mae: 1.900584, mean_q: 2.335112, mean_eps: 0.968236
  89159/250000: episode: 130, duration: 23.744s, episode steps: 581, steps per second:  24, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.012166, mae: 1.896437, mean_q: 2.329590, mean_eps: 0.968008
  89545/250000: episode: 131, duration: 15.640s, episode steps: 386, steps per second:  25, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.009269, mae: 1.882863, mean_q: 2.313646, mean_eps: 0.967833
  90182/250000: episode: 132, duration: 24.389s, episode steps: 637, steps per second:  26, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.010208, mae: 1.882529, mean_q: 2.315200, mean_eps: 0.967649
  90750/250000: episode: 133, duration: 23.417s, episode steps: 568, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.011541, mae: 1.883107, mean_q: 2.314107, mean_eps: 0.967432
  91372/250000: episode: 134, duration: 24.819s, episode steps: 622, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.010338, mae: 1.870471, mean_q: 2.299562, mean_eps: 0.967218
  92006/250000: episode: 135, duration: 25.961s, episode steps: 634, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.009994, mae: 1.893544, mean_q: 2.326392, mean_eps: 0.966992
  92623/250000: episode: 136, duration: 25.105s, episode steps: 617, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.010577, mae: 1.883412, mean_q: 2.313065, mean_eps: 0.966767
  93287/250000: episode: 137, duration: 26.209s, episode steps: 664, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.009867, mae: 1.898162, mean_q: 2.333355, mean_eps: 0.966537
  93673/250000: episode: 138, duration: 15.230s, episode steps: 386, steps per second:  25, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.010845, mae: 1.881522, mean_q: 2.313903, mean_eps: 0.966347
  94180/250000: episode: 139, duration: 20.943s, episode steps: 507, steps per second:  24, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.009899, mae: 1.885173, mean_q: 2.316407, mean_eps: 0.966187
  94636/250000: episode: 140, duration: 19.490s, episode steps: 456, steps per second:  23, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.010406, mae: 1.875276, mean_q: 2.305278, mean_eps: 0.966014
  95756/250000: episode: 141, duration: 44.765s, episode steps: 1120, steps per second:  25, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.010691, mae: 1.897923, mean_q: 2.332028, mean_eps: 0.965730
  96727/250000: episode: 142, duration: 39.375s, episode steps: 971, steps per second:  25, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.010552, mae: 1.884056, mean_q: 2.317388, mean_eps: 0.965354
  97133/250000: episode: 143, duration: 16.031s, episode steps: 406, steps per second:  25, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.010988, mae: 1.901273, mean_q: 2.336356, mean_eps: 0.965105
  97810/250000: episode: 144, duration: 28.429s, episode steps: 677, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.010305, mae: 1.873668, mean_q: 2.304057, mean_eps: 0.964910
  98810/250000: episode: 145, duration: 39.306s, episode steps: 1000, steps per second:  25, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.009658, mae: 1.867480, mean_q: 2.293120, mean_eps: 0.964608
  99189/250000: episode: 146, duration: 15.378s, episode steps: 379, steps per second:  25, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.009504, mae: 1.870919, mean_q: 2.299353, mean_eps: 0.964360
 100350/250000: episode: 147, duration: 46.421s, episode steps: 1161, steps per second:  25, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.010450, mae: 1.899302, mean_q: 2.332973, mean_eps: 0.964083
 100974/250000: episode: 148, duration: 26.360s, episode steps: 624, steps per second:  24, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.011897, mae: 1.926413, mean_q: 2.366056, mean_eps: 0.963762
 101502/250000: episode: 149, duration: 20.816s, episode steps: 528, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.010538, mae: 1.897420, mean_q: 2.328134, mean_eps: 0.963554
 102673/250000: episode: 150, duration: 46.998s, episode steps: 1171, steps per second:  25, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011429, mae: 1.932065, mean_q: 2.371827, mean_eps: 0.963248
 103331/250000: episode: 151, duration: 26.232s, episode steps: 658, steps per second:  25, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.010755, mae: 1.940739, mean_q: 2.385139, mean_eps: 0.962919
 103863/250000: episode: 152, duration: 21.731s, episode steps: 532, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.011560, mae: 1.923049, mean_q: 2.360627, mean_eps: 0.962705
 104248/250000: episode: 153, duration: 16.246s, episode steps: 385, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010818, mae: 1.914689, mean_q: 2.347417, mean_eps: 0.962541
 105214/250000: episode: 154, duration: 37.539s, episode steps: 966, steps per second:  26, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.012050, mae: 1.931143, mean_q: 2.368607, mean_eps: 0.962297
 106556/250000: episode: 155, duration: 55.454s, episode steps: 1342, steps per second:  24, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.011316, mae: 1.916494, mean_q: 2.351677, mean_eps: 0.961882
 106958/250000: episode: 156, duration: 16.248s, episode steps: 402, steps per second:  25, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.009393, mae: 1.884489, mean_q: 2.314358, mean_eps: 0.961568
 107444/250000: episode: 157, duration: 20.094s, episode steps: 486, steps per second:  24, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012076, mae: 1.900644, mean_q: 2.329921, mean_eps: 0.961408
 108201/250000: episode: 158, duration: 30.391s, episode steps: 757, steps per second:  25, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.011028, mae: 1.946264, mean_q: 2.389529, mean_eps: 0.961184
 108899/250000: episode: 159, duration: 28.568s, episode steps: 698, steps per second:  24, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.010264, mae: 1.926927, mean_q: 2.363844, mean_eps: 0.960922
 109555/250000: episode: 160, duration: 26.050s, episode steps: 656, steps per second:  25, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.011605, mae: 1.925659, mean_q: 2.363757, mean_eps: 0.960679
 110791/250000: episode: 161, duration: 55.585s, episode steps: 1236, steps per second:  22, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.010750, mae: 1.938602, mean_q: 2.380011, mean_eps: 0.960338
 111436/250000: episode: 162, duration: 26.029s, episode steps: 645, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.010288, mae: 1.929230, mean_q: 2.368906, mean_eps: 0.960000
 111925/250000: episode: 163, duration: 20.041s, episode steps: 489, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.011829, mae: 1.937977, mean_q: 2.377320, mean_eps: 0.959795
 112721/250000: episode: 164, duration: 33.087s, episode steps: 796, steps per second:  24, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013251, mae: 1.938091, mean_q: 2.378098, mean_eps: 0.959563
 113387/250000: episode: 165, duration: 26.437s, episode steps: 666, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.009631, mae: 1.930700, mean_q: 2.367380, mean_eps: 0.959301
 114286/250000: episode: 166, duration: 37.203s, episode steps: 899, steps per second:  24, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.011832, mae: 1.933269, mean_q: 2.372190, mean_eps: 0.959019
 115323/250000: episode: 167, duration: 40.714s, episode steps: 1037, steps per second:  25, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.010888, mae: 1.933455, mean_q: 2.371732, mean_eps: 0.958671
 116239/250000: episode: 168, duration: 42.856s, episode steps: 916, steps per second:  21, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.011070, mae: 1.930492, mean_q: 2.368381, mean_eps: 0.958319
 116717/250000: episode: 169, duration: 26.452s, episode steps: 478, steps per second:  18, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.009399, mae: 1.931078, mean_q: 2.369754, mean_eps: 0.958068
 117604/250000: episode: 170, duration: 53.889s, episode steps: 887, steps per second:  16, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011349, mae: 1.925512, mean_q: 2.361392, mean_eps: 0.957822
 118046/250000: episode: 171, duration: 25.204s, episode steps: 442, steps per second:  18, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.012208, mae: 1.927121, mean_q: 2.362746, mean_eps: 0.957583
 118627/250000: episode: 172, duration: 23.470s, episode steps: 581, steps per second:  25, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.011379, mae: 1.920616, mean_q: 2.355586, mean_eps: 0.957399
 119291/250000: episode: 173, duration: 27.636s, episode steps: 664, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.011390, mae: 1.932271, mean_q: 2.368970, mean_eps: 0.957175
 119993/250000: episode: 174, duration: 28.983s, episode steps: 702, steps per second:  24, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011118, mae: 1.915276, mean_q: 2.348171, mean_eps: 0.956929
 120609/250000: episode: 175, duration: 24.096s, episode steps: 616, steps per second:  26, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.011293, mae: 1.917897, mean_q: 2.351661, mean_eps: 0.956691
 121367/250000: episode: 176, duration: 31.812s, episode steps: 758, steps per second:  24, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.009300, mae: 1.914742, mean_q: 2.347821, mean_eps: 0.956444
 122088/250000: episode: 177, duration: 29.102s, episode steps: 721, steps per second:  25, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.011674, mae: 1.912426, mean_q: 2.345727, mean_eps: 0.956179
 122467/250000: episode: 178, duration: 15.859s, episode steps: 379, steps per second:  24, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012290, mae: 1.928048, mean_q: 2.365251, mean_eps: 0.955981
 123585/250000: episode: 179, duration: 45.297s, episode steps: 1118, steps per second:  25, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.011498, mae: 1.933587, mean_q: 2.371853, mean_eps: 0.955711
 124138/250000: episode: 180, duration: 22.077s, episode steps: 553, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.013284, mae: 1.930903, mean_q: 2.371369, mean_eps: 0.955410
 124623/250000: episode: 181, duration: 20.185s, episode steps: 485, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.011984, mae: 1.916998, mean_q: 2.353875, mean_eps: 0.955223
 125157/250000: episode: 182, duration: 21.847s, episode steps: 534, steps per second:  24, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.011636, mae: 1.910779, mean_q: 2.344937, mean_eps: 0.955040
 125557/250000: episode: 183, duration: 16.384s, episode steps: 400, steps per second:  24, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.010681, mae: 1.914906, mean_q: 2.350937, mean_eps: 0.954871
 126192/250000: episode: 184, duration: 25.499s, episode steps: 635, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.011455, mae: 1.917883, mean_q: 2.350472, mean_eps: 0.954685
 126813/250000: episode: 185, duration: 25.321s, episode steps: 621, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.011702, mae: 1.916865, mean_q: 2.351022, mean_eps: 0.954459
 127473/250000: episode: 186, duration: 26.805s, episode steps: 660, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.010695, mae: 1.923449, mean_q: 2.361156, mean_eps: 0.954228
 127864/250000: episode: 187, duration: 17.222s, episode steps: 391, steps per second:  23, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010182, mae: 1.947603, mean_q: 2.389966, mean_eps: 0.954040
 129090/250000: episode: 188, duration: 48.856s, episode steps: 1226, steps per second:  25, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.011054, mae: 1.906177, mean_q: 2.338533, mean_eps: 0.953749
 130205/250000: episode: 189, duration: 45.816s, episode steps: 1115, steps per second:  24, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.012784, mae: 1.912464, mean_q: 2.345573, mean_eps: 0.953327
 130792/250000: episode: 190, duration: 24.170s, episode steps: 587, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.011169, mae: 1.917493, mean_q: 2.352836, mean_eps: 0.953021
 131742/250000: episode: 191, duration: 38.332s, episode steps: 950, steps per second:  25, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.009665, mae: 1.915498, mean_q: 2.348584, mean_eps: 0.952744
 132400/250000: episode: 192, duration: 26.941s, episode steps: 658, steps per second:  24, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.011594, mae: 1.900242, mean_q: 2.331614, mean_eps: 0.952455
 132982/250000: episode: 193, duration: 24.666s, episode steps: 582, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012195, mae: 1.919903, mean_q: 2.354805, mean_eps: 0.952232
 133747/250000: episode: 194, duration: 30.967s, episode steps: 765, steps per second:  25, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.008906, mae: 1.913826, mean_q: 2.348065, mean_eps: 0.951989
 134777/250000: episode: 195, duration: 43.056s, episode steps: 1030, steps per second:  24, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012085, mae: 1.913796, mean_q: 2.347876, mean_eps: 0.951666
 135165/250000: episode: 196, duration: 19.265s, episode steps: 388, steps per second:  20, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012005, mae: 1.908590, mean_q: 2.342088, mean_eps: 0.951410
 135964/250000: episode: 197, duration: 32.796s, episode steps: 799, steps per second:  24, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.010024, mae: 1.902307, mean_q: 2.333553, mean_eps: 0.951197
 136425/250000: episode: 198, duration: 18.373s, episode steps: 461, steps per second:  25, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.010245, mae: 1.912660, mean_q: 2.347154, mean_eps: 0.950970
 136894/250000: episode: 199, duration: 18.996s, episode steps: 469, steps per second:  25, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.009246, mae: 1.896651, mean_q: 2.326212, mean_eps: 0.950802
 137293/250000: episode: 200, duration: 17.086s, episode steps: 399, steps per second:  23, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.010458, mae: 1.907577, mean_q: 2.341583, mean_eps: 0.950646
 138045/250000: episode: 201, duration: 30.302s, episode steps: 752, steps per second:  25, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.011179, mae: 1.917222, mean_q: 2.350595, mean_eps: 0.950439
 139093/250000: episode: 202, duration: 43.284s, episode steps: 1048, steps per second:  24, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.011438, mae: 1.921142, mean_q: 2.357473, mean_eps: 0.950115
 140036/250000: episode: 203, duration: 37.612s, episode steps: 943, steps per second:  25, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.011676, mae: 1.903581, mean_q: 2.334807, mean_eps: 0.949757
 140552/250000: episode: 204, duration: 21.453s, episode steps: 516, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.010612, mae: 1.910553, mean_q: 2.340375, mean_eps: 0.949495
 141815/250000: episode: 205, duration: 52.873s, episode steps: 1263, steps per second:  24, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.011147, mae: 1.916317, mean_q: 2.350265, mean_eps: 0.949174
 142422/250000: episode: 206, duration: 26.042s, episode steps: 607, steps per second:  23, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.011622, mae: 1.907645, mean_q: 2.336509, mean_eps: 0.948838
 143384/250000: episode: 207, duration: 40.277s, episode steps: 962, steps per second:  24, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.011343, mae: 1.921882, mean_q: 2.355263, mean_eps: 0.948555
 143821/250000: episode: 208, duration: 19.414s, episode steps: 437, steps per second:  23, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.010069, mae: 1.919882, mean_q: 2.353088, mean_eps: 0.948303
 144695/250000: episode: 209, duration: 35.168s, episode steps: 874, steps per second:  25, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.011208, mae: 1.912405, mean_q: 2.343992, mean_eps: 0.948067
 145339/250000: episode: 210, duration: 26.829s, episode steps: 644, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.010842, mae: 1.907473, mean_q: 2.341012, mean_eps: 0.947794
 145725/250000: episode: 211, duration: 15.160s, episode steps: 386, steps per second:  25, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010524, mae: 1.907397, mean_q: 2.337637, mean_eps: 0.947608
 146531/250000: episode: 212, duration: 33.537s, episode steps: 806, steps per second:  24, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.010428, mae: 1.896384, mean_q: 2.324979, mean_eps: 0.947394
 147150/250000: episode: 213, duration: 26.128s, episode steps: 619, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.012159, mae: 1.914849, mean_q: 2.348842, mean_eps: 0.947138
 147557/250000: episode: 214, duration: 16.004s, episode steps: 407, steps per second:  25, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.010607, mae: 1.942259, mean_q: 2.383830, mean_eps: 0.946953
 148159/250000: episode: 215, duration: 24.201s, episode steps: 602, steps per second:  25, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.010825, mae: 1.926534, mean_q: 2.362621, mean_eps: 0.946771
 148521/250000: episode: 216, duration: 15.692s, episode steps: 362, steps per second:  23, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.011576, mae: 1.935976, mean_q: 2.372277, mean_eps: 0.946598
 149015/250000: episode: 217, duration: 20.962s, episode steps: 494, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.013122, mae: 1.884352, mean_q: 2.311635, mean_eps: 0.946444
 150003/250000: episode: 218, duration: 46.580s, episode steps: 988, steps per second:  21, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.010313, mae: 1.926141, mean_q: 2.363267, mean_eps: 0.946177
 150581/250000: episode: 219, duration: 24.713s, episode steps: 578, steps per second:  23, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.011024, mae: 1.971456, mean_q: 2.415368, mean_eps: 0.945895
 151124/250000: episode: 220, duration: 23.375s, episode steps: 543, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.010836, mae: 1.945870, mean_q: 2.383633, mean_eps: 0.945693
 152245/250000: episode: 221, duration: 46.750s, episode steps: 1121, steps per second:  24, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.011749, mae: 1.949547, mean_q: 2.389208, mean_eps: 0.945394
 153197/250000: episode: 222, duration: 42.440s, episode steps: 952, steps per second:  22, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.010902, mae: 1.948592, mean_q: 2.386153, mean_eps: 0.945020
 153844/250000: episode: 223, duration: 26.786s, episode steps: 647, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.011286, mae: 1.942752, mean_q: 2.380405, mean_eps: 0.944733
 154239/250000: episode: 224, duration: 15.776s, episode steps: 395, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.010554, mae: 1.926037, mean_q: 2.358412, mean_eps: 0.944546
 154829/250000: episode: 225, duration: 25.952s, episode steps: 590, steps per second:  23, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.010798, mae: 1.960235, mean_q: 2.400741, mean_eps: 0.944368
 155221/250000: episode: 226, duration: 16.914s, episode steps: 392, steps per second:  23, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.014757, mae: 1.935129, mean_q: 2.368682, mean_eps: 0.944191
 155815/250000: episode: 227, duration: 24.563s, episode steps: 594, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012712, mae: 1.943442, mean_q: 2.378971, mean_eps: 0.944014
 156601/250000: episode: 228, duration: 33.031s, episode steps: 786, steps per second:  24, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011152, mae: 1.950716, mean_q: 2.390935, mean_eps: 0.943765
 157065/250000: episode: 229, duration: 43.791s, episode steps: 464, steps per second:  11, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.011317, mae: 1.955489, mean_q: 2.395330, mean_eps: 0.943540
 157715/250000: episode: 230, duration: 97.474s, episode steps: 650, steps per second:   7, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011671, mae: 1.953991, mean_q: 2.392606, mean_eps: 0.943340
 158340/250000: episode: 231, duration: 38.624s, episode steps: 625, steps per second:  16, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.011576, mae: 1.923165, mean_q: 2.355235, mean_eps: 0.943111
 159151/250000: episode: 232, duration: 35.900s, episode steps: 811, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.011025, mae: 1.944940, mean_q: 2.383482, mean_eps: 0.942852
 159550/250000: episode: 233, duration: 16.458s, episode steps: 399, steps per second:  24, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.012701, mae: 1.946467, mean_q: 2.385083, mean_eps: 0.942634
 160219/250000: episode: 234, duration: 29.339s, episode steps: 669, steps per second:  23, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.012758, mae: 1.964251, mean_q: 2.407855, mean_eps: 0.942442
 160618/250000: episode: 235, duration: 16.671s, episode steps: 399, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012874, mae: 1.943096, mean_q: 2.380737, mean_eps: 0.942250
 161154/250000: episode: 236, duration: 21.978s, episode steps: 536, steps per second:  24, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.012046, mae: 1.952866, mean_q: 2.393001, mean_eps: 0.942081
 161803/250000: episode: 237, duration: 26.588s, episode steps: 649, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.012992, mae: 1.945474, mean_q: 2.384446, mean_eps: 0.941868
 162436/250000: episode: 238, duration: 27.068s, episode steps: 633, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009413, mae: 1.955133, mean_q: 2.396273, mean_eps: 0.941638
 163119/250000: episode: 239, duration: 29.183s, episode steps: 683, steps per second:  23, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012141, mae: 1.950317, mean_q: 2.389343, mean_eps: 0.941401
 163525/250000: episode: 240, duration: 16.197s, episode steps: 406, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.011922, mae: 1.926665, mean_q: 2.360769, mean_eps: 0.941204
 164134/250000: episode: 241, duration: 24.510s, episode steps: 609, steps per second:  25, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011676, mae: 1.916919, mean_q: 2.349881, mean_eps: 0.941021
 164522/250000: episode: 242, duration: 17.195s, episode steps: 388, steps per second:  23, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.010336, mae: 1.928242, mean_q: 2.364111, mean_eps: 0.940842
 165147/250000: episode: 243, duration: 26.980s, episode steps: 625, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.011428, mae: 1.949058, mean_q: 2.388288, mean_eps: 0.940660
 165689/250000: episode: 244, duration: 23.274s, episode steps: 542, steps per second:  23, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.010084, mae: 1.950420, mean_q: 2.391922, mean_eps: 0.940450
 166331/250000: episode: 245, duration: 28.867s, episode steps: 642, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.011570, mae: 1.954197, mean_q: 2.395291, mean_eps: 0.940236
 167186/250000: episode: 246, duration: 34.754s, episode steps: 855, steps per second:  25, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012326, mae: 1.944252, mean_q: 2.380689, mean_eps: 0.939967
 167796/250000: episode: 247, duration: 26.162s, episode steps: 610, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.010392, mae: 1.926926, mean_q: 2.362440, mean_eps: 0.939704
 168659/250000: episode: 248, duration: 34.769s, episode steps: 863, steps per second:  25, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012660, mae: 1.955893, mean_q: 2.395312, mean_eps: 0.939439
 169373/250000: episode: 249, duration: 30.440s, episode steps: 714, steps per second:  23, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.010446, mae: 1.937196, mean_q: 2.373100, mean_eps: 0.939154
 169932/250000: episode: 250, duration: 23.039s, episode steps: 559, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.011114, mae: 1.931137, mean_q: 2.365399, mean_eps: 0.938925
 171067/250000: episode: 251, duration: 49.858s, episode steps: 1135, steps per second:  23, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.010735, mae: 1.938544, mean_q: 2.373275, mean_eps: 0.938621
 172062/250000: episode: 252, duration: 40.081s, episode steps: 995, steps per second:  25, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.011887, mae: 1.946810, mean_q: 2.385164, mean_eps: 0.938237
 172849/250000: episode: 253, duration: 32.342s, episode steps: 787, steps per second:  24, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.011778, mae: 1.932223, mean_q: 2.366648, mean_eps: 0.937916
 173337/250000: episode: 254, duration: 19.745s, episode steps: 488, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.010434, mae: 1.943934, mean_q: 2.380085, mean_eps: 0.937686
 173981/250000: episode: 255, duration: 26.484s, episode steps: 644, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.010700, mae: 1.924795, mean_q: 2.356907, mean_eps: 0.937482
 174494/250000: episode: 256, duration: 20.148s, episode steps: 513, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.011646, mae: 1.931213, mean_q: 2.364833, mean_eps: 0.937274
 175265/250000: episode: 257, duration: 31.303s, episode steps: 771, steps per second:  25, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.012012, mae: 1.934488, mean_q: 2.369126, mean_eps: 0.937043
 175667/250000: episode: 258, duration: 17.194s, episode steps: 402, steps per second:  23, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.012454, mae: 1.933913, mean_q: 2.369116, mean_eps: 0.936832
 176330/250000: episode: 259, duration: 26.482s, episode steps: 663, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.010810, mae: 1.919935, mean_q: 2.354670, mean_eps: 0.936641
 177012/250000: episode: 260, duration: 28.798s, episode steps: 682, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.010782, mae: 1.913403, mean_q: 2.343877, mean_eps: 0.936399
 177603/250000: episode: 261, duration: 24.296s, episode steps: 591, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.011902, mae: 1.946173, mean_q: 2.381817, mean_eps: 0.936170
 178135/250000: episode: 262, duration: 23.500s, episode steps: 532, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.010594, mae: 1.940656, mean_q: 2.377145, mean_eps: 0.935968
 178778/250000: episode: 263, duration: 29.176s, episode steps: 643, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.012584, mae: 1.928549, mean_q: 2.359851, mean_eps: 0.935756
 179176/250000: episode: 264, duration: 16.530s, episode steps: 398, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010029, mae: 1.944551, mean_q: 2.380101, mean_eps: 0.935569
 179623/250000: episode: 265, duration: 18.413s, episode steps: 447, steps per second:  24, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.009954, mae: 1.932194, mean_q: 2.365722, mean_eps: 0.935417
 180251/250000: episode: 266, duration: 26.255s, episode steps: 628, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.012028, mae: 1.923314, mean_q: 2.356849, mean_eps: 0.935223
 180758/250000: episode: 267, duration: 21.242s, episode steps: 507, steps per second:  24, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.010076, mae: 1.938765, mean_q: 2.374075, mean_eps: 0.935019
 181428/250000: episode: 268, duration: 26.872s, episode steps: 670, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.011392, mae: 1.945449, mean_q: 2.383832, mean_eps: 0.934807
 182128/250000: episode: 269, duration: 29.117s, episode steps: 700, steps per second:  24, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012170, mae: 1.933164, mean_q: 2.365854, mean_eps: 0.934561
 182785/250000: episode: 270, duration: 27.436s, episode steps: 657, steps per second:  24, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.011558, mae: 1.934171, mean_q: 2.370364, mean_eps: 0.934316
 183777/250000: episode: 271, duration: 41.560s, episode steps: 992, steps per second:  24, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.011542, mae: 1.929411, mean_q: 2.363323, mean_eps: 0.934018
 184411/250000: episode: 272, duration: 25.608s, episode steps: 634, steps per second:  25, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.010000, mae: 1.940711, mean_q: 2.377064, mean_eps: 0.933726
 184937/250000: episode: 273, duration: 23.158s, episode steps: 526, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.012207, mae: 1.937900, mean_q: 2.372469, mean_eps: 0.933517
 185388/250000: episode: 274, duration: 18.096s, episode steps: 451, steps per second:  25, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.011348, mae: 1.935412, mean_q: 2.365546, mean_eps: 0.933342
 185774/250000: episode: 275, duration: 15.944s, episode steps: 386, steps per second:  24, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.010596, mae: 1.933149, mean_q: 2.369588, mean_eps: 0.933191
 186614/250000: episode: 276, duration: 34.788s, episode steps: 840, steps per second:  24, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012344, mae: 1.949574, mean_q: 2.383398, mean_eps: 0.932970
 187327/250000: episode: 277, duration: 31.211s, episode steps: 713, steps per second:  23, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.012721, mae: 1.931715, mean_q: 2.364339, mean_eps: 0.932691
 187832/250000: episode: 278, duration: 22.240s, episode steps: 505, steps per second:  23, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.012002, mae: 1.929167, mean_q: 2.361992, mean_eps: 0.932472
 188350/250000: episode: 279, duration: 21.519s, episode steps: 518, steps per second:  24, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.010597, mae: 1.945545, mean_q: 2.383618, mean_eps: 0.932288
 189159/250000: episode: 280, duration: 32.894s, episode steps: 809, steps per second:  25, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.010412, mae: 1.943900, mean_q: 2.381199, mean_eps: 0.932049
 189780/250000: episode: 281, duration: 25.357s, episode steps: 621, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.011649, mae: 1.945276, mean_q: 2.383055, mean_eps: 0.931792
 190298/250000: episode: 282, duration: 21.651s, episode steps: 518, steps per second:  24, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.010983, mae: 1.937408, mean_q: 2.372981, mean_eps: 0.931586
 190682/250000: episode: 283, duration: 16.278s, episode steps: 384, steps per second:  24, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012458, mae: 1.941604, mean_q: 2.380583, mean_eps: 0.931424
 191082/250000: episode: 284, duration: 16.729s, episode steps: 400, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.010179, mae: 1.926434, mean_q: 2.360975, mean_eps: 0.931282
 191678/250000: episode: 285, duration: 23.754s, episode steps: 596, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.011516, mae: 1.953577, mean_q: 2.390554, mean_eps: 0.931103
 192210/250000: episode: 286, duration: 22.064s, episode steps: 532, steps per second:  24, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011134, mae: 1.938482, mean_q: 2.374101, mean_eps: 0.930900
 192770/250000: episode: 287, duration: 23.275s, episode steps: 560, steps per second:  24, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.012566, mae: 1.931717, mean_q: 2.364615, mean_eps: 0.930704
 193398/250000: episode: 288, duration: 27.739s, episode steps: 628, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012771, mae: 1.930493, mean_q: 2.363346, mean_eps: 0.930490
 194210/250000: episode: 289, duration: 35.292s, episode steps: 812, steps per second:  23, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011533, mae: 1.920657, mean_q: 2.351068, mean_eps: 0.930231
 194823/250000: episode: 290, duration: 25.947s, episode steps: 613, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.013664, mae: 1.920977, mean_q: 2.351582, mean_eps: 0.929974
 195391/250000: episode: 291, duration: 23.026s, episode steps: 568, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.011864, mae: 1.950499, mean_q: 2.388156, mean_eps: 0.929762
 195773/250000: episode: 292, duration: 16.314s, episode steps: 382, steps per second:  23, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011722, mae: 1.960809, mean_q: 2.401526, mean_eps: 0.929590
 196470/250000: episode: 293, duration: 28.796s, episode steps: 697, steps per second:  24, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.011884, mae: 1.931392, mean_q: 2.365130, mean_eps: 0.929396
 197326/250000: episode: 294, duration: 35.287s, episode steps: 856, steps per second:  24, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.010743, mae: 1.953129, mean_q: 2.392817, mean_eps: 0.929117
 198147/250000: episode: 295, duration: 33.726s, episode steps: 821, steps per second:  24, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.012444, mae: 1.928652, mean_q: 2.362329, mean_eps: 0.928815
 198821/250000: episode: 296, duration: 28.617s, episode steps: 674, steps per second:  24, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011074, mae: 1.932656, mean_q: 2.365749, mean_eps: 0.928546
 199567/250000: episode: 297, duration: 30.771s, episode steps: 746, steps per second:  24, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012221, mae: 1.949541, mean_q: 2.387765, mean_eps: 0.928290
 200202/250000: episode: 298, duration: 27.138s, episode steps: 635, steps per second:  23, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012409, mae: 1.946014, mean_q: 2.383258, mean_eps: 0.928042
 200746/250000: episode: 299, duration: 22.781s, episode steps: 544, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011335, mae: 1.969551, mean_q: 2.410888, mean_eps: 0.927829
 201135/250000: episode: 300, duration: 16.015s, episode steps: 389, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.012066, mae: 1.981042, mean_q: 2.424599, mean_eps: 0.927662
 201534/250000: episode: 301, duration: 17.464s, episode steps: 399, steps per second:  23, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.010931, mae: 1.972416, mean_q: 2.413368, mean_eps: 0.927520
 202143/250000: episode: 302, duration: 26.147s, episode steps: 609, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.010569, mae: 1.974266, mean_q: 2.417384, mean_eps: 0.927338
 202651/250000: episode: 303, duration: 20.255s, episode steps: 508, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.010771, mae: 1.978429, mean_q: 2.422860, mean_eps: 0.927137
 203711/250000: episode: 304, duration: 44.982s, episode steps: 1060, steps per second:  24, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.010761, mae: 1.974890, mean_q: 2.416487, mean_eps: 0.926855
 204177/250000: episode: 305, duration: 18.580s, episode steps: 466, steps per second:  25, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.012082, mae: 1.992458, mean_q: 2.437798, mean_eps: 0.926580
 205078/250000: episode: 306, duration: 36.774s, episode steps: 901, steps per second:  25, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.012258, mae: 1.976741, mean_q: 2.417099, mean_eps: 0.926334
 205889/250000: episode: 307, duration: 34.105s, episode steps: 811, steps per second:  24, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.011640, mae: 1.965421, mean_q: 2.405297, mean_eps: 0.926026
 206926/250000: episode: 308, duration: 44.186s, episode steps: 1037, steps per second:  23, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.012218, mae: 1.984630, mean_q: 2.429532, mean_eps: 0.925693
 207611/250000: episode: 309, duration: 28.374s, episode steps: 685, steps per second:  24, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.011996, mae: 1.994111, mean_q: 2.439582, mean_eps: 0.925384
 208493/250000: episode: 310, duration: 36.852s, episode steps: 882, steps per second:  24, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012672, mae: 1.991590, mean_q: 2.436241, mean_eps: 0.925101
 208918/250000: episode: 311, duration: 16.830s, episode steps: 425, steps per second:  25, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010487, mae: 1.983061, mean_q: 2.426851, mean_eps: 0.924866
 209981/250000: episode: 312, duration: 44.975s, episode steps: 1063, steps per second:  24, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.013008, mae: 1.962413, mean_q: 2.401528, mean_eps: 0.924598
 210992/250000: episode: 313, duration: 42.855s, episode steps: 1011, steps per second:  24, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.011896, mae: 1.984727, mean_q: 2.427581, mean_eps: 0.924225
 211448/250000: episode: 314, duration: 18.711s, episode steps: 456, steps per second:  24, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.012345, mae: 1.972649, mean_q: 2.414738, mean_eps: 0.923962
 211989/250000: episode: 315, duration: 22.139s, episode steps: 541, steps per second:  24, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013105, mae: 1.952094, mean_q: 2.388233, mean_eps: 0.923782
 212518/250000: episode: 316, duration: 22.861s, episode steps: 529, steps per second:  23, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.012510, mae: 1.970764, mean_q: 2.413964, mean_eps: 0.923589
 213197/250000: episode: 317, duration: 28.366s, episode steps: 679, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.012078, mae: 1.989278, mean_q: 2.435748, mean_eps: 0.923371
 213852/250000: episode: 318, duration: 25.939s, episode steps: 655, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.013154, mae: 1.977559, mean_q: 2.419031, mean_eps: 0.923131
 214631/250000: episode: 319, duration: 32.870s, episode steps: 779, steps per second:  24, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.012014, mae: 1.983448, mean_q: 2.426832, mean_eps: 0.922874
 215376/250000: episode: 320, duration: 34.244s, episode steps: 745, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.013588, mae: 2.001764, mean_q: 2.449063, mean_eps: 0.922599
 215865/250000: episode: 321, duration: 21.561s, episode steps: 489, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.011982, mae: 1.987898, mean_q: 2.432820, mean_eps: 0.922377
 216628/250000: episode: 322, duration: 31.639s, episode steps: 763, steps per second:  24, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.012869, mae: 1.991154, mean_q: 2.437955, mean_eps: 0.922151
 217267/250000: episode: 323, duration: 26.047s, episode steps: 639, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013546, mae: 1.970666, mean_q: 2.410853, mean_eps: 0.921899
 218281/250000: episode: 324, duration: 41.918s, episode steps: 1014, steps per second:  24, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.011753, mae: 1.963358, mean_q: 2.401713, mean_eps: 0.921601
 218910/250000: episode: 325, duration: 26.719s, episode steps: 629, steps per second:  24, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.011551, mae: 1.977202, mean_q: 2.418334, mean_eps: 0.921305
 219444/250000: episode: 326, duration: 22.422s, episode steps: 534, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012197, mae: 1.975341, mean_q: 2.416003, mean_eps: 0.921097
 219850/250000: episode: 327, duration: 16.146s, episode steps: 406, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.013392, mae: 1.962376, mean_q: 2.399836, mean_eps: 0.920927
 220243/250000: episode: 328, duration: 16.445s, episode steps: 393, steps per second:  24, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.013919, mae: 1.994060, mean_q: 2.436739, mean_eps: 0.920783
 220682/250000: episode: 329, duration: 18.124s, episode steps: 439, steps per second:  24, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012786, mae: 1.976877, mean_q: 2.414572, mean_eps: 0.920634
 221076/250000: episode: 330, duration: 17.139s, episode steps: 394, steps per second:  23, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.014408, mae: 1.954094, mean_q: 2.389623, mean_eps: 0.920484
 221518/250000: episode: 331, duration: 20.667s, episode steps: 442, steps per second:  21, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.012862, mae: 1.993271, mean_q: 2.436952, mean_eps: 0.920333
 222324/250000: episode: 332, duration: 36.172s, episode steps: 806, steps per second:  22, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.011962, mae: 1.963668, mean_q: 2.400334, mean_eps: 0.920109
 222709/250000: episode: 333, duration: 17.220s, episode steps: 385, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.012614, mae: 1.983655, mean_q: 2.425929, mean_eps: 0.919894
 223765/250000: episode: 334, duration: 43.860s, episode steps: 1056, steps per second:  24, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.013008, mae: 1.984397, mean_q: 2.427422, mean_eps: 0.919634
 224151/250000: episode: 335, duration: 17.304s, episode steps: 386, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.011919, mae: 1.972428, mean_q: 2.411860, mean_eps: 0.919375
 224800/250000: episode: 336, duration: 26.667s, episode steps: 649, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011954, mae: 1.988604, mean_q: 2.433721, mean_eps: 0.919189
 225606/250000: episode: 337, duration: 34.105s, episode steps: 806, steps per second:  24, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.012093, mae: 1.966862, mean_q: 2.405870, mean_eps: 0.918927
 226101/250000: episode: 338, duration: 20.102s, episode steps: 495, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.010614, mae: 1.955500, mean_q: 2.392510, mean_eps: 0.918693
 226869/250000: episode: 339, duration: 31.894s, episode steps: 768, steps per second:  24, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011575, mae: 1.977927, mean_q: 2.417832, mean_eps: 0.918465
 227663/250000: episode: 340, duration: 33.212s, episode steps: 794, steps per second:  24, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011623, mae: 1.979333, mean_q: 2.420824, mean_eps: 0.918184
 228408/250000: episode: 341, duration: 30.833s, episode steps: 745, steps per second:  24, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012448, mae: 1.991752, mean_q: 2.437509, mean_eps: 0.917908
 228880/250000: episode: 342, duration: 20.095s, episode steps: 472, steps per second:  23, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.012408, mae: 1.961345, mean_q: 2.399152, mean_eps: 0.917689
 229376/250000: episode: 343, duration: 20.213s, episode steps: 496, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.013131, mae: 1.960971, mean_q: 2.397226, mean_eps: 0.917515
 229915/250000: episode: 344, duration: 24.220s, episode steps: 539, steps per second:  22, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.011521, mae: 1.962503, mean_q: 2.400508, mean_eps: 0.917328
 230403/250000: episode: 345, duration: 22.861s, episode steps: 488, steps per second:  21, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.010547, mae: 1.975450, mean_q: 2.416095, mean_eps: 0.917143
 231002/250000: episode: 346, duration: 26.704s, episode steps: 599, steps per second:  22, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.010730, mae: 1.959398, mean_q: 2.396176, mean_eps: 0.916947
 232079/250000: episode: 347, duration: 45.766s, episode steps: 1077, steps per second:  24, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.012665, mae: 1.978827, mean_q: 2.419505, mean_eps: 0.916646
 232695/250000: episode: 348, duration: 25.066s, episode steps: 616, steps per second:  25, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.010170, mae: 1.977415, mean_q: 2.418435, mean_eps: 0.916341
 233443/250000: episode: 349, duration: 31.709s, episode steps: 748, steps per second:  24, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.012255, mae: 1.990582, mean_q: 2.434944, mean_eps: 0.916096
 234457/250000: episode: 350, duration: 43.026s, episode steps: 1014, steps per second:  24, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.011574, mae: 1.976013, mean_q: 2.415815, mean_eps: 0.915778
 234880/250000: episode: 351, duration: 18.051s, episode steps: 423, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.012858, mae: 1.985094, mean_q: 2.426329, mean_eps: 0.915520
 235417/250000: episode: 352, duration: 21.913s, episode steps: 537, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.013484, mae: 2.015370, mean_q: 2.463406, mean_eps: 0.915347
 236486/250000: episode: 353, duration: 45.761s, episode steps: 1069, steps per second:  23, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.011693, mae: 1.989125, mean_q: 2.432240, mean_eps: 0.915057
 236885/250000: episode: 354, duration: 16.548s, episode steps: 399, steps per second:  24, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.011233, mae: 1.994132, mean_q: 2.438466, mean_eps: 0.914793
 237276/250000: episode: 355, duration: 16.754s, episode steps: 391, steps per second:  23, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.012289, mae: 1.986576, mean_q: 2.428631, mean_eps: 0.914651
 238215/250000: episode: 356, duration: 40.158s, episode steps: 939, steps per second:  23, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013242, mae: 1.973555, mean_q: 2.413005, mean_eps: 0.914412
 238887/250000: episode: 357, duration: 28.528s, episode steps: 672, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.013066, mae: 1.981870, mean_q: 2.422592, mean_eps: 0.914122
 239670/250000: episode: 358, duration: 33.755s, episode steps: 783, steps per second:  23, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.013028, mae: 1.984268, mean_q: 2.425492, mean_eps: 0.913860
 240363/250000: episode: 359, duration: 29.496s, episode steps: 693, steps per second:  23, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.011313, mae: 1.965832, mean_q: 2.403273, mean_eps: 0.913594
 240905/250000: episode: 360, duration: 22.290s, episode steps: 542, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.014313, mae: 1.971856, mean_q: 2.408957, mean_eps: 0.913372
 241573/250000: episode: 361, duration: 28.337s, episode steps: 668, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.012209, mae: 1.973007, mean_q: 2.411428, mean_eps: 0.913154
 242644/250000: episode: 362, duration: 46.481s, episode steps: 1071, steps per second:  23, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.012113, mae: 1.997523, mean_q: 2.442389, mean_eps: 0.912841
 243202/250000: episode: 363, duration: 22.399s, episode steps: 558, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.012640, mae: 1.971924, mean_q: 2.411186, mean_eps: 0.912548
 244202/250000: episode: 364, duration: 42.755s, episode steps: 1000, steps per second:  23, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.011744, mae: 1.973977, mean_q: 2.416225, mean_eps: 0.912267
 245042/250000: episode: 365, duration: 34.805s, episode steps: 840, steps per second:  24, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.011825, mae: 1.984997, mean_q: 2.428479, mean_eps: 0.911936
 245929/250000: episode: 366, duration: 37.283s, episode steps: 887, steps per second:  24, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.011159, mae: 1.982885, mean_q: 2.423924, mean_eps: 0.911625
 246620/250000: episode: 367, duration: 28.972s, episode steps: 691, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.010452, mae: 1.986613, mean_q: 2.430334, mean_eps: 0.911341
 247210/250000: episode: 368, duration: 25.476s, episode steps: 590, steps per second:  23, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.012071, mae: 1.978418, mean_q: 2.422594, mean_eps: 0.911111
 247597/250000: episode: 369, duration: 16.675s, episode steps: 387, steps per second:  23, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.011345, mae: 1.999786, mean_q: 2.448667, mean_eps: 0.910935
 248216/250000: episode: 370, duration: 25.356s, episode steps: 619, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.014089, mae: 1.983564, mean_q: 2.424866, mean_eps: 0.910754
 249320/250000: episode: 371, duration: 46.661s, episode steps: 1104, steps per second:  24, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.012303, mae: 1.963569, mean_q: 2.402192, mean_eps: 0.910444
done, took 10086.136 seconds
########################################################
PROCESO TERMINADO
########################################################