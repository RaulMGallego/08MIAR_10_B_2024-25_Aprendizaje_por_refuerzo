['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_260000.h5f.index', './models\\dqn_weights_270000.h5f.index', './models\\dqn_weights_280000.h5f.index', './models\\dqn_weights_290000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_300000.h5f.index', './models\\dqn_weights_310000.h5f.index', './models\\dqn_weights_320000.h5f.index', './models\\dqn_weights_330000.h5f.index', './models\\dqn_weights_340000.h5f.index', './models\\dqn_weights_350000.h5f.index', './models\\dqn_weights_360000.h5f.index', './models\\dqn_weights_370000.h5f.index', './models\\dqn_weights_380000.h5f.index', './models\\dqn_weights_390000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_400000.h5f.index', './models\\dqn_weights_410000.h5f.index', './models\\dqn_weights_420000.h5f.index', './models\\dqn_weights_430000.h5f.index', './models\\dqn_weights_440000.h5f.index', './models\\dqn_weights_450000.h5f.index', './models\\dqn_weights_460000.h5f.index', './models\\dqn_weights_470000.h5f.index', './models\\dqn_weights_480000.h5f.index', './models\\dqn_weights_490000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_500000.h5f.index', './models\\dqn_weights_510000.h5f.index', './models\\dqn_weights_520000.h5f.index', './models\\dqn_weights_530000.h5f.index', './models\\dqn_weights_540000.h5f.index', './models\\dqn_weights_550000.h5f.index', './models\\dqn_weights_560000.h5f.index', './models\\dqn_weights_570000.h5f.index', './models\\dqn_weights_580000.h5f.index', './models\\dqn_weights_590000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_600000.h5f.index', './models\\dqn_weights_610000.h5f.index', './models\\dqn_weights_620000.h5f.index', './models\\dqn_weights_630000.h5f.index', './models\\dqn_weights_640000.h5f.index', './models\\dqn_weights_650000.h5f.index', './models\\dqn_weights_660000.h5f.index', './models\\dqn_weights_670000.h5f.index', './models\\dqn_weights_680000.h5f.index', './models\\dqn_weights_690000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_700000.h5f.index', './models\\dqn_weights_710000.h5f.index', './models\\dqn_weights_720000.h5f.index', './models\\dqn_weights_730000.h5f.index', './models\\dqn_weights_740000.h5f.index', './models\\dqn_weights_750000.h5f.index', './models\\dqn_weights_760000.h5f.index', './models\\dqn_weights_770000.h5f.index', './models\\dqn_weights_780000.h5f.index', './models\\dqn_weights_790000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_800000.h5f.index', './models\\dqn_weights_810000.h5f.index', './models\\dqn_weights_820000.h5f.index', './models\\dqn_weights_830000.h5f.index', './models\\dqn_weights_840000.h5f.index', './models\\dqn_weights_850000.h5f.index', './models\\dqn_weights_860000.h5f.index', './models\\dqn_weights_870000.h5f.index', './models\\dqn_weights_880000.h5f.index', './models\\dqn_weights_890000.h5f.index', './models\\dqn_weights_90000.h5f.index', './models\\dqn_weights_900000.h5f.index', './models\\dqn_weights_910000.h5f.index', './models\\dqn_weights_920000.h5f.index', './models\\dqn_weights_930000.h5f.index', './models\\dqn_weights_940000.h5f.index', './models\\dqn_weights_950000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_950000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
C:\Users\Rociniel\Anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    420/250000: episode: 1, duration: 2.953s, episode steps: 420, steps per second: 142, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 6.036s, episode steps: 711, steps per second: 118, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 6.207s, episode steps: 810, steps per second: 130, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2827/250000: episode: 4, duration: 7.108s, episode steps: 886, steps per second: 125, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3345/250000: episode: 5, duration: 3.169s, episode steps: 518, steps per second: 163, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3990/250000: episode: 6, duration: 4.585s, episode steps: 645, steps per second: 141, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4451/250000: episode: 7, duration: 2.970s, episode steps: 461, steps per second: 155, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5247/250000: episode: 8, duration: 4.789s, episode steps: 796, steps per second: 166, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5629/250000: episode: 9, duration: 2.253s, episode steps: 382, steps per second: 170, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7090/250000: episode: 10, duration: 9.152s, episode steps: 1461, steps per second: 160, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7471/250000: episode: 11, duration: 2.330s, episode steps: 381, steps per second: 164, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8016/250000: episode: 12, duration: 3.280s, episode steps: 545, steps per second: 166, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8762/250000: episode: 13, duration: 4.538s, episode steps: 746, steps per second: 164, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9279/250000: episode: 14, duration: 3.861s, episode steps: 517, steps per second: 134, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9896/250000: episode: 15, duration: 3.848s, episode steps: 617, steps per second: 160, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
C:\Users\Rociniel\Anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
  10641/250000: episode: 16, duration: 46.748s, episode steps: 745, steps per second:  16, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.018662, mae: 2.889173, mean_q: 3.502385, mean_eps: 0.996284
  11333/250000: episode: 17, duration: 48.395s, episode steps: 692, steps per second:  14, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.015350, mae: 2.855261, mean_q: 3.462111, mean_eps: 0.996044
  11995/250000: episode: 18, duration: 44.644s, episode steps: 662, steps per second:  15, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.016934, mae: 2.854882, mean_q: 3.458288, mean_eps: 0.995801
  12668/250000: episode: 19, duration: 45.752s, episode steps: 673, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.015676, mae: 2.867797, mean_q: 3.476498, mean_eps: 0.995561
  13633/250000: episode: 20, duration: 69.054s, episode steps: 965, steps per second:  14, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.015296, mae: 2.841502, mean_q: 3.443344, mean_eps: 0.995266
  14134/250000: episode: 21, duration: 35.841s, episode steps: 501, steps per second:  14, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.017593, mae: 2.812300, mean_q: 3.404898, mean_eps: 0.995002
  14806/250000: episode: 22, duration: 47.824s, episode steps: 672, steps per second:  14, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.018587, mae: 2.858242, mean_q: 3.458466, mean_eps: 0.994791
  15479/250000: episode: 23, duration: 46.175s, episode steps: 673, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.014046, mae: 2.803982, mean_q: 3.395217, mean_eps: 0.994549
  16119/250000: episode: 24, duration: 44.402s, episode steps: 640, steps per second:  14, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.013431, mae: 2.802038, mean_q: 3.391174, mean_eps: 0.994313
  16745/250000: episode: 25, duration: 42.787s, episode steps: 626, steps per second:  15, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.015829, mae: 2.805063, mean_q: 3.400563, mean_eps: 0.994084
  17555/250000: episode: 26, duration: 56.823s, episode steps: 810, steps per second:  14, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.015355, mae: 2.773368, mean_q: 3.355297, mean_eps: 0.993826
  18374/250000: episode: 27, duration: 55.795s, episode steps: 819, steps per second:  15, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.014063, mae: 2.785439, mean_q: 3.370247, mean_eps: 0.993533
  19038/250000: episode: 28, duration: 46.526s, episode steps: 664, steps per second:  14, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.013618, mae: 2.800629, mean_q: 3.387136, mean_eps: 0.993266
  19830/250000: episode: 29, duration: 55.361s, episode steps: 792, steps per second:  14, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.014080, mae: 2.775008, mean_q: 3.357414, mean_eps: 0.993004
  20388/250000: episode: 30, duration: 37.421s, episode steps: 558, steps per second:  15, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014458, mae: 2.795223, mean_q: 3.383604, mean_eps: 0.992761
  21049/250000: episode: 31, duration: 46.258s, episode steps: 661, steps per second:  14, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.013247, mae: 2.779857, mean_q: 3.366884, mean_eps: 0.992542
  22051/250000: episode: 32, duration: 68.978s, episode steps: 1002, steps per second:  15, episode reward:  9.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.015216, mae: 2.752663, mean_q: 3.331193, mean_eps: 0.992242
  22746/250000: episode: 33, duration: 48.715s, episode steps: 695, steps per second:  14, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013112, mae: 2.791793, mean_q: 3.377640, mean_eps: 0.991937
  23520/250000: episode: 34, duration: 56.482s, episode steps: 774, steps per second:  14, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014744, mae: 2.778759, mean_q: 3.362371, mean_eps: 0.991672
  24058/250000: episode: 35, duration: 36.488s, episode steps: 538, steps per second:  15, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.013239, mae: 2.742664, mean_q: 3.319226, mean_eps: 0.991436
  24638/250000: episode: 36, duration: 40.390s, episode steps: 580, steps per second:  14, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.013694, mae: 2.728919, mean_q: 3.300372, mean_eps: 0.991235
  25655/250000: episode: 37, duration: 69.498s, episode steps: 1017, steps per second:  15, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.015556, mae: 2.739940, mean_q: 3.313211, mean_eps: 0.990947
  26177/250000: episode: 38, duration: 35.524s, episode steps: 522, steps per second:  15, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.012458, mae: 2.756841, mean_q: 3.336316, mean_eps: 0.990670
  26531/250000: episode: 39, duration: 25.469s, episode steps: 354, steps per second:  14, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.018549, mae: 2.775348, mean_q: 3.354501, mean_eps: 0.990513
  28001/250000: episode: 40, duration: 104.073s, episode steps: 1470, steps per second:  14, episode reward: 15.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.014249, mae: 2.738292, mean_q: 3.310798, mean_eps: 0.990184
  28681/250000: episode: 41, duration: 46.795s, episode steps: 680, steps per second:  15, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.011080, mae: 2.725584, mean_q: 3.296304, mean_eps: 0.989797
  29216/250000: episode: 42, duration: 38.430s, episode steps: 535, steps per second:  14, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.013782, mae: 2.720964, mean_q: 3.290857, mean_eps: 0.989579
  30242/250000: episode: 43, duration: 70.211s, episode steps: 1026, steps per second:  15, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013101, mae: 2.716737, mean_q: 3.285022, mean_eps: 0.989298
  30895/250000: episode: 44, duration: 44.794s, episode steps: 653, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.012094, mae: 2.733996, mean_q: 3.306141, mean_eps: 0.988996
  31438/250000: episode: 45, duration: 37.456s, episode steps: 543, steps per second:  14, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.010386, mae: 2.734540, mean_q: 3.309648, mean_eps: 0.988780
  32310/250000: episode: 46, duration: 60.697s, episode steps: 872, steps per second:  14, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013015, mae: 2.748051, mean_q: 3.323936, mean_eps: 0.988525
  33310/250000: episode: 47, duration: 71.148s, episode steps: 1000, steps per second:  14, episode reward:  7.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.013689, mae: 2.745975, mean_q: 3.319918, mean_eps: 0.988188
  33852/250000: episode: 48, duration: 38.232s, episode steps: 542, steps per second:  14, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.010046, mae: 2.739001, mean_q: 3.313226, mean_eps: 0.987911
  34538/250000: episode: 49, duration: 46.961s, episode steps: 686, steps per second:  15, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.012774, mae: 2.717348, mean_q: 3.285003, mean_eps: 0.987690
  36066/250000: episode: 50, duration: 105.594s, episode steps: 1528, steps per second:  14, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.013288, mae: 2.730807, mean_q: 3.301650, mean_eps: 0.987291
  36603/250000: episode: 51, duration: 36.665s, episode steps: 537, steps per second:  15, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.010466, mae: 2.707442, mean_q: 3.275596, mean_eps: 0.986920
  37309/250000: episode: 52, duration: 49.486s, episode steps: 706, steps per second:  14, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010601, mae: 2.707455, mean_q: 3.274912, mean_eps: 0.986696
  37805/250000: episode: 53, duration: 36.451s, episode steps: 496, steps per second:  14, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.014899, mae: 2.710736, mean_q: 3.278435, mean_eps: 0.986479
  38344/250000: episode: 54, duration: 37.303s, episode steps: 539, steps per second:  14, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.010522, mae: 2.682788, mean_q: 3.245426, mean_eps: 0.986293
  39059/250000: episode: 55, duration: 50.078s, episode steps: 715, steps per second:  14, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.012347, mae: 2.686168, mean_q: 3.249083, mean_eps: 0.986068
  39562/250000: episode: 56, duration: 36.065s, episode steps: 503, steps per second:  14, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013400, mae: 2.682837, mean_q: 3.243238, mean_eps: 0.985848
  40252/250000: episode: 57, duration: 47.628s, episode steps: 690, steps per second:  14, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.011989, mae: 2.673641, mean_q: 3.234358, mean_eps: 0.985634
  41224/250000: episode: 58, duration: 66.404s, episode steps: 972, steps per second:  15, episode reward: 10.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.011380, mae: 2.691468, mean_q: 3.254593, mean_eps: 0.985335
  41627/250000: episode: 59, duration: 28.174s, episode steps: 403, steps per second:  14, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.011174, mae: 2.679698, mean_q: 3.240171, mean_eps: 0.985087
  42204/250000: episode: 60, duration: 39.614s, episode steps: 577, steps per second:  15, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.015144, mae: 2.705559, mean_q: 3.271927, mean_eps: 0.984911
  42874/250000: episode: 61, duration: 46.743s, episode steps: 670, steps per second:  14, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.013847, mae: 2.680652, mean_q: 3.240994, mean_eps: 0.984686
  43487/250000: episode: 62, duration: 43.208s, episode steps: 613, steps per second:  14, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.012971, mae: 2.698463, mean_q: 3.263518, mean_eps: 0.984455
  44095/250000: episode: 63, duration: 41.066s, episode steps: 608, steps per second:  15, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.011683, mae: 2.684446, mean_q: 3.247255, mean_eps: 0.984236
  44584/250000: episode: 64, duration: 34.222s, episode steps: 489, steps per second:  14, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.011403, mae: 2.685015, mean_q: 3.245695, mean_eps: 0.984038
  45086/250000: episode: 65, duration: 35.115s, episode steps: 502, steps per second:  14, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012704, mae: 2.686175, mean_q: 3.247959, mean_eps: 0.983860
  45618/250000: episode: 66, duration: 37.059s, episode steps: 532, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.009526, mae: 2.664863, mean_q: 3.223072, mean_eps: 0.983673
  46233/250000: episode: 67, duration: 43.445s, episode steps: 615, steps per second:  14, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.011133, mae: 2.673073, mean_q: 3.233564, mean_eps: 0.983467
  46928/250000: episode: 68, duration: 50.983s, episode steps: 695, steps per second:  14, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.012695, mae: 2.669981, mean_q: 3.225786, mean_eps: 0.983231
  47693/250000: episode: 69, duration: 53.190s, episode steps: 765, steps per second:  14, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.014172, mae: 2.686253, mean_q: 3.248930, mean_eps: 0.982968
  48299/250000: episode: 70, duration: 42.121s, episode steps: 606, steps per second:  14, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.010355, mae: 2.681542, mean_q: 3.243158, mean_eps: 0.982721
  48847/250000: episode: 71, duration: 37.331s, episode steps: 548, steps per second:  15, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010673, mae: 2.702906, mean_q: 3.267468, mean_eps: 0.982514
  49250/250000: episode: 72, duration: 28.412s, episode steps: 403, steps per second:  14, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013773, mae: 2.680382, mean_q: 3.238921, mean_eps: 0.982343
  49774/250000: episode: 73, duration: 35.403s, episode steps: 524, steps per second:  15, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.013135, mae: 2.695906, mean_q: 3.258939, mean_eps: 0.982176
  50439/250000: episode: 74, duration: 46.277s, episode steps: 665, steps per second:  14, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.017595, mae: 2.686770, mean_q: 3.247192, mean_eps: 0.981962
  51274/250000: episode: 75, duration: 60.400s, episode steps: 835, steps per second:  14, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.014285, mae: 2.676448, mean_q: 3.234262, mean_eps: 0.981692
  51941/250000: episode: 76, duration: 47.337s, episode steps: 667, steps per second:  14, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.012945, mae: 2.668834, mean_q: 3.227309, mean_eps: 0.981421
  52381/250000: episode: 77, duration: 30.754s, episode steps: 440, steps per second:  14, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.013102, mae: 2.656242, mean_q: 3.209751, mean_eps: 0.981222
  53209/250000: episode: 78, duration: 57.268s, episode steps: 828, steps per second:  14, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.013378, mae: 2.669738, mean_q: 3.227104, mean_eps: 0.980993
  53871/250000: episode: 79, duration: 46.009s, episode steps: 662, steps per second:  14, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.014718, mae: 2.685049, mean_q: 3.243598, mean_eps: 0.980726
  54452/250000: episode: 80, duration: 40.177s, episode steps: 581, steps per second:  14, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.011781, mae: 2.661194, mean_q: 3.217289, mean_eps: 0.980502
  55198/250000: episode: 81, duration: 51.891s, episode steps: 746, steps per second:  14, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.012962, mae: 2.646669, mean_q: 3.198361, mean_eps: 0.980263
  55957/250000: episode: 82, duration: 52.702s, episode steps: 759, steps per second:  14, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012097, mae: 2.677697, mean_q: 3.238676, mean_eps: 0.979992
  56633/250000: episode: 83, duration: 47.217s, episode steps: 676, steps per second:  14, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.011539, mae: 2.666767, mean_q: 3.226079, mean_eps: 0.979733
  57402/250000: episode: 84, duration: 52.268s, episode steps: 769, steps per second:  15, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.011639, mae: 2.666630, mean_q: 3.223377, mean_eps: 0.979474
  58112/250000: episode: 85, duration: 49.032s, episode steps: 710, steps per second:  14, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.012589, mae: 2.671938, mean_q: 3.230790, mean_eps: 0.979208
  58765/250000: episode: 86, duration: 45.837s, episode steps: 653, steps per second:  14, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.012647, mae: 2.667738, mean_q: 3.226488, mean_eps: 0.978962
  59176/250000: episode: 87, duration: 28.447s, episode steps: 411, steps per second:  14, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014045, mae: 2.666368, mean_q: 3.223508, mean_eps: 0.978771
  59587/250000: episode: 88, duration: 29.104s, episode steps: 411, steps per second:  14, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.010216, mae: 2.651307, mean_q: 3.206952, mean_eps: 0.978623
  60047/250000: episode: 89, duration: 31.081s, episode steps: 460, steps per second:  15, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011252, mae: 2.651816, mean_q: 3.203871, mean_eps: 0.978466
  60561/250000: episode: 90, duration: 36.227s, episode steps: 514, steps per second:  14, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.011756, mae: 2.669714, mean_q: 3.227854, mean_eps: 0.978291
  61516/250000: episode: 91, duration: 66.427s, episode steps: 955, steps per second:  14, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011214, mae: 2.649533, mean_q: 3.202087, mean_eps: 0.978026
  62117/250000: episode: 92, duration: 42.357s, episode steps: 601, steps per second:  14, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012550, mae: 2.650911, mean_q: 3.202565, mean_eps: 0.977746
  62787/250000: episode: 93, duration: 46.268s, episode steps: 670, steps per second:  14, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.012785, mae: 2.667529, mean_q: 3.224071, mean_eps: 0.977517
  63363/250000: episode: 94, duration: 40.017s, episode steps: 576, steps per second:  14, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013002, mae: 2.678340, mean_q: 3.237205, mean_eps: 0.977293
  64089/250000: episode: 95, duration: 50.234s, episode steps: 726, steps per second:  14, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.013270, mae: 2.675164, mean_q: 3.233010, mean_eps: 0.977059
  64802/250000: episode: 96, duration: 48.891s, episode steps: 713, steps per second:  15, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.014592, mae: 2.667130, mean_q: 3.223140, mean_eps: 0.976799
  65389/250000: episode: 97, duration: 42.023s, episode steps: 587, steps per second:  14, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.012603, mae: 2.658097, mean_q: 3.212196, mean_eps: 0.976565
  65796/250000: episode: 98, duration: 28.410s, episode steps: 407, steps per second:  14, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.014949, mae: 2.688064, mean_q: 3.246741, mean_eps: 0.976387
  66387/250000: episode: 99, duration: 41.329s, episode steps: 591, steps per second:  14, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.010278, mae: 2.645953, mean_q: 3.196891, mean_eps: 0.976208
  67048/250000: episode: 100, duration: 45.518s, episode steps: 661, steps per second:  15, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.014070, mae: 2.633456, mean_q: 3.181479, mean_eps: 0.975982
  67649/250000: episode: 101, duration: 41.877s, episode steps: 601, steps per second:  14, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012633, mae: 2.659025, mean_q: 3.211612, mean_eps: 0.975755
  68308/250000: episode: 102, duration: 45.946s, episode steps: 659, steps per second:  14, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.011985, mae: 2.653200, mean_q: 3.204592, mean_eps: 0.975528
  69073/250000: episode: 103, duration: 53.501s, episode steps: 765, steps per second:  14, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.013104, mae: 2.679182, mean_q: 3.235465, mean_eps: 0.975272
  69452/250000: episode: 104, duration: 26.041s, episode steps: 379, steps per second:  15, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013591, mae: 2.648751, mean_q: 3.199251, mean_eps: 0.975066
  70144/250000: episode: 105, duration: 48.937s, episode steps: 692, steps per second:  14, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.012902, mae: 2.647678, mean_q: 3.198797, mean_eps: 0.974873
  71101/250000: episode: 106, duration: 66.212s, episode steps: 957, steps per second:  14, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014351, mae: 2.634662, mean_q: 3.183026, mean_eps: 0.974576
  71785/250000: episode: 107, duration: 47.816s, episode steps: 684, steps per second:  14, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.012842, mae: 2.641167, mean_q: 3.190838, mean_eps: 0.974280
  72211/250000: episode: 108, duration: 29.170s, episode steps: 426, steps per second:  15, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.016823, mae: 2.667324, mean_q: 3.220941, mean_eps: 0.974081
  72604/250000: episode: 109, duration: 27.288s, episode steps: 393, steps per second:  14, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.012838, mae: 2.666070, mean_q: 3.224763, mean_eps: 0.973934
  72993/250000: episode: 110, duration: 27.076s, episode steps: 389, steps per second:  14, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.009959, mae: 2.669222, mean_q: 3.224945, mean_eps: 0.973793
  73534/250000: episode: 111, duration: 37.583s, episode steps: 541, steps per second:  14, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.011634, mae: 2.654401, mean_q: 3.206940, mean_eps: 0.973625
  74576/250000: episode: 112, duration: 72.505s, episode steps: 1042, steps per second:  14, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011890, mae: 2.670011, mean_q: 3.227413, mean_eps: 0.973341
  75151/250000: episode: 113, duration: 39.206s, episode steps: 575, steps per second:  15, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.014258, mae: 2.654945, mean_q: 3.205605, mean_eps: 0.973050
  75537/250000: episode: 114, duration: 27.362s, episode steps: 386, steps per second:  14, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011144, mae: 2.633920, mean_q: 3.184068, mean_eps: 0.972876
  76229/250000: episode: 115, duration: 47.093s, episode steps: 692, steps per second:  15, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.014247, mae: 2.656761, mean_q: 3.209430, mean_eps: 0.972682
  77047/250000: episode: 116, duration: 56.859s, episode steps: 818, steps per second:  14, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.013406, mae: 2.631706, mean_q: 3.180821, mean_eps: 0.972410
  77603/250000: episode: 117, duration: 38.701s, episode steps: 556, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015624, mae: 2.644761, mean_q: 3.196571, mean_eps: 0.972163
  78123/250000: episode: 118, duration: 35.418s, episode steps: 520, steps per second:  15, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.015975, mae: 2.657871, mean_q: 3.210529, mean_eps: 0.971970
  78740/250000: episode: 119, duration: 43.534s, episode steps: 617, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.010580, mae: 2.623738, mean_q: 3.171664, mean_eps: 0.971765
  79356/250000: episode: 120, duration: 44.116s, episode steps: 616, steps per second:  14, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.011554, mae: 2.655860, mean_q: 3.208198, mean_eps: 0.971543
  80313/250000: episode: 121, duration: 66.722s, episode steps: 957, steps per second:  14, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.011219, mae: 2.665028, mean_q: 3.218283, mean_eps: 0.971260
  80805/250000: episode: 122, duration: 33.733s, episode steps: 492, steps per second:  15, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013042, mae: 2.666959, mean_q: 3.220761, mean_eps: 0.970998
  81311/250000: episode: 123, duration: 35.526s, episode steps: 506, steps per second:  14, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013173, mae: 2.661344, mean_q: 3.217638, mean_eps: 0.970819
  81987/250000: episode: 124, duration: 46.692s, episode steps: 676, steps per second:  14, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012049, mae: 2.626440, mean_q: 3.171540, mean_eps: 0.970607
  82656/250000: episode: 125, duration: 46.734s, episode steps: 669, steps per second:  14, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.012617, mae: 2.640989, mean_q: 3.190868, mean_eps: 0.970365
  83241/250000: episode: 126, duration: 41.361s, episode steps: 585, steps per second:  14, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.013352, mae: 2.650339, mean_q: 3.201042, mean_eps: 0.970139
  83818/250000: episode: 127, duration: 39.014s, episode steps: 577, steps per second:  15, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.010255, mae: 2.656348, mean_q: 3.207546, mean_eps: 0.969929
  84204/250000: episode: 128, duration: 27.466s, episode steps: 386, steps per second:  14, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011973, mae: 2.648299, mean_q: 3.198133, mean_eps: 0.969756
  85070/250000: episode: 129, duration: 60.782s, episode steps: 866, steps per second:  14, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012843, mae: 2.629358, mean_q: 3.177096, mean_eps: 0.969531
  86106/250000: episode: 130, duration: 71.453s, episode steps: 1036, steps per second:  14, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.013122, mae: 2.651515, mean_q: 3.202831, mean_eps: 0.969188
  86913/250000: episode: 131, duration: 56.904s, episode steps: 807, steps per second:  14, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.012570, mae: 2.660236, mean_q: 3.215218, mean_eps: 0.968856
  87577/250000: episode: 132, duration: 45.329s, episode steps: 664, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.013302, mae: 2.644037, mean_q: 3.194604, mean_eps: 0.968591
  87962/250000: episode: 133, duration: 27.030s, episode steps: 385, steps per second:  14, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012253, mae: 2.667739, mean_q: 3.222252, mean_eps: 0.968403
  88676/250000: episode: 134, duration: 49.712s, episode steps: 714, steps per second:  14, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013799, mae: 2.677086, mean_q: 3.234808, mean_eps: 0.968206
  89324/250000: episode: 135, duration: 45.028s, episode steps: 648, steps per second:  14, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.012217, mae: 2.643582, mean_q: 3.192483, mean_eps: 0.967961
  89872/250000: episode: 136, duration: 39.243s, episode steps: 548, steps per second:  14, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.010868, mae: 2.653453, mean_q: 3.205499, mean_eps: 0.967745
  90697/250000: episode: 137, duration: 61.690s, episode steps: 825, steps per second:  13, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.012715, mae: 2.650546, mean_q: 3.201533, mean_eps: 0.967498
  91222/250000: episode: 138, duration: 38.990s, episode steps: 525, steps per second:  13, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.016221, mae: 2.653501, mean_q: 3.203908, mean_eps: 0.967254
  91736/250000: episode: 139, duration: 42.003s, episode steps: 514, steps per second:  12, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.011612, mae: 2.655490, mean_q: 3.209061, mean_eps: 0.967068
  92537/250000: episode: 140, duration: 65.237s, episode steps: 801, steps per second:  12, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.015522, mae: 2.633367, mean_q: 3.181150, mean_eps: 0.966831
  93074/250000: episode: 141, duration: 45.021s, episode steps: 537, steps per second:  12, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.014136, mae: 2.643400, mean_q: 3.191806, mean_eps: 0.966590
  93853/250000: episode: 142, duration: 85.543s, episode steps: 779, steps per second:   9, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011283, mae: 2.659780, mean_q: 3.213583, mean_eps: 0.966353
  94382/250000: episode: 143, duration: 46.463s, episode steps: 529, steps per second:  11, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.011002, mae: 2.646264, mean_q: 3.198089, mean_eps: 0.966118
  95712/250000: episode: 144, duration: 119.215s, episode steps: 1330, steps per second:  11, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.012632, mae: 2.647249, mean_q: 3.198423, mean_eps: 0.965783
  96882/250000: episode: 145, duration: 104.113s, episode steps: 1170, steps per second:  11, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012036, mae: 2.660004, mean_q: 3.213567, mean_eps: 0.965333
  97423/250000: episode: 146, duration: 39.073s, episode steps: 541, steps per second:  14, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012889, mae: 2.643515, mean_q: 3.194022, mean_eps: 0.965025
  98001/250000: episode: 147, duration: 40.493s, episode steps: 578, steps per second:  14, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.009781, mae: 2.657890, mean_q: 3.211619, mean_eps: 0.964824
  98516/250000: episode: 148, duration: 37.176s, episode steps: 515, steps per second:  14, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.011514, mae: 2.673866, mean_q: 3.229441, mean_eps: 0.964627
  98899/250000: episode: 149, duration: 26.905s, episode steps: 383, steps per second:  14, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.015331, mae: 2.663641, mean_q: 3.217005, mean_eps: 0.964466
  99512/250000: episode: 150, duration: 43.217s, episode steps: 613, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.011867, mae: 2.678844, mean_q: 3.236316, mean_eps: 0.964287
 100168/250000: episode: 151, duration: 46.680s, episode steps: 656, steps per second:  14, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.013102, mae: 2.656711, mean_q: 3.209627, mean_eps: 0.964058
 100588/250000: episode: 152, duration: 29.812s, episode steps: 420, steps per second:  14, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014307, mae: 2.645168, mean_q: 3.197371, mean_eps: 0.963865
 101240/250000: episode: 153, duration: 46.651s, episode steps: 652, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.015220, mae: 2.659017, mean_q: 3.212062, mean_eps: 0.963672
 101802/250000: episode: 154, duration: 38.869s, episode steps: 562, steps per second:  14, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.014958, mae: 2.656654, mean_q: 3.209662, mean_eps: 0.963453
 102283/250000: episode: 155, duration: 34.537s, episode steps: 481, steps per second:  14, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.011704, mae: 2.685497, mean_q: 3.245271, mean_eps: 0.963265
 103000/250000: episode: 156, duration: 50.478s, episode steps: 717, steps per second:  14, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.013067, mae: 2.678238, mean_q: 3.236924, mean_eps: 0.963050
 103825/250000: episode: 157, duration: 58.830s, episode steps: 825, steps per second:  14, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.011810, mae: 2.663779, mean_q: 3.218718, mean_eps: 0.962772
 104251/250000: episode: 158, duration: 29.494s, episode steps: 426, steps per second:  14, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011678, mae: 2.654354, mean_q: 3.209306, mean_eps: 0.962546
 104925/250000: episode: 159, duration: 47.761s, episode steps: 674, steps per second:  14, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.012488, mae: 2.643005, mean_q: 3.192516, mean_eps: 0.962348
 105447/250000: episode: 160, duration: 37.111s, episode steps: 522, steps per second:  14, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.011469, mae: 2.650171, mean_q: 3.202256, mean_eps: 0.962133
 106263/250000: episode: 161, duration: 58.143s, episode steps: 816, steps per second:  14, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.012377, mae: 2.644224, mean_q: 3.193938, mean_eps: 0.961893
 106667/250000: episode: 162, duration: 29.677s, episode steps: 404, steps per second:  14, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012180, mae: 2.676595, mean_q: 3.235017, mean_eps: 0.961673
 107245/250000: episode: 163, duration: 43.112s, episode steps: 578, steps per second:  13, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.014061, mae: 2.660302, mean_q: 3.214845, mean_eps: 0.961496
 107953/250000: episode: 164, duration: 51.122s, episode steps: 708, steps per second:  14, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.012844, mae: 2.649970, mean_q: 3.201571, mean_eps: 0.961264
 108678/250000: episode: 165, duration: 50.926s, episode steps: 725, steps per second:  14, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.013765, mae: 2.648445, mean_q: 3.200683, mean_eps: 0.961006
 109179/250000: episode: 166, duration: 37.434s, episode steps: 501, steps per second:  13, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.014676, mae: 2.642598, mean_q: 3.192744, mean_eps: 0.960786
 110142/250000: episode: 167, duration: 67.946s, episode steps: 963, steps per second:  14, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013611, mae: 2.668515, mean_q: 3.221874, mean_eps: 0.960522
 111077/250000: episode: 168, duration: 67.710s, episode steps: 935, steps per second:  14, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.013197, mae: 2.658999, mean_q: 3.210763, mean_eps: 0.960180
 111816/250000: episode: 169, duration: 52.048s, episode steps: 739, steps per second:  14, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.013950, mae: 2.645100, mean_q: 3.194320, mean_eps: 0.959879
 112317/250000: episode: 170, duration: 36.881s, episode steps: 501, steps per second:  14, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.013727, mae: 2.649865, mean_q: 3.201065, mean_eps: 0.959656
 113129/250000: episode: 171, duration: 60.228s, episode steps: 812, steps per second:  13, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.011853, mae: 2.659437, mean_q: 3.212213, mean_eps: 0.959419
 114619/250000: episode: 172, duration: 106.530s, episode steps: 1490, steps per second:  14, episode reward: 24.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013377, mae: 2.655033, mean_q: 3.206112, mean_eps: 0.959005
 115268/250000: episode: 173, duration: 47.456s, episode steps: 649, steps per second:  14, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.013067, mae: 2.670943, mean_q: 3.223502, mean_eps: 0.958621
 116187/250000: episode: 174, duration: 69.410s, episode steps: 919, steps per second:  13, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.012095, mae: 2.646066, mean_q: 3.195272, mean_eps: 0.958339
 117021/250000: episode: 175, duration: 60.428s, episode steps: 834, steps per second:  14, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.013355, mae: 2.647477, mean_q: 3.198233, mean_eps: 0.958023
 117406/250000: episode: 176, duration: 28.144s, episode steps: 385, steps per second:  14, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014427, mae: 2.680336, mean_q: 3.236604, mean_eps: 0.957803
 118343/250000: episode: 177, duration: 75.403s, episode steps: 937, steps per second:  12, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.012483, mae: 2.680451, mean_q: 3.238765, mean_eps: 0.957565
 118957/250000: episode: 178, duration: 49.273s, episode steps: 614, steps per second:  12, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.011941, mae: 2.661955, mean_q: 3.217018, mean_eps: 0.957286
 119723/250000: episode: 179, duration: 62.382s, episode steps: 766, steps per second:  12, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.013378, mae: 2.672706, mean_q: 3.229465, mean_eps: 0.957038
 120417/250000: episode: 180, duration: 56.999s, episode steps: 694, steps per second:  12, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.015543, mae: 2.649225, mean_q: 3.199934, mean_eps: 0.956775
 121213/250000: episode: 181, duration: 63.148s, episode steps: 796, steps per second:  13, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.015278, mae: 2.669416, mean_q: 3.224665, mean_eps: 0.956506
 122008/250000: episode: 182, duration: 60.456s, episode steps: 795, steps per second:  13, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.012921, mae: 2.662402, mean_q: 3.216060, mean_eps: 0.956220
 122434/250000: episode: 183, duration: 34.382s, episode steps: 426, steps per second:  12, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014558, mae: 2.648546, mean_q: 3.196202, mean_eps: 0.956001
 122998/250000: episode: 184, duration: 45.933s, episode steps: 564, steps per second:  12, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011813, mae: 2.667968, mean_q: 3.222706, mean_eps: 0.955822
 123761/250000: episode: 185, duration: 62.226s, episode steps: 763, steps per second:  12, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011367, mae: 2.654183, mean_q: 3.205168, mean_eps: 0.955583
 124312/250000: episode: 186, duration: 45.100s, episode steps: 551, steps per second:  12, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.010977, mae: 2.635630, mean_q: 3.187121, mean_eps: 0.955347
 124855/250000: episode: 187, duration: 48.176s, episode steps: 543, steps per second:  11, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.012598, mae: 2.654691, mean_q: 3.206942, mean_eps: 0.955150
 125566/250000: episode: 188, duration: 62.197s, episode steps: 711, steps per second:  11, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.012716, mae: 2.640148, mean_q: 3.189436, mean_eps: 0.954924
 126199/250000: episode: 189, duration: 56.867s, episode steps: 633, steps per second:  11, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011011, mae: 2.663961, mean_q: 3.217387, mean_eps: 0.954682
 127049/250000: episode: 190, duration: 76.743s, episode steps: 850, steps per second:  11, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012480, mae: 2.643405, mean_q: 3.191602, mean_eps: 0.954415
 128063/250000: episode: 191, duration: 90.667s, episode steps: 1014, steps per second:  11, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.013719, mae: 2.630065, mean_q: 3.178277, mean_eps: 0.954080
 128872/250000: episode: 192, duration: 75.517s, episode steps: 809, steps per second:  11, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011704, mae: 2.643983, mean_q: 3.194295, mean_eps: 0.953752
 129390/250000: episode: 193, duration: 42.801s, episode steps: 518, steps per second:  12, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.013985, mae: 2.623714, mean_q: 3.165176, mean_eps: 0.953513
 129893/250000: episode: 194, duration: 40.660s, episode steps: 503, steps per second:  12, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.011459, mae: 2.647255, mean_q: 3.196908, mean_eps: 0.953329
 131022/250000: episode: 195, duration: 95.229s, episode steps: 1129, steps per second:  12, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013223, mae: 2.643722, mean_q: 3.192279, mean_eps: 0.953035
 132152/250000: episode: 196, duration: 93.879s, episode steps: 1130, steps per second:  12, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.014418, mae: 2.623153, mean_q: 3.168078, mean_eps: 0.952629
 132821/250000: episode: 197, duration: 54.918s, episode steps: 669, steps per second:  12, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013408, mae: 2.637636, mean_q: 3.187601, mean_eps: 0.952305
 133456/250000: episode: 198, duration: 51.878s, episode steps: 635, steps per second:  12, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.013926, mae: 2.634844, mean_q: 3.184630, mean_eps: 0.952070
 133959/250000: episode: 199, duration: 41.379s, episode steps: 503, steps per second:  12, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.013925, mae: 2.619578, mean_q: 3.164946, mean_eps: 0.951866
 134483/250000: episode: 200, duration: 42.560s, episode steps: 524, steps per second:  12, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013969, mae: 2.618681, mean_q: 3.165144, mean_eps: 0.951681
 134958/250000: episode: 201, duration: 39.415s, episode steps: 475, steps per second:  12, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.012418, mae: 2.624434, mean_q: 3.169293, mean_eps: 0.951501
 135990/250000: episode: 202, duration: 84.551s, episode steps: 1032, steps per second:  12, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012530, mae: 2.633522, mean_q: 3.181475, mean_eps: 0.951229
 136669/250000: episode: 203, duration: 55.205s, episode steps: 679, steps per second:  12, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011456, mae: 2.632689, mean_q: 3.182072, mean_eps: 0.950921
 137279/250000: episode: 204, duration: 50.179s, episode steps: 610, steps per second:  12, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.013843, mae: 2.632351, mean_q: 3.178780, mean_eps: 0.950689
 137899/250000: episode: 205, duration: 51.581s, episode steps: 620, steps per second:  12, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012201, mae: 2.652538, mean_q: 3.207735, mean_eps: 0.950468
 138643/250000: episode: 206, duration: 60.976s, episode steps: 744, steps per second:  12, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012316, mae: 2.629172, mean_q: 3.176547, mean_eps: 0.950223
 139277/250000: episode: 207, duration: 53.028s, episode steps: 634, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.012066, mae: 2.624059, mean_q: 3.170064, mean_eps: 0.949974
 139812/250000: episode: 208, duration: 44.580s, episode steps: 535, steps per second:  12, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013304, mae: 2.624977, mean_q: 3.171867, mean_eps: 0.949764
 140541/250000: episode: 209, duration: 59.535s, episode steps: 729, steps per second:  12, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011971, mae: 2.620113, mean_q: 3.164136, mean_eps: 0.949537
 141114/250000: episode: 210, duration: 47.263s, episode steps: 573, steps per second:  12, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014372, mae: 2.623508, mean_q: 3.167917, mean_eps: 0.949302
 141706/250000: episode: 211, duration: 48.582s, episode steps: 592, steps per second:  12, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.010787, mae: 2.628792, mean_q: 3.175269, mean_eps: 0.949092
 142185/250000: episode: 212, duration: 39.959s, episode steps: 479, steps per second:  12, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011590, mae: 2.632546, mean_q: 3.181022, mean_eps: 0.948899
 143092/250000: episode: 213, duration: 79.877s, episode steps: 907, steps per second:  11, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012685, mae: 2.628589, mean_q: 3.173624, mean_eps: 0.948650
 143899/250000: episode: 214, duration: 72.360s, episode steps: 807, steps per second:  11, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.012921, mae: 2.631597, mean_q: 3.177244, mean_eps: 0.948342
 144760/250000: episode: 215, duration: 75.307s, episode steps: 861, steps per second:  11, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.012294, mae: 2.641217, mean_q: 3.190341, mean_eps: 0.948042
 145243/250000: episode: 216, duration: 42.328s, episode steps: 483, steps per second:  11, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.014137, mae: 2.622840, mean_q: 3.169230, mean_eps: 0.947800
 145640/250000: episode: 217, duration: 34.998s, episode steps: 397, steps per second:  11, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011208, mae: 2.641618, mean_q: 3.192680, mean_eps: 0.947642
 146427/250000: episode: 218, duration: 66.554s, episode steps: 787, steps per second:  12, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.012303, mae: 2.645275, mean_q: 3.194604, mean_eps: 0.947428
 147092/250000: episode: 219, duration: 54.843s, episode steps: 665, steps per second:  12, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.012327, mae: 2.645457, mean_q: 3.193263, mean_eps: 0.947167
 147623/250000: episode: 220, duration: 43.926s, episode steps: 531, steps per second:  12, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.012653, mae: 2.655064, mean_q: 3.206428, mean_eps: 0.946952
 148203/250000: episode: 221, duration: 48.032s, episode steps: 580, steps per second:  12, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.016264, mae: 2.641211, mean_q: 3.191341, mean_eps: 0.946752
 148951/250000: episode: 222, duration: 63.199s, episode steps: 748, steps per second:  12, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.012350, mae: 2.659880, mean_q: 3.213899, mean_eps: 0.946513
 150154/250000: episode: 223, duration: 100.078s, episode steps: 1203, steps per second:  12, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.013993, mae: 2.645074, mean_q: 3.196649, mean_eps: 0.946161
 150755/250000: episode: 224, duration: 49.543s, episode steps: 601, steps per second:  12, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.013059, mae: 2.650704, mean_q: 3.203170, mean_eps: 0.945837
 151269/250000: episode: 225, duration: 43.100s, episode steps: 514, steps per second:  12, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.014452, mae: 2.681342, mean_q: 3.237172, mean_eps: 0.945636
 152123/250000: episode: 226, duration: 70.179s, episode steps: 854, steps per second:  12, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.013749, mae: 2.653604, mean_q: 3.203517, mean_eps: 0.945389
 152895/250000: episode: 227, duration: 64.056s, episode steps: 772, steps per second:  12, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.012320, mae: 2.675384, mean_q: 3.230103, mean_eps: 0.945097
 153260/250000: episode: 228, duration: 30.540s, episode steps: 365, steps per second:  12, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.013386, mae: 2.645827, mean_q: 3.194769, mean_eps: 0.944893
 153651/250000: episode: 229, duration: 32.100s, episode steps: 391, steps per second:  12, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.013941, mae: 2.689118, mean_q: 3.248359, mean_eps: 0.944757
 154385/250000: episode: 230, duration: 63.363s, episode steps: 734, steps per second:  12, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.013441, mae: 2.666116, mean_q: 3.218727, mean_eps: 0.944554
 154950/250000: episode: 231, duration: 47.606s, episode steps: 565, steps per second:  12, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.012502, mae: 2.667332, mean_q: 3.220613, mean_eps: 0.944320
 155378/250000: episode: 232, duration: 37.016s, episode steps: 428, steps per second:  12, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.013353, mae: 2.635862, mean_q: 3.183286, mean_eps: 0.944141
 156081/250000: episode: 233, duration: 60.392s, episode steps: 703, steps per second:  12, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013376, mae: 2.669273, mean_q: 3.225922, mean_eps: 0.943937
 157176/250000: episode: 234, duration: 93.501s, episode steps: 1095, steps per second:  12, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014223, mae: 2.660411, mean_q: 3.212521, mean_eps: 0.943614
 157806/250000: episode: 235, duration: 54.426s, episode steps: 630, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.014579, mae: 2.652893, mean_q: 3.203544, mean_eps: 0.943304
 158322/250000: episode: 236, duration: 43.040s, episode steps: 516, steps per second:  12, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.017180, mae: 2.662956, mean_q: 3.215832, mean_eps: 0.943097
 158719/250000: episode: 237, duration: 32.545s, episode steps: 397, steps per second:  12, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.013975, mae: 2.632666, mean_q: 3.179024, mean_eps: 0.942933
 159385/250000: episode: 238, duration: 55.447s, episode steps: 666, steps per second:  12, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012814, mae: 2.634070, mean_q: 3.179931, mean_eps: 0.942741
 159862/250000: episode: 239, duration: 39.902s, episode steps: 477, steps per second:  12, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.013166, mae: 2.672037, mean_q: 3.226626, mean_eps: 0.942535
 160227/250000: episode: 240, duration: 29.587s, episode steps: 365, steps per second:  12, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.012172, mae: 2.666867, mean_q: 3.221217, mean_eps: 0.942384
 160931/250000: episode: 241, duration: 58.722s, episode steps: 704, steps per second:  12, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012559, mae: 2.663819, mean_q: 3.215706, mean_eps: 0.942192
 161766/250000: episode: 242, duration: 69.958s, episode steps: 835, steps per second:  12, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.012427, mae: 2.638072, mean_q: 3.185233, mean_eps: 0.941915
 162244/250000: episode: 243, duration: 40.246s, episode steps: 478, steps per second:  12, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011431, mae: 2.629452, mean_q: 3.175379, mean_eps: 0.941679
 162797/250000: episode: 244, duration: 45.894s, episode steps: 553, steps per second:  12, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.013434, mae: 2.652706, mean_q: 3.202640, mean_eps: 0.941493
 163303/250000: episode: 245, duration: 41.246s, episode steps: 506, steps per second:  12, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.014833, mae: 2.669792, mean_q: 3.224732, mean_eps: 0.941302
 163969/250000: episode: 246, duration: 55.825s, episode steps: 666, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.015623, mae: 2.666325, mean_q: 3.218693, mean_eps: 0.941091
 165298/250000: episode: 247, duration: 109.594s, episode steps: 1329, steps per second:  12, episode reward: 14.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014129, mae: 2.647249, mean_q: 3.195790, mean_eps: 0.940732
 166226/250000: episode: 248, duration: 77.024s, episode steps: 928, steps per second:  12, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.014413, mae: 2.651878, mean_q: 3.202367, mean_eps: 0.940326
 167360/250000: episode: 249, duration: 93.736s, episode steps: 1134, steps per second:  12, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.012685, mae: 2.657116, mean_q: 3.209042, mean_eps: 0.939955
 168379/250000: episode: 250, duration: 85.098s, episode steps: 1019, steps per second:  12, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.012560, mae: 2.648404, mean_q: 3.198119, mean_eps: 0.939568
 168968/250000: episode: 251, duration: 51.477s, episode steps: 589, steps per second:  11, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.013220, mae: 2.655458, mean_q: 3.206048, mean_eps: 0.939278
 169692/250000: episode: 252, duration: 64.828s, episode steps: 724, steps per second:  11, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.013490, mae: 2.660625, mean_q: 3.212479, mean_eps: 0.939042
 170331/250000: episode: 253, duration: 53.339s, episode steps: 639, steps per second:  12, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012632, mae: 2.649010, mean_q: 3.199351, mean_eps: 0.938796
 171236/250000: episode: 254, duration: 79.017s, episode steps: 905, steps per second:  11, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.014179, mae: 2.658253, mean_q: 3.209294, mean_eps: 0.938518
 171878/250000: episode: 255, duration: 58.826s, episode steps: 642, steps per second:  11, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013382, mae: 2.659787, mean_q: 3.210383, mean_eps: 0.938240
 172557/250000: episode: 256, duration: 59.308s, episode steps: 679, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.017688, mae: 2.645406, mean_q: 3.192373, mean_eps: 0.938002
 173381/250000: episode: 257, duration: 69.098s, episode steps: 824, steps per second:  12, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.011879, mae: 2.637180, mean_q: 3.184065, mean_eps: 0.937731
 173997/250000: episode: 258, duration: 50.922s, episode steps: 616, steps per second:  12, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.013472, mae: 2.679854, mean_q: 3.236996, mean_eps: 0.937472
 174569/250000: episode: 259, duration: 47.814s, episode steps: 572, steps per second:  12, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.015566, mae: 2.656709, mean_q: 3.207937, mean_eps: 0.937258
 175209/250000: episode: 260, duration: 53.448s, episode steps: 640, steps per second:  12, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.011592, mae: 2.662390, mean_q: 3.215683, mean_eps: 0.937040
 175857/250000: episode: 261, duration: 49.296s, episode steps: 648, steps per second:  13, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.014389, mae: 2.640621, mean_q: 3.187152, mean_eps: 0.936808
 176442/250000: episode: 262, duration: 44.480s, episode steps: 585, steps per second:  13, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012484, mae: 2.652824, mean_q: 3.204062, mean_eps: 0.936586
 177137/250000: episode: 263, duration: 57.400s, episode steps: 695, steps per second:  12, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013638, mae: 2.650675, mean_q: 3.201369, mean_eps: 0.936356
 177653/250000: episode: 264, duration: 43.536s, episode steps: 516, steps per second:  12, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.013156, mae: 2.610435, mean_q: 3.153097, mean_eps: 0.936137
 178091/250000: episode: 265, duration: 36.436s, episode steps: 438, steps per second:  12, episode reward: 11.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012244, mae: 2.641527, mean_q: 3.190889, mean_eps: 0.935966
 178964/250000: episode: 266, duration: 76.047s, episode steps: 873, steps per second:  11, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.013662, mae: 2.646458, mean_q: 3.196353, mean_eps: 0.935731
 179597/250000: episode: 267, duration: 53.050s, episode steps: 633, steps per second:  12, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.014804, mae: 2.670632, mean_q: 3.222618, mean_eps: 0.935459
 180528/250000: episode: 268, duration: 79.181s, episode steps: 931, steps per second:  12, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.013589, mae: 2.648347, mean_q: 3.197432, mean_eps: 0.935178
 181103/250000: episode: 269, duration: 47.991s, episode steps: 575, steps per second:  12, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.015630, mae: 2.656607, mean_q: 3.205720, mean_eps: 0.934907
 181483/250000: episode: 270, duration: 32.354s, episode steps: 380, steps per second:  12, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.012787, mae: 2.650163, mean_q: 3.200658, mean_eps: 0.934735
 182403/250000: episode: 271, duration: 84.166s, episode steps: 920, steps per second:  11, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.012664, mae: 2.642067, mean_q: 3.189219, mean_eps: 0.934501
 183204/250000: episode: 272, duration: 60.665s, episode steps: 801, steps per second:  13, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012894, mae: 2.660346, mean_q: 3.211898, mean_eps: 0.934191
 183855/250000: episode: 273, duration: 49.134s, episode steps: 651, steps per second:  13, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.013745, mae: 2.643785, mean_q: 3.191283, mean_eps: 0.933930
 184255/250000: episode: 274, duration: 33.225s, episode steps: 400, steps per second:  12, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.012275, mae: 2.647382, mean_q: 3.198149, mean_eps: 0.933741
 184852/250000: episode: 275, duration: 46.502s, episode steps: 597, steps per second:  13, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.011178, mae: 2.643715, mean_q: 3.191528, mean_eps: 0.933561
 185374/250000: episode: 276, duration: 38.153s, episode steps: 522, steps per second:  14, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.014195, mae: 2.655342, mean_q: 3.204876, mean_eps: 0.933360
 186084/250000: episode: 277, duration: 55.222s, episode steps: 710, steps per second:  13, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.012323, mae: 2.638152, mean_q: 3.186493, mean_eps: 0.933138
 186920/250000: episode: 278, duration: 67.208s, episode steps: 836, steps per second:  12, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012769, mae: 2.636552, mean_q: 3.183663, mean_eps: 0.932860
 187275/250000: episode: 279, duration: 30.903s, episode steps: 355, steps per second:  11, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.013055, mae: 2.674843, mean_q: 3.231791, mean_eps: 0.932645
 188250/250000: episode: 280, duration: 81.617s, episode steps: 975, steps per second:  12, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.013951, mae: 2.635497, mean_q: 3.180332, mean_eps: 0.932406
 188714/250000: episode: 281, duration: 39.531s, episode steps: 464, steps per second:  12, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.013040, mae: 2.621309, mean_q: 3.166184, mean_eps: 0.932146
 189222/250000: episode: 282, duration: 42.769s, episode steps: 508, steps per second:  12, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.015326, mae: 2.646789, mean_q: 3.197327, mean_eps: 0.931972
 189829/250000: episode: 283, duration: 50.515s, episode steps: 607, steps per second:  12, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.012185, mae: 2.650552, mean_q: 3.200388, mean_eps: 0.931771
 190640/250000: episode: 284, duration: 68.951s, episode steps: 811, steps per second:  12, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.013383, mae: 2.650626, mean_q: 3.201830, mean_eps: 0.931516
 191041/250000: episode: 285, duration: 33.995s, episode steps: 401, steps per second:  12, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.010445, mae: 2.656316, mean_q: 3.210222, mean_eps: 0.931298
 191840/250000: episode: 286, duration: 67.755s, episode steps: 799, steps per second:  12, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.012600, mae: 2.637793, mean_q: 3.183027, mean_eps: 0.931082
 192626/250000: episode: 287, duration: 66.416s, episode steps: 786, steps per second:  12, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.015188, mae: 2.627292, mean_q: 3.169826, mean_eps: 0.930796
 193002/250000: episode: 288, duration: 32.328s, episode steps: 376, steps per second:  12, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.015933, mae: 2.670118, mean_q: 3.220310, mean_eps: 0.930587
 193356/250000: episode: 289, duration: 29.890s, episode steps: 354, steps per second:  12, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.012102, mae: 2.639615, mean_q: 3.186577, mean_eps: 0.930456
 193970/250000: episode: 290, duration: 51.210s, episode steps: 614, steps per second:  12, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.012946, mae: 2.645768, mean_q: 3.192612, mean_eps: 0.930282
 194676/250000: episode: 291, duration: 60.421s, episode steps: 706, steps per second:  12, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013313, mae: 2.634668, mean_q: 3.178606, mean_eps: 0.930044
 195419/250000: episode: 292, duration: 63.349s, episode steps: 743, steps per second:  12, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.012536, mae: 2.662873, mean_q: 3.213267, mean_eps: 0.929783
 196230/250000: episode: 293, duration: 69.462s, episode steps: 811, steps per second:  12, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012700, mae: 2.649137, mean_q: 3.197143, mean_eps: 0.929503
 197134/250000: episode: 294, duration: 76.767s, episode steps: 904, steps per second:  12, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.012027, mae: 2.650794, mean_q: 3.199069, mean_eps: 0.929194
 197655/250000: episode: 295, duration: 44.761s, episode steps: 521, steps per second:  12, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.014126, mae: 2.676778, mean_q: 3.229991, mean_eps: 0.928938
 198472/250000: episode: 296, duration: 74.845s, episode steps: 817, steps per second:  11, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011576, mae: 2.656671, mean_q: 3.207824, mean_eps: 0.928698
 199288/250000: episode: 297, duration: 71.111s, episode steps: 816, steps per second:  11, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.013052, mae: 2.640874, mean_q: 3.189217, mean_eps: 0.928404
 200255/250000: episode: 298, duration: 84.667s, episode steps: 967, steps per second:  11, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012070, mae: 2.637836, mean_q: 3.184307, mean_eps: 0.928083
 200647/250000: episode: 299, duration: 34.375s, episode steps: 392, steps per second:  11, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.012005, mae: 2.641050, mean_q: 3.188402, mean_eps: 0.927838
 201230/250000: episode: 300, duration: 53.921s, episode steps: 583, steps per second:  11, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014414, mae: 2.648083, mean_q: 3.195981, mean_eps: 0.927662
 201863/250000: episode: 301, duration: 56.086s, episode steps: 633, steps per second:  11, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011578, mae: 2.640453, mean_q: 3.188540, mean_eps: 0.927443
 202362/250000: episode: 302, duration: 44.740s, episode steps: 499, steps per second:  11, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.013673, mae: 2.661496, mean_q: 3.212882, mean_eps: 0.927240
 202979/250000: episode: 303, duration: 53.015s, episode steps: 617, steps per second:  12, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.013218, mae: 2.660478, mean_q: 3.211122, mean_eps: 0.927039
 203750/250000: episode: 304, duration: 68.424s, episode steps: 771, steps per second:  11, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.013510, mae: 2.652887, mean_q: 3.201912, mean_eps: 0.926789
 204357/250000: episode: 305, duration: 46.781s, episode steps: 607, steps per second:  13, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012312, mae: 2.649332, mean_q: 3.197418, mean_eps: 0.926541
 205286/250000: episode: 306, duration: 76.623s, episode steps: 929, steps per second:  12, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.014473, mae: 2.649900, mean_q: 3.198282, mean_eps: 0.926264
 206034/250000: episode: 307, duration: 65.119s, episode steps: 748, steps per second:  11, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.013279, mae: 2.651078, mean_q: 3.200844, mean_eps: 0.925962
 206814/250000: episode: 308, duration: 65.982s, episode steps: 780, steps per second:  12, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012555, mae: 2.644103, mean_q: 3.191130, mean_eps: 0.925687
 207195/250000: episode: 309, duration: 29.826s, episode steps: 381, steps per second:  13, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011913, mae: 2.651737, mean_q: 3.200759, mean_eps: 0.925479
 207815/250000: episode: 310, duration: 54.260s, episode steps: 620, steps per second:  11, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.013174, mae: 2.643223, mean_q: 3.192063, mean_eps: 0.925299
 208593/250000: episode: 311, duration: 67.520s, episode steps: 778, steps per second:  12, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.014611, mae: 2.650405, mean_q: 3.196720, mean_eps: 0.925047
 209526/250000: episode: 312, duration: 80.305s, episode steps: 933, steps per second:  12, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.013231, mae: 2.669877, mean_q: 3.221626, mean_eps: 0.924738
 209913/250000: episode: 313, duration: 33.471s, episode steps: 387, steps per second:  12, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013200, mae: 2.627624, mean_q: 3.171841, mean_eps: 0.924501
 210763/250000: episode: 314, duration: 74.666s, episode steps: 850, steps per second:  11, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012350, mae: 2.664290, mean_q: 3.216261, mean_eps: 0.924278
 211396/250000: episode: 315, duration: 54.335s, episode steps: 633, steps per second:  12, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.017322, mae: 2.653605, mean_q: 3.204165, mean_eps: 0.924012
 212084/250000: episode: 316, duration: 60.337s, episode steps: 688, steps per second:  11, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.011574, mae: 2.640553, mean_q: 3.188386, mean_eps: 0.923774
 213349/250000: episode: 317, duration: 110.349s, episode steps: 1265, steps per second:  11, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.013630, mae: 2.644002, mean_q: 3.192946, mean_eps: 0.923422
 214084/250000: episode: 318, duration: 64.033s, episode steps: 735, steps per second:  11, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.014542, mae: 2.655773, mean_q: 3.203517, mean_eps: 0.923062
 215010/250000: episode: 319, duration: 80.853s, episode steps: 926, steps per second:  11, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.014840, mae: 2.655014, mean_q: 3.203421, mean_eps: 0.922763
 215576/250000: episode: 320, duration: 49.282s, episode steps: 566, steps per second:  11, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.015064, mae: 2.656114, mean_q: 3.203664, mean_eps: 0.922495
 216123/250000: episode: 321, duration: 48.973s, episode steps: 547, steps per second:  11, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.011460, mae: 2.646807, mean_q: 3.195192, mean_eps: 0.922295
 217295/250000: episode: 322, duration: 88.784s, episode steps: 1172, steps per second:  13, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.013177, mae: 2.635849, mean_q: 3.179721, mean_eps: 0.921985
 217692/250000: episode: 323, duration: 30.735s, episode steps: 397, steps per second:  13, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012263, mae: 2.636958, mean_q: 3.183301, mean_eps: 0.921703
 218386/250000: episode: 324, duration: 51.844s, episode steps: 694, steps per second:  13, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.017185, mae: 2.659962, mean_q: 3.209799, mean_eps: 0.921506
 219010/250000: episode: 325, duration: 47.207s, episode steps: 624, steps per second:  13, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.012428, mae: 2.646035, mean_q: 3.192686, mean_eps: 0.921269
 219393/250000: episode: 326, duration: 32.020s, episode steps: 383, steps per second:  12, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.015539, mae: 2.654536, mean_q: 3.202761, mean_eps: 0.921087
 220109/250000: episode: 327, duration: 55.516s, episode steps: 716, steps per second:  13, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.012600, mae: 2.649749, mean_q: 3.197497, mean_eps: 0.920889
 221256/250000: episode: 328, duration: 88.123s, episode steps: 1147, steps per second:  13, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.013026, mae: 2.638224, mean_q: 3.183491, mean_eps: 0.920554
 221934/250000: episode: 329, duration: 51.125s, episode steps: 678, steps per second:  13, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012980, mae: 2.644445, mean_q: 3.190886, mean_eps: 0.920226
 222328/250000: episode: 330, duration: 30.107s, episode steps: 394, steps per second:  13, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.016830, mae: 2.631039, mean_q: 3.171168, mean_eps: 0.920033
 223657/250000: episode: 331, duration: 99.777s, episode steps: 1329, steps per second:  13, episode reward: 20.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.013279, mae: 2.637261, mean_q: 3.183421, mean_eps: 0.919723
 224303/250000: episode: 332, duration: 48.194s, episode steps: 646, steps per second:  13, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014138, mae: 2.642844, mean_q: 3.188515, mean_eps: 0.919367
 225135/250000: episode: 333, duration: 63.458s, episode steps: 832, steps per second:  13, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013585, mae: 2.647270, mean_q: 3.193594, mean_eps: 0.919102
 226084/250000: episode: 334, duration: 72.112s, episode steps: 949, steps per second:  13, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.013501, mae: 2.622046, mean_q: 3.163367, mean_eps: 0.918781
 226556/250000: episode: 335, duration: 36.254s, episode steps: 472, steps per second:  13, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.012619, mae: 2.629252, mean_q: 3.175020, mean_eps: 0.918526
 227369/250000: episode: 336, duration: 61.412s, episode steps: 813, steps per second:  13, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.013044, mae: 2.637226, mean_q: 3.182080, mean_eps: 0.918294
 228167/250000: episode: 337, duration: 59.253s, episode steps: 798, steps per second:  13, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.014710, mae: 2.660919, mean_q: 3.212698, mean_eps: 0.918004
 229092/250000: episode: 338, duration: 69.884s, episode steps: 925, steps per second:  13, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013877, mae: 2.641302, mean_q: 3.187008, mean_eps: 0.917694
 229993/250000: episode: 339, duration: 67.439s, episode steps: 901, steps per second:  13, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.014104, mae: 2.670822, mean_q: 3.222305, mean_eps: 0.917365
 230530/250000: episode: 340, duration: 39.614s, episode steps: 537, steps per second:  14, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.011710, mae: 2.636749, mean_q: 3.181118, mean_eps: 0.917106
 231251/250000: episode: 341, duration: 54.747s, episode steps: 721, steps per second:  13, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.012671, mae: 2.649641, mean_q: 3.195738, mean_eps: 0.916880
 232088/250000: episode: 342, duration: 63.257s, episode steps: 837, steps per second:  13, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.013115, mae: 2.664113, mean_q: 3.216126, mean_eps: 0.916600
 233033/250000: episode: 343, duration: 70.903s, episode steps: 945, steps per second:  13, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012447, mae: 2.646199, mean_q: 3.192487, mean_eps: 0.916278
 233871/250000: episode: 344, duration: 63.404s, episode steps: 838, steps per second:  13, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.014246, mae: 2.660423, mean_q: 3.210819, mean_eps: 0.915957
 234432/250000: episode: 345, duration: 43.617s, episode steps: 561, steps per second:  13, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.010317, mae: 2.638993, mean_q: 3.184814, mean_eps: 0.915706
 235064/250000: episode: 346, duration: 47.684s, episode steps: 632, steps per second:  13, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.014853, mae: 2.658489, mean_q: 3.207477, mean_eps: 0.915491
 235695/250000: episode: 347, duration: 47.727s, episode steps: 631, steps per second:  13, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.014036, mae: 2.658983, mean_q: 3.206969, mean_eps: 0.915264
 236565/250000: episode: 348, duration: 65.910s, episode steps: 870, steps per second:  13, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.012056, mae: 2.654126, mean_q: 3.202940, mean_eps: 0.914993
 237162/250000: episode: 349, duration: 45.768s, episode steps: 597, steps per second:  13, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.014754, mae: 2.663295, mean_q: 3.214470, mean_eps: 0.914729
 238236/250000: episode: 350, duration: 81.218s, episode steps: 1074, steps per second:  13, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012677, mae: 2.653977, mean_q: 3.203929, mean_eps: 0.914429
 239272/250000: episode: 351, duration: 78.804s, episode steps: 1036, steps per second:  13, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.013793, mae: 2.658312, mean_q: 3.208889, mean_eps: 0.914049
 239843/250000: episode: 352, duration: 44.887s, episode steps: 571, steps per second:  13, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.012508, mae: 2.667821, mean_q: 3.219735, mean_eps: 0.913760
 240622/250000: episode: 353, duration: 59.524s, episode steps: 779, steps per second:  13, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.014953, mae: 2.655764, mean_q: 3.204661, mean_eps: 0.913516
 241628/250000: episode: 354, duration: 75.991s, episode steps: 1006, steps per second:  13, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013737, mae: 2.642622, mean_q: 3.188271, mean_eps: 0.913195
 242483/250000: episode: 355, duration: 64.825s, episode steps: 855, steps per second:  13, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.013104, mae: 2.645538, mean_q: 3.192603, mean_eps: 0.912861
 243372/250000: episode: 356, duration: 68.158s, episode steps: 889, steps per second:  13, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014463, mae: 2.658096, mean_q: 3.208306, mean_eps: 0.912547
 243950/250000: episode: 357, duration: 44.759s, episode steps: 578, steps per second:  13, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.013092, mae: 2.659128, mean_q: 3.210914, mean_eps: 0.912282
 244371/250000: episode: 358, duration: 31.356s, episode steps: 421, steps per second:  13, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012499, mae: 2.613258, mean_q: 3.155516, mean_eps: 0.912102
 244763/250000: episode: 359, duration: 30.616s, episode steps: 392, steps per second:  13, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.014385, mae: 2.668582, mean_q: 3.217345, mean_eps: 0.911956
 245242/250000: episode: 360, duration: 35.729s, episode steps: 479, steps per second:  13, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.013868, mae: 2.643512, mean_q: 3.187651, mean_eps: 0.911799
 245689/250000: episode: 361, duration: 36.431s, episode steps: 447, steps per second:  12, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.015280, mae: 2.649298, mean_q: 3.196769, mean_eps: 0.911632
 246196/250000: episode: 362, duration: 38.000s, episode steps: 507, steps per second:  13, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.014937, mae: 2.661465, mean_q: 3.212000, mean_eps: 0.911461
 247194/250000: episode: 363, duration: 78.926s, episode steps: 998, steps per second:  13, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013666, mae: 2.639447, mean_q: 3.184367, mean_eps: 0.911190
 248298/250000: episode: 364, duration: 84.047s, episode steps: 1104, steps per second:  13, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.015673, mae: 2.659286, mean_q: 3.208516, mean_eps: 0.910811