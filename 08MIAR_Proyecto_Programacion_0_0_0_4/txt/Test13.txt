['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_260000.h5f.index', './models\\dqn_weights_270000.h5f.index', './models\\dqn_weights_280000.h5f.index', './models\\dqn_weights_290000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_300000.h5f.index', './models\\dqn_weights_310000.h5f.index', './models\\dqn_weights_320000.h5f.index', './models\\dqn_weights_330000.h5f.index', './models\\dqn_weights_340000.h5f.index', './models\\dqn_weights_350000.h5f.index', './models\\dqn_weights_360000.h5f.index', './models\\dqn_weights_370000.h5f.index', './models\\dqn_weights_380000.h5f.index', './models\\dqn_weights_390000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_400000.h5f.index', './models\\dqn_weights_410000.h5f.index', './models\\dqn_weights_420000.h5f.index', './models\\dqn_weights_430000.h5f.index', './models\\dqn_weights_440000.h5f.index', './models\\dqn_weights_450000.h5f.index', './models\\dqn_weights_460000.h5f.index', './models\\dqn_weights_470000.h5f.index', './models\\dqn_weights_480000.h5f.index', './models\\dqn_weights_490000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_500000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_500000.h5f
Hay pesos anteriores y se van a cargar
Training for 1000000 steps ...
    420/1000000: episode: 1, duration: 2.735s, episode steps: 420, steps per second: 154, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/1000000: episode: 2, duration: 4.636s, episode steps: 711, steps per second: 153, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/1000000: episode: 3, duration: 6.776s, episode steps: 810, steps per second: 120, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2831/1000000: episode: 4, duration: 5.917s, episode steps: 890, steps per second: 150, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3206/1000000: episode: 5, duration: 2.572s, episode steps: 375, steps per second: 146, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3716/1000000: episode: 6, duration: 4.162s, episode steps: 510, steps per second: 123, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4245/1000000: episode: 7, duration: 4.144s, episode steps: 529, steps per second: 128, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4700/1000000: episode: 8, duration: 3.437s, episode steps: 455, steps per second: 132, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5149/1000000: episode: 9, duration: 3.865s, episode steps: 449, steps per second: 116, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5739/1000000: episode: 10, duration: 5.106s, episode steps: 590, steps per second: 116, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6558/1000000: episode: 11, duration: 6.441s, episode steps: 819, steps per second: 127, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7340/1000000: episode: 12, duration: 5.500s, episode steps: 782, steps per second: 142, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7944/1000000: episode: 13, duration: 4.124s, episode steps: 604, steps per second: 146, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8736/1000000: episode: 14, duration: 4.348s, episode steps: 792, steps per second: 182, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9076/1000000: episode: 15, duration: 2.007s, episode steps: 340, steps per second: 169, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9771/1000000: episode: 16, duration: 3.484s, episode steps: 695, steps per second: 200, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10162/1000000: episode: 17, duration: 10.374s, episode steps: 391, steps per second:  38, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.023865, mae: 2.630066, mean_q: 3.196614, mean_eps: 0.996370
  10681/1000000: episode: 18, duration: 23.445s, episode steps: 519, steps per second:  22, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.015904, mae: 2.592715, mean_q: 3.149285, mean_eps: 0.996248
  11210/1000000: episode: 19, duration: 24.043s, episode steps: 529, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.015645, mae: 2.573797, mean_q: 3.120829, mean_eps: 0.996059
  11680/1000000: episode: 20, duration: 21.033s, episode steps: 470, steps per second:  22, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014320, mae: 2.613696, mean_q: 3.170143, mean_eps: 0.995880
  12362/1000000: episode: 21, duration: 30.838s, episode steps: 682, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.012336, mae: 2.574362, mean_q: 3.124242, mean_eps: 0.995673
  12948/1000000: episode: 22, duration: 26.316s, episode steps: 586, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.016662, mae: 2.544884, mean_q: 3.088364, mean_eps: 0.995445
  13766/1000000: episode: 23, duration: 36.451s, episode steps: 818, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.013097, mae: 2.525897, mean_q: 3.066018, mean_eps: 0.995192
  14459/1000000: episode: 24, duration: 29.996s, episode steps: 693, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011036, mae: 2.538933, mean_q: 3.083424, mean_eps: 0.994920
  14946/1000000: episode: 25, duration: 21.101s, episode steps: 487, steps per second:  23, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.014763, mae: 2.516811, mean_q: 3.053115, mean_eps: 0.994707
  15640/1000000: episode: 26, duration: 31.281s, episode steps: 694, steps per second:  22, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.012639, mae: 2.533678, mean_q: 3.077893, mean_eps: 0.994495
  16006/1000000: episode: 27, duration: 16.641s, episode steps: 366, steps per second:  22, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.011273, mae: 2.520784, mean_q: 3.060680, mean_eps: 0.994304
  17359/1000000: episode: 28, duration: 61.984s, episode steps: 1353, steps per second:  22, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.011825, mae: 2.521911, mean_q: 3.062015, mean_eps: 0.993994
  17958/1000000: episode: 29, duration: 30.152s, episode steps: 599, steps per second:  20, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.013564, mae: 2.508745, mean_q: 3.045892, mean_eps: 0.993643
  18648/1000000: episode: 30, duration: 30.796s, episode steps: 690, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.012326, mae: 2.529216, mean_q: 3.070822, mean_eps: 0.993411
  19274/1000000: episode: 31, duration: 28.735s, episode steps: 626, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013152, mae: 2.502491, mean_q: 3.039246, mean_eps: 0.993174
  19909/1000000: episode: 32, duration: 28.369s, episode steps: 635, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.011629, mae: 2.499998, mean_q: 3.035108, mean_eps: 0.992947
  20556/1000000: episode: 33, duration: 29.330s, episode steps: 647, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.011706, mae: 2.479232, mean_q: 3.010391, mean_eps: 0.992716
  21454/1000000: episode: 34, duration: 41.272s, episode steps: 898, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.010703, mae: 2.477735, mean_q: 3.006638, mean_eps: 0.992439
  22756/1000000: episode: 35, duration: 60.710s, episode steps: 1302, steps per second:  21, episode reward: 12.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012148, mae: 2.509017, mean_q: 3.046689, mean_eps: 0.992043
  23437/1000000: episode: 36, duration: 30.654s, episode steps: 681, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.010540, mae: 2.494865, mean_q: 3.030264, mean_eps: 0.991685
  23911/1000000: episode: 37, duration: 21.889s, episode steps: 474, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011206, mae: 2.512251, mean_q: 3.049564, mean_eps: 0.991477
  24801/1000000: episode: 38, duration: 40.433s, episode steps: 890, steps per second:  22, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.011959, mae: 2.479778, mean_q: 3.012138, mean_eps: 0.991232
  25305/1000000: episode: 39, duration: 23.928s, episode steps: 504, steps per second:  21, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010705, mae: 2.492587, mean_q: 3.029845, mean_eps: 0.990981
  26019/1000000: episode: 40, duration: 32.318s, episode steps: 714, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.011602, mae: 2.465869, mean_q: 2.993273, mean_eps: 0.990762
  26793/1000000: episode: 41, duration: 34.767s, episode steps: 774, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.011471, mae: 2.493110, mean_q: 3.024478, mean_eps: 0.990494
  27570/1000000: episode: 42, duration: 34.958s, episode steps: 777, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011447, mae: 2.502892, mean_q: 3.036688, mean_eps: 0.990214
  28355/1000000: episode: 43, duration: 36.027s, episode steps: 785, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.010041, mae: 2.499221, mean_q: 3.032106, mean_eps: 0.989934
  28867/1000000: episode: 44, duration: 22.506s, episode steps: 512, steps per second:  23, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.011731, mae: 2.504256, mean_q: 3.039655, mean_eps: 0.989700
  29470/1000000: episode: 45, duration: 27.902s, episode steps: 603, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.010974, mae: 2.498770, mean_q: 3.034531, mean_eps: 0.989500
  30287/1000000: episode: 46, duration: 35.872s, episode steps: 817, steps per second:  23, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.010919, mae: 2.516964, mean_q: 3.054066, mean_eps: 0.989244
  30706/1000000: episode: 47, duration: 19.137s, episode steps: 419, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.012940, mae: 2.499856, mean_q: 3.032696, mean_eps: 0.989021
  31159/1000000: episode: 48, duration: 20.987s, episode steps: 453, steps per second:  22, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.013007, mae: 2.521625, mean_q: 3.061807, mean_eps: 0.988864
  31567/1000000: episode: 49, duration: 19.501s, episode steps: 408, steps per second:  21, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013001, mae: 2.510598, mean_q: 3.049820, mean_eps: 0.988710
  32114/1000000: episode: 50, duration: 36.886s, episode steps: 547, steps per second:  15, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.013997, mae: 2.508919, mean_q: 3.043525, mean_eps: 0.988538
  32522/1000000: episode: 51, duration: 22.727s, episode steps: 408, steps per second:  18, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.010769, mae: 2.530124, mean_q: 3.070794, mean_eps: 0.988366
  33330/1000000: episode: 52, duration: 36.176s, episode steps: 808, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.010744, mae: 2.500653, mean_q: 3.035727, mean_eps: 0.988147
  33762/1000000: episode: 53, duration: 19.369s, episode steps: 432, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013684, mae: 2.502758, mean_q: 3.037242, mean_eps: 0.987923
  34585/1000000: episode: 54, duration: 35.628s, episode steps: 823, steps per second:  23, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.010914, mae: 2.507986, mean_q: 3.044258, mean_eps: 0.987697
  35222/1000000: episode: 55, duration: 28.882s, episode steps: 637, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.010330, mae: 2.549147, mean_q: 3.097112, mean_eps: 0.987435
  35873/1000000: episode: 56, duration: 28.260s, episode steps: 651, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012453, mae: 2.514358, mean_q: 3.050923, mean_eps: 0.987203
  36727/1000000: episode: 57, duration: 37.792s, episode steps: 854, steps per second:  23, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.011196, mae: 2.513624, mean_q: 3.051710, mean_eps: 0.986932
  37188/1000000: episode: 58, duration: 19.668s, episode steps: 461, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012763, mae: 2.510467, mean_q: 3.044754, mean_eps: 0.986696
  37558/1000000: episode: 59, duration: 17.006s, episode steps: 370, steps per second:  22, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.010741, mae: 2.498333, mean_q: 3.032640, mean_eps: 0.986546
  37919/1000000: episode: 60, duration: 16.487s, episode steps: 361, steps per second:  22, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.009845, mae: 2.526106, mean_q: 3.068819, mean_eps: 0.986414
  38307/1000000: episode: 61, duration: 17.677s, episode steps: 388, steps per second:  22, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.011212, mae: 2.542797, mean_q: 3.083556, mean_eps: 0.986280
  39433/1000000: episode: 62, duration: 53.754s, episode steps: 1126, steps per second:  21, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.012454, mae: 2.506635, mean_q: 3.042194, mean_eps: 0.986007
  39972/1000000: episode: 63, duration: 22.983s, episode steps: 539, steps per second:  23, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010573, mae: 2.521800, mean_q: 3.061226, mean_eps: 0.985707
  40665/1000000: episode: 64, duration: 32.561s, episode steps: 693, steps per second:  21, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011480, mae: 2.512174, mean_q: 3.048194, mean_eps: 0.985486
  41142/1000000: episode: 65, duration: 22.281s, episode steps: 477, steps per second:  21, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.010267, mae: 2.511480, mean_q: 3.047924, mean_eps: 0.985275
  41711/1000000: episode: 66, duration: 25.038s, episode steps: 569, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.013713, mae: 2.502295, mean_q: 3.036095, mean_eps: 0.985087
  43404/1000000: episode: 67, duration: 76.335s, episode steps: 1693, steps per second:  22, episode reward: 25.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.011282, mae: 2.492203, mean_q: 3.025072, mean_eps: 0.984680
  44260/1000000: episode: 68, duration: 39.455s, episode steps: 856, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.011086, mae: 2.519110, mean_q: 3.059579, mean_eps: 0.984221
  44652/1000000: episode: 69, duration: 18.744s, episode steps: 392, steps per second:  21, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.013659, mae: 2.496085, mean_q: 3.030148, mean_eps: 0.983997
  45281/1000000: episode: 70, duration: 29.394s, episode steps: 629, steps per second:  21, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.014388, mae: 2.532186, mean_q: 3.072932, mean_eps: 0.983812
  46109/1000000: episode: 71, duration: 37.215s, episode steps: 828, steps per second:  22, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.011993, mae: 2.508645, mean_q: 3.047798, mean_eps: 0.983549
  46515/1000000: episode: 72, duration: 18.808s, episode steps: 406, steps per second:  22, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.014046, mae: 2.521417, mean_q: 3.060147, mean_eps: 0.983328
  47102/1000000: episode: 73, duration: 27.042s, episode steps: 587, steps per second:  22, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.012695, mae: 2.502338, mean_q: 3.038484, mean_eps: 0.983149
  47483/1000000: episode: 74, duration: 17.068s, episode steps: 381, steps per second:  22, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010600, mae: 2.478496, mean_q: 3.012306, mean_eps: 0.982975
  48115/1000000: episode: 75, duration: 29.227s, episode steps: 632, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.010854, mae: 2.527128, mean_q: 3.070172, mean_eps: 0.982793
  48770/1000000: episode: 76, duration: 27.655s, episode steps: 655, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.012108, mae: 2.490025, mean_q: 3.024815, mean_eps: 0.982561
  49534/1000000: episode: 77, duration: 37.225s, episode steps: 764, steps per second:  21, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.011235, mae: 2.489327, mean_q: 3.024482, mean_eps: 0.982305
  50067/1000000: episode: 78, duration: 22.724s, episode steps: 533, steps per second:  23, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.011207, mae: 2.489879, mean_q: 3.022810, mean_eps: 0.982072
  50693/1000000: episode: 79, duration: 28.494s, episode steps: 626, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012878, mae: 2.539889, mean_q: 3.082158, mean_eps: 0.981863
  51433/1000000: episode: 80, duration: 33.878s, episode steps: 740, steps per second:  22, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.015101, mae: 2.536357, mean_q: 3.077321, mean_eps: 0.981617
  52281/1000000: episode: 81, duration: 38.518s, episode steps: 848, steps per second:  22, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.012414, mae: 2.502348, mean_q: 3.035225, mean_eps: 0.981331
  52830/1000000: episode: 82, duration: 24.999s, episode steps: 549, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012817, mae: 2.522593, mean_q: 3.061231, mean_eps: 0.981080
  53627/1000000: episode: 83, duration: 35.445s, episode steps: 797, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.014312, mae: 2.523842, mean_q: 3.062050, mean_eps: 0.980838
  54664/1000000: episode: 84, duration: 47.192s, episode steps: 1037, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.012299, mae: 2.542356, mean_q: 3.086827, mean_eps: 0.980508
  55309/1000000: episode: 85, duration: 29.853s, episode steps: 645, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.013326, mae: 2.523468, mean_q: 3.063470, mean_eps: 0.980205
  55878/1000000: episode: 86, duration: 25.237s, episode steps: 569, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.011733, mae: 2.542313, mean_q: 3.084322, mean_eps: 0.979986
  56591/1000000: episode: 87, duration: 32.295s, episode steps: 713, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.013654, mae: 2.524535, mean_q: 3.060910, mean_eps: 0.979756
  57364/1000000: episode: 88, duration: 35.019s, episode steps: 773, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.013889, mae: 2.509096, mean_q: 3.043421, mean_eps: 0.979489
  58071/1000000: episode: 89, duration: 32.450s, episode steps: 707, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.012102, mae: 2.499801, mean_q: 3.033756, mean_eps: 0.979222
  58803/1000000: episode: 90, duration: 32.797s, episode steps: 732, steps per second:  22, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011500, mae: 2.512861, mean_q: 3.051749, mean_eps: 0.978963
  59525/1000000: episode: 91, duration: 33.131s, episode steps: 722, steps per second:  22, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010754, mae: 2.521461, mean_q: 3.062263, mean_eps: 0.978701
  59978/1000000: episode: 92, duration: 19.999s, episode steps: 453, steps per second:  23, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.011945, mae: 2.484972, mean_q: 3.012032, mean_eps: 0.978489
  60861/1000000: episode: 93, duration: 40.233s, episode steps: 883, steps per second:  22, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.012128, mae: 2.502123, mean_q: 3.036035, mean_eps: 0.978249
  61364/1000000: episode: 94, duration: 22.894s, episode steps: 503, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.011868, mae: 2.522419, mean_q: 3.059938, mean_eps: 0.978000
  61869/1000000: episode: 95, duration: 22.626s, episode steps: 505, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.011865, mae: 2.488853, mean_q: 3.019656, mean_eps: 0.977818
  62459/1000000: episode: 96, duration: 26.444s, episode steps: 590, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.012453, mae: 2.515746, mean_q: 3.051043, mean_eps: 0.977621
  63249/1000000: episode: 97, duration: 35.051s, episode steps: 790, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010644, mae: 2.517136, mean_q: 3.053279, mean_eps: 0.977373
  63890/1000000: episode: 98, duration: 29.097s, episode steps: 641, steps per second:  22, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.011445, mae: 2.501166, mean_q: 3.035017, mean_eps: 0.977115
  64635/1000000: episode: 99, duration: 33.465s, episode steps: 745, steps per second:  22, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.011793, mae: 2.500683, mean_q: 3.034625, mean_eps: 0.976866
  65161/1000000: episode: 100, duration: 24.570s, episode steps: 526, steps per second:  21, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.012721, mae: 2.502264, mean_q: 3.035038, mean_eps: 0.976637
  65962/1000000: episode: 101, duration: 35.998s, episode steps: 801, steps per second:  22, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011247, mae: 2.518067, mean_q: 3.054596, mean_eps: 0.976398
  66600/1000000: episode: 102, duration: 29.320s, episode steps: 638, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011658, mae: 2.516536, mean_q: 3.051860, mean_eps: 0.976139
  67111/1000000: episode: 103, duration: 23.169s, episode steps: 511, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.011709, mae: 2.519842, mean_q: 3.057489, mean_eps: 0.975933
  67828/1000000: episode: 104, duration: 32.508s, episode steps: 717, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.012885, mae: 2.511694, mean_q: 3.045875, mean_eps: 0.975712
  68857/1000000: episode: 105, duration: 46.638s, episode steps: 1029, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.011391, mae: 2.494243, mean_q: 3.024172, mean_eps: 0.975397
  69430/1000000: episode: 106, duration: 26.404s, episode steps: 573, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011563, mae: 2.541787, mean_q: 3.083656, mean_eps: 0.975108
  69896/1000000: episode: 107, duration: 21.348s, episode steps: 466, steps per second:  22, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.011094, mae: 2.517775, mean_q: 3.056561, mean_eps: 0.974922
  70340/1000000: episode: 108, duration: 20.315s, episode steps: 444, steps per second:  22, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.011786, mae: 2.499292, mean_q: 3.032204, mean_eps: 0.974758
  71004/1000000: episode: 109, duration: 29.595s, episode steps: 664, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.012562, mae: 2.505144, mean_q: 3.039165, mean_eps: 0.974559
  71978/1000000: episode: 110, duration: 45.145s, episode steps: 974, steps per second:  22, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.011165, mae: 2.507131, mean_q: 3.042418, mean_eps: 0.974264
  72913/1000000: episode: 111, duration: 43.551s, episode steps: 935, steps per second:  21, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010278, mae: 2.499219, mean_q: 3.032633, mean_eps: 0.973919
  73520/1000000: episode: 112, duration: 30.638s, episode steps: 607, steps per second:  20, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012192, mae: 2.504011, mean_q: 3.038754, mean_eps: 0.973642
  74193/1000000: episode: 113, duration: 31.161s, episode steps: 673, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.013598, mae: 2.509159, mean_q: 3.043064, mean_eps: 0.973412
  75215/1000000: episode: 114, duration: 45.254s, episode steps: 1022, steps per second:  23, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.013194, mae: 2.521219, mean_q: 3.057446, mean_eps: 0.973107
  75674/1000000: episode: 115, duration: 20.894s, episode steps: 459, steps per second:  22, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.011122, mae: 2.518277, mean_q: 3.054198, mean_eps: 0.972840
  76384/1000000: episode: 116, duration: 31.183s, episode steps: 710, steps per second:  23, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.015701, mae: 2.480732, mean_q: 3.008095, mean_eps: 0.972630
  76920/1000000: episode: 117, duration: 24.294s, episode steps: 536, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011418, mae: 2.511633, mean_q: 3.048182, mean_eps: 0.972406
  78171/1000000: episode: 118, duration: 55.448s, episode steps: 1251, steps per second:  23, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.011255, mae: 2.514931, mean_q: 3.052595, mean_eps: 0.972084
  79370/1000000: episode: 119, duration: 55.026s, episode steps: 1199, steps per second:  22, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.011850, mae: 2.501443, mean_q: 3.032195, mean_eps: 0.971643
  80301/1000000: episode: 120, duration: 45.524s, episode steps: 931, steps per second:  20, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.012129, mae: 2.488954, mean_q: 3.019226, mean_eps: 0.971259
  81369/1000000: episode: 121, duration: 48.895s, episode steps: 1068, steps per second:  22, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.010776, mae: 2.505635, mean_q: 3.038956, mean_eps: 0.970899
  81959/1000000: episode: 122, duration: 26.523s, episode steps: 590, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.013633, mae: 2.502241, mean_q: 3.034555, mean_eps: 0.970601
  82650/1000000: episode: 123, duration: 32.452s, episode steps: 691, steps per second:  21, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012719, mae: 2.510474, mean_q: 3.045249, mean_eps: 0.970371
  83240/1000000: episode: 124, duration: 26.914s, episode steps: 590, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.011201, mae: 2.500086, mean_q: 3.032883, mean_eps: 0.970140
  83667/1000000: episode: 125, duration: 20.007s, episode steps: 427, steps per second:  21, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.011852, mae: 2.528123, mean_q: 3.067219, mean_eps: 0.969957
  84363/1000000: episode: 126, duration: 32.300s, episode steps: 696, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.011754, mae: 2.519773, mean_q: 3.056712, mean_eps: 0.969755
  84855/1000000: episode: 127, duration: 22.061s, episode steps: 492, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.010608, mae: 2.504776, mean_q: 3.036959, mean_eps: 0.969541
  85588/1000000: episode: 128, duration: 34.670s, episode steps: 733, steps per second:  21, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.012896, mae: 2.523263, mean_q: 3.060606, mean_eps: 0.969321
  86218/1000000: episode: 129, duration: 28.124s, episode steps: 630, steps per second:  22, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012685, mae: 2.498360, mean_q: 3.030354, mean_eps: 0.969075
  86719/1000000: episode: 130, duration: 23.397s, episode steps: 501, steps per second:  21, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.009756, mae: 2.495490, mean_q: 3.025942, mean_eps: 0.968872
  87464/1000000: episode: 131, duration: 34.800s, episode steps: 745, steps per second:  21, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012101, mae: 2.508596, mean_q: 3.042123, mean_eps: 0.968648
  88129/1000000: episode: 132, duration: 30.169s, episode steps: 665, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.011028, mae: 2.526290, mean_q: 3.065718, mean_eps: 0.968393
  88925/1000000: episode: 133, duration: 36.319s, episode steps: 796, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.010960, mae: 2.492143, mean_q: 3.021526, mean_eps: 0.968130
  89614/1000000: episode: 134, duration: 31.016s, episode steps: 689, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.011321, mae: 2.496849, mean_q: 3.026987, mean_eps: 0.967863
  90259/1000000: episode: 135, duration: 30.334s, episode steps: 645, steps per second:  21, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.011940, mae: 2.509012, mean_q: 3.041430, mean_eps: 0.967623
  90988/1000000: episode: 136, duration: 33.070s, episode steps: 729, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.012119, mae: 2.505558, mean_q: 3.036523, mean_eps: 0.967376
  91866/1000000: episode: 137, duration: 39.949s, episode steps: 878, steps per second:  22, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.011714, mae: 2.514910, mean_q: 3.049847, mean_eps: 0.967087
  92266/1000000: episode: 138, duration: 19.203s, episode steps: 400, steps per second:  21, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.011208, mae: 2.502321, mean_q: 3.032545, mean_eps: 0.966856
  92870/1000000: episode: 139, duration: 27.512s, episode steps: 604, steps per second:  22, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014447, mae: 2.542028, mean_q: 3.081869, mean_eps: 0.966676
  93672/1000000: episode: 140, duration: 37.908s, episode steps: 802, steps per second:  21, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.011181, mae: 2.512660, mean_q: 3.047474, mean_eps: 0.966423
  94605/1000000: episode: 141, duration: 42.144s, episode steps: 933, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.012698, mae: 2.514857, mean_q: 3.049021, mean_eps: 0.966110
  95206/1000000: episode: 142, duration: 27.761s, episode steps: 601, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.012654, mae: 2.509570, mean_q: 3.042002, mean_eps: 0.965834
  95663/1000000: episode: 143, duration: 21.340s, episode steps: 457, steps per second:  21, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010830, mae: 2.489923, mean_q: 3.019170, mean_eps: 0.965644
  96480/1000000: episode: 144, duration: 38.185s, episode steps: 817, steps per second:  21, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.011119, mae: 2.531421, mean_q: 3.067688, mean_eps: 0.965415
  97066/1000000: episode: 145, duration: 26.469s, episode steps: 586, steps per second:  22, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.011807, mae: 2.535053, mean_q: 3.073515, mean_eps: 0.965162
  97665/1000000: episode: 146, duration: 27.412s, episode steps: 599, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.012793, mae: 2.512156, mean_q: 3.044254, mean_eps: 0.964948
  98198/1000000: episode: 147, duration: 25.112s, episode steps: 533, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.010272, mae: 2.510906, mean_q: 3.042417, mean_eps: 0.964744
  98957/1000000: episode: 148, duration: 35.189s, episode steps: 759, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.010297, mae: 2.481160, mean_q: 3.008246, mean_eps: 0.964512
  99794/1000000: episode: 149, duration: 38.152s, episode steps: 837, steps per second:  22, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.012095, mae: 2.490927, mean_q: 3.020587, mean_eps: 0.964225
 100552/1000000: episode: 150, duration: 34.812s, episode steps: 758, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.012707, mae: 2.529792, mean_q: 3.066225, mean_eps: 0.963938
 101405/1000000: episode: 151, duration: 39.898s, episode steps: 853, steps per second:  21, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.013994, mae: 2.535263, mean_q: 3.073388, mean_eps: 0.963648
 102083/1000000: episode: 152, duration: 31.411s, episode steps: 678, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012847, mae: 2.516346, mean_q: 3.050315, mean_eps: 0.963372
 102482/1000000: episode: 153, duration: 17.647s, episode steps: 399, steps per second:  23, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.012392, mae: 2.501514, mean_q: 3.033338, mean_eps: 0.963178
 103068/1000000: episode: 154, duration: 27.670s, episode steps: 586, steps per second:  21, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.012253, mae: 2.533283, mean_q: 3.071339, mean_eps: 0.963001
 103721/1000000: episode: 155, duration: 30.277s, episode steps: 653, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.012478, mae: 2.536839, mean_q: 3.075954, mean_eps: 0.962778
 104342/1000000: episode: 156, duration: 27.869s, episode steps: 621, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.014398, mae: 2.520572, mean_q: 3.053780, mean_eps: 0.962548
 104954/1000000: episode: 157, duration: 28.342s, episode steps: 612, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.012111, mae: 2.516615, mean_q: 3.050041, mean_eps: 0.962327
 105632/1000000: episode: 158, duration: 31.423s, episode steps: 678, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.012612, mae: 2.543903, mean_q: 3.082540, mean_eps: 0.962095
 106163/1000000: episode: 159, duration: 24.609s, episode steps: 531, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.012270, mae: 2.514617, mean_q: 3.046822, mean_eps: 0.961877
 107013/1000000: episode: 160, duration: 39.413s, episode steps: 850, steps per second:  22, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012988, mae: 2.524411, mean_q: 3.057498, mean_eps: 0.961628
 107807/1000000: episode: 161, duration: 36.158s, episode steps: 794, steps per second:  22, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.014598, mae: 2.490722, mean_q: 3.016537, mean_eps: 0.961332
 108430/1000000: episode: 162, duration: 29.087s, episode steps: 623, steps per second:  21, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.014479, mae: 2.509156, mean_q: 3.039455, mean_eps: 0.961078
 108959/1000000: episode: 163, duration: 24.566s, episode steps: 529, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.011638, mae: 2.532624, mean_q: 3.070598, mean_eps: 0.960870
 109824/1000000: episode: 164, duration: 39.216s, episode steps: 865, steps per second:  22, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.012437, mae: 2.516972, mean_q: 3.050317, mean_eps: 0.960620
 110409/1000000: episode: 165, duration: 26.932s, episode steps: 585, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013005, mae: 2.539160, mean_q: 3.075965, mean_eps: 0.960358
 110930/1000000: episode: 166, duration: 23.072s, episode steps: 521, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.014096, mae: 2.515168, mean_q: 3.046230, mean_eps: 0.960159
 111609/1000000: episode: 167, duration: 31.656s, episode steps: 679, steps per second:  21, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012005, mae: 2.539866, mean_q: 3.078033, mean_eps: 0.959943
 112259/1000000: episode: 168, duration: 30.236s, episode steps: 650, steps per second:  21, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.013091, mae: 2.515029, mean_q: 3.048913, mean_eps: 0.959704
 112801/1000000: episode: 169, duration: 25.808s, episode steps: 542, steps per second:  21, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.013755, mae: 2.519337, mean_q: 3.054078, mean_eps: 0.959489
 113453/1000000: episode: 170, duration: 30.006s, episode steps: 652, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012370, mae: 2.524673, mean_q: 3.060142, mean_eps: 0.959274
 113848/1000000: episode: 171, duration: 17.604s, episode steps: 395, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.016757, mae: 2.532974, mean_q: 3.068158, mean_eps: 0.959086
 114873/1000000: episode: 172, duration: 47.792s, episode steps: 1025, steps per second:  21, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.012841, mae: 2.504767, mean_q: 3.034281, mean_eps: 0.958830
 115714/1000000: episode: 173, duration: 37.988s, episode steps: 841, steps per second:  22, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012593, mae: 2.523562, mean_q: 3.057530, mean_eps: 0.958494
 116302/1000000: episode: 174, duration: 27.525s, episode steps: 588, steps per second:  21, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.014755, mae: 2.496678, mean_q: 3.024855, mean_eps: 0.958237
 117022/1000000: episode: 175, duration: 33.058s, episode steps: 720, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.011897, mae: 2.493099, mean_q: 3.020062, mean_eps: 0.958002
 117567/1000000: episode: 176, duration: 24.768s, episode steps: 545, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.014458, mae: 2.528513, mean_q: 3.062597, mean_eps: 0.957774
 118430/1000000: episode: 177, duration: 41.342s, episode steps: 863, steps per second:  21, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.011916, mae: 2.536432, mean_q: 3.072811, mean_eps: 0.957521
 119038/1000000: episode: 178, duration: 27.896s, episode steps: 608, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.012415, mae: 2.516702, mean_q: 3.049486, mean_eps: 0.957256
 119744/1000000: episode: 179, duration: 32.839s, episode steps: 706, steps per second:  21, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.013898, mae: 2.503578, mean_q: 3.034886, mean_eps: 0.957020
 120309/1000000: episode: 180, duration: 25.227s, episode steps: 565, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.014964, mae: 2.523236, mean_q: 3.059236, mean_eps: 0.956791
 120925/1000000: episode: 181, duration: 28.380s, episode steps: 616, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.011009, mae: 2.528781, mean_q: 3.064913, mean_eps: 0.956578
 121468/1000000: episode: 182, duration: 25.765s, episode steps: 543, steps per second:  21, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013938, mae: 2.506136, mean_q: 3.039731, mean_eps: 0.956369
 121852/1000000: episode: 183, duration: 17.556s, episode steps: 384, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.015051, mae: 2.519039, mean_q: 3.053742, mean_eps: 0.956203
 122330/1000000: episode: 184, duration: 21.760s, episode steps: 478, steps per second:  22, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012532, mae: 2.484368, mean_q: 3.009912, mean_eps: 0.956048
 123062/1000000: episode: 185, duration: 34.549s, episode steps: 732, steps per second:  21, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.013597, mae: 2.512433, mean_q: 3.045380, mean_eps: 0.955829
 123882/1000000: episode: 186, duration: 38.265s, episode steps: 820, steps per second:  21, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.011789, mae: 2.502217, mean_q: 3.032861, mean_eps: 0.955550
 124512/1000000: episode: 187, duration: 30.794s, episode steps: 630, steps per second:  20, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.011064, mae: 2.500025, mean_q: 3.029402, mean_eps: 0.955289
 124943/1000000: episode: 188, duration: 20.170s, episode steps: 431, steps per second:  21, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011139, mae: 2.501751, mean_q: 3.029184, mean_eps: 0.955099
 125651/1000000: episode: 189, duration: 33.490s, episode steps: 708, steps per second:  21, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.012303, mae: 2.519593, mean_q: 3.053020, mean_eps: 0.954893
 126260/1000000: episode: 190, duration: 28.583s, episode steps: 609, steps per second:  21, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011859, mae: 2.498850, mean_q: 3.028099, mean_eps: 0.954657
 126816/1000000: episode: 191, duration: 26.116s, episode steps: 556, steps per second:  21, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.013298, mae: 2.490045, mean_q: 3.015872, mean_eps: 0.954447
 127403/1000000: episode: 192, duration: 26.516s, episode steps: 587, steps per second:  22, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012975, mae: 2.514544, mean_q: 3.047157, mean_eps: 0.954241
 128079/1000000: episode: 193, duration: 31.435s, episode steps: 676, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.012985, mae: 2.522763, mean_q: 3.056705, mean_eps: 0.954014
 129248/1000000: episode: 194, duration: 54.644s, episode steps: 1169, steps per second:  21, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.013768, mae: 2.515920, mean_q: 3.049672, mean_eps: 0.953682
 129894/1000000: episode: 195, duration: 30.129s, episode steps: 646, steps per second:  21, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012391, mae: 2.493869, mean_q: 3.023380, mean_eps: 0.953355
 130423/1000000: episode: 196, duration: 25.346s, episode steps: 529, steps per second:  21, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.011832, mae: 2.486874, mean_q: 3.014997, mean_eps: 0.953143
 130963/1000000: episode: 197, duration: 24.800s, episode steps: 540, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011334, mae: 2.497890, mean_q: 3.026959, mean_eps: 0.952951
 131460/1000000: episode: 198, duration: 22.763s, episode steps: 497, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.016647, mae: 2.492640, mean_q: 3.019674, mean_eps: 0.952764
 132139/1000000: episode: 199, duration: 31.786s, episode steps: 679, steps per second:  21, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.012381, mae: 2.496562, mean_q: 3.024794, mean_eps: 0.952553
 133236/1000000: episode: 200, duration: 51.740s, episode steps: 1097, steps per second:  21, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.011775, mae: 2.493942, mean_q: 3.022753, mean_eps: 0.952233
 133626/1000000: episode: 201, duration: 17.659s, episode steps: 390, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.011841, mae: 2.500897, mean_q: 3.030469, mean_eps: 0.951965
 134137/1000000: episode: 202, duration: 24.791s, episode steps: 511, steps per second:  21, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011461, mae: 2.515939, mean_q: 3.049657, mean_eps: 0.951802
 134693/1000000: episode: 203, duration: 25.792s, episode steps: 556, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011745, mae: 2.499808, mean_q: 3.031211, mean_eps: 0.951610
 135374/1000000: episode: 204, duration: 31.739s, episode steps: 681, steps per second:  21, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.011536, mae: 2.474838, mean_q: 2.999093, mean_eps: 0.951388
 136157/1000000: episode: 205, duration: 37.272s, episode steps: 783, steps per second:  21, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.011381, mae: 2.508184, mean_q: 3.038379, mean_eps: 0.951124
 136750/1000000: episode: 206, duration: 27.286s, episode steps: 593, steps per second:  22, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.013891, mae: 2.510175, mean_q: 3.040978, mean_eps: 0.950877
 137146/1000000: episode: 207, duration: 18.289s, episode steps: 396, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.010328, mae: 2.467378, mean_q: 2.988067, mean_eps: 0.950699
 137731/1000000: episode: 208, duration: 26.668s, episode steps: 585, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013892, mae: 2.491763, mean_q: 3.020301, mean_eps: 0.950522
 138330/1000000: episode: 209, duration: 28.529s, episode steps: 599, steps per second:  21, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.012092, mae: 2.510368, mean_q: 3.043052, mean_eps: 0.950309
 138719/1000000: episode: 210, duration: 17.702s, episode steps: 389, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.011217, mae: 2.481600, mean_q: 3.010011, mean_eps: 0.950131
 139351/1000000: episode: 211, duration: 29.426s, episode steps: 632, steps per second:  21, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.013001, mae: 2.501218, mean_q: 3.029820, mean_eps: 0.949948
 139906/1000000: episode: 212, duration: 26.496s, episode steps: 555, steps per second:  21, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.013331, mae: 2.489261, mean_q: 3.020704, mean_eps: 0.949734
 140717/1000000: episode: 213, duration: 36.823s, episode steps: 811, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.013045, mae: 2.504732, mean_q: 3.036274, mean_eps: 0.949488
 141383/1000000: episode: 214, duration: 31.200s, episode steps: 666, steps per second:  21, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012611, mae: 2.511597, mean_q: 3.042563, mean_eps: 0.949222
 142173/1000000: episode: 215, duration: 36.623s, episode steps: 790, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013612, mae: 2.506593, mean_q: 3.035721, mean_eps: 0.948960
 142953/1000000: episode: 216, duration: 37.159s, episode steps: 780, steps per second:  21, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013081, mae: 2.503466, mean_q: 3.032539, mean_eps: 0.948677
 143602/1000000: episode: 217, duration: 29.895s, episode steps: 649, steps per second:  22, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.011768, mae: 2.521360, mean_q: 3.056045, mean_eps: 0.948420
 144463/1000000: episode: 218, duration: 40.962s, episode steps: 861, steps per second:  21, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.012990, mae: 2.499395, mean_q: 3.029167, mean_eps: 0.948148
 145305/1000000: episode: 219, duration: 39.613s, episode steps: 842, steps per second:  21, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.011635, mae: 2.504598, mean_q: 3.034267, mean_eps: 0.947842
 145889/1000000: episode: 220, duration: 26.948s, episode steps: 584, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.013231, mae: 2.508463, mean_q: 3.041180, mean_eps: 0.947585
 146296/1000000: episode: 221, duration: 19.141s, episode steps: 407, steps per second:  21, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.012760, mae: 2.483112, mean_q: 3.008476, mean_eps: 0.947407
 147025/1000000: episode: 222, duration: 34.867s, episode steps: 729, steps per second:  21, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.014003, mae: 2.509554, mean_q: 3.039296, mean_eps: 0.947202
 147689/1000000: episode: 223, duration: 30.685s, episode steps: 664, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.014312, mae: 2.484095, mean_q: 3.011995, mean_eps: 0.946951
 148315/1000000: episode: 224, duration: 29.176s, episode steps: 626, steps per second:  21, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011878, mae: 2.506777, mean_q: 3.036583, mean_eps: 0.946719
 148960/1000000: episode: 225, duration: 30.416s, episode steps: 645, steps per second:  21, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.012799, mae: 2.490837, mean_q: 3.016332, mean_eps: 0.946491
 149471/1000000: episode: 226, duration: 26.021s, episode steps: 511, steps per second:  20, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.012015, mae: 2.504310, mean_q: 3.033359, mean_eps: 0.946283
 150267/1000000: episode: 227, duration: 36.716s, episode steps: 796, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011250, mae: 2.528814, mean_q: 3.065923, mean_eps: 0.946048
 151005/1000000: episode: 228, duration: 34.652s, episode steps: 738, steps per second:  21, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014022, mae: 2.528390, mean_q: 3.065986, mean_eps: 0.945771
 152083/1000000: episode: 229, duration: 52.860s, episode steps: 1078, steps per second:  20, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.013685, mae: 2.516977, mean_q: 3.049544, mean_eps: 0.945444
 152972/1000000: episode: 230, duration: 41.835s, episode steps: 889, steps per second:  21, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.012893, mae: 2.520195, mean_q: 3.053345, mean_eps: 0.945091
 153680/1000000: episode: 231, duration: 33.201s, episode steps: 708, steps per second:  21, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.012729, mae: 2.515730, mean_q: 3.049337, mean_eps: 0.944803
 154737/1000000: episode: 232, duration: 52.944s, episode steps: 1057, steps per second:  20, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012416, mae: 2.511196, mean_q: 3.041792, mean_eps: 0.944485
 155618/1000000: episode: 233, duration: 40.824s, episode steps: 881, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.013285, mae: 2.495693, mean_q: 3.024742, mean_eps: 0.944136
 156441/1000000: episode: 234, duration: 39.054s, episode steps: 823, steps per second:  21, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012920, mae: 2.518520, mean_q: 3.049819, mean_eps: 0.943829
 157422/1000000: episode: 235, duration: 46.580s, episode steps: 981, steps per second:  21, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.013262, mae: 2.491863, mean_q: 3.017637, mean_eps: 0.943504
 157821/1000000: episode: 236, duration: 19.240s, episode steps: 399, steps per second:  21, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.011247, mae: 2.531044, mean_q: 3.067761, mean_eps: 0.943256
 158693/1000000: episode: 237, duration: 40.760s, episode steps: 872, steps per second:  21, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.011902, mae: 2.499797, mean_q: 3.028638, mean_eps: 0.943027
 159253/1000000: episode: 238, duration: 27.372s, episode steps: 560, steps per second:  20, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.013742, mae: 2.489917, mean_q: 3.012583, mean_eps: 0.942769
 159868/1000000: episode: 239, duration: 32.040s, episode steps: 615, steps per second:  19, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.013103, mae: 2.510924, mean_q: 3.042150, mean_eps: 0.942558
 160908/1000000: episode: 240, duration: 49.152s, episode steps: 1040, steps per second:  21, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.012418, mae: 2.500636, mean_q: 3.030033, mean_eps: 0.942261
 161659/1000000: episode: 241, duration: 35.545s, episode steps: 751, steps per second:  21, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.012210, mae: 2.517027, mean_q: 3.047436, mean_eps: 0.941938
 162904/1000000: episode: 242, duration: 62.377s, episode steps: 1245, steps per second:  20, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.011808, mae: 2.500721, mean_q: 3.029442, mean_eps: 0.941579
 163994/1000000: episode: 243, duration: 51.383s, episode steps: 1090, steps per second:  21, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013636, mae: 2.513828, mean_q: 3.043572, mean_eps: 0.941159
 164756/1000000: episode: 244, duration: 41.292s, episode steps: 762, steps per second:  18, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.015194, mae: 2.500635, mean_q: 3.026205, mean_eps: 0.940825
 165631/1000000: episode: 245, duration: 41.517s, episode steps: 875, steps per second:  21, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014243, mae: 2.514584, mean_q: 3.044830, mean_eps: 0.940531
 166692/1000000: episode: 246, duration: 50.980s, episode steps: 1061, steps per second:  21, episode reward:  6.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013002, mae: 2.517466, mean_q: 3.049314, mean_eps: 0.940182
 167403/1000000: episode: 247, duration: 38.355s, episode steps: 711, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.011654, mae: 2.487413, mean_q: 3.010600, mean_eps: 0.939863
 168187/1000000: episode: 248, duration: 38.254s, episode steps: 784, steps per second:  20, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011215, mae: 2.527650, mean_q: 3.059542, mean_eps: 0.939594
 168840/1000000: episode: 249, duration: 32.547s, episode steps: 653, steps per second:  20, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.014244, mae: 2.519133, mean_q: 3.049605, mean_eps: 0.939336
 169317/1000000: episode: 250, duration: 24.364s, episode steps: 477, steps per second:  20, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.011771, mae: 2.522258, mean_q: 3.054011, mean_eps: 0.939132
 169724/1000000: episode: 251, duration: 22.262s, episode steps: 407, steps per second:  18, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014903, mae: 2.513455, mean_q: 3.040622, mean_eps: 0.938973
 170662/1000000: episode: 252, duration: 47.055s, episode steps: 938, steps per second:  20, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.013397, mae: 2.519192, mean_q: 3.048540, mean_eps: 0.938731
 171544/1000000: episode: 253, duration: 42.945s, episode steps: 882, steps per second:  21, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.013264, mae: 2.526775, mean_q: 3.058598, mean_eps: 0.938403
 172263/1000000: episode: 254, duration: 39.369s, episode steps: 719, steps per second:  18, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013671, mae: 2.488634, mean_q: 3.012273, mean_eps: 0.938115
 173350/1000000: episode: 255, duration: 54.380s, episode steps: 1087, steps per second:  20, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.013549, mae: 2.509791, mean_q: 3.039593, mean_eps: 0.937790
 174679/1000000: episode: 256, duration: 70.218s, episode steps: 1329, steps per second:  19, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012592, mae: 2.510685, mean_q: 3.041760, mean_eps: 0.937355
 175808/1000000: episode: 257, duration: 54.714s, episode steps: 1129, steps per second:  21, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.012256, mae: 2.509395, mean_q: 3.039538, mean_eps: 0.936913
 176267/1000000: episode: 258, duration: 22.010s, episode steps: 459, steps per second:  21, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.013257, mae: 2.504353, mean_q: 3.032806, mean_eps: 0.936627
 176663/1000000: episode: 259, duration: 20.075s, episode steps: 396, steps per second:  20, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.016146, mae: 2.519743, mean_q: 3.049819, mean_eps: 0.936473
 177323/1000000: episode: 260, duration: 34.464s, episode steps: 660, steps per second:  19, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.013561, mae: 2.513749, mean_q: 3.043574, mean_eps: 0.936283
 177730/1000000: episode: 261, duration: 20.674s, episode steps: 407, steps per second:  20, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.013338, mae: 2.501406, mean_q: 3.027889, mean_eps: 0.936091
 178348/1000000: episode: 262, duration: 29.686s, episode steps: 618, steps per second:  21, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012668, mae: 2.523414, mean_q: 3.055178, mean_eps: 0.935906
 179443/1000000: episode: 263, duration: 57.213s, episode steps: 1095, steps per second:  19, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.012697, mae: 2.500041, mean_q: 3.028089, mean_eps: 0.935598
 180270/1000000: episode: 264, duration: 44.986s, episode steps: 827, steps per second:  18, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.011812, mae: 2.517312, mean_q: 3.049448, mean_eps: 0.935252
 181303/1000000: episode: 265, duration: 58.348s, episode steps: 1033, steps per second:  18, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.013186, mae: 2.510089, mean_q: 3.040315, mean_eps: 0.934917
 182570/1000000: episode: 266, duration: 70.654s, episode steps: 1267, steps per second:  18, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013760, mae: 2.503384, mean_q: 3.032175, mean_eps: 0.934503
 183146/1000000: episode: 267, duration: 32.381s, episode steps: 576, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.012723, mae: 2.510316, mean_q: 3.040174, mean_eps: 0.934171
 184331/1000000: episode: 268, duration: 66.411s, episode steps: 1185, steps per second:  18, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.012742, mae: 2.509995, mean_q: 3.039180, mean_eps: 0.933854
 184852/1000000: episode: 269, duration: 29.802s, episode steps: 521, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.013381, mae: 2.514667, mean_q: 3.043638, mean_eps: 0.933548
 185226/1000000: episode: 270, duration: 21.346s, episode steps: 374, steps per second:  18, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011558, mae: 2.516100, mean_q: 3.048006, mean_eps: 0.933386
 185864/1000000: episode: 271, duration: 36.653s, episode steps: 638, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014278, mae: 2.531776, mean_q: 3.064130, mean_eps: 0.933204
 186506/1000000: episode: 272, duration: 36.118s, episode steps: 642, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.010830, mae: 2.487220, mean_q: 3.013075, mean_eps: 0.932974
 187219/1000000: episode: 273, duration: 39.671s, episode steps: 713, steps per second:  18, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.012119, mae: 2.495071, mean_q: 3.021879, mean_eps: 0.932730
 188182/1000000: episode: 274, duration: 54.376s, episode steps: 963, steps per second:  18, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.012726, mae: 2.494254, mean_q: 3.020214, mean_eps: 0.932428
 188697/1000000: episode: 275, duration: 29.222s, episode steps: 515, steps per second:  18, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.014752, mae: 2.518051, mean_q: 3.048970, mean_eps: 0.932162
 189111/1000000: episode: 276, duration: 22.910s, episode steps: 414, steps per second:  18, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012869, mae: 2.501050, mean_q: 3.028487, mean_eps: 0.931995
 189833/1000000: episode: 277, duration: 41.149s, episode steps: 722, steps per second:  18, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.012406, mae: 2.506071, mean_q: 3.035340, mean_eps: 0.931790
 190442/1000000: episode: 278, duration: 34.890s, episode steps: 609, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.012060, mae: 2.501333, mean_q: 3.028042, mean_eps: 0.931550
 191267/1000000: episode: 279, duration: 47.131s, episode steps: 825, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.015034, mae: 2.497219, mean_q: 3.023943, mean_eps: 0.931293
 192061/1000000: episode: 280, duration: 45.252s, episode steps: 794, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.014312, mae: 2.513605, mean_q: 3.043084, mean_eps: 0.931001
 192718/1000000: episode: 281, duration: 37.083s, episode steps: 657, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.014680, mae: 2.509936, mean_q: 3.037067, mean_eps: 0.930740
 193215/1000000: episode: 282, duration: 27.991s, episode steps: 497, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.012816, mae: 2.505356, mean_q: 3.033572, mean_eps: 0.930532
 193810/1000000: episode: 283, duration: 33.571s, episode steps: 595, steps per second:  18, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.011799, mae: 2.498483, mean_q: 3.025290, mean_eps: 0.930336
 194684/1000000: episode: 284, duration: 49.289s, episode steps: 874, steps per second:  18, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013754, mae: 2.508196, mean_q: 3.037761, mean_eps: 0.930071
 195878/1000000: episode: 285, duration: 68.813s, episode steps: 1194, steps per second:  17, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.013522, mae: 2.516267, mean_q: 3.047866, mean_eps: 0.929699
 196587/1000000: episode: 286, duration: 41.140s, episode steps: 709, steps per second:  17, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.013401, mae: 2.517623, mean_q: 3.047649, mean_eps: 0.929356
 196959/1000000: episode: 287, duration: 21.298s, episode steps: 372, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.011691, mae: 2.502032, mean_q: 3.030161, mean_eps: 0.929162
 197692/1000000: episode: 288, duration: 41.861s, episode steps: 733, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012452, mae: 2.498085, mean_q: 3.027306, mean_eps: 0.928963
 198355/1000000: episode: 289, duration: 36.325s, episode steps: 663, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.014104, mae: 2.511369, mean_q: 3.041470, mean_eps: 0.928712
 198939/1000000: episode: 290, duration: 32.955s, episode steps: 584, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.012374, mae: 2.515243, mean_q: 3.046279, mean_eps: 0.928487
 199733/1000000: episode: 291, duration: 45.299s, episode steps: 794, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.012324, mae: 2.516654, mean_q: 3.047319, mean_eps: 0.928239
 200352/1000000: episode: 292, duration: 34.634s, episode steps: 619, steps per second:  18, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.014309, mae: 2.536384, mean_q: 3.072790, mean_eps: 0.927985
 200977/1000000: episode: 293, duration: 36.147s, episode steps: 625, steps per second:  17, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.014573, mae: 2.551576, mean_q: 3.089352, mean_eps: 0.927761
 201915/1000000: episode: 294, duration: 55.094s, episode steps: 938, steps per second:  17, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.013337, mae: 2.539391, mean_q: 3.076130, mean_eps: 0.927479
 202848/1000000: episode: 295, duration: 53.312s, episode steps: 933, steps per second:  18, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013420, mae: 2.543148, mean_q: 3.080895, mean_eps: 0.927143
 203453/1000000: episode: 296, duration: 34.434s, episode steps: 605, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.013009, mae: 2.515551, mean_q: 3.046992, mean_eps: 0.926866
 204122/1000000: episode: 297, duration: 38.224s, episode steps: 669, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.012179, mae: 2.527488, mean_q: 3.062147, mean_eps: 0.926636
 204624/1000000: episode: 298, duration: 28.407s, episode steps: 502, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.017376, mae: 2.518824, mean_q: 3.048042, mean_eps: 0.926426
 205445/1000000: episode: 299, duration: 46.827s, episode steps: 821, steps per second:  18, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.015291, mae: 2.515661, mean_q: 3.044079, mean_eps: 0.926188
 206141/1000000: episode: 300, duration: 40.316s, episode steps: 696, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.015406, mae: 2.535770, mean_q: 3.070067, mean_eps: 0.925914
 206839/1000000: episode: 301, duration: 39.185s, episode steps: 698, steps per second:  18, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.012806, mae: 2.526163, mean_q: 3.060951, mean_eps: 0.925664
 207583/1000000: episode: 302, duration: 42.977s, episode steps: 744, steps per second:  17, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013767, mae: 2.531659, mean_q: 3.065170, mean_eps: 0.925404
 208560/1000000: episode: 303, duration: 55.506s, episode steps: 977, steps per second:  18, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.013066, mae: 2.523384, mean_q: 3.056072, mean_eps: 0.925095
 209084/1000000: episode: 304, duration: 29.221s, episode steps: 524, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.016519, mae: 2.547318, mean_q: 3.084673, mean_eps: 0.924825
 209499/1000000: episode: 305, duration: 24.154s, episode steps: 415, steps per second:  17, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.014259, mae: 2.527379, mean_q: 3.060785, mean_eps: 0.924656
 209894/1000000: episode: 306, duration: 22.202s, episode steps: 395, steps per second:  18, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015506, mae: 2.533059, mean_q: 3.071128, mean_eps: 0.924509
 210430/1000000: episode: 307, duration: 30.734s, episode steps: 536, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.010908, mae: 2.524534, mean_q: 3.058583, mean_eps: 0.924342
 211051/1000000: episode: 308, duration: 35.302s, episode steps: 621, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.016379, mae: 2.518178, mean_q: 3.048868, mean_eps: 0.924134
 211703/1000000: episode: 309, duration: 38.463s, episode steps: 652, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.013816, mae: 2.533168, mean_q: 3.071002, mean_eps: 0.923905
 212470/1000000: episode: 310, duration: 43.244s, episode steps: 767, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.014019, mae: 2.537380, mean_q: 3.072290, mean_eps: 0.923649
 213256/1000000: episode: 311, duration: 44.666s, episode steps: 786, steps per second:  18, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.013644, mae: 2.511303, mean_q: 3.039942, mean_eps: 0.923370
 213641/1000000: episode: 312, duration: 21.874s, episode steps: 385, steps per second:  18, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.011674, mae: 2.515653, mean_q: 3.045349, mean_eps: 0.923159
 214578/1000000: episode: 313, duration: 52.993s, episode steps: 937, steps per second:  18, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012973, mae: 2.529408, mean_q: 3.062620, mean_eps: 0.922920
 215273/1000000: episode: 314, duration: 38.683s, episode steps: 695, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.010867, mae: 2.530648, mean_q: 3.063149, mean_eps: 0.922627
 215935/1000000: episode: 315, duration: 37.983s, episode steps: 662, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.014020, mae: 2.547795, mean_q: 3.083834, mean_eps: 0.922383
 216317/1000000: episode: 316, duration: 21.649s, episode steps: 382, steps per second:  18, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.014034, mae: 2.470186, mean_q: 2.988132, mean_eps: 0.922195
 216973/1000000: episode: 317, duration: 37.845s, episode steps: 656, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012572, mae: 2.549680, mean_q: 3.086457, mean_eps: 0.922007
 217672/1000000: episode: 318, duration: 39.941s, episode steps: 699, steps per second:  18, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.013078, mae: 2.493242, mean_q: 3.018830, mean_eps: 0.921764
 218643/1000000: episode: 319, duration: 54.458s, episode steps: 971, steps per second:  18, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.014037, mae: 2.525708, mean_q: 3.056890, mean_eps: 0.921464
 219358/1000000: episode: 320, duration: 41.233s, episode steps: 715, steps per second:  17, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.014486, mae: 2.508791, mean_q: 3.035752, mean_eps: 0.921160
 219790/1000000: episode: 321, duration: 24.111s, episode steps: 432, steps per second:  18, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.014228, mae: 2.535735, mean_q: 3.069187, mean_eps: 0.920953
 220313/1000000: episode: 322, duration: 29.750s, episode steps: 523, steps per second:  18, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013169, mae: 2.520666, mean_q: 3.050600, mean_eps: 0.920781
 221013/1000000: episode: 323, duration: 40.792s, episode steps: 700, steps per second:  17, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.012812, mae: 2.520372, mean_q: 3.051723, mean_eps: 0.920561
 221732/1000000: episode: 324, duration: 40.920s, episode steps: 719, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.012153, mae: 2.537805, mean_q: 3.074653, mean_eps: 0.920306
 222221/1000000: episode: 325, duration: 28.575s, episode steps: 489, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.014478, mae: 2.541120, mean_q: 3.074836, mean_eps: 0.920089
 222824/1000000: episode: 326, duration: 34.308s, episode steps: 603, steps per second:  18, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.013998, mae: 2.519167, mean_q: 3.049266, mean_eps: 0.919892
 223404/1000000: episode: 327, duration: 33.519s, episode steps: 580, steps per second:  17, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013332, mae: 2.529669, mean_q: 3.061893, mean_eps: 0.919680
 224111/1000000: episode: 328, duration: 40.935s, episode steps: 707, steps per second:  17, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013608, mae: 2.526103, mean_q: 3.058562, mean_eps: 0.919448
 224504/1000000: episode: 329, duration: 22.345s, episode steps: 393, steps per second:  18, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.017636, mae: 2.539752, mean_q: 3.075397, mean_eps: 0.919250
 225130/1000000: episode: 330, duration: 36.239s, episode steps: 626, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.014535, mae: 2.503190, mean_q: 3.031347, mean_eps: 0.919066
 225645/1000000: episode: 331, duration: 29.162s, episode steps: 515, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.012903, mae: 2.530138, mean_q: 3.063399, mean_eps: 0.918860
 226296/1000000: episode: 332, duration: 37.960s, episode steps: 651, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.014283, mae: 2.540996, mean_q: 3.076969, mean_eps: 0.918651
 227091/1000000: episode: 333, duration: 45.512s, episode steps: 795, steps per second:  17, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.013589, mae: 2.505779, mean_q: 3.033493, mean_eps: 0.918391
 227849/1000000: episode: 334, duration: 43.499s, episode steps: 758, steps per second:  17, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.014457, mae: 2.524228, mean_q: 3.054352, mean_eps: 0.918111
 228474/1000000: episode: 335, duration: 36.213s, episode steps: 625, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012790, mae: 2.529899, mean_q: 3.063173, mean_eps: 0.917862
 229416/1000000: episode: 336, duration: 54.188s, episode steps: 942, steps per second:  17, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012604, mae: 2.513279, mean_q: 3.041519, mean_eps: 0.917580
 229915/1000000: episode: 337, duration: 28.479s, episode steps: 499, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.015400, mae: 2.530067, mean_q: 3.062725, mean_eps: 0.917321
 230800/1000000: episode: 338, duration: 50.693s, episode steps: 885, steps per second:  17, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014029, mae: 2.508465, mean_q: 3.036089, mean_eps: 0.917072
 231661/1000000: episode: 339, duration: 49.394s, episode steps: 861, steps per second:  17, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.011542, mae: 2.496218, mean_q: 3.022577, mean_eps: 0.916757
 232038/1000000: episode: 340, duration: 20.949s, episode steps: 377, steps per second:  18, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.014072, mae: 2.531312, mean_q: 3.065681, mean_eps: 0.916534
 232653/1000000: episode: 341, duration: 35.938s, episode steps: 615, steps per second:  17, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.013827, mae: 2.531550, mean_q: 3.064314, mean_eps: 0.916355
 233348/1000000: episode: 342, duration: 39.616s, episode steps: 695, steps per second:  18, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.012475, mae: 2.508569, mean_q: 3.038615, mean_eps: 0.916120
 233890/1000000: episode: 343, duration: 31.167s, episode steps: 542, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.013111, mae: 2.506615, mean_q: 3.034637, mean_eps: 0.915898
 234502/1000000: episode: 344, duration: 34.473s, episode steps: 612, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.014405, mae: 2.524039, mean_q: 3.057326, mean_eps: 0.915689
 235065/1000000: episode: 345, duration: 32.377s, episode steps: 563, steps per second:  17, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.015145, mae: 2.537022, mean_q: 3.071408, mean_eps: 0.915478
 235743/1000000: episode: 346, duration: 39.495s, episode steps: 678, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.012596, mae: 2.521723, mean_q: 3.053272, mean_eps: 0.915255
 236260/1000000: episode: 347, duration: 29.444s, episode steps: 517, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.011842, mae: 2.523142, mean_q: 3.055255, mean_eps: 0.915040
 236894/1000000: episode: 348, duration: 37.169s, episode steps: 634, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.013574, mae: 2.513732, mean_q: 3.042493, mean_eps: 0.914833
 237888/1000000: episode: 349, duration: 56.727s, episode steps: 994, steps per second:  18, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.013172, mae: 2.513818, mean_q: 3.046007, mean_eps: 0.914540
 238644/1000000: episode: 350, duration: 42.882s, episode steps: 756, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012923, mae: 2.533189, mean_q: 3.066944, mean_eps: 0.914225
 239240/1000000: episode: 351, duration: 30.799s, episode steps: 596, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013010, mae: 2.511439, mean_q: 3.039735, mean_eps: 0.913982
 240218/1000000: episode: 352, duration: 52.468s, episode steps: 978, steps per second:  19, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.014649, mae: 2.529893, mean_q: 3.060773, mean_eps: 0.913698
 240735/1000000: episode: 353, duration: 30.106s, episode steps: 517, steps per second:  17, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.013966, mae: 2.547330, mean_q: 3.085169, mean_eps: 0.913429
 241156/1000000: episode: 354, duration: 24.903s, episode steps: 421, steps per second:  17, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.013290, mae: 2.506877, mean_q: 3.036012, mean_eps: 0.913260
 241532/1000000: episode: 355, duration: 21.260s, episode steps: 376, steps per second:  18, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.012340, mae: 2.494950, mean_q: 3.019481, mean_eps: 0.913117
 241969/1000000: episode: 356, duration: 25.263s, episode steps: 437, steps per second:  17, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.013996, mae: 2.492603, mean_q: 3.015623, mean_eps: 0.912970
 242555/1000000: episode: 357, duration: 33.691s, episode steps: 586, steps per second:  17, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.013593, mae: 2.512708, mean_q: 3.040824, mean_eps: 0.912786
 243257/1000000: episode: 358, duration: 40.940s, episode steps: 702, steps per second:  17, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012427, mae: 2.506689, mean_q: 3.033996, mean_eps: 0.912554
 243796/1000000: episode: 359, duration: 30.784s, episode steps: 539, steps per second:  18, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.010578, mae: 2.493093, mean_q: 3.016640, mean_eps: 0.912331
 244342/1000000: episode: 360, duration: 30.966s, episode steps: 546, steps per second:  18, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.013922, mae: 2.526932, mean_q: 3.058413, mean_eps: 0.912136
 244799/1000000: episode: 361, duration: 26.673s, episode steps: 457, steps per second:  17, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.013652, mae: 2.500670, mean_q: 3.027031, mean_eps: 0.911955
 245513/1000000: episode: 362, duration: 40.112s, episode steps: 714, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.014126, mae: 2.510393, mean_q: 3.038717, mean_eps: 0.911744
 246547/1000000: episode: 363, duration: 59.715s, episode steps: 1034, steps per second:  17, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012143, mae: 2.515115, mean_q: 3.045334, mean_eps: 0.911429
 247161/1000000: episode: 364, duration: 35.185s, episode steps: 614, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011940, mae: 2.531635, mean_q: 3.065699, mean_eps: 0.911133
 247794/1000000: episode: 365, duration: 36.597s, episode steps: 633, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.010889, mae: 2.524889, mean_q: 3.057335, mean_eps: 0.910908
 248182/1000000: episode: 366, duration: 22.296s, episode steps: 388, steps per second:  17, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.014744, mae: 2.484522, mean_q: 3.004507, mean_eps: 0.910724
 248748/1000000: episode: 367, duration: 32.424s, episode steps: 566, steps per second:  17, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.011622, mae: 2.517498, mean_q: 3.046939, mean_eps: 0.910553
 249434/1000000: episode: 368, duration: 39.933s, episode steps: 686, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.014256, mae: 2.491842, mean_q: 3.015035, mean_eps: 0.910328
 250102/1000000: episode: 369, duration: 38.155s, episode steps: 668, steps per second:  18, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.015215, mae: 2.534259, mean_q: 3.068330, mean_eps: 0.910084
 250781/1000000: episode: 370, duration: 38.261s, episode steps: 679, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013518, mae: 2.547187, mean_q: 3.085860, mean_eps: 0.909841
 251275/1000000: episode: 371, duration: 29.059s, episode steps: 494, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.015006, mae: 2.515074, mean_q: 3.045941, mean_eps: 0.909630
 251832/1000000: episode: 372, duration: 32.098s, episode steps: 557, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.014886, mae: 2.523447, mean_q: 3.057365, mean_eps: 0.909441
 252592/1000000: episode: 373, duration: 44.094s, episode steps: 760, steps per second:  17, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.014091, mae: 2.545175, mean_q: 3.082163, mean_eps: 0.909204
 253132/1000000: episode: 374, duration: 31.634s, episode steps: 540, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.015471, mae: 2.544868, mean_q: 3.081182, mean_eps: 0.908970
 253898/1000000: episode: 375, duration: 43.621s, episode steps: 766, steps per second:  18, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.014369, mae: 2.520411, mean_q: 3.050369, mean_eps: 0.908735
 254363/1000000: episode: 376, duration: 26.620s, episode steps: 465, steps per second:  17, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.014259, mae: 2.549179, mean_q: 3.086205, mean_eps: 0.908513
 254963/1000000: episode: 377, duration: 31.786s, episode steps: 600, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.014127, mae: 2.523969, mean_q: 3.054946, mean_eps: 0.908322
 255716/1000000: episode: 378, duration: 42.989s, episode steps: 753, steps per second:  18, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014363, mae: 2.528386, mean_q: 3.060211, mean_eps: 0.908078
 256368/1000000: episode: 379, duration: 36.709s, episode steps: 652, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.015350, mae: 2.561958, mean_q: 3.099857, mean_eps: 0.907826
 256951/1000000: episode: 380, duration: 32.699s, episode steps: 583, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.016270, mae: 2.525173, mean_q: 3.055408, mean_eps: 0.907603
 257662/1000000: episode: 381, duration: 41.178s, episode steps: 711, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.013074, mae: 2.549743, mean_q: 3.085657, mean_eps: 0.907370
 258481/1000000: episode: 382, duration: 47.284s, episode steps: 819, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.012988, mae: 2.556077, mean_q: 3.093547, mean_eps: 0.907094
 259851/1000000: episode: 383, duration: 78.563s, episode steps: 1370, steps per second:  17, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.014275, mae: 2.519794, mean_q: 3.048055, mean_eps: 0.906700
 260477/1000000: episode: 384, duration: 32.106s, episode steps: 626, steps per second:  19, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.013351, mae: 2.528008, mean_q: 3.059962, mean_eps: 0.906341
 261155/1000000: episode: 385, duration: 34.561s, episode steps: 678, steps per second:  20, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.016849, mae: 2.541131, mean_q: 3.076154, mean_eps: 0.906106
 261715/1000000: episode: 386, duration: 32.523s, episode steps: 560, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.014938, mae: 2.532669, mean_q: 3.068220, mean_eps: 0.905884
 262786/1000000: episode: 387, duration: 61.171s, episode steps: 1071, steps per second:  18, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.014350, mae: 2.531358, mean_q: 3.066719, mean_eps: 0.905590
 263566/1000000: episode: 388, duration: 44.737s, episode steps: 780, steps per second:  17, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.014676, mae: 2.533522, mean_q: 3.065958, mean_eps: 0.905257
 264024/1000000: episode: 389, duration: 26.582s, episode steps: 458, steps per second:  17, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013754, mae: 2.518198, mean_q: 3.047711, mean_eps: 0.905034
 265055/1000000: episode: 390, duration: 60.101s, episode steps: 1031, steps per second:  17, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012551, mae: 2.529655, mean_q: 3.060561, mean_eps: 0.904766
 265461/1000000: episode: 391, duration: 23.019s, episode steps: 406, steps per second:  18, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.012484, mae: 2.512385, mean_q: 3.043251, mean_eps: 0.904507
 266149/1000000: episode: 392, duration: 40.152s, episode steps: 688, steps per second:  17, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.013404, mae: 2.545147, mean_q: 3.080385, mean_eps: 0.904310
 267097/1000000: episode: 393, duration: 53.909s, episode steps: 948, steps per second:  18, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.013772, mae: 2.524479, mean_q: 3.054819, mean_eps: 0.904015
 267918/1000000: episode: 394, duration: 46.800s, episode steps: 821, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.012988, mae: 2.538893, mean_q: 3.073189, mean_eps: 0.903697
 268476/1000000: episode: 395, duration: 32.389s, episode steps: 558, steps per second:  17, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.012817, mae: 2.545235, mean_q: 3.083493, mean_eps: 0.903449
 269111/1000000: episode: 396, duration: 34.910s, episode steps: 635, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.013280, mae: 2.516606, mean_q: 3.045937, mean_eps: 0.903235
 270610/1000000: episode: 397, duration: 80.207s, episode steps: 1499, steps per second:  19, episode reward: 16.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.012612, mae: 2.518373, mean_q: 3.048125, mean_eps: 0.902850
 271066/1000000: episode: 398, duration: 26.050s, episode steps: 456, steps per second:  18, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013628, mae: 2.547551, mean_q: 3.083082, mean_eps: 0.902498
 271567/1000000: episode: 399, duration: 25.139s, episode steps: 501, steps per second:  20, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012078, mae: 2.536204, mean_q: 3.071885, mean_eps: 0.902326
 272056/1000000: episode: 400, duration: 23.670s, episode steps: 489, steps per second:  21, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.015472, mae: 2.523146, mean_q: 3.053929, mean_eps: 0.902148
 272894/1000000: episode: 401, duration: 42.983s, episode steps: 838, steps per second:  19, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012789, mae: 2.535810, mean_q: 3.068911, mean_eps: 0.901909
 273440/1000000: episode: 402, duration: 31.607s, episode steps: 546, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013054, mae: 2.513214, mean_q: 3.040400, mean_eps: 0.901660
 274377/1000000: episode: 403, duration: 60.631s, episode steps: 937, steps per second:  15, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.014424, mae: 2.533056, mean_q: 3.067552, mean_eps: 0.901393
 275184/1000000: episode: 404, duration: 48.113s, episode steps: 807, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.014018, mae: 2.528531, mean_q: 3.060596, mean_eps: 0.901079
 275777/1000000: episode: 405, duration: 34.696s, episode steps: 593, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011980, mae: 2.525839, mean_q: 3.056992, mean_eps: 0.900827
 276247/1000000: episode: 406, duration: 26.632s, episode steps: 470, steps per second:  18, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.014856, mae: 2.525400, mean_q: 3.053701, mean_eps: 0.900636
 277264/1000000: episode: 407, duration: 58.548s, episode steps: 1017, steps per second:  17, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.014088, mae: 2.534029, mean_q: 3.065779, mean_eps: 0.900369
 277955/1000000: episode: 408, duration: 35.694s, episode steps: 691, steps per second:  19, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013008, mae: 2.509469, mean_q: 3.035106, mean_eps: 0.900061
 278419/1000000: episode: 409, duration: 23.069s, episode steps: 464, steps per second:  20, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.013360, mae: 2.527565, mean_q: 3.059763, mean_eps: 0.899853
 278810/1000000: episode: 410, duration: 19.289s, episode steps: 391, steps per second:  20, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.013670, mae: 2.515654, mean_q: 3.044481, mean_eps: 0.899699
 279432/1000000: episode: 411, duration: 30.814s, episode steps: 622, steps per second:  20, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.012370, mae: 2.529609, mean_q: 3.062588, mean_eps: 0.899517
 280301/1000000: episode: 412, duration: 48.225s, episode steps: 869, steps per second:  18, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010819, mae: 2.505901, mean_q: 3.032886, mean_eps: 0.899248
 280874/1000000: episode: 413, duration: 28.464s, episode steps: 573, steps per second:  20, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.013904, mae: 2.534394, mean_q: 3.067953, mean_eps: 0.898988
 281349/1000000: episode: 414, duration: 24.076s, episode steps: 475, steps per second:  20, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.013455, mae: 2.513037, mean_q: 3.041254, mean_eps: 0.898800
 282158/1000000: episode: 415, duration: 43.234s, episode steps: 809, steps per second:  19, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.013017, mae: 2.519752, mean_q: 3.051390, mean_eps: 0.898569
 282693/1000000: episode: 416, duration: 31.241s, episode steps: 535, steps per second:  17, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.012113, mae: 2.503903, mean_q: 3.030048, mean_eps: 0.898327
 283370/1000000: episode: 417, duration: 38.458s, episode steps: 677, steps per second:  18, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.011633, mae: 2.528224, mean_q: 3.059324, mean_eps: 0.898108
 283922/1000000: episode: 418, duration: 31.795s, episode steps: 552, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.014443, mae: 2.508023, mean_q: 3.035826, mean_eps: 0.897887
 284881/1000000: episode: 419, duration: 55.543s, episode steps: 959, steps per second:  17, episode reward:  9.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013636, mae: 2.515089, mean_q: 3.044061, mean_eps: 0.897615
 285550/1000000: episode: 420, duration: 38.930s, episode steps: 669, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013669, mae: 2.525632, mean_q: 3.057326, mean_eps: 0.897322
 286389/1000000: episode: 421, duration: 48.954s, episode steps: 839, steps per second:  17, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.014651, mae: 2.528870, mean_q: 3.061080, mean_eps: 0.897051
 287597/1000000: episode: 422, duration: 67.852s, episode steps: 1208, steps per second:  18, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.014465, mae: 2.520569, mean_q: 3.049075, mean_eps: 0.896682
 288147/1000000: episode: 423, duration: 31.668s, episode steps: 550, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.013573, mae: 2.509928, mean_q: 3.037383, mean_eps: 0.896366
 288863/1000000: episode: 424, duration: 41.986s, episode steps: 716, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.013864, mae: 2.515004, mean_q: 3.042039, mean_eps: 0.896139
 289952/1000000: episode: 425, duration: 64.850s, episode steps: 1089, steps per second:  17, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.014940, mae: 2.509864, mean_q: 3.034898, mean_eps: 0.895814
 290574/1000000: episode: 426, duration: 36.034s, episode steps: 622, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.012947, mae: 2.527238, mean_q: 3.057508, mean_eps: 0.895506
 291531/1000000: episode: 427, duration: 56.682s, episode steps: 957, steps per second:  17, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.014114, mae: 2.553214, mean_q: 3.088603, mean_eps: 0.895221
 292160/1000000: episode: 428, duration: 36.676s, episode steps: 629, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.013115, mae: 2.550562, mean_q: 3.087436, mean_eps: 0.894936
 292765/1000000: episode: 429, duration: 34.936s, episode steps: 605, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.012673, mae: 2.526963, mean_q: 3.056985, mean_eps: 0.894714
 293547/1000000: episode: 430, duration: 45.783s, episode steps: 782, steps per second:  17, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.013524, mae: 2.508050, mean_q: 3.034673, mean_eps: 0.894464
 294439/1000000: episode: 431, duration: 50.964s, episode steps: 892, steps per second:  18, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.016350, mae: 2.522514, mean_q: 3.050709, mean_eps: 0.894163
 295085/1000000: episode: 432, duration: 39.037s, episode steps: 646, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.013049, mae: 2.530558, mean_q: 3.060827, mean_eps: 0.893886
 295530/1000000: episode: 433, duration: 24.990s, episode steps: 445, steps per second:  18, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.014409, mae: 2.523839, mean_q: 3.052726, mean_eps: 0.893689
 296023/1000000: episode: 434, duration: 29.032s, episode steps: 493, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.010846, mae: 2.529188, mean_q: 3.058675, mean_eps: 0.893521
 296654/1000000: episode: 435, duration: 36.700s, episode steps: 631, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.013542, mae: 2.526973, mean_q: 3.056413, mean_eps: 0.893318
 297201/1000000: episode: 436, duration: 32.647s, episode steps: 547, steps per second:  17, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.013245, mae: 2.520353, mean_q: 3.049910, mean_eps: 0.893106
 297581/1000000: episode: 437, duration: 22.466s, episode steps: 380, steps per second:  17, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.013979, mae: 2.526480, mean_q: 3.057126, mean_eps: 0.892939
 298080/1000000: episode: 438, duration: 29.303s, episode steps: 499, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.015385, mae: 2.523651, mean_q: 3.052993, mean_eps: 0.892781
 298787/1000000: episode: 439, duration: 40.482s, episode steps: 707, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.013012, mae: 2.530544, mean_q: 3.061159, mean_eps: 0.892564
 299303/1000000: episode: 440, duration: 30.363s, episode steps: 516, steps per second:  17, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.013934, mae: 2.524599, mean_q: 3.052097, mean_eps: 0.892344
 299948/1000000: episode: 441, duration: 37.315s, episode steps: 645, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.012520, mae: 2.519468, mean_q: 3.047793, mean_eps: 0.892135
 300491/1000000: episode: 442, duration: 31.262s, episode steps: 543, steps per second:  17, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013656, mae: 2.546625, mean_q: 3.082148, mean_eps: 0.891922
 301406/1000000: episode: 443, duration: 53.723s, episode steps: 915, steps per second:  17, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013594, mae: 2.532589, mean_q: 3.064524, mean_eps: 0.891659
 301839/1000000: episode: 444, duration: 24.200s, episode steps: 433, steps per second:  18, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012844, mae: 2.544356, mean_q: 3.079072, mean_eps: 0.891416
 302596/1000000: episode: 445, duration: 44.860s, episode steps: 757, steps per second:  17, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.013588, mae: 2.564072, mean_q: 3.101318, mean_eps: 0.891202
 303209/1000000: episode: 446, duration: 35.585s, episode steps: 613, steps per second:  17, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.013967, mae: 2.545849, mean_q: 3.079089, mean_eps: 0.890955
 303597/1000000: episode: 447, duration: 23.845s, episode steps: 388, steps per second:  16, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.012540, mae: 2.519081, mean_q: 3.047123, mean_eps: 0.890775
 304476/1000000: episode: 448, duration: 50.588s, episode steps: 879, steps per second:  17, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013328, mae: 2.551348, mean_q: 3.085982, mean_eps: 0.890547
 305438/1000000: episode: 449, duration: 56.185s, episode steps: 962, steps per second:  17, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.013292, mae: 2.542099, mean_q: 3.076836, mean_eps: 0.890216
 306234/1000000: episode: 450, duration: 46.145s, episode steps: 796, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.015068, mae: 2.530913, mean_q: 3.062586, mean_eps: 0.889899
 306747/1000000: episode: 451, duration: 30.248s, episode steps: 513, steps per second:  17, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.013540, mae: 2.523266, mean_q: 3.051296, mean_eps: 0.889664
 307245/1000000: episode: 452, duration: 29.389s, episode steps: 498, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.016797, mae: 2.534576, mean_q: 3.066535, mean_eps: 0.889481
 307816/1000000: episode: 453, duration: 33.383s, episode steps: 571, steps per second:  17, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.012244, mae: 2.532073, mean_q: 3.063938, mean_eps: 0.889289
 308206/1000000: episode: 454, duration: 23.162s, episode steps: 390, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.014527, mae: 2.548862, mean_q: 3.081826, mean_eps: 0.889116
 309003/1000000: episode: 455, duration: 46.852s, episode steps: 797, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.012265, mae: 2.540010, mean_q: 3.073993, mean_eps: 0.888903
 309662/1000000: episode: 456, duration: 38.712s, episode steps: 659, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.013376, mae: 2.565625, mean_q: 3.102251, mean_eps: 0.888640
 310502/1000000: episode: 457, duration: 49.340s, episode steps: 840, steps per second:  17, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.014350, mae: 2.556300, mean_q: 3.091564, mean_eps: 0.888370
 310948/1000000: episode: 458, duration: 26.698s, episode steps: 446, steps per second:  17, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.015483, mae: 2.562752, mean_q: 3.101276, mean_eps: 0.888139
 311783/1000000: episode: 459, duration: 49.009s, episode steps: 835, steps per second:  17, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.012786, mae: 2.536487, mean_q: 3.068401, mean_eps: 0.887909
 312532/1000000: episode: 460, duration: 44.188s, episode steps: 749, steps per second:  17, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013739, mae: 2.556000, mean_q: 3.091792, mean_eps: 0.887624
 313063/1000000: episode: 461, duration: 30.796s, episode steps: 531, steps per second:  17, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.014519, mae: 2.541570, mean_q: 3.074488, mean_eps: 0.887393
 313780/1000000: episode: 462, duration: 42.294s, episode steps: 717, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012762, mae: 2.551041, mean_q: 3.086296, mean_eps: 0.887169
 314643/1000000: episode: 463, duration: 50.533s, episode steps: 863, steps per second:  17, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.012778, mae: 2.538207, mean_q: 3.073937, mean_eps: 0.886884
 315169/1000000: episode: 464, duration: 30.779s, episode steps: 526, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.013931, mae: 2.537388, mean_q: 3.070089, mean_eps: 0.886634
 315800/1000000: episode: 465, duration: 37.257s, episode steps: 631, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.015670, mae: 2.531599, mean_q: 3.063285, mean_eps: 0.886426
 316321/1000000: episode: 466, duration: 30.958s, episode steps: 521, steps per second:  17, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.016676, mae: 2.556291, mean_q: 3.092760, mean_eps: 0.886218
 316936/1000000: episode: 467, duration: 37.177s, episode steps: 615, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.014982, mae: 2.541148, mean_q: 3.074023, mean_eps: 0.886014
 317430/1000000: episode: 468, duration: 28.388s, episode steps: 494, steps per second:  17, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.013576, mae: 2.551575, mean_q: 3.086909, mean_eps: 0.885814
 317814/1000000: episode: 469, duration: 22.667s, episode steps: 384, steps per second:  17, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012640, mae: 2.521751, mean_q: 3.051210, mean_eps: 0.885656
 318218/1000000: episode: 470, duration: 23.424s, episode steps: 404, steps per second:  17, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.015754, mae: 2.551079, mean_q: 3.089360, mean_eps: 0.885514
 319283/1000000: episode: 471, duration: 61.626s, episode steps: 1065, steps per second:  17, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012526, mae: 2.528118, mean_q: 3.058441, mean_eps: 0.885250
 320042/1000000: episode: 472, duration: 44.869s, episode steps: 759, steps per second:  17, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.012692, mae: 2.534825, mean_q: 3.065805, mean_eps: 0.884922
 320695/1000000: episode: 473, duration: 38.450s, episode steps: 653, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.015240, mae: 2.559194, mean_q: 3.095606, mean_eps: 0.884668
 321216/1000000: episode: 474, duration: 31.018s, episode steps: 521, steps per second:  17, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.013441, mae: 2.540244, mean_q: 3.071692, mean_eps: 0.884457
 321852/1000000: episode: 475, duration: 40.355s, episode steps: 636, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013447, mae: 2.532495, mean_q: 3.062532, mean_eps: 0.884248
 322629/1000000: episode: 476, duration: 50.624s, episode steps: 777, steps per second:  15, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.016679, mae: 2.541139, mean_q: 3.073782, mean_eps: 0.883994
 323291/1000000: episode: 477, duration: 39.539s, episode steps: 662, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.014248, mae: 2.529018, mean_q: 3.057354, mean_eps: 0.883734
 323869/1000000: episode: 478, duration: 39.131s, episode steps: 578, steps per second:  15, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012573, mae: 2.546336, mean_q: 3.079193, mean_eps: 0.883511
 324552/1000000: episode: 479, duration: 40.715s, episode steps: 683, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.010994, mae: 2.533084, mean_q: 3.063724, mean_eps: 0.883284
 325063/1000000: episode: 480, duration: 34.284s, episode steps: 511, steps per second:  15, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013137, mae: 2.552023, mean_q: 3.082825, mean_eps: 0.883070
 325746/1000000: episode: 481, duration: 41.039s, episode steps: 683, steps per second:  17, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.012296, mae: 2.557091, mean_q: 3.091388, mean_eps: 0.882855
 326342/1000000: episode: 482, duration: 36.258s, episode steps: 596, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.014053, mae: 2.538759, mean_q: 3.070278, mean_eps: 0.882624
 327183/1000000: episode: 483, duration: 50.188s, episode steps: 841, steps per second:  17, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012701, mae: 2.528152, mean_q: 3.056954, mean_eps: 0.882366
 327609/1000000: episode: 484, duration: 24.557s, episode steps: 426, steps per second:  17, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.014424, mae: 2.544930, mean_q: 3.080056, mean_eps: 0.882137
 328287/1000000: episode: 485, duration: 44.054s, episode steps: 678, steps per second:  15, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012836, mae: 2.548281, mean_q: 3.081641, mean_eps: 0.881939
 328905/1000000: episode: 486, duration: 36.734s, episode steps: 618, steps per second:  17, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014837, mae: 2.541314, mean_q: 3.074099, mean_eps: 0.881705
 329470/1000000: episode: 487, duration: 33.170s, episode steps: 565, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.013931, mae: 2.527964, mean_q: 3.056962, mean_eps: 0.881492
 330044/1000000: episode: 488, duration: 34.264s, episode steps: 574, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012862, mae: 2.556163, mean_q: 3.090978, mean_eps: 0.881288
 330609/1000000: episode: 489, duration: 35.308s, episode steps: 565, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013377, mae: 2.556157, mean_q: 3.092884, mean_eps: 0.881083
 331622/1000000: episode: 490, duration: 58.963s, episode steps: 1013, steps per second:  17, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013724, mae: 2.544197, mean_q: 3.078803, mean_eps: 0.880798
 332054/1000000: episode: 491, duration: 25.839s, episode steps: 432, steps per second:  17, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.014335, mae: 2.552357, mean_q: 3.088181, mean_eps: 0.880538
 332975/1000000: episode: 492, duration: 54.241s, episode steps: 921, steps per second:  17, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.014616, mae: 2.533893, mean_q: 3.063791, mean_eps: 0.880295
 333511/1000000: episode: 493, duration: 31.938s, episode steps: 536, steps per second:  17, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.013242, mae: 2.533788, mean_q: 3.065198, mean_eps: 0.880033
 333910/1000000: episode: 494, duration: 24.173s, episode steps: 399, steps per second:  17, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.013160, mae: 2.545137, mean_q: 3.077852, mean_eps: 0.879864
 334922/1000000: episode: 495, duration: 60.634s, episode steps: 1012, steps per second:  17, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.013533, mae: 2.542185, mean_q: 3.075630, mean_eps: 0.879610
 335424/1000000: episode: 496, duration: 29.597s, episode steps: 502, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.015471, mae: 2.528899, mean_q: 3.063120, mean_eps: 0.879338
 336053/1000000: episode: 497, duration: 37.685s, episode steps: 629, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.016784, mae: 2.545294, mean_q: 3.078005, mean_eps: 0.879134
 336421/1000000: episode: 498, duration: 21.466s, episode steps: 368, steps per second:  17, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.012295, mae: 2.532031, mean_q: 3.062354, mean_eps: 0.878954
 337066/1000000: episode: 499, duration: 39.739s, episode steps: 645, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.011662, mae: 2.529938, mean_q: 3.060733, mean_eps: 0.878772
 337752/1000000: episode: 500, duration: 40.458s, episode steps: 686, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.013557, mae: 2.538021, mean_q: 3.069131, mean_eps: 0.878533
 338175/1000000: episode: 501, duration: 25.446s, episode steps: 423, steps per second:  17, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.011373, mae: 2.534672, mean_q: 3.067045, mean_eps: 0.878334
 339318/1000000: episode: 502, duration: 67.843s, episode steps: 1143, steps per second:  17, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013104, mae: 2.543259, mean_q: 3.075944, mean_eps: 0.878051
 339877/1000000: episode: 503, duration: 32.433s, episode steps: 559, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013510, mae: 2.541024, mean_q: 3.073398, mean_eps: 0.877745
 340377/1000000: episode: 504, duration: 29.692s, episode steps: 500, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.013708, mae: 2.552310, mean_q: 3.088499, mean_eps: 0.877554
 341107/1000000: episode: 505, duration: 43.275s, episode steps: 730, steps per second:  17, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.015080, mae: 2.545834, mean_q: 3.077280, mean_eps: 0.877333
 342052/1000000: episode: 506, duration: 55.903s, episode steps: 945, steps per second:  17, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.014273, mae: 2.544673, mean_q: 3.078633, mean_eps: 0.877032
 342784/1000000: episode: 507, duration: 45.094s, episode steps: 732, steps per second:  16, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.014892, mae: 2.541222, mean_q: 3.075062, mean_eps: 0.876730
 343294/1000000: episode: 508, duration: 30.845s, episode steps: 510, steps per second:  17, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.014312, mae: 2.544557, mean_q: 3.078800, mean_eps: 0.876506
 343787/1000000: episode: 509, duration: 29.385s, episode steps: 493, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.011214, mae: 2.525012, mean_q: 3.054144, mean_eps: 0.876326
 344714/1000000: episode: 510, duration: 53.673s, episode steps: 927, steps per second:  17, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013752, mae: 2.543730, mean_q: 3.076838, mean_eps: 0.876070
 345393/1000000: episode: 511, duration: 41.875s, episode steps: 679, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.013724, mae: 2.550378, mean_q: 3.084249, mean_eps: 0.875781
 345934/1000000: episode: 512, duration: 31.741s, episode steps: 541, steps per second:  17, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.013928, mae: 2.550095, mean_q: 3.082965, mean_eps: 0.875561
 346602/1000000: episode: 513, duration: 39.692s, episode steps: 668, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.016055, mae: 2.534926, mean_q: 3.067774, mean_eps: 0.875344
 347097/1000000: episode: 514, duration: 30.142s, episode steps: 495, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012667, mae: 2.543674, mean_q: 3.078527, mean_eps: 0.875134
 347595/1000000: episode: 515, duration: 29.963s, episode steps: 498, steps per second:  17, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.011859, mae: 2.516229, mean_q: 3.043930, mean_eps: 0.874955
 348234/1000000: episode: 516, duration: 37.170s, episode steps: 639, steps per second:  17, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013197, mae: 2.541090, mean_q: 3.071863, mean_eps: 0.874751
 348930/1000000: episode: 517, duration: 41.032s, episode steps: 696, steps per second:  17, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.012905, mae: 2.558760, mean_q: 3.093866, mean_eps: 0.874510
 349631/1000000: episode: 518, duration: 41.158s, episode steps: 701, steps per second:  17, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.014598, mae: 2.555580, mean_q: 3.089778, mean_eps: 0.874259
 350149/1000000: episode: 519, duration: 29.986s, episode steps: 518, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.012564, mae: 2.544888, mean_q: 3.079088, mean_eps: 0.874040
 350798/1000000: episode: 520, duration: 39.630s, episode steps: 649, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013460, mae: 2.557089, mean_q: 3.092319, mean_eps: 0.873829
 351741/1000000: episode: 521, duration: 56.772s, episode steps: 943, steps per second:  17, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013251, mae: 2.557393, mean_q: 3.093720, mean_eps: 0.873543
 352324/1000000: episode: 522, duration: 34.832s, episode steps: 583, steps per second:  17, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.011920, mae: 2.532652, mean_q: 3.065870, mean_eps: 0.873268
 352894/1000000: episode: 523, duration: 33.963s, episode steps: 570, steps per second:  17, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.012886, mae: 2.547754, mean_q: 3.083837, mean_eps: 0.873061
 353408/1000000: episode: 524, duration: 30.877s, episode steps: 514, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012717, mae: 2.541510, mean_q: 3.073656, mean_eps: 0.872866
 354258/1000000: episode: 525, duration: 51.256s, episode steps: 850, steps per second:  17, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014861, mae: 2.558637, mean_q: 3.097043, mean_eps: 0.872620
 354702/1000000: episode: 526, duration: 25.773s, episode steps: 444, steps per second:  17, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.014306, mae: 2.548382, mean_q: 3.083190, mean_eps: 0.872387
 355253/1000000: episode: 527, duration: 32.089s, episode steps: 551, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.014395, mae: 2.555033, mean_q: 3.091451, mean_eps: 0.872208
 355820/1000000: episode: 528, duration: 35.349s, episode steps: 567, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.012236, mae: 2.535190, mean_q: 3.067242, mean_eps: 0.872007
 356493/1000000: episode: 529, duration: 39.839s, episode steps: 673, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.014013, mae: 2.556051, mean_q: 3.091824, mean_eps: 0.871784
 357185/1000000: episode: 530, duration: 41.632s, episode steps: 692, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.013686, mae: 2.545243, mean_q: 3.078568, mean_eps: 0.871538
 357752/1000000: episode: 531, duration: 36.230s, episode steps: 567, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014106, mae: 2.575791, mean_q: 3.115061, mean_eps: 0.871312
 358291/1000000: episode: 532, duration: 32.356s, episode steps: 539, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.013674, mae: 2.569851, mean_q: 3.106213, mean_eps: 0.871113
 359544/1000000: episode: 533, duration: 75.327s, episode steps: 1253, steps per second:  17, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.014864, mae: 2.563173, mean_q: 3.099210, mean_eps: 0.870790
 360179/1000000: episode: 534, duration: 37.123s, episode steps: 635, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.014490, mae: 2.566170, mean_q: 3.102499, mean_eps: 0.870450
 361026/1000000: episode: 535, duration: 49.756s, episode steps: 847, steps per second:  17, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.013667, mae: 2.551418, mean_q: 3.085716, mean_eps: 0.870183
 361680/1000000: episode: 536, duration: 40.478s, episode steps: 654, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013226, mae: 2.560940, mean_q: 3.098035, mean_eps: 0.869913
 362069/1000000: episode: 537, duration: 23.106s, episode steps: 389, steps per second:  17, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.013690, mae: 2.575422, mean_q: 3.114763, mean_eps: 0.869725
 362614/1000000: episode: 538, duration: 32.937s, episode steps: 545, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012448, mae: 2.536954, mean_q: 3.070400, mean_eps: 0.869557
 363739/1000000: episode: 539, duration: 67.388s, episode steps: 1125, steps per second:  17, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.014403, mae: 2.568421, mean_q: 3.107516, mean_eps: 0.869257
 364281/1000000: episode: 540, duration: 32.408s, episode steps: 542, steps per second:  17, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.013454, mae: 2.547638, mean_q: 3.081938, mean_eps: 0.868956
 364953/1000000: episode: 541, duration: 41.009s, episode steps: 672, steps per second:  16, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.013479, mae: 2.547821, mean_q: 3.080682, mean_eps: 0.868738
 365515/1000000: episode: 542, duration: 33.217s, episode steps: 562, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.016666, mae: 2.553230, mean_q: 3.085061, mean_eps: 0.868516
 366063/1000000: episode: 543, duration: 32.940s, episode steps: 548, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.015336, mae: 2.539391, mean_q: 3.070363, mean_eps: 0.868316
 366729/1000000: episode: 544, duration: 34.152s, episode steps: 666, steps per second:  20, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.015082, mae: 2.555451, mean_q: 3.090021, mean_eps: 0.868097
 367425/1000000: episode: 545, duration: 34.685s, episode steps: 696, steps per second:  20, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.012099, mae: 2.548051, mean_q: 3.082615, mean_eps: 0.867852
 367962/1000000: episode: 546, duration: 27.937s, episode steps: 537, steps per second:  19, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.015938, mae: 2.550103, mean_q: 3.083350, mean_eps: 0.867630
 368595/1000000: episode: 547, duration: 34.967s, episode steps: 633, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.014187, mae: 2.548656, mean_q: 3.081891, mean_eps: 0.867420
 369288/1000000: episode: 548, duration: 36.055s, episode steps: 693, steps per second:  19, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.011911, mae: 2.546969, mean_q: 3.080721, mean_eps: 0.867182
 370038/1000000: episode: 549, duration: 42.954s, episode steps: 750, steps per second:  17, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.013467, mae: 2.547813, mean_q: 3.080807, mean_eps: 0.866922
 370555/1000000: episode: 550, duration: 31.428s, episode steps: 517, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.015437, mae: 2.540470, mean_q: 3.072028, mean_eps: 0.866693
 371428/1000000: episode: 551, duration: 51.866s, episode steps: 873, steps per second:  17, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013149, mae: 2.554993, mean_q: 3.089772, mean_eps: 0.866444
 372550/1000000: episode: 552, duration: 69.276s, episode steps: 1122, steps per second:  16, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.014852, mae: 2.558495, mean_q: 3.093278, mean_eps: 0.866084
 373149/1000000: episode: 553, duration: 36.467s, episode steps: 599, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.012424, mae: 2.552431, mean_q: 3.086453, mean_eps: 0.865774
 373878/1000000: episode: 554, duration: 44.723s, episode steps: 729, steps per second:  16, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013055, mae: 2.568039, mean_q: 3.104582, mean_eps: 0.865535
 374546/1000000: episode: 555, duration: 40.837s, episode steps: 668, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.011029, mae: 2.531856, mean_q: 3.060941, mean_eps: 0.865284
 375138/1000000: episode: 556, duration: 33.178s, episode steps: 592, steps per second:  18, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.015204, mae: 2.565190, mean_q: 3.100696, mean_eps: 0.865057
 376046/1000000: episode: 557, duration: 48.362s, episode steps: 908, steps per second:  19, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014143, mae: 2.551652, mean_q: 3.084893, mean_eps: 0.864787
 376673/1000000: episode: 558, duration: 33.092s, episode steps: 627, steps per second:  19, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.013732, mae: 2.567821, mean_q: 3.104933, mean_eps: 0.864510
 377074/1000000: episode: 559, duration: 27.110s, episode steps: 401, steps per second:  15, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.014010, mae: 2.543995, mean_q: 3.073769, mean_eps: 0.864325
 377832/1000000: episode: 560, duration: 48.060s, episode steps: 758, steps per second:  16, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.013805, mae: 2.553316, mean_q: 3.087024, mean_eps: 0.864117
 378512/1000000: episode: 561, duration: 42.869s, episode steps: 680, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.014297, mae: 2.551794, mean_q: 3.085083, mean_eps: 0.863859
 379084/1000000: episode: 562, duration: 37.148s, episode steps: 572, steps per second:  15, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.015386, mae: 2.555323, mean_q: 3.089953, mean_eps: 0.863633
 379463/1000000: episode: 563, duration: 24.246s, episode steps: 379, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.012976, mae: 2.538655, mean_q: 3.070671, mean_eps: 0.863462
 380069/1000000: episode: 564, duration: 37.443s, episode steps: 606, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.013100, mae: 2.517597, mean_q: 3.048470, mean_eps: 0.863284
 380654/1000000: episode: 565, duration: 36.246s, episode steps: 585, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.015178, mae: 2.539191, mean_q: 3.069836, mean_eps: 0.863070
 381325/1000000: episode: 566, duration: 43.870s, episode steps: 671, steps per second:  15, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013787, mae: 2.539974, mean_q: 3.070429, mean_eps: 0.862844
 381787/1000000: episode: 567, duration: 29.148s, episode steps: 462, steps per second:  16, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.014528, mae: 2.553896, mean_q: 3.088216, mean_eps: 0.862640
 382323/1000000: episode: 568, duration: 34.315s, episode steps: 536, steps per second:  16, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.015517, mae: 2.532783, mean_q: 3.065927, mean_eps: 0.862461
 383024/1000000: episode: 569, duration: 42.182s, episode steps: 701, steps per second:  17, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.014041, mae: 2.533288, mean_q: 3.063456, mean_eps: 0.862238
 383487/1000000: episode: 570, duration: 24.277s, episode steps: 463, steps per second:  19, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.013862, mae: 2.542361, mean_q: 3.075031, mean_eps: 0.862029
 383909/1000000: episode: 571, duration: 22.887s, episode steps: 422, steps per second:  18, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.013212, mae: 2.537954, mean_q: 3.068829, mean_eps: 0.861869
 384246/1000000: episode: 572, duration: 17.377s, episode steps: 337, steps per second:  19, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.015682, mae: 2.544548, mean_q: 3.076157, mean_eps: 0.861732
 384904/1000000: episode: 573, duration: 35.160s, episode steps: 658, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015640, mae: 2.564239, mean_q: 3.099853, mean_eps: 0.861553
 385793/1000000: episode: 574, duration: 48.158s, episode steps: 889, steps per second:  18, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.013986, mae: 2.542657, mean_q: 3.073769, mean_eps: 0.861275
 386783/1000000: episode: 575, duration: 51.989s, episode steps: 990, steps per second:  19, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.015904, mae: 2.533716, mean_q: 3.062312, mean_eps: 0.860936
 387867/1000000: episode: 576, duration: 61.769s, episode steps: 1084, steps per second:  18, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.012875, mae: 2.566802, mean_q: 3.104595, mean_eps: 0.860563
 388668/1000000: episode: 577, duration: 43.052s, episode steps: 801, steps per second:  19, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.016052, mae: 2.554945, mean_q: 3.089210, mean_eps: 0.860224
 389600/1000000: episode: 578, duration: 50.087s, episode steps: 932, steps per second:  19, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.014300, mae: 2.540180, mean_q: 3.071393, mean_eps: 0.859912
 390234/1000000: episode: 579, duration: 38.382s, episode steps: 634, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.014663, mae: 2.535726, mean_q: 3.063359, mean_eps: 0.859630
 391045/1000000: episode: 580, duration: 50.027s, episode steps: 811, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012914, mae: 2.531891, mean_q: 3.062154, mean_eps: 0.859370
 391878/1000000: episode: 581, duration: 53.005s, episode steps: 833, steps per second:  16, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013662, mae: 2.542531, mean_q: 3.073788, mean_eps: 0.859074
 392250/1000000: episode: 582, duration: 22.593s, episode steps: 372, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011878, mae: 2.559650, mean_q: 3.094335, mean_eps: 0.858857
 392754/1000000: episode: 583, duration: 31.687s, episode steps: 504, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.014596, mae: 2.544134, mean_q: 3.078166, mean_eps: 0.858699
 393824/1000000: episode: 584, duration: 65.693s, episode steps: 1070, steps per second:  16, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.014410, mae: 2.562674, mean_q: 3.099053, mean_eps: 0.858416
 394368/1000000: episode: 585, duration: 33.277s, episode steps: 544, steps per second:  16, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.015805, mae: 2.559085, mean_q: 3.094927, mean_eps: 0.858126
 395249/1000000: episode: 586, duration: 54.720s, episode steps: 881, steps per second:  16, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.014361, mae: 2.558799, mean_q: 3.093644, mean_eps: 0.857869
 396183/1000000: episode: 587, duration: 57.784s, episode steps: 934, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.015115, mae: 2.564799, mean_q: 3.100573, mean_eps: 0.857542
 396708/1000000: episode: 588, duration: 32.589s, episode steps: 525, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012561, mae: 2.536443, mean_q: 3.067433, mean_eps: 0.857280
 397245/1000000: episode: 589, duration: 34.556s, episode steps: 537, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.013699, mae: 2.564860, mean_q: 3.100833, mean_eps: 0.857089
 397769/1000000: episode: 590, duration: 33.315s, episode steps: 524, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.015545, mae: 2.533239, mean_q: 3.061954, mean_eps: 0.856897
 398397/1000000: episode: 591, duration: 39.130s, episode steps: 628, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.015628, mae: 2.544981, mean_q: 3.076495, mean_eps: 0.856690
 399054/1000000: episode: 592, duration: 40.834s, episode steps: 657, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.014437, mae: 2.591024, mean_q: 3.133360, mean_eps: 0.856459
 399590/1000000: episode: 593, duration: 33.073s, episode steps: 536, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012916, mae: 2.548353, mean_q: 3.081657, mean_eps: 0.856244
 400357/1000000: episode: 594, duration: 48.689s, episode steps: 767, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.013325, mae: 2.532933, mean_q: 3.062742, mean_eps: 0.856009
 400842/1000000: episode: 595, duration: 29.420s, episode steps: 485, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.014757, mae: 2.535008, mean_q: 3.065559, mean_eps: 0.855784
 401410/1000000: episode: 596, duration: 35.722s, episode steps: 568, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012398, mae: 2.542450, mean_q: 3.075479, mean_eps: 0.855595
 402397/1000000: episode: 597, duration: 61.451s, episode steps: 987, steps per second:  16, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.014319, mae: 2.545816, mean_q: 3.078247, mean_eps: 0.855315
 403522/1000000: episode: 598, duration: 68.738s, episode steps: 1125, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.016101, mae: 2.557327, mean_q: 3.090957, mean_eps: 0.854934
 404162/1000000: episode: 599, duration: 39.107s, episode steps: 640, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.012690, mae: 2.546811, mean_q: 3.078877, mean_eps: 0.854617
 404678/1000000: episode: 600, duration: 31.887s, episode steps: 516, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.013695, mae: 2.546128, mean_q: 3.080767, mean_eps: 0.854409
 405440/1000000: episode: 601, duration: 47.013s, episode steps: 762, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.014255, mae: 2.548707, mean_q: 3.081792, mean_eps: 0.854179
 406144/1000000: episode: 602, duration: 44.742s, episode steps: 704, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.016614, mae: 2.543290, mean_q: 3.075788, mean_eps: 0.853916
 406794/1000000: episode: 603, duration: 41.654s, episode steps: 650, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.017076, mae: 2.551871, mean_q: 3.084925, mean_eps: 0.853672
 407814/1000000: episode: 604, duration: 64.501s, episode steps: 1020, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013901, mae: 2.531648, mean_q: 3.061530, mean_eps: 0.853371
 408215/1000000: episode: 605, duration: 23.803s, episode steps: 401, steps per second:  17, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.013727, mae: 2.523231, mean_q: 3.050624, mean_eps: 0.853115
 408797/1000000: episode: 606, duration: 36.863s, episode steps: 582, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.014162, mae: 2.538706, mean_q: 3.067750, mean_eps: 0.852938
 409533/1000000: episode: 607, duration: 44.823s, episode steps: 736, steps per second:  16, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013643, mae: 2.545575, mean_q: 3.076859, mean_eps: 0.852700
 410043/1000000: episode: 608, duration: 31.812s, episode steps: 510, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.013543, mae: 2.545041, mean_q: 3.074434, mean_eps: 0.852476
 410746/1000000: episode: 609, duration: 44.551s, episode steps: 703, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.013704, mae: 2.546753, mean_q: 3.079126, mean_eps: 0.852258
 411368/1000000: episode: 610, duration: 37.838s, episode steps: 622, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012864, mae: 2.522662, mean_q: 3.050613, mean_eps: 0.852020
 411907/1000000: episode: 611, duration: 34.873s, episode steps: 539, steps per second:  15, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014212, mae: 2.537966, mean_q: 3.068465, mean_eps: 0.851811
 412394/1000000: episode: 612, duration: 29.803s, episode steps: 487, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.014701, mae: 2.526997, mean_q: 3.055615, mean_eps: 0.851626
 412743/1000000: episode: 613, duration: 21.922s, episode steps: 349, steps per second:  16, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.016504, mae: 2.523608, mean_q: 3.049451, mean_eps: 0.851476
 413913/1000000: episode: 614, duration: 73.385s, episode steps: 1170, steps per second:  16, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013406, mae: 2.552679, mean_q: 3.086380, mean_eps: 0.851202
 414571/1000000: episode: 615, duration: 39.920s, episode steps: 658, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.013587, mae: 2.535198, mean_q: 3.066789, mean_eps: 0.850873
 415588/1000000: episode: 616, duration: 62.856s, episode steps: 1017, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.014039, mae: 2.533157, mean_q: 3.063633, mean_eps: 0.850572
 415949/1000000: episode: 617, duration: 22.434s, episode steps: 361, steps per second:  16, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.015602, mae: 2.546273, mean_q: 3.078264, mean_eps: 0.850324
 416846/1000000: episode: 618, duration: 56.499s, episode steps: 897, steps per second:  16, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.013500, mae: 2.558947, mean_q: 3.094069, mean_eps: 0.850097
 417542/1000000: episode: 619, duration: 42.386s, episode steps: 696, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.017341, mae: 2.541997, mean_q: 3.072482, mean_eps: 0.849810
 418382/1000000: episode: 620, duration: 52.703s, episode steps: 840, steps per second:  16, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.014024, mae: 2.527922, mean_q: 3.056639, mean_eps: 0.849534
 418933/1000000: episode: 621, duration: 33.556s, episode steps: 551, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.014544, mae: 2.541225, mean_q: 3.071530, mean_eps: 0.849283
 419613/1000000: episode: 622, duration: 41.655s, episode steps: 680, steps per second:  16, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.015776, mae: 2.535083, mean_q: 3.065782, mean_eps: 0.849061
 420120/1000000: episode: 623, duration: 31.820s, episode steps: 507, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014652, mae: 2.560751, mean_q: 3.095612, mean_eps: 0.848848
 420655/1000000: episode: 624, duration: 33.426s, episode steps: 535, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.014104, mae: 2.538446, mean_q: 3.068272, mean_eps: 0.848661
 421348/1000000: episode: 625, duration: 42.518s, episode steps: 693, steps per second:  16, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.014533, mae: 2.525004, mean_q: 3.051656, mean_eps: 0.848440
 421798/1000000: episode: 626, duration: 28.850s, episode steps: 450, steps per second:  16, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.013149, mae: 2.525250, mean_q: 3.051812, mean_eps: 0.848234
 422321/1000000: episode: 627, duration: 32.464s, episode steps: 523, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012659, mae: 2.545604, mean_q: 3.077931, mean_eps: 0.848058
 422797/1000000: episode: 628, duration: 29.394s, episode steps: 476, steps per second:  16, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.013078, mae: 2.529315, mean_q: 3.057883, mean_eps: 0.847878
 423468/1000000: episode: 629, duration: 40.407s, episode steps: 671, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.013275, mae: 2.540738, mean_q: 3.073134, mean_eps: 0.847672
 424179/1000000: episode: 630, duration: 44.382s, episode steps: 711, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.015550, mae: 2.536417, mean_q: 3.066293, mean_eps: 0.847424
 424594/1000000: episode: 631, duration: 25.938s, episode steps: 415, steps per second:  16, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.013621, mae: 2.534512, mean_q: 3.066270, mean_eps: 0.847221
 425336/1000000: episode: 632, duration: 46.695s, episode steps: 742, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.013531, mae: 2.527363, mean_q: 3.056659, mean_eps: 0.847013
 426219/1000000: episode: 633, duration: 54.606s, episode steps: 883, steps per second:  16, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013700, mae: 2.536884, mean_q: 3.070803, mean_eps: 0.846721
 426822/1000000: episode: 634, duration: 37.023s, episode steps: 603, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.012988, mae: 2.540363, mean_q: 3.070534, mean_eps: 0.846453
 427718/1000000: episode: 635, duration: 55.285s, episode steps: 896, steps per second:  16, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.014792, mae: 2.540094, mean_q: 3.070132, mean_eps: 0.846183
 428452/1000000: episode: 636, duration: 37.897s, episode steps: 734, steps per second:  19, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.014372, mae: 2.547188, mean_q: 3.078264, mean_eps: 0.845890
 429082/1000000: episode: 637, duration: 33.251s, episode steps: 630, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.015933, mae: 2.538015, mean_q: 3.067890, mean_eps: 0.845644
 429833/1000000: episode: 638, duration: 40.244s, episode steps: 751, steps per second:  19, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014772, mae: 2.532652, mean_q: 3.061137, mean_eps: 0.845395
 430218/1000000: episode: 639, duration: 20.236s, episode steps: 385, steps per second:  19, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.012740, mae: 2.516349, mean_q: 3.040995, mean_eps: 0.845191
 431062/1000000: episode: 640, duration: 44.777s, episode steps: 844, steps per second:  19, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.014427, mae: 2.538368, mean_q: 3.067690, mean_eps: 0.844970
 431851/1000000: episode: 641, duration: 40.978s, episode steps: 789, steps per second:  19, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.015680, mae: 2.548048, mean_q: 3.080083, mean_eps: 0.844676
 432855/1000000: episode: 642, duration: 55.106s, episode steps: 1004, steps per second:  18, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.013247, mae: 2.521044, mean_q: 3.046622, mean_eps: 0.844353
 433497/1000000: episode: 643, duration: 33.337s, episode steps: 642, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.013718, mae: 2.534733, mean_q: 3.063076, mean_eps: 0.844057
 433888/1000000: episode: 644, duration: 20.940s, episode steps: 391, steps per second:  19, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013319, mae: 2.532383, mean_q: 3.062146, mean_eps: 0.843871
 434410/1000000: episode: 645, duration: 27.254s, episode steps: 522, steps per second:  19, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.016181, mae: 2.539478, mean_q: 3.070382, mean_eps: 0.843707
 434971/1000000: episode: 646, duration: 33.093s, episode steps: 561, steps per second:  17, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.013220, mae: 2.518880, mean_q: 3.046015, mean_eps: 0.843512
 435583/1000000: episode: 647, duration: 32.034s, episode steps: 612, steps per second:  19, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.012611, mae: 2.551946, mean_q: 3.084160, mean_eps: 0.843301
 436268/1000000: episode: 648, duration: 37.258s, episode steps: 685, steps per second:  18, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.015129, mae: 2.546454, mean_q: 3.077595, mean_eps: 0.843067
 437117/1000000: episode: 649, duration: 49.217s, episode steps: 849, steps per second:  17, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.014013, mae: 2.519914, mean_q: 3.047586, mean_eps: 0.842791
 437615/1000000: episode: 650, duration: 26.459s, episode steps: 498, steps per second:  19, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.015043, mae: 2.547684, mean_q: 3.082138, mean_eps: 0.842548
 438310/1000000: episode: 651, duration: 36.010s, episode steps: 695, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.014643, mae: 2.550126, mean_q: 3.082661, mean_eps: 0.842334
 439261/1000000: episode: 652, duration: 53.298s, episode steps: 951, steps per second:  18, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013727, mae: 2.531727, mean_q: 3.059167, mean_eps: 0.842037
 440357/1000000: episode: 653, duration: 60.485s, episode steps: 1096, steps per second:  18, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014640, mae: 2.538611, mean_q: 3.067938, mean_eps: 0.841668
 441031/1000000: episode: 654, duration: 36.981s, episode steps: 674, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.012602, mae: 2.522718, mean_q: 3.049460, mean_eps: 0.841350
 441417/1000000: episode: 655, duration: 21.322s, episode steps: 386, steps per second:  18, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.015579, mae: 2.535811, mean_q: 3.064195, mean_eps: 0.841159
 442001/1000000: episode: 656, duration: 37.361s, episode steps: 584, steps per second:  16, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.013562, mae: 2.536265, mean_q: 3.068092, mean_eps: 0.840984
 442419/1000000: episode: 657, duration: 26.113s, episode steps: 418, steps per second:  16, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.014277, mae: 2.537883, mean_q: 3.068699, mean_eps: 0.840804
 443350/1000000: episode: 658, duration: 64.049s, episode steps: 931, steps per second:  15, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.015503, mae: 2.545057, mean_q: 3.077463, mean_eps: 0.840562
 444427/1000000: episode: 659, duration: 65.388s, episode steps: 1077, steps per second:  16, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.014627, mae: 2.534890, mean_q: 3.063922, mean_eps: 0.840200
 445121/1000000: episode: 660, duration: 44.075s, episode steps: 694, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.014927, mae: 2.542852, mean_q: 3.073034, mean_eps: 0.839881
 445562/1000000: episode: 661, duration: 27.978s, episode steps: 441, steps per second:  16, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.015876, mae: 2.514207, mean_q: 3.040914, mean_eps: 0.839677
 446217/1000000: episode: 662, duration: 40.640s, episode steps: 655, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.014312, mae: 2.526522, mean_q: 3.054706, mean_eps: 0.839480
 446857/1000000: episode: 663, duration: 43.609s, episode steps: 640, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.015874, mae: 2.553123, mean_q: 3.084920, mean_eps: 0.839246
 447443/1000000: episode: 664, duration: 38.497s, episode steps: 586, steps per second:  15, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.013470, mae: 2.552483, mean_q: 3.084548, mean_eps: 0.839026
 448173/1000000: episode: 665, duration: 45.805s, episode steps: 730, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.013055, mae: 2.558539, mean_q: 3.091501, mean_eps: 0.838789
 448692/1000000: episode: 666, duration: 31.711s, episode steps: 519, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.014181, mae: 2.538857, mean_q: 3.067675, mean_eps: 0.838564
 449778/1000000: episode: 667, duration: 68.282s, episode steps: 1086, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014887, mae: 2.524846, mean_q: 3.052677, mean_eps: 0.838276
 450177/1000000: episode: 668, duration: 24.784s, episode steps: 399, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015391, mae: 2.540414, mean_q: 3.069565, mean_eps: 0.838008
 451043/1000000: episode: 669, duration: 53.701s, episode steps: 866, steps per second:  16, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.013968, mae: 2.551931, mean_q: 3.083236, mean_eps: 0.837780
 451970/1000000: episode: 670, duration: 57.950s, episode steps: 927, steps per second:  16, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.013585, mae: 2.558701, mean_q: 3.093211, mean_eps: 0.837458
 452376/1000000: episode: 671, duration: 26.368s, episode steps: 406, steps per second:  15, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.010829, mae: 2.537826, mean_q: 3.065537, mean_eps: 0.837218
 453002/1000000: episode: 672, duration: 38.696s, episode steps: 626, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.016909, mae: 2.565645, mean_q: 3.100442, mean_eps: 0.837032
 453670/1000000: episode: 673, duration: 42.736s, episode steps: 668, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.017863, mae: 2.538745, mean_q: 3.066167, mean_eps: 0.836799
 454379/1000000: episode: 674, duration: 43.775s, episode steps: 709, steps per second:  16, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.013545, mae: 2.573901, mean_q: 3.110315, mean_eps: 0.836551
 455317/1000000: episode: 675, duration: 59.160s, episode steps: 938, steps per second:  16, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.013937, mae: 2.535759, mean_q: 3.062805, mean_eps: 0.836255
 456270/1000000: episode: 676, duration: 59.637s, episode steps: 953, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.015279, mae: 2.552368, mean_q: 3.083368, mean_eps: 0.835914
 456865/1000000: episode: 677, duration: 37.218s, episode steps: 595, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.015720, mae: 2.539223, mean_q: 3.067567, mean_eps: 0.835636
 457730/1000000: episode: 678, duration: 55.329s, episode steps: 865, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.014800, mae: 2.542030, mean_q: 3.072492, mean_eps: 0.835373
 458538/1000000: episode: 679, duration: 50.312s, episode steps: 808, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.015938, mae: 2.554926, mean_q: 3.086486, mean_eps: 0.835072
 459367/1000000: episode: 680, duration: 51.376s, episode steps: 829, steps per second:  16, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.013553, mae: 2.562130, mean_q: 3.096185, mean_eps: 0.834777
 460005/1000000: episode: 681, duration: 39.563s, episode steps: 638, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014463, mae: 2.547709, mean_q: 3.077405, mean_eps: 0.834513
 461312/1000000: episode: 682, duration: 81.787s, episode steps: 1307, steps per second:  16, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.013495, mae: 2.544389, mean_q: 3.075182, mean_eps: 0.834163
 462099/1000000: episode: 683, duration: 49.268s, episode steps: 787, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.014286, mae: 2.562705, mean_q: 3.098410, mean_eps: 0.833787
 462734/1000000: episode: 684, duration: 38.966s, episode steps: 635, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.015289, mae: 2.531046, mean_q: 3.058792, mean_eps: 0.833530
 463105/1000000: episode: 685, duration: 24.208s, episode steps: 371, steps per second:  15, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.013589, mae: 2.555545, mean_q: 3.092059, mean_eps: 0.833349
 464071/1000000: episode: 686, duration: 59.220s, episode steps: 966, steps per second:  16, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.013877, mae: 2.556398, mean_q: 3.089567, mean_eps: 0.833108
 464642/1000000: episode: 687, duration: 35.905s, episode steps: 571, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.014828, mae: 2.552745, mean_q: 3.084973, mean_eps: 0.832832
 465187/1000000: episode: 688, duration: 33.697s, episode steps: 545, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.015408, mae: 2.553414, mean_q: 3.084609, mean_eps: 0.832631
 465976/1000000: episode: 689, duration: 49.133s, episode steps: 789, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.012705, mae: 2.533593, mean_q: 3.062640, mean_eps: 0.832391
 466447/1000000: episode: 690, duration: 29.351s, episode steps: 471, steps per second:  16, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.013047, mae: 2.545837, mean_q: 3.077738, mean_eps: 0.832164
 467245/1000000: episode: 691, duration: 50.395s, episode steps: 798, steps per second:  16, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.016587, mae: 2.559477, mean_q: 3.092682, mean_eps: 0.831935
 467889/1000000: episode: 692, duration: 34.597s, episode steps: 644, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.015274, mae: 2.560440, mean_q: 3.095484, mean_eps: 0.831676
 468344/1000000: episode: 693, duration: 24.022s, episode steps: 455, steps per second:  19, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.015514, mae: 2.561347, mean_q: 3.092842, mean_eps: 0.831478
 468978/1000000: episode: 694, duration: 34.367s, episode steps: 634, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.014473, mae: 2.542757, mean_q: 3.072608, mean_eps: 0.831282
 469928/1000000: episode: 695, duration: 56.189s, episode steps: 950, steps per second:  17, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.013172, mae: 2.576303, mean_q: 3.113064, mean_eps: 0.830997
 470769/1000000: episode: 696, duration: 52.969s, episode steps: 841, steps per second:  16, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014728, mae: 2.533181, mean_q: 3.059061, mean_eps: 0.830675
 471512/1000000: episode: 697, duration: 46.848s, episode steps: 743, steps per second:  16, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.014417, mae: 2.565776, mean_q: 3.100106, mean_eps: 0.830390
 472310/1000000: episode: 698, duration: 49.750s, episode steps: 798, steps per second:  16, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.015015, mae: 2.545614, mean_q: 3.076030, mean_eps: 0.830112
 473425/1000000: episode: 699, duration: 69.442s, episode steps: 1115, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014555, mae: 2.561125, mean_q: 3.094243, mean_eps: 0.829768
 473955/1000000: episode: 700, duration: 31.940s, episode steps: 530, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.013341, mae: 2.561875, mean_q: 3.096726, mean_eps: 0.829472
 474352/1000000: episode: 701, duration: 24.450s, episode steps: 397, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014481, mae: 2.532290, mean_q: 3.060703, mean_eps: 0.829305
 474752/1000000: episode: 702, duration: 25.675s, episode steps: 400, steps per second:  16, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.013132, mae: 2.554514, mean_q: 3.086100, mean_eps: 0.829162
 475396/1000000: episode: 703, duration: 39.709s, episode steps: 644, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.013996, mae: 2.551564, mean_q: 3.083603, mean_eps: 0.828974
 476177/1000000: episode: 704, duration: 49.161s, episode steps: 781, steps per second:  16, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013535, mae: 2.554857, mean_q: 3.086851, mean_eps: 0.828717
 477145/1000000: episode: 705, duration: 59.878s, episode steps: 968, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.014615, mae: 2.548737, mean_q: 3.080744, mean_eps: 0.828402
 477662/1000000: episode: 706, duration: 32.353s, episode steps: 517, steps per second:  16, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.013683, mae: 2.555024, mean_q: 3.086685, mean_eps: 0.828135
 478034/1000000: episode: 707, duration: 24.244s, episode steps: 372, steps per second:  15, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.013153, mae: 2.548246, mean_q: 3.081196, mean_eps: 0.827975
 479193/1000000: episode: 708, duration: 71.643s, episode steps: 1159, steps per second:  16, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.014967, mae: 2.552438, mean_q: 3.084568, mean_eps: 0.827699
 479632/1000000: episode: 709, duration: 28.041s, episode steps: 439, steps per second:  16, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.012843, mae: 2.575547, mean_q: 3.111680, mean_eps: 0.827412
 480021/1000000: episode: 710, duration: 24.664s, episode steps: 389, steps per second:  16, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.015228, mae: 2.572341, mean_q: 3.110298, mean_eps: 0.827263
 480651/1000000: episode: 711, duration: 38.893s, episode steps: 630, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.014302, mae: 2.542099, mean_q: 3.072602, mean_eps: 0.827079
 481329/1000000: episode: 712, duration: 43.588s, episode steps: 678, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.015471, mae: 2.562452, mean_q: 3.094826, mean_eps: 0.826844
 482094/1000000: episode: 713, duration: 47.829s, episode steps: 765, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.014975, mae: 2.532916, mean_q: 3.057547, mean_eps: 0.826584
 482899/1000000: episode: 714, duration: 49.842s, episode steps: 805, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.015844, mae: 2.564336, mean_q: 3.099354, mean_eps: 0.826301
 483518/1000000: episode: 715, duration: 38.125s, episode steps: 619, steps per second:  16, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.013387, mae: 2.581045, mean_q: 3.117830, mean_eps: 0.826045
 484266/1000000: episode: 716, duration: 46.737s, episode steps: 748, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.015993, mae: 2.529440, mean_q: 3.053672, mean_eps: 0.825799
 484924/1000000: episode: 717, duration: 42.082s, episode steps: 658, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.014626, mae: 2.569202, mean_q: 3.103278, mean_eps: 0.825546
 485445/1000000: episode: 718, duration: 32.821s, episode steps: 521, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.014958, mae: 2.550350, mean_q: 3.080835, mean_eps: 0.825334
 485796/1000000: episode: 719, duration: 22.119s, episode steps: 351, steps per second:  16, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.014939, mae: 2.543910, mean_q: 3.069329, mean_eps: 0.825177
 486512/1000000: episode: 720, duration: 45.294s, episode steps: 716, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.015871, mae: 2.560558, mean_q: 3.090409, mean_eps: 0.824985
 487041/1000000: episode: 721, duration: 33.144s, episode steps: 529, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.013310, mae: 2.568735, mean_q: 3.102598, mean_eps: 0.824761
 487665/1000000: episode: 722, duration: 41.040s, episode steps: 624, steps per second:  15, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.013326, mae: 2.544696, mean_q: 3.075520, mean_eps: 0.824553
 488316/1000000: episode: 723, duration: 40.503s, episode steps: 651, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.015008, mae: 2.571550, mean_q: 3.106802, mean_eps: 0.824324
 488884/1000000: episode: 724, duration: 35.583s, episode steps: 568, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.014311, mae: 2.555063, mean_q: 3.087638, mean_eps: 0.824105
 489409/1000000: episode: 725, duration: 33.961s, episode steps: 525, steps per second:  15, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.014828, mae: 2.556059, mean_q: 3.088489, mean_eps: 0.823907
 490133/1000000: episode: 726, duration: 44.815s, episode steps: 724, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014701, mae: 2.539347, mean_q: 3.066796, mean_eps: 0.823682
 490597/1000000: episode: 727, duration: 29.361s, episode steps: 464, steps per second:  16, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.015605, mae: 2.571563, mean_q: 3.105885, mean_eps: 0.823468
 491357/1000000: episode: 728, duration: 49.897s, episode steps: 760, steps per second:  15, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014033, mae: 2.557324, mean_q: 3.090804, mean_eps: 0.823248
 492103/1000000: episode: 729, duration: 45.306s, episode steps: 746, steps per second:  16, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.013560, mae: 2.567158, mean_q: 3.103762, mean_eps: 0.822977
 492778/1000000: episode: 730, duration: 43.143s, episode steps: 675, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.014892, mae: 2.562410, mean_q: 3.096114, mean_eps: 0.822722
 493274/1000000: episode: 731, duration: 32.641s, episode steps: 496, steps per second:  15, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.012762, mae: 2.555355, mean_q: 3.086596, mean_eps: 0.822511
 494102/1000000: episode: 732, duration: 52.509s, episode steps: 828, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.014289, mae: 2.542527, mean_q: 3.071207, mean_eps: 0.822272
 494648/1000000: episode: 733, duration: 35.231s, episode steps: 546, steps per second:  15, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.017888, mae: 2.550997, mean_q: 3.078793, mean_eps: 0.822025
 495442/1000000: episode: 734, duration: 53.434s, episode steps: 794, steps per second:  15, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.016171, mae: 2.549326, mean_q: 3.077878, mean_eps: 0.821784
 496138/1000000: episode: 735, duration: 44.221s, episode steps: 696, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.013542, mae: 2.558963, mean_q: 3.090214, mean_eps: 0.821516
 496928/1000000: episode: 736, duration: 51.262s, episode steps: 790, steps per second:  15, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.014570, mae: 2.537389, mean_q: 3.064128, mean_eps: 0.821248
 497321/1000000: episode: 737, duration: 25.286s, episode steps: 393, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.013196, mae: 2.560731, mean_q: 3.094722, mean_eps: 0.821035
 497718/1000000: episode: 738, duration: 25.398s, episode steps: 397, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013445, mae: 2.529822, mean_q: 3.057346, mean_eps: 0.820893
 498536/1000000: episode: 739, duration: 52.789s, episode steps: 818, steps per second:  15, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.014281, mae: 2.539340, mean_q: 3.066986, mean_eps: 0.820675
 499276/1000000: episode: 740, duration: 55.229s, episode steps: 740, steps per second:  13, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.013299, mae: 2.562452, mean_q: 3.096053, mean_eps: 0.820395
 500152/1000000: episode: 741, duration: 57.194s, episode steps: 876, steps per second:  15, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014668, mae: 2.544052, mean_q: 3.072901, mean_eps: 0.820104
 500935/1000000: episode: 742, duration: 52.257s, episode steps: 783, steps per second:  15, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.014469, mae: 2.539093, mean_q: 3.066573, mean_eps: 0.819805
 501716/1000000: episode: 743, duration: 51.116s, episode steps: 781, steps per second:  15, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.014908, mae: 2.532198, mean_q: 3.059657, mean_eps: 0.819523
 502293/1000000: episode: 744, duration: 37.527s, episode steps: 577, steps per second:  15, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.015724, mae: 2.539872, mean_q: 3.070300, mean_eps: 0.819279
 502750/1000000: episode: 745, duration: 30.363s, episode steps: 457, steps per second:  15, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.013663, mae: 2.540168, mean_q: 3.070718, mean_eps: 0.819092
 503243/1000000: episode: 746, duration: 31.982s, episode steps: 493, steps per second:  15, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.015868, mae: 2.568050, mean_q: 3.102315, mean_eps: 0.818921
 503885/1000000: episode: 747, duration: 41.327s, episode steps: 642, steps per second:  16, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.015904, mae: 2.544090, mean_q: 3.073337, mean_eps: 0.818717
 504628/1000000: episode: 748, duration: 49.890s, episode steps: 743, steps per second:  15, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.014113, mae: 2.535031, mean_q: 3.064998, mean_eps: 0.818468
 505300/1000000: episode: 749, duration: 43.658s, episode steps: 672, steps per second:  15, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014142, mae: 2.527472, mean_q: 3.053648, mean_eps: 0.818214
 506059/1000000: episode: 750, duration: 49.167s, episode steps: 759, steps per second:  15, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.014398, mae: 2.562615, mean_q: 3.095636, mean_eps: 0.817956
 506853/1000000: episode: 751, duration: 52.101s, episode steps: 794, steps per second:  15, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.014023, mae: 2.535167, mean_q: 3.062353, mean_eps: 0.817676
 507568/1000000: episode: 752, duration: 45.912s, episode steps: 715, steps per second:  16, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013215, mae: 2.548143, mean_q: 3.076933, mean_eps: 0.817404
 508078/1000000: episode: 753, duration: 34.544s, episode steps: 510, steps per second:  15, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.017471, mae: 2.528253, mean_q: 3.050667, mean_eps: 0.817184
 509022/1000000: episode: 754, duration: 55.570s, episode steps: 944, steps per second:  17, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.014770, mae: 2.538799, mean_q: 3.065482, mean_eps: 0.816922
 509587/1000000: episode: 755, duration: 34.616s, episode steps: 565, steps per second:  16, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.015933, mae: 2.540449, mean_q: 3.069424, mean_eps: 0.816651
 510184/1000000: episode: 756, duration: 40.487s, episode steps: 597, steps per second:  15, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015598, mae: 2.555524, mean_q: 3.088083, mean_eps: 0.816442
 510672/1000000: episode: 757, duration: 32.423s, episode steps: 488, steps per second:  15, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.015146, mae: 2.543245, mean_q: 3.074994, mean_eps: 0.816247
 511526/1000000: episode: 758, duration: 55.814s, episode steps: 854, steps per second:  15, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.016509, mae: 2.558109, mean_q: 3.088760, mean_eps: 0.816005
 512187/1000000: episode: 759, duration: 44.082s, episode steps: 661, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.012839, mae: 2.539166, mean_q: 3.068364, mean_eps: 0.815732
 513369/1000000: episode: 760, duration: 77.424s, episode steps: 1182, steps per second:  15, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.013548, mae: 2.544491, mean_q: 3.075071, mean_eps: 0.815400
 514066/1000000: episode: 761, duration: 46.268s, episode steps: 697, steps per second:  15, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.016285, mae: 2.537437, mean_q: 3.064736, mean_eps: 0.815062
 514755/1000000: episode: 762, duration: 44.308s, episode steps: 689, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.016328, mae: 2.546278, mean_q: 3.076573, mean_eps: 0.814812
 515148/1000000: episode: 763, duration: 25.457s, episode steps: 393, steps per second:  15, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.015310, mae: 2.577772, mean_q: 3.115516, mean_eps: 0.814618
 515691/1000000: episode: 764, duration: 34.676s, episode steps: 543, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.014551, mae: 2.552443, mean_q: 3.084769, mean_eps: 0.814450
 516234/1000000: episode: 765, duration: 37.581s, episode steps: 543, steps per second:  14, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.014668, mae: 2.549385, mean_q: 3.080457, mean_eps: 0.814254
 516612/1000000: episode: 766, duration: 24.024s, episode steps: 378, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.013883, mae: 2.534557, mean_q: 3.064636, mean_eps: 0.814088
 517285/1000000: episode: 767, duration: 44.531s, episode steps: 673, steps per second:  15, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.015254, mae: 2.542656, mean_q: 3.072593, mean_eps: 0.813899
 518066/1000000: episode: 768, duration: 50.880s, episode steps: 781, steps per second:  15, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.014924, mae: 2.546910, mean_q: 3.077710, mean_eps: 0.813637
 518967/1000000: episode: 769, duration: 49.570s, episode steps: 901, steps per second:  18, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.012878, mae: 2.540067, mean_q: 3.069869, mean_eps: 0.813334
 519360/1000000: episode: 770, duration: 26.300s, episode steps: 393, steps per second:  15, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.016090, mae: 2.556546, mean_q: 3.086763, mean_eps: 0.813102
 520106/1000000: episode: 771, duration: 49.140s, episode steps: 746, steps per second:  15, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.014496, mae: 2.526886, mean_q: 3.053694, mean_eps: 0.812896
 520786/1000000: episode: 772, duration: 44.269s, episode steps: 680, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.014882, mae: 2.572070, mean_q: 3.107367, mean_eps: 0.812639
 521785/1000000: episode: 773, duration: 65.686s, episode steps: 999, steps per second:  15, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.014683, mae: 2.562059, mean_q: 3.094167, mean_eps: 0.812337
 522213/1000000: episode: 774, duration: 27.696s, episode steps: 428, steps per second:  15, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.015533, mae: 2.562252, mean_q: 3.093473, mean_eps: 0.812080
 522758/1000000: episode: 775, duration: 35.960s, episode steps: 545, steps per second:  15, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.013727, mae: 2.538147, mean_q: 3.065589, mean_eps: 0.811905
 523617/1000000: episode: 776, duration: 56.921s, episode steps: 859, steps per second:  15, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013118, mae: 2.546079, mean_q: 3.074519, mean_eps: 0.811652
 524236/1000000: episode: 777, duration: 40.392s, episode steps: 619, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.013867, mae: 2.526158, mean_q: 3.049139, mean_eps: 0.811387
 524815/1000000: episode: 778, duration: 35.172s, episode steps: 579, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.013719, mae: 2.552096, mean_q: 3.082280, mean_eps: 0.811171
 525598/1000000: episode: 779, duration: 51.757s, episode steps: 783, steps per second:  15, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.013355, mae: 2.536143, mean_q: 3.062142, mean_eps: 0.810926
 526657/1000000: episode: 780, duration: 73.780s, episode steps: 1059, steps per second:  14, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.013275, mae: 2.546000, mean_q: 3.074556, mean_eps: 0.810594
 527288/1000000: episode: 781, duration: 42.923s, episode steps: 631, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.014937, mae: 2.563321, mean_q: 3.095203, mean_eps: 0.810290
 528339/1000000: episode: 782, duration: 70.393s, episode steps: 1051, steps per second:  15, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.014900, mae: 2.543065, mean_q: 3.071975, mean_eps: 0.809988
 529036/1000000: episode: 783, duration: 45.804s, episode steps: 697, steps per second:  15, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.014256, mae: 2.547029, mean_q: 3.075510, mean_eps: 0.809673
 529656/1000000: episode: 784, duration: 42.806s, episode steps: 620, steps per second:  14, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.013745, mae: 2.555731, mean_q: 3.089791, mean_eps: 0.809436
 530653/1000000: episode: 785, duration: 66.798s, episode steps: 997, steps per second:  15, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.015218, mae: 2.552769, mean_q: 3.083569, mean_eps: 0.809145
 531218/1000000: episode: 786, duration: 38.766s, episode steps: 565, steps per second:  15, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.013559, mae: 2.546854, mean_q: 3.076412, mean_eps: 0.808863
 531759/1000000: episode: 787, duration: 33.449s, episode steps: 541, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.013265, mae: 2.552970, mean_q: 3.082948, mean_eps: 0.808664
 532122/1000000: episode: 788, duration: 25.067s, episode steps: 363, steps per second:  14, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.017177, mae: 2.552224, mean_q: 3.084098, mean_eps: 0.808502
 533467/1000000: episode: 789, duration: 90.124s, episode steps: 1345, steps per second:  15, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014370, mae: 2.551676, mean_q: 3.082568, mean_eps: 0.808194
 533841/1000000: episode: 790, duration: 25.217s, episode steps: 374, steps per second:  15, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.015154, mae: 2.554653, mean_q: 3.085654, mean_eps: 0.807885
 534760/1000000: episode: 791, duration: 61.687s, episode steps: 919, steps per second:  15, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.014760, mae: 2.556892, mean_q: 3.087900, mean_eps: 0.807652
 535491/1000000: episode: 792, duration: 40.361s, episode steps: 731, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.014462, mae: 2.561684, mean_q: 3.093263, mean_eps: 0.807355
 536823/1000000: episode: 793, duration: 87.289s, episode steps: 1332, steps per second:  15, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.014905, mae: 2.550241, mean_q: 3.079478, mean_eps: 0.806984
 537456/1000000: episode: 794, duration: 42.168s, episode steps: 633, steps per second:  15, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.014310, mae: 2.569460, mean_q: 3.104759, mean_eps: 0.806630
 538115/1000000: episode: 795, duration: 44.429s, episode steps: 659, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015912, mae: 2.553404, mean_q: 3.084810, mean_eps: 0.806398
 539630/1000000: episode: 796, duration: 100.742s, episode steps: 1515, steps per second:  15, episode reward: 16.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.014735, mae: 2.550952, mean_q: 3.080961, mean_eps: 0.806006
 540317/1000000: episode: 797, duration: 50.245s, episode steps: 687, steps per second:  14, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.014990, mae: 2.553678, mean_q: 3.084542, mean_eps: 0.805609
 541008/1000000: episode: 798, duration: 48.336s, episode steps: 691, steps per second:  14, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.014683, mae: 2.553672, mean_q: 3.084216, mean_eps: 0.805362
 541809/1000000: episode: 799, duration: 54.049s, episode steps: 801, steps per second:  15, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.015536, mae: 2.534690, mean_q: 3.061066, mean_eps: 0.805093
 542752/1000000: episode: 800, duration: 64.704s, episode steps: 943, steps per second:  15, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013940, mae: 2.556689, mean_q: 3.088779, mean_eps: 0.804779
 543531/1000000: episode: 801, duration: 53.894s, episode steps: 779, steps per second:  14, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.014062, mae: 2.528742, mean_q: 3.055147, mean_eps: 0.804470
 544038/1000000: episode: 802, duration: 35.661s, episode steps: 507, steps per second:  14, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014100, mae: 2.572061, mean_q: 3.108089, mean_eps: 0.804238
 544494/1000000: episode: 803, duration: 30.606s, episode steps: 456, steps per second:  15, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.014616, mae: 2.545035, mean_q: 3.073597, mean_eps: 0.804064
 545278/1000000: episode: 804, duration: 43.089s, episode steps: 784, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014521, mae: 2.534359, mean_q: 3.063243, mean_eps: 0.803841
 545916/1000000: episode: 805, duration: 38.126s, episode steps: 638, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.014918, mae: 2.546354, mean_q: 3.074780, mean_eps: 0.803585
 546772/1000000: episode: 806, duration: 50.982s, episode steps: 856, steps per second:  17, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.014824, mae: 2.540746, mean_q: 3.069387, mean_eps: 0.803317
 547372/1000000: episode: 807, duration: 34.910s, episode steps: 600, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.014412, mae: 2.543995, mean_q: 3.069397, mean_eps: 0.803055
 548299/1000000: episode: 808, duration: 60.381s, episode steps: 927, steps per second:  15, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.014843, mae: 2.551321, mean_q: 3.079734, mean_eps: 0.802780
 549065/1000000: episode: 809, duration: 51.525s, episode steps: 766, steps per second:  15, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.014190, mae: 2.564840, mean_q: 3.097638, mean_eps: 0.802474
 549562/1000000: episode: 810, duration: 33.070s, episode steps: 497, steps per second:  15, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.015125, mae: 2.561229, mean_q: 3.094563, mean_eps: 0.802247
 550170/1000000: episode: 811, duration: 43.326s, episode steps: 608, steps per second:  14, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.014968, mae: 2.562340, mean_q: 3.094538, mean_eps: 0.802048
 550747/1000000: episode: 812, duration: 38.266s, episode steps: 577, steps per second:  15, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012640, mae: 2.544011, mean_q: 3.071045, mean_eps: 0.801835
 551399/1000000: episode: 813, duration: 44.372s, episode steps: 652, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.012590, mae: 2.548234, mean_q: 3.078032, mean_eps: 0.801614
 551828/1000000: episode: 814, duration: 29.901s, episode steps: 429, steps per second:  14, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.015496, mae: 2.572738, mean_q: 3.106577, mean_eps: 0.801420
 552369/1000000: episode: 815, duration: 36.053s, episode steps: 541, steps per second:  15, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.013175, mae: 2.555788, mean_q: 3.091162, mean_eps: 0.801245
 552820/1000000: episode: 816, duration: 30.133s, episode steps: 451, steps per second:  15, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.013387, mae: 2.550595, mean_q: 3.081616, mean_eps: 0.801066
 553708/1000000: episode: 817, duration: 60.372s, episode steps: 888, steps per second:  15, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.012820, mae: 2.565910, mean_q: 3.100892, mean_eps: 0.800826
 554458/1000000: episode: 818, duration: 50.534s, episode steps: 750, steps per second:  15, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014236, mae: 2.550643, mean_q: 3.080106, mean_eps: 0.800530
 554915/1000000: episode: 819, duration: 30.867s, episode steps: 457, steps per second:  15, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.013291, mae: 2.587809, mean_q: 3.126608, mean_eps: 0.800313
 555475/1000000: episode: 820, duration: 37.982s, episode steps: 560, steps per second:  15, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.013481, mae: 2.579590, mean_q: 3.112609, mean_eps: 0.800130
 556155/1000000: episode: 821, duration: 45.803s, episode steps: 680, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.014709, mae: 2.543772, mean_q: 3.069217, mean_eps: 0.799907
 556856/1000000: episode: 822, duration: 47.031s, episode steps: 701, steps per second:  15, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.014651, mae: 2.552930, mean_q: 3.081709, mean_eps: 0.799659
 557271/1000000: episode: 823, duration: 29.204s, episode steps: 415, steps per second:  14, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.014575, mae: 2.590260, mean_q: 3.126298, mean_eps: 0.799458
 557798/1000000: episode: 824, duration: 35.695s, episode steps: 527, steps per second:  15, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.015124, mae: 2.545577, mean_q: 3.074782, mean_eps: 0.799288
 558316/1000000: episode: 825, duration: 35.086s, episode steps: 518, steps per second:  15, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.016654, mae: 2.568363, mean_q: 3.100525, mean_eps: 0.799100
 558852/1000000: episode: 826, duration: 37.751s, episode steps: 536, steps per second:  14, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013372, mae: 2.563616, mean_q: 3.093557, mean_eps: 0.798910
 559647/1000000: episode: 827, duration: 54.433s, episode steps: 795, steps per second:  15, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.013098, mae: 2.556590, mean_q: 3.087046, mean_eps: 0.798671
 560146/1000000: episode: 828, duration: 33.670s, episode steps: 499, steps per second:  15, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.013219, mae: 2.555509, mean_q: 3.086440, mean_eps: 0.798437
 560760/1000000: episode: 829, duration: 42.344s, episode steps: 614, steps per second:  15, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.015568, mae: 2.576938, mean_q: 3.110953, mean_eps: 0.798237
 561410/1000000: episode: 830, duration: 44.232s, episode steps: 650, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.016203, mae: 2.571972, mean_q: 3.107248, mean_eps: 0.798010
 562139/1000000: episode: 831, duration: 49.131s, episode steps: 729, steps per second:  15, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.014296, mae: 2.545154, mean_q: 3.075464, mean_eps: 0.797761
 562672/1000000: episode: 832, duration: 37.216s, episode steps: 533, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.014589, mae: 2.545051, mean_q: 3.076355, mean_eps: 0.797535
 563179/1000000: episode: 833, duration: 33.988s, episode steps: 507, steps per second:  15, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.013836, mae: 2.541537, mean_q: 3.069596, mean_eps: 0.797347
 563862/1000000: episode: 834, duration: 47.047s, episode steps: 683, steps per second:  15, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.016679, mae: 2.562771, mean_q: 3.093936, mean_eps: 0.797133
 564354/1000000: episode: 835, duration: 33.206s, episode steps: 492, steps per second:  15, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.015302, mae: 2.578142, mean_q: 3.114015, mean_eps: 0.796921
 565016/1000000: episode: 836, duration: 42.596s, episode steps: 662, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.013711, mae: 2.563158, mean_q: 3.096187, mean_eps: 0.796714
 565774/1000000: episode: 837, duration: 43.654s, episode steps: 758, steps per second:  17, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.014467, mae: 2.571422, mean_q: 3.104520, mean_eps: 0.796458
 566752/1000000: episode: 838, duration: 56.239s, episode steps: 978, steps per second:  17, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.014212, mae: 2.578271, mean_q: 3.114117, mean_eps: 0.796146
 567370/1000000: episode: 839, duration: 42.544s, episode steps: 618, steps per second:  15, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.016717, mae: 2.548462, mean_q: 3.077504, mean_eps: 0.795858
 568001/1000000: episode: 840, duration: 43.168s, episode steps: 631, steps per second:  15, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.014871, mae: 2.544122, mean_q: 3.071836, mean_eps: 0.795633
 568586/1000000: episode: 841, duration: 39.875s, episode steps: 585, steps per second:  15, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.013825, mae: 2.566650, mean_q: 3.100149, mean_eps: 0.795414
 569569/1000000: episode: 842, duration: 66.207s, episode steps: 983, steps per second:  15, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013623, mae: 2.554752, mean_q: 3.084476, mean_eps: 0.795132
 570434/1000000: episode: 843, duration: 58.388s, episode steps: 865, steps per second:  15, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.013577, mae: 2.549814, mean_q: 3.078222, mean_eps: 0.794799
 571114/1000000: episode: 844, duration: 46.815s, episode steps: 680, steps per second:  15, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.013554, mae: 2.554751, mean_q: 3.086249, mean_eps: 0.794521
 571481/1000000: episode: 845, duration: 24.364s, episode steps: 367, steps per second:  15, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.015851, mae: 2.572065, mean_q: 3.106301, mean_eps: 0.794333
 572129/1000000: episode: 846, duration: 43.794s, episode steps: 648, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.012560, mae: 2.540829, mean_q: 3.067621, mean_eps: 0.794150
 572665/1000000: episode: 847, duration: 36.771s, episode steps: 536, steps per second:  15, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.016521, mae: 2.572781, mean_q: 3.105788, mean_eps: 0.793937
 573223/1000000: episode: 848, duration: 37.625s, episode steps: 558, steps per second:  15, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.013818, mae: 2.563731, mean_q: 3.095508, mean_eps: 0.793740
 574086/1000000: episode: 849, duration: 57.534s, episode steps: 863, steps per second:  15, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.013679, mae: 2.545622, mean_q: 3.072372, mean_eps: 0.793485
 574975/1000000: episode: 850, duration: 60.162s, episode steps: 889, steps per second:  15, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.014699, mae: 2.557771, mean_q: 3.088335, mean_eps: 0.793169
 575630/1000000: episode: 851, duration: 44.174s, episode steps: 655, steps per second:  15, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.013021, mae: 2.536563, mean_q: 3.062868, mean_eps: 0.792891
 576439/1000000: episode: 852, duration: 54.263s, episode steps: 809, steps per second:  15, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.015012, mae: 2.553458, mean_q: 3.081154, mean_eps: 0.792628
 577143/1000000: episode: 853, duration: 47.122s, episode steps: 704, steps per second:  15, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.015176, mae: 2.568567, mean_q: 3.101359, mean_eps: 0.792356
 577645/1000000: episode: 854, duration: 33.641s, episode steps: 502, steps per second:  15, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.014625, mae: 2.575671, mean_q: 3.109496, mean_eps: 0.792138
 578256/1000000: episode: 855, duration: 42.417s, episode steps: 611, steps per second:  14, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.016821, mae: 2.556351, mean_q: 3.085308, mean_eps: 0.791938
 578651/1000000: episode: 856, duration: 26.102s, episode steps: 395, steps per second:  15, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.014118, mae: 2.585195, mean_q: 3.120214, mean_eps: 0.791757
 579659/1000000: episode: 857, duration: 68.170s, episode steps: 1008, steps per second:  15, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.015211, mae: 2.567628, mean_q: 3.100978, mean_eps: 0.791505
 580521/1000000: episode: 858, duration: 59.546s, episode steps: 862, steps per second:  14, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.014814, mae: 2.545053, mean_q: 3.072102, mean_eps: 0.791168
 581403/1000000: episode: 859, duration: 63.517s, episode steps: 882, steps per second:  14, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.013203, mae: 2.556863, mean_q: 3.088982, mean_eps: 0.790854
 582046/1000000: episode: 860, duration: 45.390s, episode steps: 643, steps per second:  14, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.016688, mae: 2.563084, mean_q: 3.092138, mean_eps: 0.790579
 582782/1000000: episode: 861, duration: 56.448s, episode steps: 736, steps per second:  13, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.015283, mae: 2.573223, mean_q: 3.106924, mean_eps: 0.790331
 583443/1000000: episode: 862, duration: 47.198s, episode steps: 661, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.015180, mae: 2.561373, mean_q: 3.091250, mean_eps: 0.790080
 584081/1000000: episode: 863, duration: 45.584s, episode steps: 638, steps per second:  14, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.016702, mae: 2.554024, mean_q: 3.081146, mean_eps: 0.789846
 585424/1000000: episode: 864, duration: 90.238s, episode steps: 1343, steps per second:  15, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013365, mae: 2.570288, mean_q: 3.103400, mean_eps: 0.789489
 585982/1000000: episode: 865, duration: 34.448s, episode steps: 558, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.016335, mae: 2.560784, mean_q: 3.091565, mean_eps: 0.789147
 586890/1000000: episode: 866, duration: 66.145s, episode steps: 908, steps per second:  14, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.014364, mae: 2.572040, mean_q: 3.106107, mean_eps: 0.788883
 587417/1000000: episode: 867, duration: 34.366s, episode steps: 527, steps per second:  15, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.013018, mae: 2.566616, mean_q: 3.101126, mean_eps: 0.788625
 587847/1000000: episode: 868, duration: 28.878s, episode steps: 430, steps per second:  15, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.015565, mae: 2.577067, mean_q: 3.114547, mean_eps: 0.788452
 588375/1000000: episode: 869, duration: 38.598s, episode steps: 528, steps per second:  14, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.014918, mae: 2.545785, mean_q: 3.073989, mean_eps: 0.788280
 589510/1000000: episode: 870, duration: 74.447s, episode steps: 1135, steps per second:  15, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.014872, mae: 2.555620, mean_q: 3.084674, mean_eps: 0.787981
 589935/1000000: episode: 871, duration: 25.820s, episode steps: 425, steps per second:  16, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.015594, mae: 2.555445, mean_q: 3.083439, mean_eps: 0.787700
 590621/1000000: episode: 872, duration: 42.052s, episode steps: 686, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.013944, mae: 2.552594, mean_q: 3.079578, mean_eps: 0.787500
 591330/1000000: episode: 873, duration: 41.230s, episode steps: 709, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.015363, mae: 2.566196, mean_q: 3.097704, mean_eps: 0.787249
 591700/1000000: episode: 874, duration: 22.036s, episode steps: 370, steps per second:  17, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.013456, mae: 2.550033, mean_q: 3.079180, mean_eps: 0.787055
 593023/1000000: episode: 875, duration: 75.719s, episode steps: 1323, steps per second:  17, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.015296, mae: 2.569225, mean_q: 3.100316, mean_eps: 0.786750
 593559/1000000: episode: 876, duration: 31.767s, episode steps: 536, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.017543, mae: 2.563894, mean_q: 3.096802, mean_eps: 0.786416
 594050/1000000: episode: 877, duration: 28.408s, episode steps: 491, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.015965, mae: 2.559849, mean_q: 3.088974, mean_eps: 0.786231
 594653/1000000: episode: 878, duration: 39.092s, episode steps: 603, steps per second:  15, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.014194, mae: 2.552869, mean_q: 3.081296, mean_eps: 0.786033
 595515/1000000: episode: 879, duration: 59.232s, episode steps: 862, steps per second:  15, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.015740, mae: 2.560867, mean_q: 3.090303, mean_eps: 0.785770
 596160/1000000: episode: 880, duration: 44.795s, episode steps: 645, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.015132, mae: 2.565305, mean_q: 3.095786, mean_eps: 0.785499
 596804/1000000: episode: 881, duration: 46.123s, episode steps: 644, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.015714, mae: 2.549493, mean_q: 3.076765, mean_eps: 0.785267
 597469/1000000: episode: 882, duration: 45.712s, episode steps: 665, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.014584, mae: 2.588971, mean_q: 3.123618, mean_eps: 0.785031
 597918/1000000: episode: 883, duration: 31.083s, episode steps: 449, steps per second:  14, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.016193, mae: 2.545249, mean_q: 3.069407, mean_eps: 0.784830
 598819/1000000: episode: 884, duration: 63.213s, episode steps: 901, steps per second:  14, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013964, mae: 2.563906, mean_q: 3.094269, mean_eps: 0.784588
 599292/1000000: episode: 885, duration: 32.656s, episode steps: 473, steps per second:  14, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.016811, mae: 2.547167, mean_q: 3.072867, mean_eps: 0.784341
 599875/1000000: episode: 886, duration: 39.993s, episode steps: 583, steps per second:  15, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.014254, mae: 2.557051, mean_q: 3.087883, mean_eps: 0.784150
 600278/1000000: episode: 887, duration: 28.693s, episode steps: 403, steps per second:  14, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.015824, mae: 2.558925, mean_q: 3.088141, mean_eps: 0.783973
 600935/1000000: episode: 888, duration: 44.983s, episode steps: 657, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.014368, mae: 2.576739, mean_q: 3.113898, mean_eps: 0.783782
 601492/1000000: episode: 889, duration: 38.325s, episode steps: 557, steps per second:  15, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.012295, mae: 2.577540, mean_q: 3.114375, mean_eps: 0.783564
 602137/1000000: episode: 890, duration: 44.545s, episode steps: 645, steps per second:  14, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.013225, mae: 2.562412, mean_q: 3.094664, mean_eps: 0.783347
 602632/1000000: episode: 891, duration: 33.541s, episode steps: 495, steps per second:  15, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012555, mae: 2.567237, mean_q: 3.101298, mean_eps: 0.783142
 603268/1000000: episode: 892, duration: 43.942s, episode steps: 636, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012237, mae: 2.573692, mean_q: 3.108955, mean_eps: 0.782939
 603685/1000000: episode: 893, duration: 28.712s, episode steps: 417, steps per second:  15, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.017341, mae: 2.575252, mean_q: 3.106884, mean_eps: 0.782749
 604471/1000000: episode: 894, duration: 54.530s, episode steps: 786, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.016440, mae: 2.552439, mean_q: 3.083385, mean_eps: 0.782532
 605114/1000000: episode: 895, duration: 44.577s, episode steps: 643, steps per second:  14, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.016537, mae: 2.581592, mean_q: 3.118760, mean_eps: 0.782275
 605490/1000000: episode: 896, duration: 25.441s, episode steps: 376, steps per second:  15, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.013096, mae: 2.573938, mean_q: 3.106075, mean_eps: 0.782091
 606127/1000000: episode: 897, duration: 44.430s, episode steps: 637, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.014523, mae: 2.584464, mean_q: 3.118292, mean_eps: 0.781909
 606720/1000000: episode: 898, duration: 41.477s, episode steps: 593, steps per second:  14, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.015698, mae: 2.547590, mean_q: 3.074335, mean_eps: 0.781688
 607079/1000000: episode: 899, duration: 23.967s, episode steps: 359, steps per second:  15, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014705, mae: 2.584411, mean_q: 3.120365, mean_eps: 0.781517
 607457/1000000: episode: 900, duration: 26.974s, episode steps: 378, steps per second:  14, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014987, mae: 2.537834, mean_q: 3.063089, mean_eps: 0.781384
 608248/1000000: episode: 901, duration: 57.022s, episode steps: 791, steps per second:  14, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012654, mae: 2.569566, mean_q: 3.100337, mean_eps: 0.781173
 608761/1000000: episode: 902, duration: 35.855s, episode steps: 513, steps per second:  14, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.014260, mae: 2.549951, mean_q: 3.077977, mean_eps: 0.780939
 609723/1000000: episode: 903, duration: 66.539s, episode steps: 962, steps per second:  14, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.014169, mae: 2.533896, mean_q: 3.060513, mean_eps: 0.780673
 610504/1000000: episode: 904, duration: 54.040s, episode steps: 781, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.014730, mae: 2.560856, mean_q: 3.091546, mean_eps: 0.780360
 611229/1000000: episode: 905, duration: 51.031s, episode steps: 725, steps per second:  14, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.014220, mae: 2.550418, mean_q: 3.079271, mean_eps: 0.780088
 612753/1000000: episode: 906, duration: 107.085s, episode steps: 1524, steps per second:  14, episode reward: 16.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.014423, mae: 2.560160, mean_q: 3.090286, mean_eps: 0.779683
 613300/1000000: episode: 907, duration: 40.026s, episode steps: 547, steps per second:  14, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.012929, mae: 2.560643, mean_q: 3.093316, mean_eps: 0.779311
 614256/1000000: episode: 908, duration: 66.702s, episode steps: 956, steps per second:  14, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.013923, mae: 2.545623, mean_q: 3.073673, mean_eps: 0.779041
 614993/1000000: episode: 909, duration: 52.139s, episode steps: 737, steps per second:  14, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.015989, mae: 2.585412, mean_q: 3.121066, mean_eps: 0.778735
 615779/1000000: episode: 910, duration: 54.813s, episode steps: 786, steps per second:  14, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.016072, mae: 2.549980, mean_q: 3.080900, mean_eps: 0.778461
 616489/1000000: episode: 911, duration: 50.549s, episode steps: 710, steps per second:  14, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013003, mae: 2.546524, mean_q: 3.072659, mean_eps: 0.778192
 617373/1000000: episode: 912, duration: 54.350s, episode steps: 884, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.012944, mae: 2.561759, mean_q: 3.093751, mean_eps: 0.777904
 617857/1000000: episode: 913, duration: 29.382s, episode steps: 484, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.014791, mae: 2.571013, mean_q: 3.102664, mean_eps: 0.777658
 618447/1000000: episode: 914, duration: 35.562s, episode steps: 590, steps per second:  17, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014277, mae: 2.547728, mean_q: 3.076823, mean_eps: 0.777465
 618875/1000000: episode: 915, duration: 28.894s, episode steps: 428, steps per second:  15, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.015536, mae: 2.555377, mean_q: 3.084226, mean_eps: 0.777282
 619503/1000000: episode: 916, duration: 41.012s, episode steps: 628, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.013847, mae: 2.575524, mean_q: 3.111352, mean_eps: 0.777092
 620139/1000000: episode: 917, duration: 43.673s, episode steps: 636, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.017601, mae: 2.557764, mean_q: 3.087296, mean_eps: 0.776865
 620546/1000000: episode: 918, duration: 29.686s, episode steps: 407, steps per second:  14, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.011583, mae: 2.590664, mean_q: 3.126909, mean_eps: 0.776677
 621138/1000000: episode: 919, duration: 41.804s, episode steps: 592, steps per second:  14, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.013780, mae: 2.536620, mean_q: 3.062670, mean_eps: 0.776497
 622077/1000000: episode: 920, duration: 65.642s, episode steps: 939, steps per second:  14, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.015217, mae: 2.540495, mean_q: 3.066484, mean_eps: 0.776221
 622735/1000000: episode: 921, duration: 46.136s, episode steps: 658, steps per second:  14, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.014810, mae: 2.578404, mean_q: 3.110479, mean_eps: 0.775934
 623728/1000000: episode: 922, duration: 68.793s, episode steps: 993, steps per second:  14, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.013885, mae: 2.568378, mean_q: 3.101310, mean_eps: 0.775637
 624644/1000000: episode: 923, duration: 64.333s, episode steps: 916, steps per second:  14, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.015250, mae: 2.559328, mean_q: 3.090281, mean_eps: 0.775294
 625611/1000000: episode: 924, duration: 66.815s, episode steps: 967, steps per second:  14, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.015819, mae: 2.566644, mean_q: 3.099355, mean_eps: 0.774955
 625984/1000000: episode: 925, duration: 27.418s, episode steps: 373, steps per second:  14, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.014613, mae: 2.568592, mean_q: 3.099339, mean_eps: 0.774713
 626697/1000000: episode: 926, duration: 48.859s, episode steps: 713, steps per second:  15, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.015312, mae: 2.563955, mean_q: 3.096693, mean_eps: 0.774518
 627385/1000000: episode: 927, duration: 47.339s, episode steps: 688, steps per second:  15, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.013986, mae: 2.555011, mean_q: 3.084432, mean_eps: 0.774265
 627753/1000000: episode: 928, duration: 26.627s, episode steps: 368, steps per second:  14, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.015646, mae: 2.574154, mean_q: 3.108440, mean_eps: 0.774075
 628314/1000000: episode: 929, duration: 39.321s, episode steps: 561, steps per second:  14, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.013454, mae: 2.535051, mean_q: 3.060582, mean_eps: 0.773908
 628984/1000000: episode: 930, duration: 53.146s, episode steps: 670, steps per second:  13, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.012041, mae: 2.535804, mean_q: 3.063396, mean_eps: 0.773687
 629488/1000000: episode: 931, duration: 37.259s, episode steps: 504, steps per second:  14, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.016421, mae: 2.594462, mean_q: 3.132604, mean_eps: 0.773476
 629885/1000000: episode: 932, duration: 27.938s, episode steps: 397, steps per second:  14, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.016221, mae: 2.605316, mean_q: 3.142601, mean_eps: 0.773313
 630411/1000000: episode: 933, duration: 40.822s, episode steps: 526, steps per second:  13, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.015532, mae: 2.540534, mean_q: 3.064788, mean_eps: 0.773147
 630925/1000000: episode: 934, duration: 36.868s, episode steps: 514, steps per second:  14, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014218, mae: 2.543313, mean_q: 3.068824, mean_eps: 0.772960
 631429/1000000: episode: 935, duration: 36.106s, episode steps: 504, steps per second:  14, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.013982, mae: 2.576234, mean_q: 3.106950, mean_eps: 0.772776
 632256/1000000: episode: 936, duration: 62.890s, episode steps: 827, steps per second:  13, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.014664, mae: 2.571053, mean_q: 3.100857, mean_eps: 0.772537
 633249/1000000: episode: 937, duration: 67.854s, episode steps: 993, steps per second:  15, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.016112, mae: 2.571816, mean_q: 3.103821, mean_eps: 0.772209
 633939/1000000: episode: 938, duration: 40.355s, episode steps: 690, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.014486, mae: 2.550300, mean_q: 3.078219, mean_eps: 0.771906
 634364/1000000: episode: 939, duration: 26.680s, episode steps: 425, steps per second:  16, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.015038, mae: 2.593339, mean_q: 3.128585, mean_eps: 0.771706
 634893/1000000: episode: 940, duration: 31.969s, episode steps: 529, steps per second:  17, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.014059, mae: 2.573723, mean_q: 3.106463, mean_eps: 0.771534
 635516/1000000: episode: 941, duration: 37.032s, episode steps: 623, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.016732, mae: 2.573964, mean_q: 3.107037, mean_eps: 0.771327
 636237/1000000: episode: 942, duration: 43.734s, episode steps: 721, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.015313, mae: 2.579097, mean_q: 3.113635, mean_eps: 0.771085
 637308/1000000: episode: 943, duration: 65.090s, episode steps: 1071, steps per second:  16, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.013736, mae: 2.551608, mean_q: 3.078439, mean_eps: 0.770762
 638055/1000000: episode: 944, duration: 44.017s, episode steps: 747, steps per second:  17, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.013724, mae: 2.569684, mean_q: 3.102860, mean_eps: 0.770435
 638680/1000000: episode: 945, duration: 40.053s, episode steps: 625, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.014200, mae: 2.561972, mean_q: 3.093355, mean_eps: 0.770188
 639197/1000000: episode: 946, duration: 36.013s, episode steps: 517, steps per second:  14, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.014806, mae: 2.554110, mean_q: 3.084271, mean_eps: 0.769982
 639789/1000000: episode: 947, duration: 42.686s, episode steps: 592, steps per second:  14, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.014141, mae: 2.590111, mean_q: 3.126484, mean_eps: 0.769782
 640215/1000000: episode: 948, duration: 30.568s, episode steps: 426, steps per second:  14, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.014368, mae: 2.551009, mean_q: 3.081042, mean_eps: 0.769599
 641301/1000000: episode: 949, duration: 74.458s, episode steps: 1086, steps per second:  15, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.014778, mae: 2.562926, mean_q: 3.093559, mean_eps: 0.769327
 641811/1000000: episode: 950, duration: 36.342s, episode steps: 510, steps per second:  14, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.014184, mae: 2.533362, mean_q: 3.056912, mean_eps: 0.769040
 642433/1000000: episode: 951, duration: 46.282s, episode steps: 622, steps per second:  13, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.015212, mae: 2.556906, mean_q: 3.086749, mean_eps: 0.768836
 643238/1000000: episode: 952, duration: 58.586s, episode steps: 805, steps per second:  14, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.015232, mae: 2.551776, mean_q: 3.081047, mean_eps: 0.768579
 643954/1000000: episode: 953, duration: 51.975s, episode steps: 716, steps per second:  14, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012728, mae: 2.546705, mean_q: 3.075105, mean_eps: 0.768305
 644349/1000000: episode: 954, duration: 29.394s, episode steps: 395, steps per second:  13, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.012852, mae: 2.566910, mean_q: 3.101218, mean_eps: 0.768105
 645417/1000000: episode: 955, duration: 74.314s, episode steps: 1068, steps per second:  14, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.016980, mae: 2.552877, mean_q: 3.081313, mean_eps: 0.767842
 646458/1000000: episode: 956, duration: 74.359s, episode steps: 1041, steps per second:  14, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.016855, mae: 2.554700, mean_q: 3.082206, mean_eps: 0.767462
 647310/1000000: episode: 957, duration: 61.096s, episode steps: 852, steps per second:  14, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.017435, mae: 2.572102, mean_q: 3.103004, mean_eps: 0.767122
 648360/1000000: episode: 958, duration: 75.309s, episode steps: 1050, steps per second:  14, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013477, mae: 2.536803, mean_q: 3.063349, mean_eps: 0.766780
 648762/1000000: episode: 959, duration: 28.250s, episode steps: 402, steps per second:  14, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.013570, mae: 2.562407, mean_q: 3.094388, mean_eps: 0.766518
 649774/1000000: episode: 960, duration: 72.192s, episode steps: 1012, steps per second:  14, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.015456, mae: 2.587320, mean_q: 3.123594, mean_eps: 0.766264
 650427/1000000: episode: 961, duration: 45.905s, episode steps: 653, steps per second:  14, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.013555, mae: 2.580343, mean_q: 3.117002, mean_eps: 0.765964
 650973/1000000: episode: 962, duration: 39.244s, episode steps: 546, steps per second:  14, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.016356, mae: 2.574413, mean_q: 3.108535, mean_eps: 0.765748
 651609/1000000: episode: 963, duration: 45.254s, episode steps: 636, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.012743, mae: 2.598814, mean_q: 3.138308, mean_eps: 0.765535
 652631/1000000: episode: 964, duration: 75.953s, episode steps: 1022, steps per second:  13, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.015178, mae: 2.577586, mean_q: 3.112818, mean_eps: 0.765237
 653139/1000000: episode: 965, duration: 38.558s, episode steps: 508, steps per second:  13, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.014711, mae: 2.537098, mean_q: 3.059453, mean_eps: 0.764962
 653822/1000000: episode: 966, duration: 49.649s, episode steps: 683, steps per second:  14, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.015059, mae: 2.554081, mean_q: 3.083032, mean_eps: 0.764747
 654705/1000000: episode: 967, duration: 65.704s, episode steps: 883, steps per second:  13, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.014886, mae: 2.569250, mean_q: 3.102572, mean_eps: 0.764465
 655643/1000000: episode: 968, duration: 65.091s, episode steps: 938, steps per second:  14, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.016340, mae: 2.571295, mean_q: 3.100862, mean_eps: 0.764137
 656125/1000000: episode: 969, duration: 36.232s, episode steps: 482, steps per second:  13, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.014111, mae: 2.569775, mean_q: 3.099956, mean_eps: 0.763882
 656732/1000000: episode: 970, duration: 45.370s, episode steps: 607, steps per second:  13, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.013906, mae: 2.576753, mean_q: 3.110844, mean_eps: 0.763686
 657677/1000000: episode: 971, duration: 67.470s, episode steps: 945, steps per second:  14, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.015908, mae: 2.581939, mean_q: 3.115333, mean_eps: 0.763407
 658268/1000000: episode: 972, duration: 41.293s, episode steps: 591, steps per second:  14, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.016741, mae: 2.574780, mean_q: 3.106264, mean_eps: 0.763130
 658964/1000000: episode: 973, duration: 49.048s, episode steps: 696, steps per second:  14, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.013813, mae: 2.565557, mean_q: 3.096189, mean_eps: 0.762899
 659571/1000000: episode: 974, duration: 41.634s, episode steps: 607, steps per second:  15, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.012681, mae: 2.554787, mean_q: 3.084273, mean_eps: 0.762664
 660133/1000000: episode: 975, duration: 34.197s, episode steps: 562, steps per second:  16, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013719, mae: 2.573844, mean_q: 3.105961, mean_eps: 0.762453
 661209/1000000: episode: 976, duration: 65.928s, episode steps: 1076, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.015873, mae: 2.589950, mean_q: 3.124932, mean_eps: 0.762158
 661905/1000000: episode: 977, duration: 49.233s, episode steps: 696, steps per second:  14, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.015900, mae: 2.551751, mean_q: 3.078072, mean_eps: 0.761839
 662417/1000000: episode: 978, duration: 37.198s, episode steps: 512, steps per second:  14, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.014649, mae: 2.557637, mean_q: 3.085953, mean_eps: 0.761622
 663481/1000000: episode: 979, duration: 78.397s, episode steps: 1064, steps per second:  14, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013960, mae: 2.574355, mean_q: 3.105959, mean_eps: 0.761338
 664539/1000000: episode: 980, duration: 76.395s, episode steps: 1058, steps per second:  14, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.015192, mae: 2.554351, mean_q: 3.082322, mean_eps: 0.760956
 665228/1000000: episode: 981, duration: 50.848s, episode steps: 689, steps per second:  14, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.016813, mae: 2.550913, mean_q: 3.076921, mean_eps: 0.760642
 665732/1000000: episode: 982, duration: 36.636s, episode steps: 504, steps per second:  14, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.016189, mae: 2.565745, mean_q: 3.097487, mean_eps: 0.760428
 666345/1000000: episode: 983, duration: 43.801s, episode steps: 613, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.015652, mae: 2.580973, mean_q: 3.114721, mean_eps: 0.760226
 666872/1000000: episode: 984, duration: 38.811s, episode steps: 527, steps per second:  14, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.014743, mae: 2.587954, mean_q: 3.124742, mean_eps: 0.760021
 667393/1000000: episode: 985, duration: 37.249s, episode steps: 521, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.016222, mae: 2.551831, mean_q: 3.081455, mean_eps: 0.759832
 668052/1000000: episode: 986, duration: 47.537s, episode steps: 659, steps per second:  14, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.015781, mae: 2.567098, mean_q: 3.097977, mean_eps: 0.759620
 668672/1000000: episode: 987, duration: 46.501s, episode steps: 620, steps per second:  13, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.013637, mae: 2.590145, mean_q: 3.126734, mean_eps: 0.759390
 669614/1000000: episode: 988, duration: 66.993s, episode steps: 942, steps per second:  14, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.015803, mae: 2.560360, mean_q: 3.089791, mean_eps: 0.759109
 670338/1000000: episode: 989, duration: 53.390s, episode steps: 724, steps per second:  14, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.015593, mae: 2.559681, mean_q: 3.087440, mean_eps: 0.758809
 671355/1000000: episode: 990, duration: 71.775s, episode steps: 1017, steps per second:  14, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.016155, mae: 2.576685, mean_q: 3.110075, mean_eps: 0.758495
 671880/1000000: episode: 991, duration: 39.840s, episode steps: 525, steps per second:  13, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.014701, mae: 2.580845, mean_q: 3.114306, mean_eps: 0.758218
 672516/1000000: episode: 992, duration: 45.537s, episode steps: 636, steps per second:  14, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.015006, mae: 2.573996, mean_q: 3.107610, mean_eps: 0.758009
 672998/1000000: episode: 993, duration: 35.020s, episode steps: 482, steps per second:  14, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014126, mae: 2.584521, mean_q: 3.118754, mean_eps: 0.757808
 673740/1000000: episode: 994, duration: 53.607s, episode steps: 742, steps per second:  14, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.013792, mae: 2.583008, mean_q: 3.117029, mean_eps: 0.757588
 674791/1000000: episode: 995, duration: 74.951s, episode steps: 1051, steps per second:  14, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.014490, mae: 2.565100, mean_q: 3.096017, mean_eps: 0.757265
 675634/1000000: episode: 996, duration: 61.025s, episode steps: 843, steps per second:  14, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.014636, mae: 2.569009, mean_q: 3.101533, mean_eps: 0.756924
 676359/1000000: episode: 997, duration: 51.322s, episode steps: 725, steps per second:  14, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.015185, mae: 2.564352, mean_q: 3.096300, mean_eps: 0.756641
 677169/1000000: episode: 998, duration: 59.050s, episode steps: 810, steps per second:  14, episode reward:  6.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.016426, mae: 2.565121, mean_q: 3.093132, mean_eps: 0.756365
 678059/1000000: episode: 999, duration: 64.139s, episode steps: 890, steps per second:  14, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.015658, mae: 2.572317, mean_q: 3.104304, mean_eps: 0.756059
 678793/1000000: episode: 1000, duration: 56.357s, episode steps: 734, steps per second:  13, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016456, mae: 2.572686, mean_q: 3.103306, mean_eps: 0.755767
 679149/1000000: episode: 1001, duration: 28.633s, episode steps: 356, steps per second:  12, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.013922, mae: 2.564808, mean_q: 3.092971, mean_eps: 0.755570
 680084/1000000: episode: 1002, duration: 74.344s, episode steps: 935, steps per second:  13, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.014987, mae: 2.582535, mean_q: 3.116974, mean_eps: 0.755338
 680675/1000000: episode: 1003, duration: 48.347s, episode steps: 591, steps per second:  12, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.012390, mae: 2.558307, mean_q: 3.086760, mean_eps: 0.755064
 681295/1000000: episode: 1004, duration: 49.955s, episode steps: 620, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.014976, mae: 2.562916, mean_q: 3.093906, mean_eps: 0.754846
 682141/1000000: episode: 1005, duration: 68.682s, episode steps: 846, steps per second:  12, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014772, mae: 2.554834, mean_q: 3.082504, mean_eps: 0.754582
 682818/1000000: episode: 1006, duration: 53.729s, episode steps: 677, steps per second:  13, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.014289, mae: 2.563958, mean_q: 3.096144, mean_eps: 0.754307
 683218/1000000: episode: 1007, duration: 31.992s, episode steps: 400, steps per second:  13, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.016160, mae: 2.552771, mean_q: 3.081665, mean_eps: 0.754114
 683926/1000000: episode: 1008, duration: 58.554s, episode steps: 708, steps per second:  12, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.014935, mae: 2.561237, mean_q: 3.090348, mean_eps: 0.753914
 684435/1000000: episode: 1009, duration: 40.932s, episode steps: 509, steps per second:  12, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.014309, mae: 2.591015, mean_q: 3.125311, mean_eps: 0.753695
 684926/1000000: episode: 1010, duration: 40.687s, episode steps: 491, steps per second:  12, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.014546, mae: 2.560399, mean_q: 3.089995, mean_eps: 0.753515
 685924/1000000: episode: 1011, duration: 81.918s, episode steps: 998, steps per second:  12, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.015602, mae: 2.562098, mean_q: 3.091504, mean_eps: 0.753247
 686543/1000000: episode: 1012, duration: 51.110s, episode steps: 619, steps per second:  12, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.014764, mae: 2.562707, mean_q: 3.093297, mean_eps: 0.752956
 687037/1000000: episode: 1013, duration: 40.644s, episode steps: 494, steps per second:  12, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.014645, mae: 2.594747, mean_q: 3.130663, mean_eps: 0.752756
 687816/1000000: episode: 1014, duration: 63.722s, episode steps: 779, steps per second:  12, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.013810, mae: 2.542129, mean_q: 3.068063, mean_eps: 0.752527
 688529/1000000: episode: 1015, duration: 54.236s, episode steps: 713, steps per second:  13, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.015089, mae: 2.570039, mean_q: 3.101965, mean_eps: 0.752258
 689500/1000000: episode: 1016, duration: 72.131s, episode steps: 971, steps per second:  13, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.016091, mae: 2.572447, mean_q: 3.103080, mean_eps: 0.751955
 690115/1000000: episode: 1017, duration: 49.836s, episode steps: 615, steps per second:  12, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.013781, mae: 2.548950, mean_q: 3.074453, mean_eps: 0.751670
 690647/1000000: episode: 1018, duration: 41.467s, episode steps: 532, steps per second:  13, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.013872, mae: 2.558579, mean_q: 3.086290, mean_eps: 0.751463
 691384/1000000: episode: 1019, duration: 60.431s, episode steps: 737, steps per second:  12, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012316, mae: 2.574075, mean_q: 3.105168, mean_eps: 0.751235
 692036/1000000: episode: 1020, duration: 58.566s, episode steps: 652, steps per second:  11, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014926, mae: 2.561566, mean_q: 3.090939, mean_eps: 0.750985
 693029/1000000: episode: 1021, duration: 81.531s, episode steps: 993, steps per second:  12, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.014552, mae: 2.560881, mean_q: 3.088919, mean_eps: 0.750688
 693474/1000000: episode: 1022, duration: 35.811s, episode steps: 445, steps per second:  12, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.852 [0.000, 5.000],  loss: 0.015370, mae: 2.543078, mean_q: 3.066272, mean_eps: 0.750429
 694445/1000000: episode: 1023, duration: 79.290s, episode steps: 971, steps per second:  12, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.016058, mae: 2.559912, mean_q: 3.088204, mean_eps: 0.750174
 695080/1000000: episode: 1024, duration: 51.648s, episode steps: 635, steps per second:  12, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.014682, mae: 2.553151, mean_q: 3.079816, mean_eps: 0.749886
 695865/1000000: episode: 1025, duration: 64.696s, episode steps: 785, steps per second:  12, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.015626, mae: 2.558635, mean_q: 3.087671, mean_eps: 0.749630
 696689/1000000: episode: 1026, duration: 66.997s, episode steps: 824, steps per second:  12, episode reward:  7.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.014008, mae: 2.553434, mean_q: 3.080444, mean_eps: 0.749340
 697324/1000000: episode: 1027, duration: 51.869s, episode steps: 635, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.015986, mae: 2.549554, mean_q: 3.074615, mean_eps: 0.749078
 697875/1000000: episode: 1028, duration: 44.713s, episode steps: 551, steps per second:  12, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014549, mae: 2.569694, mean_q: 3.101664, mean_eps: 0.748865
 698283/1000000: episode: 1029, duration: 32.829s, episode steps: 408, steps per second:  12, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.013227, mae: 2.569144, mean_q: 3.100252, mean_eps: 0.748692
 699131/1000000: episode: 1030, duration: 69.342s, episode steps: 848, steps per second:  12, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.014091, mae: 2.566658, mean_q: 3.098951, mean_eps: 0.748466
 699722/1000000: episode: 1031, duration: 48.421s, episode steps: 591, steps per second:  12, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.014223, mae: 2.560666, mean_q: 3.088482, mean_eps: 0.748207
 700178/1000000: episode: 1032, duration: 36.577s, episode steps: 456, steps per second:  12, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.014081, mae: 2.613218, mean_q: 3.152020, mean_eps: 0.748018
 701116/1000000: episode: 1033, duration: 68.773s, episode steps: 938, steps per second:  14, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.014423, mae: 2.591069, mean_q: 3.127675, mean_eps: 0.747767
 701657/1000000: episode: 1034, duration: 38.383s, episode steps: 541, steps per second:  14, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.013754, mae: 2.549044, mean_q: 3.075851, mean_eps: 0.747501
 702631/1000000: episode: 1035, duration: 71.664s, episode steps: 974, steps per second:  14, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.014716, mae: 2.559031, mean_q: 3.086663, mean_eps: 0.747228
 703194/1000000: episode: 1036, duration: 47.040s, episode steps: 563, steps per second:  12, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.013150, mae: 2.598513, mean_q: 3.133920, mean_eps: 0.746952
 703855/1000000: episode: 1037, duration: 61.377s, episode steps: 661, steps per second:  11, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.016371, mae: 2.562192, mean_q: 3.091235, mean_eps: 0.746731
 704247/1000000: episode: 1038, duration: 32.373s, episode steps: 392, steps per second:  12, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.018138, mae: 2.564153, mean_q: 3.093291, mean_eps: 0.746542
 705446/1000000: episode: 1039, duration: 100.438s, episode steps: 1199, steps per second:  12, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.013262, mae: 2.559263, mean_q: 3.089495, mean_eps: 0.746255
 706071/1000000: episode: 1040, duration: 52.387s, episode steps: 625, steps per second:  12, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.013472, mae: 2.571770, mean_q: 3.103184, mean_eps: 0.745927
 706710/1000000: episode: 1041, duration: 54.086s, episode steps: 639, steps per second:  12, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.016005, mae: 2.555980, mean_q: 3.084605, mean_eps: 0.745700
 707241/1000000: episode: 1042, duration: 47.912s, episode steps: 531, steps per second:  11, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.014951, mae: 2.557003, mean_q: 3.085323, mean_eps: 0.745489
 707932/1000000: episode: 1043, duration: 65.717s, episode steps: 691, steps per second:  11, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013707, mae: 2.563157, mean_q: 3.093192, mean_eps: 0.745269
 708481/1000000: episode: 1044, duration: 51.604s, episode steps: 549, steps per second:  11, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.017172, mae: 2.547059, mean_q: 3.071013, mean_eps: 0.745046
 709045/1000000: episode: 1045, duration: 48.303s, episode steps: 564, steps per second:  12, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.015487, mae: 2.581403, mean_q: 3.113723, mean_eps: 0.744845
 709438/1000000: episode: 1046, duration: 38.109s, episode steps: 393, steps per second:  10, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.014797, mae: 2.561894, mean_q: 3.089989, mean_eps: 0.744673
 710077/1000000: episode: 1047, duration: 68.442s, episode steps: 639, steps per second:   9, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.014568, mae: 2.592184, mean_q: 3.126943, mean_eps: 0.744487
 710812/1000000: episode: 1048, duration: 83.792s, episode steps: 735, steps per second:   9, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.016382, mae: 2.564614, mean_q: 3.092436, mean_eps: 0.744240
 711930/1000000: episode: 1049, duration: 101.098s, episode steps: 1118, steps per second:  11, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.015446, mae: 2.572578, mean_q: 3.103966, mean_eps: 0.743907
 712952/1000000: episode: 1050, duration: 103.548s, episode steps: 1022, steps per second:  10, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.015317, mae: 2.577142, mean_q: 3.109522, mean_eps: 0.743522
 713600/1000000: episode: 1051, duration: 55.154s, episode steps: 648, steps per second:  12, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014710, mae: 2.556415, mean_q: 3.083633, mean_eps: 0.743221
 714286/1000000: episode: 1052, duration: 57.705s, episode steps: 686, steps per second:  12, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.015031, mae: 2.557581, mean_q: 3.083783, mean_eps: 0.742981
 715179/1000000: episode: 1053, duration: 76.935s, episode steps: 893, steps per second:  12, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.014624, mae: 2.558058, mean_q: 3.083997, mean_eps: 0.742696
 715887/1000000: episode: 1054, duration: 57.330s, episode steps: 708, steps per second:  12, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.015667, mae: 2.586315, mean_q: 3.118618, mean_eps: 0.742408
 716674/1000000: episode: 1055, duration: 57.147s, episode steps: 787, steps per second:  14, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.015057, mae: 2.570207, mean_q: 3.100218, mean_eps: 0.742139
 717616/1000000: episode: 1056, duration: 71.211s, episode steps: 942, steps per second:  13, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.014341, mae: 2.568353, mean_q: 3.098146, mean_eps: 0.741828
 718694/1000000: episode: 1057, duration: 79.740s, episode steps: 1078, steps per second:  14, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.015588, mae: 2.582088, mean_q: 3.114446, mean_eps: 0.741465
 719222/1000000: episode: 1058, duration: 43.438s, episode steps: 528, steps per second:  12, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.016723, mae: 2.557511, mean_q: 3.085485, mean_eps: 0.741175
 720314/1000000: episode: 1059, duration: 90.461s, episode steps: 1092, steps per second:  12, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.016658, mae: 2.572214, mean_q: 3.102417, mean_eps: 0.740884
 720915/1000000: episode: 1060, duration: 49.808s, episode steps: 601, steps per second:  12, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.017695, mae: 2.575112, mean_q: 3.106669, mean_eps: 0.740579
 721277/1000000: episode: 1061, duration: 30.091s, episode steps: 362, steps per second:  12, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.015361, mae: 2.580199, mean_q: 3.112727, mean_eps: 0.740405
 721684/1000000: episode: 1062, duration: 33.312s, episode steps: 407, steps per second:  12, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.015682, mae: 2.551037, mean_q: 3.076012, mean_eps: 0.740267
 722445/1000000: episode: 1063, duration: 66.360s, episode steps: 761, steps per second:  11, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.015235, mae: 2.566707, mean_q: 3.095109, mean_eps: 0.740057
 723202/1000000: episode: 1064, duration: 62.612s, episode steps: 757, steps per second:  12, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.013133, mae: 2.570823, mean_q: 3.101839, mean_eps: 0.739783
 723925/1000000: episode: 1065, duration: 60.917s, episode steps: 723, steps per second:  12, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.016051, mae: 2.555059, mean_q: 3.081227, mean_eps: 0.739517
 724544/1000000: episode: 1066, duration: 50.940s, episode steps: 619, steps per second:  12, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.014406, mae: 2.587567, mean_q: 3.120187, mean_eps: 0.739276
 725091/1000000: episode: 1067, duration: 45.950s, episode steps: 547, steps per second:  12, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.016408, mae: 2.577102, mean_q: 3.108876, mean_eps: 0.739066
 725726/1000000: episode: 1068, duration: 53.350s, episode steps: 635, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.013037, mae: 2.562302, mean_q: 3.092263, mean_eps: 0.738853
 726480/1000000: episode: 1069, duration: 65.507s, episode steps: 754, steps per second:  12, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015828, mae: 2.549346, mean_q: 3.074782, mean_eps: 0.738603
 726870/1000000: episode: 1070, duration: 32.840s, episode steps: 390, steps per second:  12, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.012286, mae: 2.578867, mean_q: 3.111629, mean_eps: 0.738397
 727479/1000000: episode: 1071, duration: 50.863s, episode steps: 609, steps per second:  12, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015795, mae: 2.583626, mean_q: 3.116966, mean_eps: 0.738217
 728177/1000000: episode: 1072, duration: 58.967s, episode steps: 698, steps per second:  12, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.013152, mae: 2.535391, mean_q: 3.056593, mean_eps: 0.737982
 729076/1000000: episode: 1073, duration: 74.247s, episode steps: 899, steps per second:  12, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.018873, mae: 2.549015, mean_q: 3.073775, mean_eps: 0.737695
 729675/1000000: episode: 1074, duration: 51.563s, episode steps: 599, steps per second:  12, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.018115, mae: 2.574235, mean_q: 3.104658, mean_eps: 0.737425
 730486/1000000: episode: 1075, duration: 67.870s, episode steps: 811, steps per second:  12, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.014615, mae: 2.574353, mean_q: 3.104539, mean_eps: 0.737171
 731414/1000000: episode: 1076, duration: 74.064s, episode steps: 928, steps per second:  13, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.013218, mae: 2.568589, mean_q: 3.098803, mean_eps: 0.736858
 731952/1000000: episode: 1077, duration: 39.927s, episode steps: 538, steps per second:  13, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.014476, mae: 2.555993, mean_q: 3.085775, mean_eps: 0.736594
 732678/1000000: episode: 1078, duration: 53.520s, episode steps: 726, steps per second:  14, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.014058, mae: 2.566386, mean_q: 3.096150, mean_eps: 0.736367
 733609/1000000: episode: 1079, duration: 67.918s, episode steps: 931, steps per second:  14, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.011952, mae: 2.564834, mean_q: 3.094323, mean_eps: 0.736068
 734471/1000000: episode: 1080, duration: 66.788s, episode steps: 862, steps per second:  13, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.016113, mae: 2.560245, mean_q: 3.087382, mean_eps: 0.735746
 735142/1000000: episode: 1081, duration: 50.139s, episode steps: 671, steps per second:  13, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.014417, mae: 2.546914, mean_q: 3.073282, mean_eps: 0.735470
 735535/1000000: episode: 1082, duration: 33.048s, episode steps: 393, steps per second:  12, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.014191, mae: 2.583961, mean_q: 3.117005, mean_eps: 0.735278
 736449/1000000: episode: 1083, duration: 76.552s, episode steps: 914, steps per second:  12, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.013746, mae: 2.542030, mean_q: 3.066531, mean_eps: 0.735043
 736967/1000000: episode: 1084, duration: 43.070s, episode steps: 518, steps per second:  12, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.016209, mae: 2.584051, mean_q: 3.118877, mean_eps: 0.734785
 737591/1000000: episode: 1085, duration: 52.773s, episode steps: 624, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.015119, mae: 2.560494, mean_q: 3.089677, mean_eps: 0.734580
 737998/1000000: episode: 1086, duration: 40.099s, episode steps: 407, steps per second:  10, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013537, mae: 2.573478, mean_q: 3.103616, mean_eps: 0.734394
 738687/1000000: episode: 1087, duration: 59.136s, episode steps: 689, steps per second:  12, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014368, mae: 2.577644, mean_q: 3.109038, mean_eps: 0.734197
 739117/1000000: episode: 1088, duration: 36.622s, episode steps: 430, steps per second:  12, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.013793, mae: 2.547327, mean_q: 3.071585, mean_eps: 0.733995
 739770/1000000: episode: 1089, duration: 56.918s, episode steps: 653, steps per second:  11, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.014664, mae: 2.545644, mean_q: 3.070021, mean_eps: 0.733800
 740301/1000000: episode: 1090, duration: 47.805s, episode steps: 531, steps per second:  11, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.013141, mae: 2.547118, mean_q: 3.073111, mean_eps: 0.733587
 740853/1000000: episode: 1091, duration: 52.541s, episode steps: 552, steps per second:  11, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.017061, mae: 2.575827, mean_q: 3.106611, mean_eps: 0.733392
 741824/1000000: episode: 1092, duration: 83.964s, episode steps: 971, steps per second:  12, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.014220, mae: 2.565352, mean_q: 3.094126, mean_eps: 0.733118
 742742/1000000: episode: 1093, duration: 78.638s, episode steps: 918, steps per second:  12, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.015332, mae: 2.578844, mean_q: 3.110676, mean_eps: 0.732778
 743240/1000000: episode: 1094, duration: 42.759s, episode steps: 498, steps per second:  12, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.013956, mae: 2.554306, mean_q: 3.079473, mean_eps: 0.732524
 743853/1000000: episode: 1095, duration: 51.507s, episode steps: 613, steps per second:  12, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.014892, mae: 2.534825, mean_q: 3.057000, mean_eps: 0.732323
 744630/1000000: episode: 1096, duration: 65.078s, episode steps: 777, steps per second:  12, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.012489, mae: 2.545822, mean_q: 3.071336, mean_eps: 0.732073
 745400/1000000: episode: 1097, duration: 56.446s, episode steps: 770, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.016397, mae: 2.573977, mean_q: 3.106887, mean_eps: 0.731795
 746033/1000000: episode: 1098, duration: 47.122s, episode steps: 633, steps per second:  13, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.016713, mae: 2.560079, mean_q: 3.087092, mean_eps: 0.731542
 746997/1000000: episode: 1099, duration: 72.424s, episode steps: 964, steps per second:  13, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014343, mae: 2.567023, mean_q: 3.097846, mean_eps: 0.731254
 748106/1000000: episode: 1100, duration: 83.207s, episode steps: 1109, steps per second:  13, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.015556, mae: 2.555965, mean_q: 3.085551, mean_eps: 0.730881
 748753/1000000: episode: 1101, duration: 48.952s, episode steps: 647, steps per second:  13, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.016114, mae: 2.550946, mean_q: 3.076601, mean_eps: 0.730565
 749394/1000000: episode: 1102, duration: 54.893s, episode steps: 641, steps per second:  12, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.015981, mae: 2.556209, mean_q: 3.082060, mean_eps: 0.730333
 750320/1000000: episode: 1103, duration: 78.373s, episode steps: 926, steps per second:  12, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.014478, mae: 2.560337, mean_q: 3.089923, mean_eps: 0.730052
 750672/1000000: episode: 1104, duration: 29.895s, episode steps: 352, steps per second:  12, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.015090, mae: 2.549924, mean_q: 3.077722, mean_eps: 0.729822
 751402/1000000: episode: 1105, duration: 61.487s, episode steps: 730, steps per second:  12, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.016945, mae: 2.576784, mean_q: 3.108588, mean_eps: 0.729627
 752547/1000000: episode: 1106, duration: 96.234s, episode steps: 1145, steps per second:  12, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.015691, mae: 2.552954, mean_q: 3.083453, mean_eps: 0.729289
 753112/1000000: episode: 1107, duration: 47.924s, episode steps: 565, steps per second:  12, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.013179, mae: 2.562958, mean_q: 3.093174, mean_eps: 0.728982
 754032/1000000: episode: 1108, duration: 76.940s, episode steps: 920, steps per second:  12, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.015708, mae: 2.566178, mean_q: 3.096879, mean_eps: 0.728715
 754550/1000000: episode: 1109, duration: 43.280s, episode steps: 518, steps per second:  12, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.017450, mae: 2.536449, mean_q: 3.059738, mean_eps: 0.728456
 755147/1000000: episode: 1110, duration: 49.746s, episode steps: 597, steps per second:  12, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.015677, mae: 2.565814, mean_q: 3.094977, mean_eps: 0.728255
 756085/1000000: episode: 1111, duration: 78.517s, episode steps: 938, steps per second:  12, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.013771, mae: 2.567498, mean_q: 3.097481, mean_eps: 0.727978
 756761/1000000: episode: 1112, duration: 56.837s, episode steps: 676, steps per second:  12, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.014633, mae: 2.558067, mean_q: 3.085301, mean_eps: 0.727687
 757538/1000000: episode: 1113, duration: 65.461s, episode steps: 777, steps per second:  12, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.014710, mae: 2.564041, mean_q: 3.092837, mean_eps: 0.727426
 758244/1000000: episode: 1114, duration: 60.425s, episode steps: 706, steps per second:  12, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.012966, mae: 2.564382, mean_q: 3.094041, mean_eps: 0.727160
 758843/1000000: episode: 1115, duration: 49.936s, episode steps: 599, steps per second:  12, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.014685, mae: 2.560838, mean_q: 3.090357, mean_eps: 0.726925
 759545/1000000: episode: 1116, duration: 59.801s, episode steps: 702, steps per second:  12, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.016579, mae: 2.547216, mean_q: 3.072631, mean_eps: 0.726690
 760323/1000000: episode: 1117, duration: 58.418s, episode steps: 778, steps per second:  13, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.014847, mae: 2.563142, mean_q: 3.091141, mean_eps: 0.726424
 760869/1000000: episode: 1118, duration: 40.288s, episode steps: 546, steps per second:  14, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.016855, mae: 2.558666, mean_q: 3.086418, mean_eps: 0.726185
 761399/1000000: episode: 1119, duration: 41.730s, episode steps: 530, steps per second:  13, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.014610, mae: 2.574512, mean_q: 3.104563, mean_eps: 0.725992
 762612/1000000: episode: 1120, duration: 95.248s, episode steps: 1213, steps per second:  13, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.014344, mae: 2.540146, mean_q: 3.065649, mean_eps: 0.725679
 763106/1000000: episode: 1121, duration: 40.906s, episode steps: 494, steps per second:  12, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.013944, mae: 2.565490, mean_q: 3.096967, mean_eps: 0.725371
 764041/1000000: episode: 1122, duration: 78.354s, episode steps: 935, steps per second:  12, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.014733, mae: 2.571509, mean_q: 3.102146, mean_eps: 0.725113
 764648/1000000: episode: 1123, duration: 51.296s, episode steps: 607, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.015113, mae: 2.576368, mean_q: 3.109054, mean_eps: 0.724836
 765213/1000000: episode: 1124, duration: 47.499s, episode steps: 565, steps per second:  12, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013880, mae: 2.568585, mean_q: 3.098404, mean_eps: 0.724625
 765589/1000000: episode: 1125, duration: 31.201s, episode steps: 376, steps per second:  12, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.016116, mae: 2.563842, mean_q: 3.093424, mean_eps: 0.724455
 766098/1000000: episode: 1126, duration: 43.091s, episode steps: 509, steps per second:  12, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.014428, mae: 2.559187, mean_q: 3.088367, mean_eps: 0.724296
 767387/1000000: episode: 1127, duration: 109.986s, episode steps: 1289, steps per second:  12, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.014287, mae: 2.556491, mean_q: 3.082476, mean_eps: 0.723973
 768084/1000000: episode: 1128, duration: 58.674s, episode steps: 697, steps per second:  12, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.015752, mae: 2.556516, mean_q: 3.082097, mean_eps: 0.723616
 768713/1000000: episode: 1129, duration: 54.314s, episode steps: 629, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.014227, mae: 2.567521, mean_q: 3.096291, mean_eps: 0.723377
 769104/1000000: episode: 1130, duration: 33.620s, episode steps: 391, steps per second:  12, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.015722, mae: 2.559655, mean_q: 3.088613, mean_eps: 0.723193
 769919/1000000: episode: 1131, duration: 69.340s, episode steps: 815, steps per second:  12, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.013972, mae: 2.568082, mean_q: 3.098071, mean_eps: 0.722976
 770608/1000000: episode: 1132, duration: 61.304s, episode steps: 689, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.016618, mae: 2.568472, mean_q: 3.098276, mean_eps: 0.722706
 771298/1000000: episode: 1133, duration: 57.968s, episode steps: 690, steps per second:  12, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014407, mae: 2.550821, mean_q: 3.076671, mean_eps: 0.722457
 771675/1000000: episode: 1134, duration: 32.480s, episode steps: 377, steps per second:  12, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.014659, mae: 2.558218, mean_q: 3.085434, mean_eps: 0.722265
 772178/1000000: episode: 1135, duration: 42.555s, episode steps: 503, steps per second:  12, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.014443, mae: 2.562293, mean_q: 3.090453, mean_eps: 0.722107
 772495/1000000: episode: 1136, duration: 25.980s, episode steps: 317, steps per second:  12, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.016451, mae: 2.582136, mean_q: 3.112100, mean_eps: 0.721959
 773251/1000000: episode: 1137, duration: 64.469s, episode steps: 756, steps per second:  12, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.015756, mae: 2.563951, mean_q: 3.090983, mean_eps: 0.721766
 774224/1000000: episode: 1138, duration: 81.514s, episode steps: 973, steps per second:  12, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.015635, mae: 2.548252, mean_q: 3.072305, mean_eps: 0.721455
 775019/1000000: episode: 1139, duration: 67.549s, episode steps: 795, steps per second:  12, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.014248, mae: 2.550509, mean_q: 3.075743, mean_eps: 0.721137
 775521/1000000: episode: 1140, duration: 41.928s, episode steps: 502, steps per second:  12, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.015863, mae: 2.568870, mean_q: 3.097161, mean_eps: 0.720903
 776148/1000000: episode: 1141, duration: 54.081s, episode steps: 627, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.015647, mae: 2.556925, mean_q: 3.081832, mean_eps: 0.720700
 776700/1000000: episode: 1142, duration: 46.693s, episode steps: 552, steps per second:  12, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.016505, mae: 2.563995, mean_q: 3.091008, mean_eps: 0.720488
 777292/1000000: episode: 1143, duration: 50.244s, episode steps: 592, steps per second:  12, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.015743, mae: 2.549519, mean_q: 3.076418, mean_eps: 0.720282
 777730/1000000: episode: 1144, duration: 34.005s, episode steps: 438, steps per second:  13, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.016344, mae: 2.572156, mean_q: 3.104024, mean_eps: 0.720096
 778999/1000000: episode: 1145, duration: 95.174s, episode steps: 1269, steps per second:  13, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.016805, mae: 2.567588, mean_q: 3.096926, mean_eps: 0.719789
 779533/1000000: episode: 1146, duration: 42.927s, episode steps: 534, steps per second:  12, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.014273, mae: 2.581446, mean_q: 3.117506, mean_eps: 0.719464
 780688/1000000: episode: 1147, duration: 97.421s, episode steps: 1155, steps per second:  12, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.015252, mae: 2.571620, mean_q: 3.102700, mean_eps: 0.719160
 781425/1000000: episode: 1148, duration: 62.705s, episode steps: 737, steps per second:  12, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.015334, mae: 2.555573, mean_q: 3.083173, mean_eps: 0.718820
 782058/1000000: episode: 1149, duration: 55.176s, episode steps: 633, steps per second:  11, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.013554, mae: 2.569900, mean_q: 3.101425, mean_eps: 0.718573
 782988/1000000: episode: 1150, duration: 77.924s, episode steps: 930, steps per second:  12, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.015230, mae: 2.545609, mean_q: 3.071762, mean_eps: 0.718292
 783691/1000000: episode: 1151, duration: 58.819s, episode steps: 703, steps per second:  12, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.015014, mae: 2.564795, mean_q: 3.093431, mean_eps: 0.717998
 784520/1000000: episode: 1152, duration: 70.792s, episode steps: 829, steps per second:  12, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.017028, mae: 2.575985, mean_q: 3.106652, mean_eps: 0.717723
 784887/1000000: episode: 1153, duration: 31.701s, episode steps: 367, steps per second:  12, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.017838, mae: 2.579741, mean_q: 3.113397, mean_eps: 0.717507
 785799/1000000: episode: 1154, duration: 76.482s, episode steps: 912, steps per second:  12, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.015487, mae: 2.563507, mean_q: 3.093167, mean_eps: 0.717277
 786343/1000000: episode: 1155, duration: 45.319s, episode steps: 544, steps per second:  12, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.013139, mae: 2.543183, mean_q: 3.068200, mean_eps: 0.717015
 786862/1000000: episode: 1156, duration: 44.792s, episode steps: 519, steps per second:  12, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.017506, mae: 2.557138, mean_q: 3.083560, mean_eps: 0.716823
 787415/1000000: episode: 1157, duration: 46.008s, episode steps: 553, steps per second:  12, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.016303, mae: 2.570147, mean_q: 3.097833, mean_eps: 0.716630
 788060/1000000: episode: 1158, duration: 55.399s, episode steps: 645, steps per second:  12, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.014192, mae: 2.560223, mean_q: 3.089812, mean_eps: 0.716415
 788749/1000000: episode: 1159, duration: 57.923s, episode steps: 689, steps per second:  12, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.015289, mae: 2.531982, mean_q: 3.052396, mean_eps: 0.716175
 789132/1000000: episode: 1160, duration: 32.485s, episode steps: 383, steps per second:  12, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.013927, mae: 2.564712, mean_q: 3.094440, mean_eps: 0.715982
 789847/1000000: episode: 1161, duration: 61.621s, episode steps: 715, steps per second:  12, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014219, mae: 2.559149, mean_q: 3.086982, mean_eps: 0.715784
 790857/1000000: episode: 1162, duration: 84.862s, episode steps: 1010, steps per second:  12, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.013325, mae: 2.558898, mean_q: 3.086047, mean_eps: 0.715473
 791714/1000000: episode: 1163, duration: 72.290s, episode steps: 857, steps per second:  12, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.014659, mae: 2.559848, mean_q: 3.089162, mean_eps: 0.715137
 792222/1000000: episode: 1164, duration: 44.826s, episode steps: 508, steps per second:  11, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.014625, mae: 2.576324, mean_q: 3.107814, mean_eps: 0.714892
 792978/1000000: episode: 1165, duration: 63.644s, episode steps: 756, steps per second:  12, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.015015, mae: 2.558364, mean_q: 3.088105, mean_eps: 0.714664
 793498/1000000: episode: 1166, duration: 43.632s, episode steps: 520, steps per second:  12, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.014632, mae: 2.562911, mean_q: 3.092753, mean_eps: 0.714434
 794211/1000000: episode: 1167, duration: 61.204s, episode steps: 713, steps per second:  12, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.015207, mae: 2.532048, mean_q: 3.054919, mean_eps: 0.714213
 795121/1000000: episode: 1168, duration: 78.040s, episode steps: 910, steps per second:  12, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.013633, mae: 2.553558, mean_q: 3.079020, mean_eps: 0.713920
 795645/1000000: episode: 1169, duration: 45.439s, episode steps: 524, steps per second:  12, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.013447, mae: 2.550570, mean_q: 3.075921, mean_eps: 0.713662
 796391/1000000: episode: 1170, duration: 63.417s, episode steps: 746, steps per second:  12, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015488, mae: 2.546542, mean_q: 3.072209, mean_eps: 0.713434
 797107/1000000: episode: 1171, duration: 61.662s, episode steps: 716, steps per second:  12, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.015864, mae: 2.551235, mean_q: 3.077730, mean_eps: 0.713171
 797630/1000000: episode: 1172, duration: 43.642s, episode steps: 523, steps per second:  12, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.017141, mae: 2.557483, mean_q: 3.083845, mean_eps: 0.712948
 797998/1000000: episode: 1173, duration: 31.244s, episode steps: 368, steps per second:  12, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.017931, mae: 2.550342, mean_q: 3.076344, mean_eps: 0.712787
 798512/1000000: episode: 1174, duration: 44.207s, episode steps: 514, steps per second:  12, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.016550, mae: 2.582793, mean_q: 3.116836, mean_eps: 0.712629
 799002/1000000: episode: 1175, duration: 42.101s, episode steps: 490, steps per second:  12, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.016070, mae: 2.549840, mean_q: 3.077961, mean_eps: 0.712448
 799944/1000000: episode: 1176, duration: 81.397s, episode steps: 942, steps per second:  12, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.015234, mae: 2.552332, mean_q: 3.079556, mean_eps: 0.712190
 800556/1000000: episode: 1177, duration: 52.264s, episode steps: 612, steps per second:  12, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.013787, mae: 2.544049, mean_q: 3.070497, mean_eps: 0.711911
 801594/1000000: episode: 1178, duration: 82.328s, episode steps: 1038, steps per second:  13, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.015847, mae: 2.562176, mean_q: 3.093324, mean_eps: 0.711613
 802201/1000000: episode: 1179, duration: 46.557s, episode steps: 607, steps per second:  13, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.013901, mae: 2.541817, mean_q: 3.069786, mean_eps: 0.711317
 802826/1000000: episode: 1180, duration: 55.583s, episode steps: 625, steps per second:  11, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012287, mae: 2.545302, mean_q: 3.074358, mean_eps: 0.711095
 803370/1000000: episode: 1181, duration: 46.232s, episode steps: 544, steps per second:  12, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.014944, mae: 2.562627, mean_q: 3.091201, mean_eps: 0.710885
 804000/1000000: episode: 1182, duration: 54.545s, episode steps: 630, steps per second:  12, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.014640, mae: 2.559766, mean_q: 3.087877, mean_eps: 0.710674
 804639/1000000: episode: 1183, duration: 54.800s, episode steps: 639, steps per second:  12, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.015424, mae: 2.558876, mean_q: 3.085816, mean_eps: 0.710446
 805290/1000000: episode: 1184, duration: 55.335s, episode steps: 651, steps per second:  12, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.014169, mae: 2.548112, mean_q: 3.073512, mean_eps: 0.710213
 806040/1000000: episode: 1185, duration: 64.428s, episode steps: 750, steps per second:  12, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.015257, mae: 2.540231, mean_q: 3.064448, mean_eps: 0.709961
 806964/1000000: episode: 1186, duration: 79.477s, episode steps: 924, steps per second:  12, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.014411, mae: 2.560715, mean_q: 3.090434, mean_eps: 0.709660
 807607/1000000: episode: 1187, duration: 53.979s, episode steps: 643, steps per second:  12, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.014600, mae: 2.560536, mean_q: 3.089381, mean_eps: 0.709378
 808198/1000000: episode: 1188, duration: 50.231s, episode steps: 591, steps per second:  12, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.014974, mae: 2.545713, mean_q: 3.069424, mean_eps: 0.709155
 808625/1000000: episode: 1189, duration: 37.456s, episode steps: 427, steps per second:  11, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.017420, mae: 2.558202, mean_q: 3.084538, mean_eps: 0.708972
 809264/1000000: episode: 1190, duration: 53.772s, episode steps: 639, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.014388, mae: 2.536409, mean_q: 3.058886, mean_eps: 0.708780
 810155/1000000: episode: 1191, duration: 76.480s, episode steps: 891, steps per second:  12, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.014660, mae: 2.553953, mean_q: 3.081268, mean_eps: 0.708505
 811237/1000000: episode: 1192, duration: 93.206s, episode steps: 1082, steps per second:  12, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.017557, mae: 2.531637, mean_q: 3.053506, mean_eps: 0.708149
 811693/1000000: episode: 1193, duration: 40.004s, episode steps: 456, steps per second:  11, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.016806, mae: 2.532662, mean_q: 3.054048, mean_eps: 0.707872
 812334/1000000: episode: 1194, duration: 54.616s, episode steps: 641, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.802 [0.000, 5.000],  loss: 0.016424, mae: 2.541735, mean_q: 3.068004, mean_eps: 0.707675
 812981/1000000: episode: 1195, duration: 57.983s, episode steps: 647, steps per second:  11, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.014386, mae: 2.532764, mean_q: 3.056411, mean_eps: 0.707443
 813981/1000000: episode: 1196, duration: 86.953s, episode steps: 1000, steps per second:  12, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.014296, mae: 2.556380, mean_q: 3.083527, mean_eps: 0.707146
 814608/1000000: episode: 1197, duration: 55.534s, episode steps: 627, steps per second:  11, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.016648, mae: 2.544066, mean_q: 3.067191, mean_eps: 0.706854
 815311/1000000: episode: 1198, duration: 60.713s, episode steps: 703, steps per second:  12, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.014077, mae: 2.531385, mean_q: 3.051355, mean_eps: 0.706615
 816194/1000000: episode: 1199, duration: 76.080s, episode steps: 883, steps per second:  12, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.016846, mae: 2.548168, mean_q: 3.073264, mean_eps: 0.706329
 817001/1000000: episode: 1200, duration: 68.756s, episode steps: 807, steps per second:  12, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.014530, mae: 2.540900, mean_q: 3.064587, mean_eps: 0.706025
 817431/1000000: episode: 1201, duration: 36.992s, episode steps: 430, steps per second:  12, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.719 [0.000, 5.000],  loss: 0.012323, mae: 2.516411, mean_q: 3.036785, mean_eps: 0.705802
 818185/1000000: episode: 1202, duration: 64.935s, episode steps: 754, steps per second:  12, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.015342, mae: 2.559850, mean_q: 3.087390, mean_eps: 0.705589
 819161/1000000: episode: 1203, duration: 76.874s, episode steps: 976, steps per second:  13, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.015854, mae: 2.541934, mean_q: 3.065125, mean_eps: 0.705277
 819984/1000000: episode: 1204, duration: 62.019s, episode steps: 823, steps per second:  13, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013371, mae: 2.551345, mean_q: 3.076058, mean_eps: 0.704954
 820564/1000000: episode: 1205, duration: 45.054s, episode steps: 580, steps per second:  13, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.013838, mae: 2.562966, mean_q: 3.090129, mean_eps: 0.704702
 821654/1000000: episode: 1206, duration: 86.065s, episode steps: 1090, steps per second:  13, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.015043, mae: 2.545880, mean_q: 3.069757, mean_eps: 0.704401
 822047/1000000: episode: 1207, duration: 33.328s, episode steps: 393, steps per second:  12, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.016906, mae: 2.545728, mean_q: 3.070847, mean_eps: 0.704134
 822997/1000000: episode: 1208, duration: 80.774s, episode steps: 950, steps per second:  12, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013448, mae: 2.540587, mean_q: 3.062411, mean_eps: 0.703892
 823830/1000000: episode: 1209, duration: 71.926s, episode steps: 833, steps per second:  12, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.015400, mae: 2.536663, mean_q: 3.063701, mean_eps: 0.703571
 824559/1000000: episode: 1210, duration: 61.918s, episode steps: 729, steps per second:  12, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.015827, mae: 2.549492, mean_q: 3.074197, mean_eps: 0.703290
 824968/1000000: episode: 1211, duration: 35.422s, episode steps: 409, steps per second:  12, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.012947, mae: 2.545660, mean_q: 3.069277, mean_eps: 0.703086
 825474/1000000: episode: 1212, duration: 42.675s, episode steps: 506, steps per second:  12, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.013837, mae: 2.526457, mean_q: 3.048468, mean_eps: 0.702921
 826009/1000000: episode: 1213, duration: 45.117s, episode steps: 535, steps per second:  12, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.014912, mae: 2.563649, mean_q: 3.091250, mean_eps: 0.702733
 826768/1000000: episode: 1214, duration: 66.213s, episode steps: 759, steps per second:  11, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.014816, mae: 2.538801, mean_q: 3.061170, mean_eps: 0.702500
 827439/1000000: episode: 1215, duration: 57.489s, episode steps: 671, steps per second:  12, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.014349, mae: 2.577745, mean_q: 3.108015, mean_eps: 0.702243
 828740/1000000: episode: 1216, duration: 112.542s, episode steps: 1301, steps per second:  12, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.016333, mae: 2.555974, mean_q: 3.081866, mean_eps: 0.701888
 829283/1000000: episode: 1217, duration: 47.538s, episode steps: 543, steps per second:  11, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.016605, mae: 2.545508, mean_q: 3.069409, mean_eps: 0.701556
 829910/1000000: episode: 1218, duration: 53.994s, episode steps: 627, steps per second:  12, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.017063, mae: 2.534897, mean_q: 3.057980, mean_eps: 0.701345
 830296/1000000: episode: 1219, duration: 33.230s, episode steps: 386, steps per second:  12, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.015561, mae: 2.539916, mean_q: 3.061491, mean_eps: 0.701163
 830867/1000000: episode: 1220, duration: 49.777s, episode steps: 571, steps per second:  11, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.014754, mae: 2.558728, mean_q: 3.085993, mean_eps: 0.700991
 831792/1000000: episode: 1221, duration: 79.500s, episode steps: 925, steps per second:  12, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.016340, mae: 2.541000, mean_q: 3.064830, mean_eps: 0.700722
 832367/1000000: episode: 1222, duration: 49.328s, episode steps: 575, steps per second:  12, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.014784, mae: 2.551986, mean_q: 3.077718, mean_eps: 0.700452
 832948/1000000: episode: 1223, duration: 49.555s, episode steps: 581, steps per second:  12, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.014872, mae: 2.541075, mean_q: 3.064801, mean_eps: 0.700244
 833624/1000000: episode: 1224, duration: 58.052s, episode steps: 676, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.015338, mae: 2.555342, mean_q: 3.082555, mean_eps: 0.700018
 834132/1000000: episode: 1225, duration: 43.786s, episode steps: 508, steps per second:  12, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.013915, mae: 2.529224, mean_q: 3.050181, mean_eps: 0.699805
 834797/1000000: episode: 1226, duration: 58.112s, episode steps: 665, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.015272, mae: 2.536656, mean_q: 3.060552, mean_eps: 0.699593
 835527/1000000: episode: 1227, duration: 62.445s, episode steps: 730, steps per second:  12, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.014450, mae: 2.543823, mean_q: 3.067826, mean_eps: 0.699342
 836191/1000000: episode: 1228, duration: 56.637s, episode steps: 664, steps per second:  12, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.015834, mae: 2.544032, mean_q: 3.068224, mean_eps: 0.699091
 836890/1000000: episode: 1229, duration: 60.116s, episode steps: 699, steps per second:  12, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.015980, mae: 2.557896, mean_q: 3.088242, mean_eps: 0.698846
 837436/1000000: episode: 1230, duration: 47.492s, episode steps: 546, steps per second:  11, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.014015, mae: 2.560823, mean_q: 3.086987, mean_eps: 0.698622
 838411/1000000: episode: 1231, duration: 85.114s, episode steps: 975, steps per second:  11, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.015644, mae: 2.554921, mean_q: 3.081127, mean_eps: 0.698348
 839132/1000000: episode: 1232, duration: 62.592s, episode steps: 721, steps per second:  12, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.014544, mae: 2.552393, mean_q: 3.079682, mean_eps: 0.698043
 840113/1000000: episode: 1233, duration: 85.039s, episode steps: 981, steps per second:  12, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.014831, mae: 2.562905, mean_q: 3.091188, mean_eps: 0.697736
 840789/1000000: episode: 1234, duration: 57.940s, episode steps: 676, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.014467, mae: 2.562905, mean_q: 3.089923, mean_eps: 0.697437
 841261/1000000: episode: 1235, duration: 41.379s, episode steps: 472, steps per second:  11, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.015892, mae: 2.559743, mean_q: 3.087706, mean_eps: 0.697231
 842024/1000000: episode: 1236, duration: 66.003s, episode steps: 763, steps per second:  12, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.014678, mae: 2.528191, mean_q: 3.046889, mean_eps: 0.697009
 842705/1000000: episode: 1237, duration: 58.772s, episode steps: 681, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.013474, mae: 2.549797, mean_q: 3.074121, mean_eps: 0.696749
 843203/1000000: episode: 1238, duration: 42.425s, episode steps: 498, steps per second:  12, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.016506, mae: 2.537928, mean_q: 3.060859, mean_eps: 0.696537
 843897/1000000: episode: 1239, duration: 57.248s, episode steps: 694, steps per second:  12, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.013779, mae: 2.540116, mean_q: 3.062176, mean_eps: 0.696322
 844792/1000000: episode: 1240, duration: 69.515s, episode steps: 895, steps per second:  13, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.014436, mae: 2.554586, mean_q: 3.080249, mean_eps: 0.696036
 845598/1000000: episode: 1241, duration: 64.636s, episode steps: 806, steps per second:  12, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015374, mae: 2.550873, mean_q: 3.074223, mean_eps: 0.695730
 846214/1000000: episode: 1242, duration: 50.213s, episode steps: 616, steps per second:  12, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.015830, mae: 2.558894, mean_q: 3.085959, mean_eps: 0.695474
 846937/1000000: episode: 1243, duration: 63.656s, episode steps: 723, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.015893, mae: 2.554233, mean_q: 3.078049, mean_eps: 0.695233
 847978/1000000: episode: 1244, duration: 90.224s, episode steps: 1041, steps per second:  12, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.013710, mae: 2.558002, mean_q: 3.083434, mean_eps: 0.694915
 848671/1000000: episode: 1245, duration: 58.614s, episode steps: 693, steps per second:  12, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.014696, mae: 2.548625, mean_q: 3.074075, mean_eps: 0.694603
 849048/1000000: episode: 1246, duration: 32.599s, episode steps: 377, steps per second:  12, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014738, mae: 2.550818, mean_q: 3.076011, mean_eps: 0.694411
 849468/1000000: episode: 1247, duration: 36.805s, episode steps: 420, steps per second:  11, episode reward: 10.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.014974, mae: 2.537072, mean_q: 3.060214, mean_eps: 0.694268
 850438/1000000: episode: 1248, duration: 82.450s, episode steps: 970, steps per second:  12, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.017710, mae: 2.551706, mean_q: 3.077697, mean_eps: 0.694017
 850920/1000000: episode: 1249, duration: 42.837s, episode steps: 482, steps per second:  11, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.018292, mae: 2.558705, mean_q: 3.084411, mean_eps: 0.693756
 851831/1000000: episode: 1250, duration: 78.007s, episode steps: 911, steps per second:  12, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.015236, mae: 2.580060, mean_q: 3.111220, mean_eps: 0.693505
 852494/1000000: episode: 1251, duration: 57.160s, episode steps: 663, steps per second:  12, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013847, mae: 2.580274, mean_q: 3.111415, mean_eps: 0.693222
 853144/1000000: episode: 1252, duration: 55.553s, episode steps: 650, steps per second:  12, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.015003, mae: 2.571070, mean_q: 3.097636, mean_eps: 0.692986
 853700/1000000: episode: 1253, duration: 48.873s, episode steps: 556, steps per second:  11, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.014408, mae: 2.571103, mean_q: 3.100989, mean_eps: 0.692769
 854793/1000000: episode: 1254, duration: 94.838s, episode steps: 1093, steps per second:  12, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.014803, mae: 2.593024, mean_q: 3.129996, mean_eps: 0.692471
 855322/1000000: episode: 1255, duration: 47.350s, episode steps: 529, steps per second:  11, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.013745, mae: 2.587922, mean_q: 3.120112, mean_eps: 0.692179
 855989/1000000: episode: 1256, duration: 56.769s, episode steps: 667, steps per second:  12, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014405, mae: 2.574430, mean_q: 3.103878, mean_eps: 0.691964
 856477/1000000: episode: 1257, duration: 42.865s, episode steps: 488, steps per second:  11, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.015790, mae: 2.564376, mean_q: 3.090086, mean_eps: 0.691756
 857203/1000000: episode: 1258, duration: 62.302s, episode steps: 726, steps per second:  12, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.015298, mae: 2.574939, mean_q: 3.102919, mean_eps: 0.691538
 858011/1000000: episode: 1259, duration: 69.391s, episode steps: 808, steps per second:  12, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.014721, mae: 2.562741, mean_q: 3.089004, mean_eps: 0.691262
 858512/1000000: episode: 1260, duration: 44.524s, episode steps: 501, steps per second:  11, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.016655, mae: 2.560468, mean_q: 3.085673, mean_eps: 0.691026
 859367/1000000: episode: 1261, duration: 73.547s, episode steps: 855, steps per second:  12, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.016721, mae: 2.554179, mean_q: 3.078140, mean_eps: 0.690782
 859837/1000000: episode: 1262, duration: 41.906s, episode steps: 470, steps per second:  11, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.014796, mae: 2.577140, mean_q: 3.106682, mean_eps: 0.690543
 860239/1000000: episode: 1263, duration: 34.258s, episode steps: 402, steps per second:  12, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.013703, mae: 2.571353, mean_q: 3.100883, mean_eps: 0.690386
 861284/1000000: episode: 1264, duration: 114.443s, episode steps: 1045, steps per second:   9, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.018173, mae: 2.577859, mean_q: 3.105760, mean_eps: 0.690126
 861678/1000000: episode: 1265, duration: 36.185s, episode steps: 394, steps per second:  11, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.016635, mae: 2.581687, mean_q: 3.111416, mean_eps: 0.689867
 862754/1000000: episode: 1266, duration: 97.208s, episode steps: 1076, steps per second:  11, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.014461, mae: 2.579554, mean_q: 3.109465, mean_eps: 0.689602
 863390/1000000: episode: 1267, duration: 59.216s, episode steps: 636, steps per second:  11, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.015995, mae: 2.590708, mean_q: 3.122392, mean_eps: 0.689294
 864111/1000000: episode: 1268, duration: 68.074s, episode steps: 721, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.016745, mae: 2.547999, mean_q: 3.071004, mean_eps: 0.689050
 864494/1000000: episode: 1269, duration: 33.850s, episode steps: 383, steps per second:  11, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.034 [0.000, 5.000],  loss: 0.012637, mae: 2.582424, mean_q: 3.115999, mean_eps: 0.688851
 865093/1000000: episode: 1270, duration: 54.787s, episode steps: 599, steps per second:  11, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.016111, mae: 2.584258, mean_q: 3.117237, mean_eps: 0.688674
 865719/1000000: episode: 1271, duration: 48.774s, episode steps: 626, steps per second:  13, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.014537, mae: 2.562623, mean_q: 3.089769, mean_eps: 0.688454
 866121/1000000: episode: 1272, duration: 31.742s, episode steps: 402, steps per second:  13, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.013570, mae: 2.568649, mean_q: 3.096785, mean_eps: 0.688269
 866737/1000000: episode: 1273, duration: 48.162s, episode steps: 616, steps per second:  13, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.013613, mae: 2.581618, mean_q: 3.112003, mean_eps: 0.688085
 867471/1000000: episode: 1274, duration: 56.865s, episode steps: 734, steps per second:  13, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.013950, mae: 2.581916, mean_q: 3.112626, mean_eps: 0.687843
 868387/1000000: episode: 1275, duration: 76.639s, episode steps: 916, steps per second:  12, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.014181, mae: 2.581545, mean_q: 3.111826, mean_eps: 0.687546
 869047/1000000: episode: 1276, duration: 57.301s, episode steps: 660, steps per second:  12, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.014417, mae: 2.546615, mean_q: 3.069445, mean_eps: 0.687262
 869740/1000000: episode: 1277, duration: 61.422s, episode steps: 693, steps per second:  11, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.015016, mae: 2.558735, mean_q: 3.083714, mean_eps: 0.687019
 870426/1000000: episode: 1278, duration: 61.895s, episode steps: 686, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.014499, mae: 2.586929, mean_q: 3.119439, mean_eps: 0.686770
 871045/1000000: episode: 1279, duration: 55.053s, episode steps: 619, steps per second:  11, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.016590, mae: 2.577960, mean_q: 3.105791, mean_eps: 0.686535
 872269/1000000: episode: 1280, duration: 110.112s, episode steps: 1224, steps per second:  11, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.014056, mae: 2.577891, mean_q: 3.108540, mean_eps: 0.686203
 873313/1000000: episode: 1281, duration: 81.545s, episode steps: 1044, steps per second:  13, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.015139, mae: 2.574391, mean_q: 3.104242, mean_eps: 0.685795
 873791/1000000: episode: 1282, duration: 38.557s, episode steps: 478, steps per second:  12, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.011517, mae: 2.546092, mean_q: 3.069397, mean_eps: 0.685521
 874601/1000000: episode: 1283, duration: 63.909s, episode steps: 810, steps per second:  13, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.015492, mae: 2.583602, mean_q: 3.114633, mean_eps: 0.685289
 875296/1000000: episode: 1284, duration: 60.467s, episode steps: 695, steps per second:  11, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015388, mae: 2.588331, mean_q: 3.121700, mean_eps: 0.685019
 875740/1000000: episode: 1285, duration: 39.923s, episode steps: 444, steps per second:  11, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.014381, mae: 2.577214, mean_q: 3.108880, mean_eps: 0.684814
 876541/1000000: episode: 1286, duration: 71.871s, episode steps: 801, steps per second:  11, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.013463, mae: 2.588340, mean_q: 3.120467, mean_eps: 0.684590
 877422/1000000: episode: 1287, duration: 77.547s, episode steps: 881, steps per second:  11, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.015570, mae: 2.558229, mean_q: 3.084726, mean_eps: 0.684286
 877870/1000000: episode: 1288, duration: 41.174s, episode steps: 448, steps per second:  11, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.016660, mae: 2.573773, mean_q: 3.103565, mean_eps: 0.684047
 878429/1000000: episode: 1289, duration: 49.578s, episode steps: 559, steps per second:  11, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014462, mae: 2.564558, mean_q: 3.090061, mean_eps: 0.683866
 878950/1000000: episode: 1290, duration: 46.629s, episode steps: 521, steps per second:  11, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.016182, mae: 2.573256, mean_q: 3.099753, mean_eps: 0.683672
 879697/1000000: episode: 1291, duration: 66.588s, episode steps: 747, steps per second:  11, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.017009, mae: 2.560388, mean_q: 3.085021, mean_eps: 0.683443
 880438/1000000: episode: 1292, duration: 65.187s, episode steps: 741, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.014349, mae: 2.552216, mean_q: 3.078529, mean_eps: 0.683176
 881143/1000000: episode: 1293, duration: 62.666s, episode steps: 705, steps per second:  11, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.015394, mae: 2.567133, mean_q: 3.092198, mean_eps: 0.682916
 882250/1000000: episode: 1294, duration: 97.408s, episode steps: 1107, steps per second:  11, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.016255, mae: 2.576880, mean_q: 3.105483, mean_eps: 0.682589
 882875/1000000: episode: 1295, duration: 55.143s, episode steps: 625, steps per second:  11, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.015836, mae: 2.585858, mean_q: 3.115181, mean_eps: 0.682278
 883824/1000000: episode: 1296, duration: 85.064s, episode steps: 949, steps per second:  11, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.014100, mae: 2.583849, mean_q: 3.114890, mean_eps: 0.681995
 884417/1000000: episode: 1297, duration: 52.286s, episode steps: 593, steps per second:  11, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.766 [0.000, 5.000],  loss: 0.016303, mae: 2.546546, mean_q: 3.069948, mean_eps: 0.681717
 884764/1000000: episode: 1298, duration: 31.095s, episode steps: 347, steps per second:  11, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.014326, mae: 2.583559, mean_q: 3.113436, mean_eps: 0.681548
 885270/1000000: episode: 1299, duration: 45.133s, episode steps: 506, steps per second:  11, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.013195, mae: 2.545109, mean_q: 3.068771, mean_eps: 0.681394
 885784/1000000: episode: 1300, duration: 46.199s, episode steps: 514, steps per second:  11, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.015352, mae: 2.563934, mean_q: 3.091819, mean_eps: 0.681211
 886883/1000000: episode: 1301, duration: 100.420s, episode steps: 1099, steps per second:  11, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.013743, mae: 2.562827, mean_q: 3.091887, mean_eps: 0.680920
 887404/1000000: episode: 1302, duration: 47.216s, episode steps: 521, steps per second:  11, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013327, mae: 2.574976, mean_q: 3.103933, mean_eps: 0.680629
 887860/1000000: episode: 1303, duration: 41.271s, episode steps: 456, steps per second:  11, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.014768, mae: 2.580923, mean_q: 3.109879, mean_eps: 0.680453
 888264/1000000: episode: 1304, duration: 35.928s, episode steps: 404, steps per second:  11, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.014823, mae: 2.566489, mean_q: 3.091291, mean_eps: 0.680298
 889240/1000000: episode: 1305, duration: 86.824s, episode steps: 976, steps per second:  11, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.016533, mae: 2.571391, mean_q: 3.098134, mean_eps: 0.680050
 889869/1000000: episode: 1306, duration: 55.609s, episode steps: 629, steps per second:  11, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.013990, mae: 2.557644, mean_q: 3.084211, mean_eps: 0.679761
 890398/1000000: episode: 1307, duration: 47.842s, episode steps: 529, steps per second:  11, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.015354, mae: 2.569977, mean_q: 3.097345, mean_eps: 0.679552
 890904/1000000: episode: 1308, duration: 45.010s, episode steps: 506, steps per second:  11, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.013613, mae: 2.585138, mean_q: 3.117772, mean_eps: 0.679366
 891526/1000000: episode: 1309, duration: 54.955s, episode steps: 622, steps per second:  11, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.015220, mae: 2.587197, mean_q: 3.119105, mean_eps: 0.679163
 892139/1000000: episode: 1310, duration: 60.326s, episode steps: 613, steps per second:  10, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.017573, mae: 2.575523, mean_q: 3.104620, mean_eps: 0.678940
 892567/1000000: episode: 1311, duration: 37.676s, episode steps: 428, steps per second:  11, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012195, mae: 2.551022, mean_q: 3.075109, mean_eps: 0.678753
 893225/1000000: episode: 1312, duration: 65.360s, episode steps: 658, steps per second:  10, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014301, mae: 2.571478, mean_q: 3.098524, mean_eps: 0.678557
 894021/1000000: episode: 1313, duration: 75.837s, episode steps: 796, steps per second:  10, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.014522, mae: 2.578658, mean_q: 3.109499, mean_eps: 0.678295
 894738/1000000: episode: 1314, duration: 69.434s, episode steps: 717, steps per second:  10, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.016118, mae: 2.589334, mean_q: 3.123707, mean_eps: 0.678023
 895264/1000000: episode: 1315, duration: 53.987s, episode steps: 526, steps per second:  10, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.013430, mae: 2.558792, mean_q: 3.086472, mean_eps: 0.677800
 895690/1000000: episode: 1316, duration: 40.730s, episode steps: 426, steps per second:  10, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.014425, mae: 2.566151, mean_q: 3.093281, mean_eps: 0.677629
 896323/1000000: episode: 1317, duration: 53.584s, episode steps: 633, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.015389, mae: 2.572445, mean_q: 3.100731, mean_eps: 0.677438
 897236/1000000: episode: 1318, duration: 74.865s, episode steps: 913, steps per second:  12, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.016522, mae: 2.570395, mean_q: 3.097804, mean_eps: 0.677160
 897869/1000000: episode: 1319, duration: 58.328s, episode steps: 633, steps per second:  11, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.014937, mae: 2.576202, mean_q: 3.104740, mean_eps: 0.676881
 898520/1000000: episode: 1320, duration: 60.908s, episode steps: 651, steps per second:  11, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.016390, mae: 2.602122, mean_q: 3.135124, mean_eps: 0.676650
 899107/1000000: episode: 1321, duration: 52.826s, episode steps: 587, steps per second:  11, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.014425, mae: 2.566548, mean_q: 3.095922, mean_eps: 0.676428
 899726/1000000: episode: 1322, duration: 51.652s, episode steps: 619, steps per second:  12, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.015673, mae: 2.567338, mean_q: 3.094296, mean_eps: 0.676210
 900807/1000000: episode: 1323, duration: 94.063s, episode steps: 1081, steps per second:  11, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.014640, mae: 2.578237, mean_q: 3.109351, mean_eps: 0.675904
 901326/1000000: episode: 1324, duration: 47.779s, episode steps: 519, steps per second:  11, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014643, mae: 2.566425, mean_q: 3.098285, mean_eps: 0.675616
 902403/1000000: episode: 1325, duration: 101.720s, episode steps: 1077, steps per second:  11, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.015281, mae: 2.606354, mean_q: 3.142153, mean_eps: 0.675329
 903210/1000000: episode: 1326, duration: 78.339s, episode steps: 807, steps per second:  10, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.017113, mae: 2.589015, mean_q: 3.122299, mean_eps: 0.674990
 903966/1000000: episode: 1327, duration: 72.895s, episode steps: 756, steps per second:  10, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.015957, mae: 2.588918, mean_q: 3.119986, mean_eps: 0.674708
 904832/1000000: episode: 1328, duration: 79.810s, episode steps: 866, steps per second:  11, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014583, mae: 2.586290, mean_q: 3.117345, mean_eps: 0.674417
 905795/1000000: episode: 1329, duration: 88.033s, episode steps: 963, steps per second:  11, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.015253, mae: 2.580304, mean_q: 3.109383, mean_eps: 0.674088
 906339/1000000: episode: 1330, duration: 48.426s, episode steps: 544, steps per second:  11, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.012750, mae: 2.589776, mean_q: 3.121913, mean_eps: 0.673816
 906965/1000000: episode: 1331, duration: 57.865s, episode steps: 626, steps per second:  11, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.013702, mae: 2.576180, mean_q: 3.104150, mean_eps: 0.673605
 907735/1000000: episode: 1332, duration: 68.837s, episode steps: 770, steps per second:  11, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.015059, mae: 2.583245, mean_q: 3.112650, mean_eps: 0.673354
 908227/1000000: episode: 1333, duration: 45.923s, episode steps: 492, steps per second:  11, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.013899, mae: 2.599785, mean_q: 3.134180, mean_eps: 0.673127
 909061/1000000: episode: 1334, duration: 75.459s, episode steps: 834, steps per second:  11, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.015438, mae: 2.601210, mean_q: 3.134585, mean_eps: 0.672888
 910254/1000000: episode: 1335, duration: 108.744s, episode steps: 1193, steps per second:  11, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014107, mae: 2.594266, mean_q: 3.127623, mean_eps: 0.672523
 910687/1000000: episode: 1336, duration: 39.094s, episode steps: 433, steps per second:  11, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.834 [0.000, 5.000],  loss: 0.017643, mae: 2.610118, mean_q: 3.145474, mean_eps: 0.672231
 911428/1000000: episode: 1337, duration: 66.400s, episode steps: 741, steps per second:  11, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.015893, mae: 2.608888, mean_q: 3.144244, mean_eps: 0.672020
 912296/1000000: episode: 1338, duration: 79.428s, episode steps: 868, steps per second:  11, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.015060, mae: 2.584482, mean_q: 3.116825, mean_eps: 0.671730
 913233/1000000: episode: 1339, duration: 85.659s, episode steps: 937, steps per second:  11, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.015046, mae: 2.582426, mean_q: 3.113662, mean_eps: 0.671405
 913746/1000000: episode: 1340, duration: 47.611s, episode steps: 513, steps per second:  11, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.016705, mae: 2.576044, mean_q: 3.101825, mean_eps: 0.671144
 914465/1000000: episode: 1341, duration: 65.938s, episode steps: 719, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.014046, mae: 2.588413, mean_q: 3.120859, mean_eps: 0.670922
 915325/1000000: episode: 1342, duration: 77.854s, episode steps: 860, steps per second:  11, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.016330, mae: 2.584538, mean_q: 3.114765, mean_eps: 0.670637
 915743/1000000: episode: 1343, duration: 38.436s, episode steps: 418, steps per second:  11, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.013661, mae: 2.596470, mean_q: 3.129338, mean_eps: 0.670408
 916230/1000000: episode: 1344, duration: 43.107s, episode steps: 487, steps per second:  11, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.016378, mae: 2.597830, mean_q: 3.130725, mean_eps: 0.670245
 917153/1000000: episode: 1345, duration: 72.260s, episode steps: 923, steps per second:  13, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.015541, mae: 2.581180, mean_q: 3.111656, mean_eps: 0.669991
 917600/1000000: episode: 1346, duration: 37.335s, episode steps: 447, steps per second:  12, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.015937, mae: 2.613501, mean_q: 3.150190, mean_eps: 0.669745
 918189/1000000: episode: 1347, duration: 53.234s, episode steps: 589, steps per second:  11, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.015440, mae: 2.571735, mean_q: 3.100440, mean_eps: 0.669558
 918843/1000000: episode: 1348, duration: 59.714s, episode steps: 654, steps per second:  11, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.015944, mae: 2.593026, mean_q: 3.123711, mean_eps: 0.669334
 919407/1000000: episode: 1349, duration: 51.859s, episode steps: 564, steps per second:  11, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.015745, mae: 2.586413, mean_q: 3.117514, mean_eps: 0.669115
 919903/1000000: episode: 1350, duration: 44.697s, episode steps: 496, steps per second:  11, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014608, mae: 2.597897, mean_q: 3.129183, mean_eps: 0.668925
 920645/1000000: episode: 1351, duration: 68.367s, episode steps: 742, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.015960, mae: 2.595789, mean_q: 3.130461, mean_eps: 0.668701
 921776/1000000: episode: 1352, duration: 102.004s, episode steps: 1131, steps per second:  11, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.015071, mae: 2.573117, mean_q: 3.101563, mean_eps: 0.668364
 922489/1000000: episode: 1353, duration: 64.359s, episode steps: 713, steps per second:  11, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014218, mae: 2.592015, mean_q: 3.125331, mean_eps: 0.668032
 923009/1000000: episode: 1354, duration: 47.282s, episode steps: 520, steps per second:  11, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.017161, mae: 2.586382, mean_q: 3.116291, mean_eps: 0.667810
 923600/1000000: episode: 1355, duration: 53.805s, episode steps: 591, steps per second:  11, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.014879, mae: 2.580130, mean_q: 3.111986, mean_eps: 0.667611
 924160/1000000: episode: 1356, duration: 50.605s, episode steps: 560, steps per second:  11, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.015639, mae: 2.601361, mean_q: 3.134684, mean_eps: 0.667404
 925034/1000000: episode: 1357, duration: 80.248s, episode steps: 874, steps per second:  11, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013457, mae: 2.583490, mean_q: 3.113343, mean_eps: 0.667145
 926039/1000000: episode: 1358, duration: 91.452s, episode steps: 1005, steps per second:  11, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.015303, mae: 2.583852, mean_q: 3.113271, mean_eps: 0.666807
 926610/1000000: episode: 1359, duration: 52.665s, episode steps: 571, steps per second:  11, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.017581, mae: 2.606333, mean_q: 3.139427, mean_eps: 0.666523
 927348/1000000: episode: 1360, duration: 66.706s, episode steps: 738, steps per second:  11, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.015012, mae: 2.571362, mean_q: 3.098474, mean_eps: 0.666288
 928190/1000000: episode: 1361, duration: 76.330s, episode steps: 842, steps per second:  11, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013713, mae: 2.575553, mean_q: 3.104368, mean_eps: 0.666004
 928679/1000000: episode: 1362, duration: 44.550s, episode steps: 489, steps per second:  11, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.017212, mae: 2.593603, mean_q: 3.126807, mean_eps: 0.665764
 929277/1000000: episode: 1363, duration: 54.437s, episode steps: 598, steps per second:  11, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.014538, mae: 2.566316, mean_q: 3.092666, mean_eps: 0.665568
 930032/1000000: episode: 1364, duration: 70.500s, episode steps: 755, steps per second:  11, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.015097, mae: 2.587819, mean_q: 3.118531, mean_eps: 0.665325
 930507/1000000: episode: 1365, duration: 43.417s, episode steps: 475, steps per second:  11, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.015915, mae: 2.555337, mean_q: 3.077967, mean_eps: 0.665104
 931038/1000000: episode: 1366, duration: 48.314s, episode steps: 531, steps per second:  11, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.017140, mae: 2.588064, mean_q: 3.117343, mean_eps: 0.664922
 932092/1000000: episode: 1367, duration: 96.677s, episode steps: 1054, steps per second:  11, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.016059, mae: 2.585334, mean_q: 3.115649, mean_eps: 0.664637
 932507/1000000: episode: 1368, duration: 38.380s, episode steps: 415, steps per second:  11, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.017920, mae: 2.573827, mean_q: 3.101969, mean_eps: 0.664373
 933547/1000000: episode: 1369, duration: 94.878s, episode steps: 1040, steps per second:  11, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.016077, mae: 2.592426, mean_q: 3.123457, mean_eps: 0.664111
 934454/1000000: episode: 1370, duration: 83.978s, episode steps: 907, steps per second:  11, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.014522, mae: 2.573599, mean_q: 3.103329, mean_eps: 0.663760
 934929/1000000: episode: 1371, duration: 42.984s, episode steps: 475, steps per second:  11, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.014346, mae: 2.571144, mean_q: 3.100751, mean_eps: 0.663511
 935411/1000000: episode: 1372, duration: 49.380s, episode steps: 482, steps per second:  10, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.015791, mae: 2.598711, mean_q: 3.130610, mean_eps: 0.663339
 936651/1000000: episode: 1373, duration: 116.946s, episode steps: 1240, steps per second:  11, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.015099, mae: 2.571727, mean_q: 3.100373, mean_eps: 0.663029
 937608/1000000: episode: 1374, duration: 88.449s, episode steps: 957, steps per second:  11, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.015347, mae: 2.591966, mean_q: 3.121596, mean_eps: 0.662634
 938211/1000000: episode: 1375, duration: 57.141s, episode steps: 603, steps per second:  11, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.015257, mae: 2.588593, mean_q: 3.117524, mean_eps: 0.662353
 938901/1000000: episode: 1376, duration: 65.054s, episode steps: 690, steps per second:  11, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.016703, mae: 2.588774, mean_q: 3.119015, mean_eps: 0.662120
 939502/1000000: episode: 1377, duration: 57.237s, episode steps: 601, steps per second:  11, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.015158, mae: 2.614425, mean_q: 3.152315, mean_eps: 0.661887
 940161/1000000: episode: 1378, duration: 61.158s, episode steps: 659, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.015294, mae: 2.578685, mean_q: 3.107081, mean_eps: 0.661660
 940864/1000000: episode: 1379, duration: 67.212s, episode steps: 703, steps per second:  10, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.014918, mae: 2.584732, mean_q: 3.114738, mean_eps: 0.661416
 941971/1000000: episode: 1380, duration: 104.193s, episode steps: 1107, steps per second:  11, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.013723, mae: 2.593314, mean_q: 3.125265, mean_eps: 0.661090
 942585/1000000: episode: 1381, duration: 58.046s, episode steps: 614, steps per second:  11, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.015471, mae: 2.591415, mean_q: 3.122132, mean_eps: 0.660780
 942961/1000000: episode: 1382, duration: 35.010s, episode steps: 376, steps per second:  11, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.012054, mae: 2.548968, mean_q: 3.072978, mean_eps: 0.660601
 944057/1000000: episode: 1383, duration: 101.914s, episode steps: 1096, steps per second:  11, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.015789, mae: 2.578116, mean_q: 3.106434, mean_eps: 0.660336
 944827/1000000: episode: 1384, duration: 74.505s, episode steps: 770, steps per second:  10, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.015360, mae: 2.591403, mean_q: 3.121746, mean_eps: 0.660001
 945499/1000000: episode: 1385, duration: 62.591s, episode steps: 672, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015754, mae: 2.598004, mean_q: 3.129437, mean_eps: 0.659742
 946008/1000000: episode: 1386, duration: 48.536s, episode steps: 509, steps per second:  10, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.015547, mae: 2.583467, mean_q: 3.113629, mean_eps: 0.659529
 946769/1000000: episode: 1387, duration: 79.215s, episode steps: 761, steps per second:  10, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.018562, mae: 2.585280, mean_q: 3.113672, mean_eps: 0.659300
 947400/1000000: episode: 1388, duration: 62.639s, episode steps: 631, steps per second:  10, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.014132, mae: 2.586332, mean_q: 3.117628, mean_eps: 0.659050
 947922/1000000: episode: 1389, duration: 52.227s, episode steps: 522, steps per second:  10, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.014079, mae: 2.592723, mean_q: 3.125198, mean_eps: 0.658842
 948564/1000000: episode: 1390, duration: 65.397s, episode steps: 642, steps per second:  10, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.014923, mae: 2.554842, mean_q: 3.079711, mean_eps: 0.658633
 949251/1000000: episode: 1391, duration: 67.120s, episode steps: 687, steps per second:  10, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.017418, mae: 2.597546, mean_q: 3.129271, mean_eps: 0.658394
 949633/1000000: episode: 1392, duration: 40.184s, episode steps: 382, steps per second:  10, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.013386, mae: 2.570683, mean_q: 3.099330, mean_eps: 0.658201
 950891/1000000: episode: 1393, duration: 121.150s, episode steps: 1258, steps per second:  10, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014262, mae: 2.589722, mean_q: 3.121877, mean_eps: 0.657906
 951353/1000000: episode: 1394, duration: 41.398s, episode steps: 462, steps per second:  11, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.015921, mae: 2.622299, mean_q: 3.158285, mean_eps: 0.657596
 952258/1000000: episode: 1395, duration: 77.547s, episode steps: 905, steps per second:  12, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.014429, mae: 2.557393, mean_q: 3.082832, mean_eps: 0.657350
 952753/1000000: episode: 1396, duration: 47.197s, episode steps: 495, steps per second:  10, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.015060, mae: 2.576834, mean_q: 3.104749, mean_eps: 0.657098
 953168/1000000: episode: 1397, duration: 38.587s, episode steps: 415, steps per second:  11, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.013145, mae: 2.593520, mean_q: 3.126169, mean_eps: 0.656934
 953857/1000000: episode: 1398, duration: 65.782s, episode steps: 689, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.013803, mae: 2.580957, mean_q: 3.108756, mean_eps: 0.656736
 954541/1000000: episode: 1399, duration: 78.902s, episode steps: 684, steps per second:   9, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.017094, mae: 2.562039, mean_q: 3.088127, mean_eps: 0.656488
 955087/1000000: episode: 1400, duration: 52.231s, episode steps: 546, steps per second:  10, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.015653, mae: 2.580741, mean_q: 3.109620, mean_eps: 0.656267
 955629/1000000: episode: 1401, duration: 55.162s, episode steps: 542, steps per second:  10, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.014308, mae: 2.562556, mean_q: 3.089640, mean_eps: 0.656071
 956006/1000000: episode: 1402, duration: 36.218s, episode steps: 377, steps per second:  10, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.015710, mae: 2.552721, mean_q: 3.076110, mean_eps: 0.655906
 956712/1000000: episode: 1403, duration: 86.451s, episode steps: 706, steps per second:   8, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.014807, mae: 2.562979, mean_q: 3.086392, mean_eps: 0.655711
 957401/1000000: episode: 1404, duration: 77.670s, episode steps: 689, steps per second:   9, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.014187, mae: 2.570429, mean_q: 3.096286, mean_eps: 0.655460