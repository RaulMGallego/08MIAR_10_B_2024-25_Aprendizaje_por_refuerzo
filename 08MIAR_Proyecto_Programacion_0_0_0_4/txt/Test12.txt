['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_250000.h5f
Hay pesos anteriores y se van a cargar
Training for 500000 steps ...
    420/500000: episode: 1, duration: 2.275s, episode steps: 420, steps per second: 185, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/500000: episode: 2, duration: 4.014s, episode steps: 711, steps per second: 177, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/500000: episode: 3, duration: 3.698s, episode steps: 810, steps per second: 219, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2827/500000: episode: 4, duration: 3.813s, episode steps: 886, steps per second: 232, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3345/500000: episode: 5, duration: 2.314s, episode steps: 518, steps per second: 224, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3990/500000: episode: 6, duration: 2.799s, episode steps: 645, steps per second: 230, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4451/500000: episode: 7, duration: 2.389s, episode steps: 461, steps per second: 193, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5247/500000: episode: 8, duration: 4.253s, episode steps: 796, steps per second: 187, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5629/500000: episode: 9, duration: 1.684s, episode steps: 382, steps per second: 227, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6751/500000: episode: 10, duration: 4.840s, episode steps: 1122, steps per second: 232, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7285/500000: episode: 11, duration: 2.689s, episode steps: 534, steps per second: 199, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7855/500000: episode: 12, duration: 4.015s, episode steps: 570, steps per second: 142, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8489/500000: episode: 13, duration: 3.641s, episode steps: 634, steps per second: 174, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9574/500000: episode: 14, duration: 4.786s, episode steps: 1085, steps per second: 227, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10243/500000: episode: 15, duration: 12.878s, episode steps: 669, steps per second:  52, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013060, mae: 2.430590, mean_q: 2.977170, mean_eps: 0.996356
  10923/500000: episode: 16, duration: 27.826s, episode steps: 680, steps per second:  24, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.012128, mae: 2.432858, mean_q: 2.976039, mean_eps: 0.996190
  11598/500000: episode: 17, duration: 27.699s, episode steps: 675, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.013280, mae: 2.451000, mean_q: 2.997898, mean_eps: 0.995946
  12367/500000: episode: 18, duration: 31.308s, episode steps: 769, steps per second:  25, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.011787, mae: 2.402895, mean_q: 2.940022, mean_eps: 0.995686
  12921/500000: episode: 19, duration: 23.296s, episode steps: 554, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012499, mae: 2.426008, mean_q: 2.968366, mean_eps: 0.995448
  13283/500000: episode: 20, duration: 14.624s, episode steps: 362, steps per second:  25, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.009528, mae: 2.395161, mean_q: 2.928656, mean_eps: 0.995283
  14077/500000: episode: 21, duration: 33.263s, episode steps: 794, steps per second:  24, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.011197, mae: 2.399694, mean_q: 2.936725, mean_eps: 0.995075
  15112/500000: episode: 22, duration: 41.821s, episode steps: 1035, steps per second:  25, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.011666, mae: 2.367094, mean_q: 2.892825, mean_eps: 0.994746
  15696/500000: episode: 23, duration: 24.564s, episode steps: 584, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.009619, mae: 2.385571, mean_q: 2.915945, mean_eps: 0.994455
  16385/500000: episode: 24, duration: 29.869s, episode steps: 689, steps per second:  23, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.009281, mae: 2.390462, mean_q: 2.924424, mean_eps: 0.994226
  17369/500000: episode: 25, duration: 45.179s, episode steps: 984, steps per second:  22, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.009331, mae: 2.387072, mean_q: 2.919790, mean_eps: 0.993924
  17856/500000: episode: 26, duration: 20.719s, episode steps: 487, steps per second:  24, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.012001, mae: 2.372365, mean_q: 2.899612, mean_eps: 0.993660
  19055/500000: episode: 27, duration: 50.799s, episode steps: 1199, steps per second:  24, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.010932, mae: 2.368862, mean_q: 2.896326, mean_eps: 0.993357
  19716/500000: episode: 28, duration: 27.999s, episode steps: 661, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.010150, mae: 2.381870, mean_q: 2.911460, mean_eps: 0.993022
  20299/500000: episode: 29, duration: 24.676s, episode steps: 583, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008950, mae: 2.380149, mean_q: 2.905352, mean_eps: 0.992798
  21038/500000: episode: 30, duration: 30.041s, episode steps: 739, steps per second:  25, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009907, mae: 2.374526, mean_q: 2.900965, mean_eps: 0.992560
  21851/500000: episode: 31, duration: 34.177s, episode steps: 813, steps per second:  24, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.009427, mae: 2.378122, mean_q: 2.905385, mean_eps: 0.992280
  22507/500000: episode: 32, duration: 27.239s, episode steps: 656, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.010412, mae: 2.392812, mean_q: 2.922355, mean_eps: 0.992016
  22881/500000: episode: 33, duration: 15.605s, episode steps: 374, steps per second:  24, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.008287, mae: 2.361579, mean_q: 2.883558, mean_eps: 0.991830
  23606/500000: episode: 34, duration: 30.599s, episode steps: 725, steps per second:  24, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.010598, mae: 2.370303, mean_q: 2.895089, mean_eps: 0.991632
  24493/500000: episode: 35, duration: 36.396s, episode steps: 887, steps per second:  24, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.009785, mae: 2.348516, mean_q: 2.865894, mean_eps: 0.991342
  25302/500000: episode: 36, duration: 33.854s, episode steps: 809, steps per second:  24, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.012476, mae: 2.377044, mean_q: 2.902888, mean_eps: 0.991037
  25799/500000: episode: 37, duration: 21.471s, episode steps: 497, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.010320, mae: 2.362745, mean_q: 2.883825, mean_eps: 0.990802
  26442/500000: episode: 38, duration: 25.992s, episode steps: 643, steps per second:  25, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.009237, mae: 2.324903, mean_q: 2.836654, mean_eps: 0.990597
  27879/500000: episode: 39, duration: 61.560s, episode steps: 1437, steps per second:  23, episode reward: 17.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.010909, mae: 2.318316, mean_q: 2.831260, mean_eps: 0.990222
  28728/500000: episode: 40, duration: 35.968s, episode steps: 849, steps per second:  24, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.008728, mae: 2.317080, mean_q: 2.830759, mean_eps: 0.989811
  29463/500000: episode: 41, duration: 30.777s, episode steps: 735, steps per second:  24, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.009725, mae: 2.364440, mean_q: 2.889419, mean_eps: 0.989526
  30371/500000: episode: 42, duration: 39.544s, episode steps: 908, steps per second:  23, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.010232, mae: 2.329755, mean_q: 2.842455, mean_eps: 0.989230
  30958/500000: episode: 43, duration: 24.441s, episode steps: 587, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.011568, mae: 2.327513, mean_q: 2.839740, mean_eps: 0.988961
  31573/500000: episode: 44, duration: 26.315s, episode steps: 615, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.011256, mae: 2.314406, mean_q: 2.824004, mean_eps: 0.988744
  32805/500000: episode: 45, duration: 51.175s, episode steps: 1232, steps per second:  24, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.009792, mae: 2.317797, mean_q: 2.828055, mean_eps: 0.988412
  33693/500000: episode: 46, duration: 37.494s, episode steps: 888, steps per second:  24, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.012410, mae: 2.317945, mean_q: 2.828877, mean_eps: 0.988030
  34300/500000: episode: 47, duration: 25.568s, episode steps: 607, steps per second:  24, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.013011, mae: 2.351915, mean_q: 2.871519, mean_eps: 0.987761
  34869/500000: episode: 48, duration: 25.333s, episode steps: 569, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.010481, mae: 2.352033, mean_q: 2.872643, mean_eps: 0.987550
  35261/500000: episode: 49, duration: 16.552s, episode steps: 392, steps per second:  24, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.011239, mae: 2.307061, mean_q: 2.817315, mean_eps: 0.987376
  35669/500000: episode: 50, duration: 17.363s, episode steps: 408, steps per second:  23, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.008721, mae: 2.316311, mean_q: 2.824154, mean_eps: 0.987232
  36059/500000: episode: 51, duration: 16.660s, episode steps: 390, steps per second:  23, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.009488, mae: 2.315458, mean_q: 2.823257, mean_eps: 0.987089
  36555/500000: episode: 52, duration: 20.182s, episode steps: 496, steps per second:  25, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.009488, mae: 2.303578, mean_q: 2.811870, mean_eps: 0.986930
  37286/500000: episode: 53, duration: 30.668s, episode steps: 731, steps per second:  24, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.010332, mae: 2.315155, mean_q: 2.826557, mean_eps: 0.986709
  38152/500000: episode: 54, duration: 35.994s, episode steps: 866, steps per second:  24, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.011164, mae: 2.314165, mean_q: 2.823965, mean_eps: 0.986422
  38986/500000: episode: 55, duration: 36.521s, episode steps: 834, steps per second:  23, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013169, mae: 2.315040, mean_q: 2.824380, mean_eps: 0.986116
  39615/500000: episode: 56, duration: 36.416s, episode steps: 629, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.009185, mae: 2.351089, mean_q: 2.870396, mean_eps: 0.985852
  40278/500000: episode: 57, duration: 31.771s, episode steps: 663, steps per second:  21, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.011188, mae: 2.324497, mean_q: 2.835157, mean_eps: 0.985619
  41414/500000: episode: 58, duration: 53.480s, episode steps: 1136, steps per second:  21, episode reward:  9.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.009851, mae: 2.329609, mean_q: 2.842516, mean_eps: 0.985295
  41826/500000: episode: 59, duration: 26.533s, episode steps: 412, steps per second:  16, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.009981, mae: 2.342871, mean_q: 2.856722, mean_eps: 0.985017
  42430/500000: episode: 60, duration: 35.484s, episode steps: 604, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.010459, mae: 2.347124, mean_q: 2.861404, mean_eps: 0.984834
  42824/500000: episode: 61, duration: 22.218s, episode steps: 394, steps per second:  18, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.009501, mae: 2.342791, mean_q: 2.857705, mean_eps: 0.984655
  43249/500000: episode: 62, duration: 26.792s, episode steps: 425, steps per second:  16, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.010823, mae: 2.338467, mean_q: 2.853404, mean_eps: 0.984507
  43923/500000: episode: 63, duration: 36.630s, episode steps: 674, steps per second:  18, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.011098, mae: 2.330299, mean_q: 2.842176, mean_eps: 0.984309
  44523/500000: episode: 64, duration: 27.257s, episode steps: 600, steps per second:  22, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.009439, mae: 2.338025, mean_q: 2.853207, mean_eps: 0.984080
  45333/500000: episode: 65, duration: 35.732s, episode steps: 810, steps per second:  23, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.009813, mae: 2.338590, mean_q: 2.852855, mean_eps: 0.983826
  45738/500000: episode: 66, duration: 17.871s, episode steps: 405, steps per second:  23, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.011737, mae: 2.337789, mean_q: 2.848975, mean_eps: 0.983607
  46328/500000: episode: 67, duration: 24.693s, episode steps: 590, steps per second:  24, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.009583, mae: 2.341228, mean_q: 2.855298, mean_eps: 0.983428
  47397/500000: episode: 68, duration: 45.844s, episode steps: 1069, steps per second:  23, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.010223, mae: 2.334796, mean_q: 2.847006, mean_eps: 0.983130
  47916/500000: episode: 69, duration: 21.498s, episode steps: 519, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.011376, mae: 2.313721, mean_q: 2.820996, mean_eps: 0.982844
  48291/500000: episode: 70, duration: 16.390s, episode steps: 375, steps per second:  23, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.010668, mae: 2.330993, mean_q: 2.842745, mean_eps: 0.982683
  48667/500000: episode: 71, duration: 15.726s, episode steps: 376, steps per second:  24, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.009186, mae: 2.335250, mean_q: 2.848338, mean_eps: 0.982548
  49812/500000: episode: 72, duration: 48.643s, episode steps: 1145, steps per second:  24, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.010967, mae: 2.338609, mean_q: 2.850716, mean_eps: 0.982274
  50424/500000: episode: 73, duration: 27.055s, episode steps: 612, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.010899, mae: 2.357850, mean_q: 2.875866, mean_eps: 0.981958
  50845/500000: episode: 74, duration: 18.397s, episode steps: 421, steps per second:  23, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.009634, mae: 2.378134, mean_q: 2.900299, mean_eps: 0.981772
  51372/500000: episode: 75, duration: 24.116s, episode steps: 527, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011965, mae: 2.372082, mean_q: 2.889741, mean_eps: 0.981601
  51963/500000: episode: 76, duration: 25.617s, episode steps: 591, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.010804, mae: 2.362158, mean_q: 2.878490, mean_eps: 0.981400
  52916/500000: episode: 77, duration: 41.132s, episode steps: 953, steps per second:  23, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.011008, mae: 2.364568, mean_q: 2.883629, mean_eps: 0.981122
  53569/500000: episode: 78, duration: 29.719s, episode steps: 653, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.011428, mae: 2.362415, mean_q: 2.879568, mean_eps: 0.980833
  53988/500000: episode: 79, duration: 18.118s, episode steps: 419, steps per second:  23, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.016935, mae: 2.393644, mean_q: 2.914277, mean_eps: 0.980640
  54588/500000: episode: 80, duration: 24.930s, episode steps: 600, steps per second:  24, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.011566, mae: 2.360513, mean_q: 2.875566, mean_eps: 0.980457
  55167/500000: episode: 81, duration: 25.305s, episode steps: 579, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.009865, mae: 2.390526, mean_q: 2.914070, mean_eps: 0.980245
  55848/500000: episode: 82, duration: 29.818s, episode steps: 681, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.012167, mae: 2.386266, mean_q: 2.907109, mean_eps: 0.980018
  56461/500000: episode: 83, duration: 26.017s, episode steps: 613, steps per second:  24, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.010272, mae: 2.369966, mean_q: 2.887132, mean_eps: 0.979785
  56925/500000: episode: 84, duration: 19.924s, episode steps: 464, steps per second:  23, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013396, mae: 2.366408, mean_q: 2.882773, mean_eps: 0.979590
  57512/500000: episode: 85, duration: 24.720s, episode steps: 587, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011996, mae: 2.379368, mean_q: 2.901043, mean_eps: 0.979402
  58071/500000: episode: 86, duration: 24.417s, episode steps: 559, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.010654, mae: 2.383915, mean_q: 2.906141, mean_eps: 0.979196
  58536/500000: episode: 87, duration: 19.158s, episode steps: 465, steps per second:  24, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.011853, mae: 2.391648, mean_q: 2.915148, mean_eps: 0.979011
  59269/500000: episode: 88, duration: 31.378s, episode steps: 733, steps per second:  23, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.010375, mae: 2.383151, mean_q: 2.902364, mean_eps: 0.978795
  59690/500000: episode: 89, duration: 17.530s, episode steps: 421, steps per second:  24, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011479, mae: 2.379045, mean_q: 2.896328, mean_eps: 0.978587
  60184/500000: episode: 90, duration: 20.405s, episode steps: 494, steps per second:  24, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.008641, mae: 2.377034, mean_q: 2.894641, mean_eps: 0.978423
  60963/500000: episode: 91, duration: 33.196s, episode steps: 779, steps per second:  23, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.011258, mae: 2.375546, mean_q: 2.893520, mean_eps: 0.978194
  61343/500000: episode: 92, duration: 15.849s, episode steps: 380, steps per second:  24, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.010781, mae: 2.364015, mean_q: 2.878327, mean_eps: 0.977985
  61811/500000: episode: 93, duration: 20.753s, episode steps: 468, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.010907, mae: 2.359713, mean_q: 2.873434, mean_eps: 0.977833
  62416/500000: episode: 94, duration: 25.032s, episode steps: 605, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.010446, mae: 2.395772, mean_q: 2.919542, mean_eps: 0.977640
  63236/500000: episode: 95, duration: 34.252s, episode steps: 820, steps per second:  24, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.012310, mae: 2.386095, mean_q: 2.906443, mean_eps: 0.977383
  63756/500000: episode: 96, duration: 23.443s, episode steps: 520, steps per second:  22, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.010614, mae: 2.363179, mean_q: 2.879545, mean_eps: 0.977142
  64494/500000: episode: 97, duration: 30.879s, episode steps: 738, steps per second:  24, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.010223, mae: 2.362406, mean_q: 2.878313, mean_eps: 0.976915
  65126/500000: episode: 98, duration: 26.082s, episode steps: 632, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.010120, mae: 2.372477, mean_q: 2.890888, mean_eps: 0.976668
  65818/500000: episode: 99, duration: 29.460s, episode steps: 692, steps per second:  23, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.009991, mae: 2.373106, mean_q: 2.892104, mean_eps: 0.976430
  66513/500000: episode: 100, duration: 29.725s, episode steps: 695, steps per second:  23, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012299, mae: 2.375571, mean_q: 2.893167, mean_eps: 0.976180
  67208/500000: episode: 101, duration: 29.719s, episode steps: 695, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.009095, mae: 2.388887, mean_q: 2.908670, mean_eps: 0.975930
  67851/500000: episode: 102, duration: 27.392s, episode steps: 643, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.010496, mae: 2.351739, mean_q: 2.863166, mean_eps: 0.975690
  68497/500000: episode: 103, duration: 27.045s, episode steps: 646, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.010605, mae: 2.341693, mean_q: 2.853764, mean_eps: 0.975457
  69172/500000: episode: 104, duration: 28.661s, episode steps: 675, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010767, mae: 2.364092, mean_q: 2.880443, mean_eps: 0.975220
  69812/500000: episode: 105, duration: 28.215s, episode steps: 640, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.010440, mae: 2.353385, mean_q: 2.868495, mean_eps: 0.974984
  70677/500000: episode: 106, duration: 35.811s, episode steps: 865, steps per second:  24, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.012245, mae: 2.365450, mean_q: 2.881200, mean_eps: 0.974712
  71389/500000: episode: 107, duration: 31.090s, episode steps: 712, steps per second:  23, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.010204, mae: 2.364572, mean_q: 2.883142, mean_eps: 0.974428
  72098/500000: episode: 108, duration: 34.572s, episode steps: 709, steps per second:  21, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.009390, mae: 2.356229, mean_q: 2.872237, mean_eps: 0.974172
  72556/500000: episode: 109, duration: 19.496s, episode steps: 458, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.009992, mae: 2.389623, mean_q: 2.910641, mean_eps: 0.973963
  73019/500000: episode: 110, duration: 19.662s, episode steps: 463, steps per second:  24, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.010085, mae: 2.369476, mean_q: 2.887336, mean_eps: 0.973797
  73830/500000: episode: 111, duration: 33.365s, episode steps: 811, steps per second:  24, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.011299, mae: 2.371833, mean_q: 2.888248, mean_eps: 0.973567
  74476/500000: episode: 112, duration: 27.606s, episode steps: 646, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012491, mae: 2.355692, mean_q: 2.868433, mean_eps: 0.973305
  75355/500000: episode: 113, duration: 37.609s, episode steps: 879, steps per second:  23, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.008847, mae: 2.356253, mean_q: 2.870939, mean_eps: 0.973031
  76224/500000: episode: 114, duration: 37.733s, episode steps: 869, steps per second:  23, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.011261, mae: 2.381540, mean_q: 2.900542, mean_eps: 0.972716
  77172/500000: episode: 115, duration: 43.402s, episode steps: 948, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.010669, mae: 2.368670, mean_q: 2.886125, mean_eps: 0.972389
  77927/500000: episode: 116, duration: 33.770s, episode steps: 755, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011355, mae: 2.360022, mean_q: 2.873990, mean_eps: 0.972083
  78359/500000: episode: 117, duration: 18.642s, episode steps: 432, steps per second:  23, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.012199, mae: 2.359882, mean_q: 2.876374, mean_eps: 0.971869
  78841/500000: episode: 118, duration: 22.018s, episode steps: 482, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.011590, mae: 2.374559, mean_q: 2.891131, mean_eps: 0.971704
  79481/500000: episode: 119, duration: 28.261s, episode steps: 640, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.011276, mae: 2.361715, mean_q: 2.876242, mean_eps: 0.971502
  80447/500000: episode: 120, duration: 42.557s, episode steps: 966, steps per second:  23, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.010036, mae: 2.362518, mean_q: 2.877967, mean_eps: 0.971213
  80994/500000: episode: 121, duration: 23.449s, episode steps: 547, steps per second:  23, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010982, mae: 2.348391, mean_q: 2.860962, mean_eps: 0.970941
  81552/500000: episode: 122, duration: 24.386s, episode steps: 558, steps per second:  23, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.010847, mae: 2.349453, mean_q: 2.861664, mean_eps: 0.970742
  82348/500000: episode: 123, duration: 33.891s, episode steps: 796, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.011255, mae: 2.375703, mean_q: 2.895624, mean_eps: 0.970499
  83307/500000: episode: 124, duration: 41.547s, episode steps: 959, steps per second:  23, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.010829, mae: 2.371279, mean_q: 2.888307, mean_eps: 0.970183
  83694/500000: episode: 125, duration: 16.899s, episode steps: 387, steps per second:  23, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012294, mae: 2.351032, mean_q: 2.860600, mean_eps: 0.969940
  84270/500000: episode: 126, duration: 23.823s, episode steps: 576, steps per second:  24, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.009643, mae: 2.349846, mean_q: 2.859452, mean_eps: 0.969766
  84630/500000: episode: 127, duration: 15.099s, episode steps: 360, steps per second:  24, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.010001, mae: 2.372290, mean_q: 2.890578, mean_eps: 0.969598
  85116/500000: episode: 128, duration: 20.444s, episode steps: 486, steps per second:  24, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.009342, mae: 2.352884, mean_q: 2.864603, mean_eps: 0.969446
  85816/500000: episode: 129, duration: 29.439s, episode steps: 700, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.011488, mae: 2.346465, mean_q: 2.857204, mean_eps: 0.969233
  86319/500000: episode: 130, duration: 21.646s, episode steps: 503, steps per second:  23, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.009331, mae: 2.351246, mean_q: 2.863688, mean_eps: 0.969016
  87070/500000: episode: 131, duration: 31.663s, episode steps: 751, steps per second:  24, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.010745, mae: 2.352070, mean_q: 2.862938, mean_eps: 0.968790
  87651/500000: episode: 132, duration: 32.750s, episode steps: 581, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011794, mae: 2.350282, mean_q: 2.860594, mean_eps: 0.968550
  88240/500000: episode: 133, duration: 29.986s, episode steps: 589, steps per second:  20, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.010663, mae: 2.353954, mean_q: 2.868435, mean_eps: 0.968340
  88978/500000: episode: 134, duration: 37.966s, episode steps: 738, steps per second:  19, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011677, mae: 2.347420, mean_q: 2.859036, mean_eps: 0.968101
  89893/500000: episode: 135, duration: 45.970s, episode steps: 915, steps per second:  20, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.011314, mae: 2.351935, mean_q: 2.862510, mean_eps: 0.967803
  90433/500000: episode: 136, duration: 27.027s, episode steps: 540, steps per second:  20, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.010477, mae: 2.329120, mean_q: 2.836999, mean_eps: 0.967541
  91049/500000: episode: 137, duration: 31.568s, episode steps: 616, steps per second:  20, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010566, mae: 2.345232, mean_q: 2.856105, mean_eps: 0.967333
  91686/500000: episode: 138, duration: 31.834s, episode steps: 637, steps per second:  20, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.010774, mae: 2.339020, mean_q: 2.846868, mean_eps: 0.967108
  92070/500000: episode: 139, duration: 19.736s, episode steps: 384, steps per second:  19, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.014330, mae: 2.353538, mean_q: 2.862699, mean_eps: 0.966924
  92514/500000: episode: 140, duration: 22.347s, episode steps: 444, steps per second:  20, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.009397, mae: 2.331967, mean_q: 2.838793, mean_eps: 0.966775
  93158/500000: episode: 141, duration: 32.330s, episode steps: 644, steps per second:  20, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.013022, mae: 2.363815, mean_q: 2.876865, mean_eps: 0.966579
  94105/500000: episode: 142, duration: 43.001s, episode steps: 947, steps per second:  22, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.011406, mae: 2.347607, mean_q: 2.857971, mean_eps: 0.966292
  94912/500000: episode: 143, duration: 36.407s, episode steps: 807, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.011102, mae: 2.363344, mean_q: 2.878097, mean_eps: 0.965977
  95696/500000: episode: 144, duration: 50.222s, episode steps: 784, steps per second:  16, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.010872, mae: 2.352768, mean_q: 2.864193, mean_eps: 0.965691
  96219/500000: episode: 145, duration: 26.607s, episode steps: 523, steps per second:  20, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.010727, mae: 2.365644, mean_q: 2.879692, mean_eps: 0.965456
  96919/500000: episode: 146, duration: 39.567s, episode steps: 700, steps per second:  18, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.011566, mae: 2.348855, mean_q: 2.858249, mean_eps: 0.965236
  97663/500000: episode: 147, duration: 40.603s, episode steps: 744, steps per second:  18, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.011592, mae: 2.353459, mean_q: 2.866493, mean_eps: 0.964976
  98286/500000: episode: 148, duration: 33.443s, episode steps: 623, steps per second:  19, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.010182, mae: 2.343588, mean_q: 2.852588, mean_eps: 0.964729
  98681/500000: episode: 149, duration: 20.834s, episode steps: 395, steps per second:  19, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012036, mae: 2.342429, mean_q: 2.848836, mean_eps: 0.964546
  99444/500000: episode: 150, duration: 39.389s, episode steps: 763, steps per second:  19, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.011659, mae: 2.348421, mean_q: 2.859933, mean_eps: 0.964338
 100200/500000: episode: 151, duration: 33.325s, episode steps: 756, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.010723, mae: 2.347611, mean_q: 2.859168, mean_eps: 0.964065
 100935/500000: episode: 152, duration: 31.933s, episode steps: 735, steps per second:  23, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.013681, mae: 2.373760, mean_q: 2.887499, mean_eps: 0.963796
 101594/500000: episode: 153, duration: 28.841s, episode steps: 659, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012163, mae: 2.350924, mean_q: 2.862849, mean_eps: 0.963545
 102352/500000: episode: 154, duration: 32.561s, episode steps: 758, steps per second:  23, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.011395, mae: 2.348799, mean_q: 2.859710, mean_eps: 0.963290
 102879/500000: episode: 155, duration: 22.385s, episode steps: 527, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.009916, mae: 2.343418, mean_q: 2.853885, mean_eps: 0.963059
 103955/500000: episode: 156, duration: 46.175s, episode steps: 1076, steps per second:  23, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012725, mae: 2.358802, mean_q: 2.872815, mean_eps: 0.962770
 104818/500000: episode: 157, duration: 36.497s, episode steps: 863, steps per second:  24, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.011710, mae: 2.379495, mean_q: 2.896712, mean_eps: 0.962421
 105845/500000: episode: 158, duration: 52.038s, episode steps: 1027, steps per second:  20, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.013201, mae: 2.337306, mean_q: 2.843851, mean_eps: 0.962080
 106242/500000: episode: 159, duration: 25.541s, episode steps: 397, steps per second:  16, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010618, mae: 2.355475, mean_q: 2.866994, mean_eps: 0.961824
 106882/500000: episode: 160, duration: 44.366s, episode steps: 640, steps per second:  14, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010362, mae: 2.365335, mean_q: 2.880064, mean_eps: 0.961638
 107503/500000: episode: 161, duration: 31.471s, episode steps: 621, steps per second:  20, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.010504, mae: 2.337897, mean_q: 2.847348, mean_eps: 0.961411
 108255/500000: episode: 162, duration: 38.311s, episode steps: 752, steps per second:  20, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.012142, mae: 2.340133, mean_q: 2.847574, mean_eps: 0.961164
 109027/500000: episode: 163, duration: 37.247s, episode steps: 772, steps per second:  21, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.012201, mae: 2.349541, mean_q: 2.860415, mean_eps: 0.960890
 109453/500000: episode: 164, duration: 32.292s, episode steps: 426, steps per second:  13, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012430, mae: 2.342870, mean_q: 2.851456, mean_eps: 0.960674
 109941/500000: episode: 165, duration: 34.844s, episode steps: 488, steps per second:  14, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.010230, mae: 2.325685, mean_q: 2.829560, mean_eps: 0.960509
 110455/500000: episode: 166, duration: 25.512s, episode steps: 514, steps per second:  20, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.010938, mae: 2.354328, mean_q: 2.865624, mean_eps: 0.960329
 111299/500000: episode: 167, duration: 41.211s, episode steps: 844, steps per second:  20, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012479, mae: 2.338583, mean_q: 2.844839, mean_eps: 0.960085
 112195/500000: episode: 168, duration: 49.107s, episode steps: 896, steps per second:  18, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011991, mae: 2.345110, mean_q: 2.853555, mean_eps: 0.959771
 112753/500000: episode: 169, duration: 24.077s, episode steps: 558, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.009613, mae: 2.352129, mean_q: 2.865689, mean_eps: 0.959509
 113799/500000: episode: 170, duration: 47.831s, episode steps: 1046, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011883, mae: 2.344097, mean_q: 2.852452, mean_eps: 0.959221
 114447/500000: episode: 171, duration: 27.008s, episode steps: 648, steps per second:  24, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.011063, mae: 2.360239, mean_q: 2.871550, mean_eps: 0.958916
 114932/500000: episode: 172, duration: 22.188s, episode steps: 485, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.012147, mae: 2.390683, mean_q: 2.911913, mean_eps: 0.958712
 115484/500000: episode: 173, duration: 25.929s, episode steps: 552, steps per second:  21, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.010061, mae: 2.331422, mean_q: 2.836921, mean_eps: 0.958526
 116044/500000: episode: 174, duration: 27.330s, episode steps: 560, steps per second:  20, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.011176, mae: 2.345175, mean_q: 2.853727, mean_eps: 0.958326
 116685/500000: episode: 175, duration: 31.076s, episode steps: 641, steps per second:  21, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.010251, mae: 2.352478, mean_q: 2.863304, mean_eps: 0.958109
 117201/500000: episode: 176, duration: 23.685s, episode steps: 516, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.011567, mae: 2.370175, mean_q: 2.881970, mean_eps: 0.957900
 117685/500000: episode: 177, duration: 22.766s, episode steps: 484, steps per second:  21, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.010419, mae: 2.361431, mean_q: 2.874110, mean_eps: 0.957720
 118076/500000: episode: 178, duration: 20.438s, episode steps: 391, steps per second:  19, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.011768, mae: 2.363102, mean_q: 2.877180, mean_eps: 0.957563
 118631/500000: episode: 179, duration: 26.912s, episode steps: 555, steps per second:  21, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.011812, mae: 2.340931, mean_q: 2.851819, mean_eps: 0.957393
 119010/500000: episode: 180, duration: 18.878s, episode steps: 379, steps per second:  20, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.013844, mae: 2.356371, mean_q: 2.866163, mean_eps: 0.957225
 119784/500000: episode: 181, duration: 72.281s, episode steps: 774, steps per second:  11, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.012231, mae: 2.344593, mean_q: 2.854682, mean_eps: 0.957017
 120258/500000: episode: 182, duration: 73.499s, episode steps: 474, steps per second:   6, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010623, mae: 2.348558, mean_q: 2.859498, mean_eps: 0.956793
 121341/500000: episode: 183, duration: 105.867s, episode steps: 1083, steps per second:  10, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012377, mae: 2.344878, mean_q: 2.852681, mean_eps: 0.956512
 121857/500000: episode: 184, duration: 39.211s, episode steps: 516, steps per second:  13, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010516, mae: 2.366593, mean_q: 2.880661, mean_eps: 0.956224
 122389/500000: episode: 185, duration: 72.116s, episode steps: 532, steps per second:   7, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.012603, mae: 2.378008, mean_q: 2.894424, mean_eps: 0.956035
 123028/500000: episode: 186, duration: 38.185s, episode steps: 639, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.011444, mae: 2.353868, mean_q: 2.865937, mean_eps: 0.955825
 123589/500000: episode: 187, duration: 30.001s, episode steps: 561, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.009753, mae: 2.359747, mean_q: 2.874713, mean_eps: 0.955609
 124139/500000: episode: 188, duration: 29.653s, episode steps: 550, steps per second:  19, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.010670, mae: 2.380854, mean_q: 2.899901, mean_eps: 0.955409
 125120/500000: episode: 189, duration: 61.241s, episode steps: 981, steps per second:  16, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.011944, mae: 2.346053, mean_q: 2.857161, mean_eps: 0.955134
 126100/500000: episode: 190, duration: 43.224s, episode steps: 980, steps per second:  23, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.011161, mae: 2.365098, mean_q: 2.877425, mean_eps: 0.954781
 127285/500000: episode: 191, duration: 51.618s, episode steps: 1185, steps per second:  23, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.009637, mae: 2.350094, mean_q: 2.859425, mean_eps: 0.954391
 127745/500000: episode: 192, duration: 22.408s, episode steps: 460, steps per second:  21, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.010536, mae: 2.329615, mean_q: 2.831870, mean_eps: 0.954094
 128280/500000: episode: 193, duration: 21.830s, episode steps: 535, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.010766, mae: 2.349741, mean_q: 2.859064, mean_eps: 0.953916
 129127/500000: episode: 194, duration: 37.753s, episode steps: 847, steps per second:  22, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012197, mae: 2.340040, mean_q: 2.846585, mean_eps: 0.953667
 129934/500000: episode: 195, duration: 33.768s, episode steps: 807, steps per second:  24, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.011156, mae: 2.353364, mean_q: 2.864389, mean_eps: 0.953369
 130669/500000: episode: 196, duration: 33.267s, episode steps: 735, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.010543, mae: 2.346063, mean_q: 2.856242, mean_eps: 0.953091
 131266/500000: episode: 197, duration: 25.252s, episode steps: 597, steps per second:  24, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.010184, mae: 2.342772, mean_q: 2.849986, mean_eps: 0.952852
 132646/500000: episode: 198, duration: 62.798s, episode steps: 1380, steps per second:  22, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011823, mae: 2.344385, mean_q: 2.853213, mean_eps: 0.952496
 133425/500000: episode: 199, duration: 34.773s, episode steps: 779, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.012239, mae: 2.351605, mean_q: 2.861619, mean_eps: 0.952107
 134137/500000: episode: 200, duration: 30.049s, episode steps: 712, steps per second:  24, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.010970, mae: 2.368827, mean_q: 2.883311, mean_eps: 0.951838
 135284/500000: episode: 201, duration: 49.694s, episode steps: 1147, steps per second:  23, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010999, mae: 2.366430, mean_q: 2.877357, mean_eps: 0.951504
 135917/500000: episode: 202, duration: 27.548s, episode steps: 633, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.010695, mae: 2.365722, mean_q: 2.877633, mean_eps: 0.951184
 136887/500000: episode: 203, duration: 42.556s, episode steps: 970, steps per second:  23, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.011664, mae: 2.356665, mean_q: 2.866496, mean_eps: 0.950895
 137434/500000: episode: 204, duration: 22.898s, episode steps: 547, steps per second:  24, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.012565, mae: 2.357523, mean_q: 2.868533, mean_eps: 0.950622
 138120/500000: episode: 205, duration: 30.237s, episode steps: 686, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.011968, mae: 2.365244, mean_q: 2.877805, mean_eps: 0.950401
 138702/500000: episode: 206, duration: 24.880s, episode steps: 582, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.013057, mae: 2.349965, mean_q: 2.858737, mean_eps: 0.950172
 139372/500000: episode: 207, duration: 29.244s, episode steps: 670, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.010302, mae: 2.360555, mean_q: 2.873925, mean_eps: 0.949947
 140004/500000: episode: 208, duration: 27.540s, episode steps: 632, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.009708, mae: 2.344657, mean_q: 2.851713, mean_eps: 0.949713
 140909/500000: episode: 209, duration: 40.201s, episode steps: 905, steps per second:  23, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.011391, mae: 2.371278, mean_q: 2.886797, mean_eps: 0.949436
 141645/500000: episode: 210, duration: 31.370s, episode steps: 736, steps per second:  23, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.012657, mae: 2.370744, mean_q: 2.883852, mean_eps: 0.949140
 142250/500000: episode: 211, duration: 26.492s, episode steps: 605, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.013784, mae: 2.356728, mean_q: 2.866955, mean_eps: 0.948899
 142751/500000: episode: 212, duration: 22.520s, episode steps: 501, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.009609, mae: 2.359092, mean_q: 2.871254, mean_eps: 0.948700
 143541/500000: episode: 213, duration: 33.375s, episode steps: 790, steps per second:  24, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.010795, mae: 2.367275, mean_q: 2.881060, mean_eps: 0.948467
 144132/500000: episode: 214, duration: 25.858s, episode steps: 591, steps per second:  23, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011298, mae: 2.352634, mean_q: 2.860057, mean_eps: 0.948219
 145114/500000: episode: 215, duration: 42.612s, episode steps: 982, steps per second:  23, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.010176, mae: 2.348810, mean_q: 2.856841, mean_eps: 0.947936
 145654/500000: episode: 216, duration: 24.603s, episode steps: 540, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011208, mae: 2.340322, mean_q: 2.846601, mean_eps: 0.947662
 146258/500000: episode: 217, duration: 24.748s, episode steps: 604, steps per second:  24, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.012881, mae: 2.348585, mean_q: 2.853452, mean_eps: 0.947456
 146732/500000: episode: 218, duration: 21.350s, episode steps: 474, steps per second:  22, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.013163, mae: 2.336521, mean_q: 2.840147, mean_eps: 0.947262
 147264/500000: episode: 219, duration: 23.627s, episode steps: 532, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.010739, mae: 2.359783, mean_q: 2.868122, mean_eps: 0.947081
 147773/500000: episode: 220, duration: 21.366s, episode steps: 509, steps per second:  24, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011294, mae: 2.380594, mean_q: 2.894750, mean_eps: 0.946894
 148573/500000: episode: 221, duration: 36.348s, episode steps: 800, steps per second:  22, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.010056, mae: 2.329193, mean_q: 2.832589, mean_eps: 0.946657
 149148/500000: episode: 222, duration: 24.065s, episode steps: 575, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.010373, mae: 2.363197, mean_q: 2.873714, mean_eps: 0.946410
 149753/500000: episode: 223, duration: 27.149s, episode steps: 605, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011443, mae: 2.352316, mean_q: 2.863164, mean_eps: 0.946198
 150311/500000: episode: 224, duration: 24.239s, episode steps: 558, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.011491, mae: 2.362446, mean_q: 2.873340, mean_eps: 0.945988
 150971/500000: episode: 225, duration: 28.656s, episode steps: 660, steps per second:  23, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011364, mae: 2.374298, mean_q: 2.885892, mean_eps: 0.945770
 151441/500000: episode: 226, duration: 20.240s, episode steps: 470, steps per second:  23, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012620, mae: 2.407663, mean_q: 2.930292, mean_eps: 0.945566
 152140/500000: episode: 227, duration: 30.203s, episode steps: 699, steps per second:  23, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.013191, mae: 2.367997, mean_q: 2.881133, mean_eps: 0.945356
 152724/500000: episode: 228, duration: 26.420s, episode steps: 584, steps per second:  22, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.013052, mae: 2.394953, mean_q: 2.910986, mean_eps: 0.945125
 153723/500000: episode: 229, duration: 43.261s, episode steps: 999, steps per second:  23, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.012175, mae: 2.370374, mean_q: 2.882941, mean_eps: 0.944840
 155010/500000: episode: 230, duration: 56.165s, episode steps: 1287, steps per second:  23, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.012956, mae: 2.385042, mean_q: 2.899726, mean_eps: 0.944428
 155505/500000: episode: 231, duration: 21.080s, episode steps: 495, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.012641, mae: 2.373428, mean_q: 2.884759, mean_eps: 0.944107
 156468/500000: episode: 232, duration: 42.588s, episode steps: 963, steps per second:  23, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.013060, mae: 2.387478, mean_q: 2.902230, mean_eps: 0.943845
 157248/500000: episode: 233, duration: 33.688s, episode steps: 780, steps per second:  23, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013003, mae: 2.363877, mean_q: 2.874175, mean_eps: 0.943532
 157997/500000: episode: 234, duration: 32.503s, episode steps: 749, steps per second:  23, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.012927, mae: 2.362367, mean_q: 2.871594, mean_eps: 0.943256
 158850/500000: episode: 235, duration: 37.822s, episode steps: 853, steps per second:  23, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012085, mae: 2.394425, mean_q: 2.910894, mean_eps: 0.942967
 159775/500000: episode: 236, duration: 39.717s, episode steps: 925, steps per second:  23, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.012477, mae: 2.379204, mean_q: 2.892316, mean_eps: 0.942648
 160224/500000: episode: 237, duration: 19.723s, episode steps: 449, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012245, mae: 2.406490, mean_q: 2.924200, mean_eps: 0.942401
 160646/500000: episode: 238, duration: 22.715s, episode steps: 422, steps per second:  19, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.010952, mae: 2.389438, mean_q: 2.904162, mean_eps: 0.942244
 161821/500000: episode: 239, duration: 50.273s, episode steps: 1175, steps per second:  23, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012071, mae: 2.393360, mean_q: 2.908776, mean_eps: 0.941956
 163004/500000: episode: 240, duration: 50.092s, episode steps: 1183, steps per second:  24, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.012822, mae: 2.384058, mean_q: 2.898255, mean_eps: 0.941532
 163862/500000: episode: 241, duration: 37.781s, episode steps: 858, steps per second:  23, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.011682, mae: 2.380116, mean_q: 2.894331, mean_eps: 0.941164
 164256/500000: episode: 242, duration: 16.412s, episode steps: 394, steps per second:  24, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014210, mae: 2.397612, mean_q: 2.914016, mean_eps: 0.940939
 165363/500000: episode: 243, duration: 48.579s, episode steps: 1107, steps per second:  23, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012648, mae: 2.382455, mean_q: 2.897329, mean_eps: 0.940669
 166062/500000: episode: 244, duration: 29.684s, episode steps: 699, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.012610, mae: 2.389872, mean_q: 2.907616, mean_eps: 0.940344
 166786/500000: episode: 245, duration: 34.441s, episode steps: 724, steps per second:  21, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.011029, mae: 2.378962, mean_q: 2.892331, mean_eps: 0.940087
 167404/500000: episode: 246, duration: 27.945s, episode steps: 618, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.013694, mae: 2.376218, mean_q: 2.889637, mean_eps: 0.939846
 168669/500000: episode: 247, duration: 56.509s, episode steps: 1265, steps per second:  22, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012149, mae: 2.377534, mean_q: 2.890414, mean_eps: 0.939507
 169172/500000: episode: 248, duration: 21.856s, episode steps: 503, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010834, mae: 2.398917, mean_q: 2.917022, mean_eps: 0.939189
 169815/500000: episode: 249, duration: 28.608s, episode steps: 643, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013072, mae: 2.354617, mean_q: 2.861394, mean_eps: 0.938983
 170525/500000: episode: 250, duration: 31.392s, episode steps: 710, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.012340, mae: 2.397392, mean_q: 2.916226, mean_eps: 0.938739
 170912/500000: episode: 251, duration: 16.925s, episode steps: 387, steps per second:  23, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.011853, mae: 2.377326, mean_q: 2.891410, mean_eps: 0.938542
 171568/500000: episode: 252, duration: 29.447s, episode steps: 656, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.013579, mae: 2.383652, mean_q: 2.895850, mean_eps: 0.938354
 172313/500000: episode: 253, duration: 33.041s, episode steps: 745, steps per second:  23, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.011464, mae: 2.360022, mean_q: 2.870759, mean_eps: 0.938102
 173046/500000: episode: 254, duration: 32.236s, episode steps: 733, steps per second:  23, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.012720, mae: 2.357896, mean_q: 2.866018, mean_eps: 0.937835
 173643/500000: episode: 255, duration: 25.842s, episode steps: 597, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.011502, mae: 2.397872, mean_q: 2.914963, mean_eps: 0.937596
 174432/500000: episode: 256, duration: 35.817s, episode steps: 789, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.013925, mae: 2.388301, mean_q: 2.901995, mean_eps: 0.937347
 175082/500000: episode: 257, duration: 27.983s, episode steps: 650, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.010959, mae: 2.367832, mean_q: 2.877860, mean_eps: 0.937088
 175911/500000: episode: 258, duration: 36.836s, episode steps: 829, steps per second:  23, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011416, mae: 2.354618, mean_q: 2.862234, mean_eps: 0.936821
 176710/500000: episode: 259, duration: 34.878s, episode steps: 799, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.013072, mae: 2.363839, mean_q: 2.873056, mean_eps: 0.936528
 177271/500000: episode: 260, duration: 24.161s, episode steps: 561, steps per second:  23, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.012588, mae: 2.395146, mean_q: 2.912160, mean_eps: 0.936284
 177782/500000: episode: 261, duration: 22.447s, episode steps: 511, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.012172, mae: 2.388065, mean_q: 2.901879, mean_eps: 0.936091
 178472/500000: episode: 262, duration: 31.172s, episode steps: 690, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.012600, mae: 2.396098, mean_q: 2.913236, mean_eps: 0.935875
 179433/500000: episode: 263, duration: 41.057s, episode steps: 961, steps per second:  23, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.012780, mae: 2.355227, mean_q: 2.861425, mean_eps: 0.935577
 180052/500000: episode: 264, duration: 28.300s, episode steps: 619, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014942, mae: 2.396998, mean_q: 2.912257, mean_eps: 0.935293
 181185/500000: episode: 265, duration: 52.833s, episode steps: 1133, steps per second:  21, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.011286, mae: 2.371314, mean_q: 2.882808, mean_eps: 0.934978
 181700/500000: episode: 266, duration: 23.100s, episode steps: 515, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.010831, mae: 2.397469, mean_q: 2.917568, mean_eps: 0.934681
 182509/500000: episode: 267, duration: 35.209s, episode steps: 809, steps per second:  23, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.010076, mae: 2.369785, mean_q: 2.879999, mean_eps: 0.934443
 183339/500000: episode: 268, duration: 35.638s, episode steps: 830, steps per second:  23, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012109, mae: 2.375704, mean_q: 2.885960, mean_eps: 0.934147
 184634/500000: episode: 269, duration: 57.318s, episode steps: 1295, steps per second:  23, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012147, mae: 2.378119, mean_q: 2.889667, mean_eps: 0.933765
 185191/500000: episode: 270, duration: 25.421s, episode steps: 557, steps per second:  22, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011760, mae: 2.370529, mean_q: 2.882205, mean_eps: 0.933432
 185725/500000: episode: 271, duration: 23.340s, episode steps: 534, steps per second:  23, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.011977, mae: 2.376438, mean_q: 2.887861, mean_eps: 0.933235
 186376/500000: episode: 272, duration: 28.558s, episode steps: 651, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011940, mae: 2.367743, mean_q: 2.878565, mean_eps: 0.933022
 187544/500000: episode: 273, duration: 50.971s, episode steps: 1168, steps per second:  23, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012008, mae: 2.383384, mean_q: 2.897726, mean_eps: 0.932695
 188164/500000: episode: 274, duration: 26.816s, episode steps: 620, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012768, mae: 2.375460, mean_q: 2.886487, mean_eps: 0.932373
 188674/500000: episode: 275, duration: 22.983s, episode steps: 510, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.010280, mae: 2.357854, mean_q: 2.868024, mean_eps: 0.932170
 189175/500000: episode: 276, duration: 22.395s, episode steps: 501, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.013247, mae: 2.390484, mean_q: 2.905381, mean_eps: 0.931987
 189786/500000: episode: 277, duration: 28.778s, episode steps: 611, steps per second:  21, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.011101, mae: 2.352103, mean_q: 2.858767, mean_eps: 0.931787
 190630/500000: episode: 278, duration: 38.790s, episode steps: 844, steps per second:  22, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.011151, mae: 2.369923, mean_q: 2.881144, mean_eps: 0.931525
 191061/500000: episode: 279, duration: 18.684s, episode steps: 431, steps per second:  23, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.014559, mae: 2.373709, mean_q: 2.886200, mean_eps: 0.931295
 191448/500000: episode: 280, duration: 17.486s, episode steps: 387, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.012706, mae: 2.387743, mean_q: 2.902485, mean_eps: 0.931149
 192103/500000: episode: 281, duration: 29.164s, episode steps: 655, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.013762, mae: 2.386322, mean_q: 2.899832, mean_eps: 0.930961
 193425/500000: episode: 282, duration: 58.107s, episode steps: 1322, steps per second:  23, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012044, mae: 2.381037, mean_q: 2.893958, mean_eps: 0.930605
 194376/500000: episode: 283, duration: 42.798s, episode steps: 951, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.012546, mae: 2.374574, mean_q: 2.885906, mean_eps: 0.930196
 194797/500000: episode: 284, duration: 18.733s, episode steps: 421, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011626, mae: 2.384715, mean_q: 2.898326, mean_eps: 0.929949
 195182/500000: episode: 285, duration: 16.813s, episode steps: 385, steps per second:  23, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.010086, mae: 2.403631, mean_q: 2.918765, mean_eps: 0.929804
 195710/500000: episode: 286, duration: 24.248s, episode steps: 528, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.012617, mae: 2.392890, mean_q: 2.907446, mean_eps: 0.929639
 196525/500000: episode: 287, duration: 35.725s, episode steps: 815, steps per second:  23, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.012038, mae: 2.371756, mean_q: 2.882662, mean_eps: 0.929398
 197154/500000: episode: 288, duration: 28.111s, episode steps: 629, steps per second:  22, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.014246, mae: 2.369513, mean_q: 2.878425, mean_eps: 0.929138
 198145/500000: episode: 289, duration: 43.420s, episode steps: 991, steps per second:  23, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.012472, mae: 2.380415, mean_q: 2.892575, mean_eps: 0.928846
 199115/500000: episode: 290, duration: 43.074s, episode steps: 970, steps per second:  23, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.011447, mae: 2.360266, mean_q: 2.867104, mean_eps: 0.928493
 199713/500000: episode: 291, duration: 26.640s, episode steps: 598, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.012459, mae: 2.372383, mean_q: 2.884361, mean_eps: 0.928211
 200203/500000: episode: 292, duration: 22.118s, episode steps: 490, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.013172, mae: 2.371215, mean_q: 2.883821, mean_eps: 0.928015
 200584/500000: episode: 293, duration: 16.888s, episode steps: 381, steps per second:  23, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012689, mae: 2.396676, mean_q: 2.912543, mean_eps: 0.927859
 201399/500000: episode: 294, duration: 36.461s, episode steps: 815, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.011635, mae: 2.397254, mean_q: 2.913964, mean_eps: 0.927644
 202163/500000: episode: 295, duration: 35.149s, episode steps: 764, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.012255, mae: 2.409083, mean_q: 2.928689, mean_eps: 0.927359
 202674/500000: episode: 296, duration: 21.815s, episode steps: 511, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.012718, mae: 2.394775, mean_q: 2.909440, mean_eps: 0.927130
 203219/500000: episode: 297, duration: 24.972s, episode steps: 545, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.012181, mae: 2.407370, mean_q: 2.923275, mean_eps: 0.926939
 203836/500000: episode: 298, duration: 26.481s, episode steps: 617, steps per second:  23, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.013166, mae: 2.387483, mean_q: 2.897103, mean_eps: 0.926731
 204418/500000: episode: 299, duration: 25.769s, episode steps: 582, steps per second:  23, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.012099, mae: 2.402616, mean_q: 2.919606, mean_eps: 0.926515
 204910/500000: episode: 300, duration: 21.497s, episode steps: 492, steps per second:  23, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.015282, mae: 2.398882, mean_q: 2.915674, mean_eps: 0.926321
 205549/500000: episode: 301, duration: 28.487s, episode steps: 639, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.015518, mae: 2.396506, mean_q: 2.910768, mean_eps: 0.926117
 206141/500000: episode: 302, duration: 26.875s, episode steps: 592, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.013002, mae: 2.371462, mean_q: 2.881304, mean_eps: 0.925895
 206940/500000: episode: 303, duration: 34.846s, episode steps: 799, steps per second:  23, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012491, mae: 2.387823, mean_q: 2.902001, mean_eps: 0.925646
 207784/500000: episode: 304, duration: 37.139s, episode steps: 844, steps per second:  23, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012069, mae: 2.407189, mean_q: 2.925218, mean_eps: 0.925350
 208173/500000: episode: 305, duration: 17.482s, episode steps: 389, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.011981, mae: 2.382976, mean_q: 2.895193, mean_eps: 0.925128
 209272/500000: episode: 306, duration: 48.896s, episode steps: 1099, steps per second:  22, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011589, mae: 2.397712, mean_q: 2.914409, mean_eps: 0.924860
 209818/500000: episode: 307, duration: 25.071s, episode steps: 546, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013099, mae: 2.403174, mean_q: 2.919340, mean_eps: 0.924564
 210170/500000: episode: 308, duration: 15.072s, episode steps: 352, steps per second:  23, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015622, mae: 2.375359, mean_q: 2.886808, mean_eps: 0.924402
 210866/500000: episode: 309, duration: 31.360s, episode steps: 696, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011482, mae: 2.409029, mean_q: 2.927756, mean_eps: 0.924214
 211407/500000: episode: 310, duration: 23.217s, episode steps: 541, steps per second:  23, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.013064, mae: 2.367392, mean_q: 2.876333, mean_eps: 0.923991
 212070/500000: episode: 311, duration: 30.374s, episode steps: 663, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012072, mae: 2.376357, mean_q: 2.888765, mean_eps: 0.923774
 212553/500000: episode: 312, duration: 22.195s, episode steps: 483, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.012339, mae: 2.397576, mean_q: 2.912425, mean_eps: 0.923568
 213120/500000: episode: 313, duration: 25.395s, episode steps: 567, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.012830, mae: 2.400005, mean_q: 2.915119, mean_eps: 0.923379
 213663/500000: episode: 314, duration: 25.377s, episode steps: 543, steps per second:  21, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.012425, mae: 2.392714, mean_q: 2.906940, mean_eps: 0.923180
 214173/500000: episode: 315, duration: 21.960s, episode steps: 510, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.012906, mae: 2.381685, mean_q: 2.891798, mean_eps: 0.922990
 214638/500000: episode: 316, duration: 19.835s, episode steps: 465, steps per second:  23, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.011162, mae: 2.396796, mean_q: 2.910779, mean_eps: 0.922814
 215540/500000: episode: 317, duration: 41.619s, episode steps: 902, steps per second:  22, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012138, mae: 2.391601, mean_q: 2.905616, mean_eps: 0.922568
 216110/500000: episode: 318, duration: 24.984s, episode steps: 570, steps per second:  23, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.013922, mae: 2.402692, mean_q: 2.919151, mean_eps: 0.922303
 216624/500000: episode: 319, duration: 23.025s, episode steps: 514, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.013855, mae: 2.379443, mean_q: 2.890705, mean_eps: 0.922108
 217317/500000: episode: 320, duration: 30.463s, episode steps: 693, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.012010, mae: 2.385378, mean_q: 2.897098, mean_eps: 0.921891
 217678/500000: episode: 321, duration: 15.830s, episode steps: 361, steps per second:  23, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.012901, mae: 2.382924, mean_q: 2.894204, mean_eps: 0.921701
 218389/500000: episode: 322, duration: 31.994s, episode steps: 711, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.014254, mae: 2.416961, mean_q: 2.937518, mean_eps: 0.921508
 219177/500000: episode: 323, duration: 34.182s, episode steps: 788, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.012969, mae: 2.374883, mean_q: 2.885405, mean_eps: 0.921238
 220306/500000: episode: 324, duration: 50.089s, episode steps: 1129, steps per second:  23, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.012807, mae: 2.407501, mean_q: 2.925118, mean_eps: 0.920893
 220929/500000: episode: 325, duration: 28.531s, episode steps: 623, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011899, mae: 2.398342, mean_q: 2.915473, mean_eps: 0.920578
 221672/500000: episode: 326, duration: 32.704s, episode steps: 743, steps per second:  23, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.012955, mae: 2.409980, mean_q: 2.928313, mean_eps: 0.920332
 222017/500000: episode: 327, duration: 15.106s, episode steps: 345, steps per second:  23, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013825, mae: 2.387923, mean_q: 2.900554, mean_eps: 0.920136
 222730/500000: episode: 328, duration: 32.246s, episode steps: 713, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.012845, mae: 2.393088, mean_q: 2.907323, mean_eps: 0.919945
 223236/500000: episode: 329, duration: 22.024s, episode steps: 506, steps per second:  23, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.012771, mae: 2.402090, mean_q: 2.917871, mean_eps: 0.919726
 224067/500000: episode: 330, duration: 38.256s, episode steps: 831, steps per second:  22, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.012932, mae: 2.367518, mean_q: 2.877775, mean_eps: 0.919486
 224792/500000: episode: 331, duration: 32.502s, episode steps: 725, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.010562, mae: 2.402680, mean_q: 2.921778, mean_eps: 0.919206
 225440/500000: episode: 332, duration: 28.890s, episode steps: 648, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.011053, mae: 2.396965, mean_q: 2.910220, mean_eps: 0.918959
 226073/500000: episode: 333, duration: 28.056s, episode steps: 633, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012100, mae: 2.390735, mean_q: 2.904457, mean_eps: 0.918728
 226996/500000: episode: 334, duration: 41.923s, episode steps: 923, steps per second:  22, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011660, mae: 2.417939, mean_q: 2.936301, mean_eps: 0.918448
 227960/500000: episode: 335, duration: 41.860s, episode steps: 964, steps per second:  23, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.011103, mae: 2.398935, mean_q: 2.915020, mean_eps: 0.918109
 228348/500000: episode: 336, duration: 19.059s, episode steps: 388, steps per second:  20, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.012471, mae: 2.382277, mean_q: 2.893386, mean_eps: 0.917865
 228854/500000: episode: 337, duration: 25.818s, episode steps: 506, steps per second:  20, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.012481, mae: 2.402470, mean_q: 2.918298, mean_eps: 0.917704
 229774/500000: episode: 338, duration: 42.255s, episode steps: 920, steps per second:  22, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.011347, mae: 2.388258, mean_q: 2.902859, mean_eps: 0.917447
 230274/500000: episode: 339, duration: 21.641s, episode steps: 500, steps per second:  23, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.013422, mae: 2.383181, mean_q: 2.894954, mean_eps: 0.917191
 230901/500000: episode: 340, duration: 28.513s, episode steps: 627, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.012624, mae: 2.395221, mean_q: 2.911460, mean_eps: 0.916988
 231516/500000: episode: 341, duration: 28.116s, episode steps: 615, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.010207, mae: 2.382358, mean_q: 2.896336, mean_eps: 0.916765
 232498/500000: episode: 342, duration: 44.035s, episode steps: 982, steps per second:  22, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012749, mae: 2.399407, mean_q: 2.914559, mean_eps: 0.916478
 232977/500000: episode: 343, duration: 21.332s, episode steps: 479, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011816, mae: 2.406652, mean_q: 2.926652, mean_eps: 0.916214
 233591/500000: episode: 344, duration: 26.769s, episode steps: 614, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.012807, mae: 2.397116, mean_q: 2.913600, mean_eps: 0.916018
 234202/500000: episode: 345, duration: 26.919s, episode steps: 611, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011051, mae: 2.379550, mean_q: 2.891307, mean_eps: 0.915797
 234967/500000: episode: 346, duration: 33.286s, episode steps: 765, steps per second:  23, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.013740, mae: 2.384778, mean_q: 2.895329, mean_eps: 0.915550
 235461/500000: episode: 347, duration: 23.511s, episode steps: 494, steps per second:  21, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.012104, mae: 2.394092, mean_q: 2.908035, mean_eps: 0.915323
 236026/500000: episode: 348, duration: 24.291s, episode steps: 565, steps per second:  23, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.011073, mae: 2.375974, mean_q: 2.887162, mean_eps: 0.915132
 236897/500000: episode: 349, duration: 39.475s, episode steps: 871, steps per second:  22, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012204, mae: 2.428002, mean_q: 2.950969, mean_eps: 0.914874
 237568/500000: episode: 350, duration: 30.440s, episode steps: 671, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011426, mae: 2.396220, mean_q: 2.911165, mean_eps: 0.914596
 238194/500000: episode: 351, duration: 26.387s, episode steps: 626, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.011825, mae: 2.379366, mean_q: 2.890784, mean_eps: 0.914363
 238579/500000: episode: 352, duration: 18.507s, episode steps: 385, steps per second:  21, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.013542, mae: 2.428343, mean_q: 2.949749, mean_eps: 0.914181
 238961/500000: episode: 353, duration: 17.432s, episode steps: 382, steps per second:  22, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.012894, mae: 2.378666, mean_q: 2.887442, mean_eps: 0.914043
 239601/500000: episode: 354, duration: 27.213s, episode steps: 640, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012103, mae: 2.387558, mean_q: 2.900283, mean_eps: 0.913858
 240453/500000: episode: 355, duration: 39.642s, episode steps: 852, steps per second:  21, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.012296, mae: 2.398983, mean_q: 2.913881, mean_eps: 0.913590
 241204/500000: episode: 356, duration: 32.812s, episode steps: 751, steps per second:  23, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.012073, mae: 2.394138, mean_q: 2.907786, mean_eps: 0.913302
 241599/500000: episode: 357, duration: 17.929s, episode steps: 395, steps per second:  22, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012099, mae: 2.387911, mean_q: 2.900891, mean_eps: 0.913096
 241965/500000: episode: 358, duration: 16.273s, episode steps: 366, steps per second:  22, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012440, mae: 2.387503, mean_q: 2.899827, mean_eps: 0.912958
 242444/500000: episode: 359, duration: 20.324s, episode steps: 479, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.012998, mae: 2.402845, mean_q: 2.920007, mean_eps: 0.912807
 242825/500000: episode: 360, duration: 18.100s, episode steps: 381, steps per second:  21, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.011290, mae: 2.386100, mean_q: 2.897315, mean_eps: 0.912652
 243555/500000: episode: 361, duration: 31.950s, episode steps: 730, steps per second:  23, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012785, mae: 2.411678, mean_q: 2.929867, mean_eps: 0.912452
 244310/500000: episode: 362, duration: 34.396s, episode steps: 755, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.014048, mae: 2.373229, mean_q: 2.882255, mean_eps: 0.912184
 245170/500000: episode: 363, duration: 37.939s, episode steps: 860, steps per second:  23, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.012195, mae: 2.415359, mean_q: 2.933427, mean_eps: 0.911894
 245928/500000: episode: 364, duration: 34.209s, episode steps: 758, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.012266, mae: 2.401503, mean_q: 2.917557, mean_eps: 0.911603
 246666/500000: episode: 365, duration: 32.448s, episode steps: 738, steps per second:  23, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.012087, mae: 2.388058, mean_q: 2.902389, mean_eps: 0.911333
 247222/500000: episode: 366, duration: 25.534s, episode steps: 556, steps per second:  22, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.015515, mae: 2.391726, mean_q: 2.903935, mean_eps: 0.911100
 247574/500000: episode: 367, duration: 15.385s, episode steps: 352, steps per second:  23, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.012499, mae: 2.367752, mean_q: 2.875884, mean_eps: 0.910937
 248212/500000: episode: 368, duration: 27.693s, episode steps: 638, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.010924, mae: 2.398512, mean_q: 2.913640, mean_eps: 0.910759
 248760/500000: episode: 369, duration: 26.095s, episode steps: 548, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012094, mae: 2.370114, mean_q: 2.879069, mean_eps: 0.910546
 249447/500000: episode: 370, duration: 30.746s, episode steps: 687, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.010074, mae: 2.377865, mean_q: 2.888720, mean_eps: 0.910323
 250216/500000: episode: 371, duration: 34.653s, episode steps: 769, steps per second:  22, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012315, mae: 2.376767, mean_q: 2.885816, mean_eps: 0.910061
 250898/500000: episode: 372, duration: 30.464s, episode steps: 682, steps per second:  22, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.012962, mae: 2.406768, mean_q: 2.922410, mean_eps: 0.909800
 251625/500000: episode: 373, duration: 32.579s, episode steps: 727, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.011455, mae: 2.398266, mean_q: 2.914048, mean_eps: 0.909546
 252876/500000: episode: 374, duration: 55.340s, episode steps: 1251, steps per second:  23, episode reward: 13.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.012740, mae: 2.419948, mean_q: 2.940111, mean_eps: 0.909190
 253274/500000: episode: 375, duration: 19.428s, episode steps: 398, steps per second:  20, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013867, mae: 2.397673, mean_q: 2.910762, mean_eps: 0.908893
 253956/500000: episode: 376, duration: 29.921s, episode steps: 682, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.012466, mae: 2.400729, mean_q: 2.914320, mean_eps: 0.908699
 254493/500000: episode: 377, duration: 24.132s, episode steps: 537, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013837, mae: 2.409270, mean_q: 2.925376, mean_eps: 0.908479
 255065/500000: episode: 378, duration: 26.735s, episode steps: 572, steps per second:  21, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.014102, mae: 2.405769, mean_q: 2.922275, mean_eps: 0.908279
 255631/500000: episode: 379, duration: 23.684s, episode steps: 566, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014874, mae: 2.427246, mean_q: 2.947913, mean_eps: 0.908075
 256350/500000: episode: 380, duration: 32.973s, episode steps: 719, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.013811, mae: 2.410153, mean_q: 2.927396, mean_eps: 0.907844
 257203/500000: episode: 381, duration: 38.202s, episode steps: 853, steps per second:  22, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.013213, mae: 2.397563, mean_q: 2.912581, mean_eps: 0.907561
 257892/500000: episode: 382, duration: 32.066s, episode steps: 689, steps per second:  21, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.014301, mae: 2.426462, mean_q: 2.945652, mean_eps: 0.907283
 258483/500000: episode: 383, duration: 24.934s, episode steps: 591, steps per second:  24, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.012531, mae: 2.400887, mean_q: 2.916117, mean_eps: 0.907053
 258863/500000: episode: 384, duration: 17.281s, episode steps: 380, steps per second:  22, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.011131, mae: 2.402897, mean_q: 2.919014, mean_eps: 0.906878
 259400/500000: episode: 385, duration: 25.278s, episode steps: 537, steps per second:  21, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.012818, mae: 2.390601, mean_q: 2.901427, mean_eps: 0.906713
 260025/500000: episode: 386, duration: 27.508s, episode steps: 625, steps per second:  23, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.014264, mae: 2.403219, mean_q: 2.918162, mean_eps: 0.906504
 261285/500000: episode: 387, duration: 55.687s, episode steps: 1260, steps per second:  23, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.013551, mae: 2.391960, mean_q: 2.905628, mean_eps: 0.906164
 262314/500000: episode: 388, duration: 46.939s, episode steps: 1029, steps per second:  22, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.012522, mae: 2.386528, mean_q: 2.897724, mean_eps: 0.905752
 262808/500000: episode: 389, duration: 21.262s, episode steps: 494, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012325, mae: 2.400369, mean_q: 2.913184, mean_eps: 0.905478
 263318/500000: episode: 390, duration: 23.928s, episode steps: 510, steps per second:  21, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.015364, mae: 2.411524, mean_q: 2.927123, mean_eps: 0.905298
 263707/500000: episode: 391, duration: 17.120s, episode steps: 389, steps per second:  23, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012741, mae: 2.413727, mean_q: 2.931689, mean_eps: 0.905136
 264354/500000: episode: 392, duration: 29.052s, episode steps: 647, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013625, mae: 2.402717, mean_q: 2.919032, mean_eps: 0.904949
 265063/500000: episode: 393, duration: 32.726s, episode steps: 709, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.013030, mae: 2.398698, mean_q: 2.912652, mean_eps: 0.904705
 265467/500000: episode: 394, duration: 17.958s, episode steps: 404, steps per second:  22, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.015029, mae: 2.394952, mean_q: 2.908101, mean_eps: 0.904505
 266144/500000: episode: 395, duration: 30.248s, episode steps: 677, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.013589, mae: 2.400705, mean_q: 2.914453, mean_eps: 0.904311
 266799/500000: episode: 396, duration: 30.229s, episode steps: 655, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012862, mae: 2.393539, mean_q: 2.904331, mean_eps: 0.904071
 267649/500000: episode: 397, duration: 37.569s, episode steps: 850, steps per second:  23, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.012676, mae: 2.431375, mean_q: 2.951376, mean_eps: 0.903799
 268190/500000: episode: 398, duration: 26.327s, episode steps: 541, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.011658, mae: 2.406014, mean_q: 2.921870, mean_eps: 0.903549
 269083/500000: episode: 399, duration: 40.797s, episode steps: 893, steps per second:  22, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.014271, mae: 2.412864, mean_q: 2.931422, mean_eps: 0.903291
 269447/500000: episode: 400, duration: 16.413s, episode steps: 364, steps per second:  22, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012285, mae: 2.432197, mean_q: 2.955852, mean_eps: 0.903065
 270530/500000: episode: 401, duration: 49.127s, episode steps: 1083, steps per second:  22, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013141, mae: 2.412113, mean_q: 2.930283, mean_eps: 0.902804
 271182/500000: episode: 402, duration: 29.959s, episode steps: 652, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.011359, mae: 2.416497, mean_q: 2.934077, mean_eps: 0.902492
 271567/500000: episode: 403, duration: 16.692s, episode steps: 385, steps per second:  23, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013117, mae: 2.397872, mean_q: 2.909809, mean_eps: 0.902305
 272837/500000: episode: 404, duration: 57.894s, episode steps: 1270, steps per second:  22, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013362, mae: 2.405291, mean_q: 2.920383, mean_eps: 0.902007
 273488/500000: episode: 405, duration: 30.030s, episode steps: 651, steps per second:  22, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.013511, mae: 2.414388, mean_q: 2.931360, mean_eps: 0.901662
 273976/500000: episode: 406, duration: 21.755s, episode steps: 488, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.012861, mae: 2.418718, mean_q: 2.939105, mean_eps: 0.901457
 274700/500000: episode: 407, duration: 32.461s, episode steps: 724, steps per second:  22, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.012383, mae: 2.411485, mean_q: 2.929843, mean_eps: 0.901239
 275218/500000: episode: 408, duration: 24.341s, episode steps: 518, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.014715, mae: 2.398819, mean_q: 2.912164, mean_eps: 0.901015
 275658/500000: episode: 409, duration: 19.695s, episode steps: 440, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.013357, mae: 2.409574, mean_q: 2.925440, mean_eps: 0.900842
 276012/500000: episode: 410, duration: 16.153s, episode steps: 354, steps per second:  22, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.016501, mae: 2.420957, mean_q: 2.940093, mean_eps: 0.900700
 276630/500000: episode: 411, duration: 29.660s, episode steps: 618, steps per second:  21, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012184, mae: 2.409948, mean_q: 2.925684, mean_eps: 0.900525
 277391/500000: episode: 412, duration: 33.710s, episode steps: 761, steps per second:  23, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.011709, mae: 2.406423, mean_q: 2.923935, mean_eps: 0.900276
 277781/500000: episode: 413, duration: 18.209s, episode steps: 390, steps per second:  21, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.013008, mae: 2.403030, mean_q: 2.919470, mean_eps: 0.900069
 278741/500000: episode: 414, duration: 43.734s, episode steps: 960, steps per second:  22, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.012991, mae: 2.390992, mean_q: 2.902732, mean_eps: 0.899826
 279672/500000: episode: 415, duration: 41.834s, episode steps: 931, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.012315, mae: 2.401493, mean_q: 2.918280, mean_eps: 0.899486
 280296/500000: episode: 416, duration: 28.275s, episode steps: 624, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.013565, mae: 2.412505, mean_q: 2.928839, mean_eps: 0.899206
 280716/500000: episode: 417, duration: 19.014s, episode steps: 420, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.012614, mae: 2.422189, mean_q: 2.942676, mean_eps: 0.899019
 281222/500000: episode: 418, duration: 23.777s, episode steps: 506, steps per second:  21, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.013015, mae: 2.388847, mean_q: 2.899453, mean_eps: 0.898852
 282495/500000: episode: 419, duration: 56.798s, episode steps: 1273, steps per second:  22, episode reward: 13.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.010970, mae: 2.407049, mean_q: 2.922050, mean_eps: 0.898531
 283122/500000: episode: 420, duration: 28.598s, episode steps: 627, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.012522, mae: 2.410867, mean_q: 2.929765, mean_eps: 0.898189
 283683/500000: episode: 421, duration: 26.264s, episode steps: 561, steps per second:  21, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.012324, mae: 2.411034, mean_q: 2.926321, mean_eps: 0.897975
 284123/500000: episode: 422, duration: 19.766s, episode steps: 440, steps per second:  22, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.011379, mae: 2.395825, mean_q: 2.906908, mean_eps: 0.897795
 284782/500000: episode: 423, duration: 29.404s, episode steps: 659, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.013874, mae: 2.388633, mean_q: 2.899939, mean_eps: 0.897597
 285429/500000: episode: 424, duration: 30.092s, episode steps: 647, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.012118, mae: 2.390557, mean_q: 2.902212, mean_eps: 0.897362
 286299/500000: episode: 425, duration: 38.239s, episode steps: 870, steps per second:  23, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.012675, mae: 2.421982, mean_q: 2.940236, mean_eps: 0.897089
 286803/500000: episode: 426, duration: 23.573s, episode steps: 504, steps per second:  21, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.010931, mae: 2.430577, mean_q: 2.953493, mean_eps: 0.896842
 287476/500000: episode: 427, duration: 30.045s, episode steps: 673, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.013214, mae: 2.403893, mean_q: 2.918573, mean_eps: 0.896630
 288196/500000: episode: 428, duration: 33.756s, episode steps: 720, steps per second:  21, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.012597, mae: 2.389525, mean_q: 2.901279, mean_eps: 0.896380
 289079/500000: episode: 429, duration: 39.227s, episode steps: 883, steps per second:  23, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.013507, mae: 2.394604, mean_q: 2.906087, mean_eps: 0.896091
 289513/500000: episode: 430, duration: 19.872s, episode steps: 434, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.013428, mae: 2.440073, mean_q: 2.961533, mean_eps: 0.895853
 290060/500000: episode: 431, duration: 25.493s, episode steps: 547, steps per second:  21, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.012899, mae: 2.401293, mean_q: 2.915143, mean_eps: 0.895677
 291243/500000: episode: 432, duration: 53.182s, episode steps: 1183, steps per second:  22, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012803, mae: 2.405136, mean_q: 2.919418, mean_eps: 0.895366
 292074/500000: episode: 433, duration: 37.396s, episode steps: 831, steps per second:  22, episode reward:  6.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013844, mae: 2.422068, mean_q: 2.942496, mean_eps: 0.895003
 292708/500000: episode: 434, duration: 29.149s, episode steps: 634, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.013170, mae: 2.413585, mean_q: 2.929028, mean_eps: 0.894740
 293482/500000: episode: 435, duration: 35.336s, episode steps: 774, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.012870, mae: 2.396390, mean_q: 2.910603, mean_eps: 0.894486
 294111/500000: episode: 436, duration: 28.357s, episode steps: 629, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.013311, mae: 2.385711, mean_q: 2.897287, mean_eps: 0.894233
 295333/500000: episode: 437, duration: 56.858s, episode steps: 1222, steps per second:  21, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.013077, mae: 2.402623, mean_q: 2.917545, mean_eps: 0.893900
 295997/500000: episode: 438, duration: 30.250s, episode steps: 664, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.013009, mae: 2.409346, mean_q: 2.927057, mean_eps: 0.893560
 296780/500000: episode: 439, duration: 34.828s, episode steps: 783, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011978, mae: 2.389276, mean_q: 2.902684, mean_eps: 0.893300
 297370/500000: episode: 440, duration: 26.939s, episode steps: 590, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.012393, mae: 2.406529, mean_q: 2.924968, mean_eps: 0.893053
 297973/500000: episode: 441, duration: 26.221s, episode steps: 603, steps per second:  23, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.014590, mae: 2.403486, mean_q: 2.916143, mean_eps: 0.892838
 298818/500000: episode: 442, duration: 40.503s, episode steps: 845, steps per second:  21, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.013824, mae: 2.414417, mean_q: 2.932587, mean_eps: 0.892577
 299467/500000: episode: 443, duration: 29.526s, episode steps: 649, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.012430, mae: 2.385994, mean_q: 2.897302, mean_eps: 0.892309
 300119/500000: episode: 444, duration: 29.961s, episode steps: 652, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012505, mae: 2.409871, mean_q: 2.922272, mean_eps: 0.892075
 300779/500000: episode: 445, duration: 29.278s, episode steps: 660, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.014189, mae: 2.424122, mean_q: 2.941645, mean_eps: 0.891839
 301364/500000: episode: 446, duration: 29.063s, episode steps: 585, steps per second:  20, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012312, mae: 2.427448, mean_q: 2.943312, mean_eps: 0.891615
 302180/500000: episode: 447, duration: 37.928s, episode steps: 816, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012700, mae: 2.411661, mean_q: 2.925938, mean_eps: 0.891363
 302775/500000: episode: 448, duration: 26.912s, episode steps: 595, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.014492, mae: 2.405240, mean_q: 2.922925, mean_eps: 0.891109
 303243/500000: episode: 449, duration: 21.122s, episode steps: 468, steps per second:  22, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.012829, mae: 2.414734, mean_q: 2.932369, mean_eps: 0.890917
 303634/500000: episode: 450, duration: 19.573s, episode steps: 391, steps per second:  20, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.014860, mae: 2.412947, mean_q: 2.925909, mean_eps: 0.890762
 304019/500000: episode: 451, duration: 19.018s, episode steps: 385, steps per second:  20, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.012612, mae: 2.376822, mean_q: 2.885518, mean_eps: 0.890623
 304990/500000: episode: 452, duration: 45.141s, episode steps: 971, steps per second:  22, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.013270, mae: 2.402705, mean_q: 2.914920, mean_eps: 0.890379
 305583/500000: episode: 453, duration: 26.681s, episode steps: 593, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.012576, mae: 2.388719, mean_q: 2.897303, mean_eps: 0.890097
 306299/500000: episode: 454, duration: 36.606s, episode steps: 716, steps per second:  20, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013824, mae: 2.390114, mean_q: 2.899506, mean_eps: 0.889862
 306908/500000: episode: 455, duration: 30.255s, episode steps: 609, steps per second:  20, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.011516, mae: 2.429093, mean_q: 2.948084, mean_eps: 0.889623
 307585/500000: episode: 456, duration: 30.692s, episode steps: 677, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.013158, mae: 2.406527, mean_q: 2.918860, mean_eps: 0.889391
 308240/500000: episode: 457, duration: 29.816s, episode steps: 655, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013261, mae: 2.398388, mean_q: 2.911704, mean_eps: 0.889152
 308675/500000: episode: 458, duration: 21.797s, episode steps: 435, steps per second:  20, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.013402, mae: 2.391046, mean_q: 2.902299, mean_eps: 0.888956
 309328/500000: episode: 459, duration: 33.652s, episode steps: 653, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.015179, mae: 2.408589, mean_q: 2.924550, mean_eps: 0.888760
 309931/500000: episode: 460, duration: 30.023s, episode steps: 603, steps per second:  20, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012871, mae: 2.413931, mean_q: 2.930261, mean_eps: 0.888534
 310549/500000: episode: 461, duration: 27.485s, episode steps: 618, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.012725, mae: 2.416491, mean_q: 2.934140, mean_eps: 0.888314
 311668/500000: episode: 462, duration: 55.704s, episode steps: 1119, steps per second:  20, episode reward:  7.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.014557, mae: 2.406663, mean_q: 2.920413, mean_eps: 0.888001
 312380/500000: episode: 463, duration: 34.556s, episode steps: 712, steps per second:  21, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.014751, mae: 2.416818, mean_q: 2.932356, mean_eps: 0.887672
 313057/500000: episode: 464, duration: 31.538s, episode steps: 677, steps per second:  21, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.012962, mae: 2.424103, mean_q: 2.943122, mean_eps: 0.887422
 313540/500000: episode: 465, duration: 21.814s, episode steps: 483, steps per second:  22, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.013639, mae: 2.420105, mean_q: 2.938457, mean_eps: 0.887213
 314037/500000: episode: 466, duration: 26.886s, episode steps: 497, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.014693, mae: 2.398856, mean_q: 2.909452, mean_eps: 0.887036
 314575/500000: episode: 467, duration: 26.912s, episode steps: 538, steps per second:  20, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.012191, mae: 2.403103, mean_q: 2.917367, mean_eps: 0.886850
 315390/500000: episode: 468, duration: 37.839s, episode steps: 815, steps per second:  22, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013157, mae: 2.423254, mean_q: 2.939792, mean_eps: 0.886606
 315844/500000: episode: 469, duration: 20.303s, episode steps: 454, steps per second:  22, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013199, mae: 2.423987, mean_q: 2.941819, mean_eps: 0.886378
 316590/500000: episode: 470, duration: 37.641s, episode steps: 746, steps per second:  20, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.013564, mae: 2.413472, mean_q: 2.929319, mean_eps: 0.886162
 317105/500000: episode: 471, duration: 24.919s, episode steps: 515, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.013025, mae: 2.395464, mean_q: 2.907291, mean_eps: 0.885935
 317736/500000: episode: 472, duration: 29.802s, episode steps: 631, steps per second:  21, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.013033, mae: 2.431370, mean_q: 2.951858, mean_eps: 0.885729
 318268/500000: episode: 473, duration: 25.147s, episode steps: 532, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012568, mae: 2.396876, mean_q: 2.909231, mean_eps: 0.885520
 319340/500000: episode: 474, duration: 51.710s, episode steps: 1072, steps per second:  21, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.013186, mae: 2.414076, mean_q: 2.929116, mean_eps: 0.885231
 320422/500000: episode: 475, duration: 51.488s, episode steps: 1082, steps per second:  21, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.012651, mae: 2.418795, mean_q: 2.935494, mean_eps: 0.884843
 321086/500000: episode: 476, duration: 30.842s, episode steps: 664, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.013513, mae: 2.396752, mean_q: 2.911885, mean_eps: 0.884529
 321767/500000: episode: 477, duration: 33.848s, episode steps: 681, steps per second:  20, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.011748, mae: 2.413232, mean_q: 2.930015, mean_eps: 0.884287
 322360/500000: episode: 478, duration: 32.163s, episode steps: 593, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.012168, mae: 2.426365, mean_q: 2.944602, mean_eps: 0.884058
 322936/500000: episode: 479, duration: 29.135s, episode steps: 576, steps per second:  20, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.014918, mae: 2.391973, mean_q: 2.904004, mean_eps: 0.883847
 323606/500000: episode: 480, duration: 31.652s, episode steps: 670, steps per second:  21, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.013399, mae: 2.406609, mean_q: 2.920302, mean_eps: 0.883623
 324221/500000: episode: 481, duration: 32.532s, episode steps: 615, steps per second:  19, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.014269, mae: 2.427187, mean_q: 2.945578, mean_eps: 0.883391
 325041/500000: episode: 482, duration: 45.888s, episode steps: 820, steps per second:  18, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.013183, mae: 2.402914, mean_q: 2.917232, mean_eps: 0.883132
 325764/500000: episode: 483, duration: 36.820s, episode steps: 723, steps per second:  20, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.015103, mae: 2.429201, mean_q: 2.947659, mean_eps: 0.882855
 326344/500000: episode: 484, duration: 38.210s, episode steps: 580, steps per second:  15, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.012127, mae: 2.417622, mean_q: 2.934860, mean_eps: 0.882621
 327056/500000: episode: 485, duration: 42.801s, episode steps: 712, steps per second:  17, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.013674, mae: 2.385657, mean_q: 2.892979, mean_eps: 0.882389
 327439/500000: episode: 486, duration: 21.655s, episode steps: 383, steps per second:  18, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.014764, mae: 2.404530, mean_q: 2.917201, mean_eps: 0.882191
 327929/500000: episode: 487, duration: 26.492s, episode steps: 490, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.013271, mae: 2.410641, mean_q: 2.924158, mean_eps: 0.882034
 328473/500000: episode: 488, duration: 33.624s, episode steps: 544, steps per second:  16, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.012225, mae: 2.401775, mean_q: 2.913506, mean_eps: 0.881847
 328954/500000: episode: 489, duration: 26.870s, episode steps: 481, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.013758, mae: 2.410290, mean_q: 2.926511, mean_eps: 0.881663
 329507/500000: episode: 490, duration: 31.303s, episode steps: 553, steps per second:  18, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.014262, mae: 2.390811, mean_q: 2.900883, mean_eps: 0.881477
 330292/500000: episode: 491, duration: 40.108s, episode steps: 785, steps per second:  20, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011424, mae: 2.406402, mean_q: 2.920869, mean_eps: 0.881237
 330995/500000: episode: 492, duration: 36.777s, episode steps: 703, steps per second:  19, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012655, mae: 2.412415, mean_q: 2.929190, mean_eps: 0.880969
 331794/500000: episode: 493, duration: 44.393s, episode steps: 799, steps per second:  18, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.013489, mae: 2.405831, mean_q: 2.921429, mean_eps: 0.880698
 332505/500000: episode: 494, duration: 38.894s, episode steps: 711, steps per second:  18, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.012713, mae: 2.402463, mean_q: 2.916217, mean_eps: 0.880426
 333355/500000: episode: 495, duration: 46.470s, episode steps: 850, steps per second:  18, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.012977, mae: 2.380044, mean_q: 2.888271, mean_eps: 0.880145
 334195/500000: episode: 496, duration: 45.816s, episode steps: 840, steps per second:  18, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.013763, mae: 2.404665, mean_q: 2.917670, mean_eps: 0.879841
 334690/500000: episode: 497, duration: 27.278s, episode steps: 495, steps per second:  18, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.013426, mae: 2.394053, mean_q: 2.907313, mean_eps: 0.879601
 335364/500000: episode: 498, duration: 37.241s, episode steps: 674, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.011802, mae: 2.396579, mean_q: 2.907879, mean_eps: 0.879391
 335903/500000: episode: 499, duration: 30.153s, episode steps: 539, steps per second:  18, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012507, mae: 2.417136, mean_q: 2.932271, mean_eps: 0.879172
 336543/500000: episode: 500, duration: 34.904s, episode steps: 640, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.012337, mae: 2.397190, mean_q: 2.910751, mean_eps: 0.878960
 337027/500000: episode: 501, duration: 27.150s, episode steps: 484, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.013666, mae: 2.425956, mean_q: 2.942965, mean_eps: 0.878758
 337419/500000: episode: 502, duration: 21.169s, episode steps: 392, steps per second:  19, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.013589, mae: 2.410626, mean_q: 2.925748, mean_eps: 0.878600
 337940/500000: episode: 503, duration: 29.746s, episode steps: 521, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.013347, mae: 2.419006, mean_q: 2.935325, mean_eps: 0.878436
 338754/500000: episode: 504, duration: 37.672s, episode steps: 814, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012151, mae: 2.384722, mean_q: 2.892991, mean_eps: 0.878195
 339274/500000: episode: 505, duration: 24.072s, episode steps: 520, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.012570, mae: 2.381570, mean_q: 2.888145, mean_eps: 0.877955
 340001/500000: episode: 506, duration: 33.186s, episode steps: 727, steps per second:  22, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.015209, mae: 2.390994, mean_q: 2.899461, mean_eps: 0.877730
 340850/500000: episode: 507, duration: 43.372s, episode steps: 849, steps per second:  20, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012667, mae: 2.400870, mean_q: 2.911830, mean_eps: 0.877447
 341407/500000: episode: 508, duration: 26.425s, episode steps: 557, steps per second:  21, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.013245, mae: 2.419451, mean_q: 2.934645, mean_eps: 0.877194
 342096/500000: episode: 509, duration: 32.521s, episode steps: 689, steps per second:  21, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014105, mae: 2.417093, mean_q: 2.931236, mean_eps: 0.876970
 342806/500000: episode: 510, duration: 35.292s, episode steps: 710, steps per second:  20, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012230, mae: 2.400026, mean_q: 2.910951, mean_eps: 0.876718
 343845/500000: episode: 511, duration: 58.308s, episode steps: 1039, steps per second:  18, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.013138, mae: 2.387440, mean_q: 2.897592, mean_eps: 0.876403
 344731/500000: episode: 512, duration: 48.651s, episode steps: 886, steps per second:  18, episode reward:  6.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.013084, mae: 2.404451, mean_q: 2.918196, mean_eps: 0.876056
 345557/500000: episode: 513, duration: 46.094s, episode steps: 826, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.011765, mae: 2.401302, mean_q: 2.914444, mean_eps: 0.875748
 346017/500000: episode: 514, duration: 25.135s, episode steps: 460, steps per second:  18, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.013672, mae: 2.409863, mean_q: 2.925048, mean_eps: 0.875516
 347038/500000: episode: 515, duration: 56.134s, episode steps: 1021, steps per second:  18, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.012396, mae: 2.401339, mean_q: 2.913090, mean_eps: 0.875250
 348098/500000: episode: 516, duration: 58.465s, episode steps: 1060, steps per second:  18, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012823, mae: 2.427087, mean_q: 2.944588, mean_eps: 0.874876
 348741/500000: episode: 517, duration: 35.353s, episode steps: 643, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.012974, mae: 2.397827, mean_q: 2.910238, mean_eps: 0.874569
 349589/500000: episode: 518, duration: 47.182s, episode steps: 848, steps per second:  18, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013001, mae: 2.411561, mean_q: 2.925636, mean_eps: 0.874300
 350261/500000: episode: 519, duration: 36.601s, episode steps: 672, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.014332, mae: 2.402274, mean_q: 2.914694, mean_eps: 0.874027
 351351/500000: episode: 520, duration: 59.966s, episode steps: 1090, steps per second:  18, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.011991, mae: 2.437402, mean_q: 2.958537, mean_eps: 0.873710
 351983/500000: episode: 521, duration: 35.300s, episode steps: 632, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.014609, mae: 2.428109, mean_q: 2.946413, mean_eps: 0.873400
 352984/500000: episode: 522, duration: 55.154s, episode steps: 1001, steps per second:  18, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.013193, mae: 2.437153, mean_q: 2.956532, mean_eps: 0.873106
 353772/500000: episode: 523, duration: 44.114s, episode steps: 788, steps per second:  18, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.013112, mae: 2.422016, mean_q: 2.937910, mean_eps: 0.872785
 354449/500000: episode: 524, duration: 38.349s, episode steps: 677, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.016495, mae: 2.434884, mean_q: 2.951242, mean_eps: 0.872520
 355532/500000: episode: 525, duration: 59.232s, episode steps: 1083, steps per second:  18, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.014471, mae: 2.434411, mean_q: 2.951843, mean_eps: 0.872204
 355964/500000: episode: 526, duration: 24.987s, episode steps: 432, steps per second:  17, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013133, mae: 2.458320, mean_q: 2.981908, mean_eps: 0.871931
 356893/500000: episode: 527, duration: 51.426s, episode steps: 929, steps per second:  18, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.013167, mae: 2.411386, mean_q: 2.924106, mean_eps: 0.871686
 357310/500000: episode: 528, duration: 23.127s, episode steps: 417, steps per second:  18, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.017717, mae: 2.424760, mean_q: 2.940371, mean_eps: 0.871443
 357846/500000: episode: 529, duration: 29.370s, episode steps: 536, steps per second:  18, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.015175, mae: 2.406210, mean_q: 2.917295, mean_eps: 0.871272
 358419/500000: episode: 530, duration: 31.421s, episode steps: 573, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.013298, mae: 2.425303, mean_q: 2.943567, mean_eps: 0.871072
 359191/500000: episode: 531, duration: 40.674s, episode steps: 772, steps per second:  19, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.013140, mae: 2.451115, mean_q: 2.971609, mean_eps: 0.870831
 359682/500000: episode: 532, duration: 22.549s, episode steps: 491, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014322, mae: 2.407409, mean_q: 2.921534, mean_eps: 0.870603
 360115/500000: episode: 533, duration: 21.590s, episode steps: 433, steps per second:  20, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.014211, mae: 2.418731, mean_q: 2.932812, mean_eps: 0.870437
 360678/500000: episode: 534, duration: 25.051s, episode steps: 563, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.014547, mae: 2.423334, mean_q: 2.938716, mean_eps: 0.870257
 361361/500000: episode: 535, duration: 37.476s, episode steps: 683, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011785, mae: 2.417516, mean_q: 2.933556, mean_eps: 0.870033
 362017/500000: episode: 536, duration: 35.263s, episode steps: 656, steps per second:  19, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.015296, mae: 2.430037, mean_q: 2.945534, mean_eps: 0.869792
 362693/500000: episode: 537, duration: 32.971s, episode steps: 676, steps per second:  21, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.012472, mae: 2.421335, mean_q: 2.934217, mean_eps: 0.869552
 363510/500000: episode: 538, duration: 40.015s, episode steps: 817, steps per second:  20, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.013725, mae: 2.416187, mean_q: 2.928721, mean_eps: 0.869283
 364297/500000: episode: 539, duration: 40.958s, episode steps: 787, steps per second:  19, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.015706, mae: 2.416197, mean_q: 2.928923, mean_eps: 0.868995
 364872/500000: episode: 540, duration: 28.990s, episode steps: 575, steps per second:  20, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.014837, mae: 2.379593, mean_q: 2.885627, mean_eps: 0.868750
 365401/500000: episode: 541, duration: 25.234s, episode steps: 529, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015248, mae: 2.430223, mean_q: 2.945672, mean_eps: 0.868551
 366013/500000: episode: 542, duration: 33.847s, episode steps: 612, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012898, mae: 2.402798, mean_q: 2.916665, mean_eps: 0.868345
 366687/500000: episode: 543, duration: 38.050s, episode steps: 674, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.013917, mae: 2.405505, mean_q: 2.916744, mean_eps: 0.868114
 367326/500000: episode: 544, duration: 35.432s, episode steps: 639, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.013101, mae: 2.426660, mean_q: 2.941340, mean_eps: 0.867878
 368170/500000: episode: 545, duration: 46.695s, episode steps: 844, steps per second:  18, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.014156, mae: 2.405156, mean_q: 2.917381, mean_eps: 0.867611
 369539/500000: episode: 546, duration: 76.170s, episode steps: 1369, steps per second:  18, episode reward: 14.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.013445, mae: 2.422298, mean_q: 2.937190, mean_eps: 0.867213
 369976/500000: episode: 547, duration: 24.287s, episode steps: 437, steps per second:  18, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.014936, mae: 2.420801, mean_q: 2.938856, mean_eps: 0.866888
 370554/500000: episode: 548, duration: 32.759s, episode steps: 578, steps per second:  18, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.012861, mae: 2.405875, mean_q: 2.916536, mean_eps: 0.866705
 370935/500000: episode: 549, duration: 22.225s, episode steps: 381, steps per second:  17, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.014622, mae: 2.403025, mean_q: 2.911706, mean_eps: 0.866532
 371485/500000: episode: 550, duration: 30.439s, episode steps: 550, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012846, mae: 2.401765, mean_q: 2.910393, mean_eps: 0.866364
 371887/500000: episode: 551, duration: 22.158s, episode steps: 402, steps per second:  18, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014666, mae: 2.416811, mean_q: 2.929333, mean_eps: 0.866193
 372676/500000: episode: 552, duration: 43.784s, episode steps: 789, steps per second:  18, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.014349, mae: 2.423851, mean_q: 2.938752, mean_eps: 0.865979
 373260/500000: episode: 553, duration: 32.943s, episode steps: 584, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.014733, mae: 2.396449, mean_q: 2.904468, mean_eps: 0.865732
 374090/500000: episode: 554, duration: 45.581s, episode steps: 830, steps per second:  18, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.012700, mae: 2.405043, mean_q: 2.918967, mean_eps: 0.865477
 375165/500000: episode: 555, duration: 58.431s, episode steps: 1075, steps per second:  18, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.013095, mae: 2.395645, mean_q: 2.904155, mean_eps: 0.865134
 375736/500000: episode: 556, duration: 28.596s, episode steps: 571, steps per second:  20, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.014730, mae: 2.398805, mean_q: 2.908036, mean_eps: 0.864838
 376284/500000: episode: 557, duration: 26.422s, episode steps: 548, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012097, mae: 2.411844, mean_q: 2.922722, mean_eps: 0.864637
 376982/500000: episode: 558, duration: 33.325s, episode steps: 698, steps per second:  21, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.014059, mae: 2.428342, mean_q: 2.944782, mean_eps: 0.864412
 377496/500000: episode: 559, duration: 25.750s, episode steps: 514, steps per second:  20, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.014972, mae: 2.426448, mean_q: 2.943005, mean_eps: 0.864194
 377889/500000: episode: 560, duration: 20.679s, episode steps: 393, steps per second:  19, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.016280, mae: 2.444921, mean_q: 2.964945, mean_eps: 0.864031
 378849/500000: episode: 561, duration: 54.139s, episode steps: 960, steps per second:  18, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.013702, mae: 2.397891, mean_q: 2.908657, mean_eps: 0.863787
 379324/500000: episode: 562, duration: 26.692s, episode steps: 475, steps per second:  18, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.015303, mae: 2.430696, mean_q: 2.945710, mean_eps: 0.863529
 380382/500000: episode: 563, duration: 59.046s, episode steps: 1058, steps per second:  18, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.012874, mae: 2.416598, mean_q: 2.929187, mean_eps: 0.863253
 380960/500000: episode: 564, duration: 32.535s, episode steps: 578, steps per second:  18, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.013469, mae: 2.415837, mean_q: 2.929963, mean_eps: 0.862959
 381704/500000: episode: 565, duration: 42.317s, episode steps: 744, steps per second:  18, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.013179, mae: 2.412298, mean_q: 2.927427, mean_eps: 0.862721
 382593/500000: episode: 566, duration: 49.376s, episode steps: 889, steps per second:  18, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.014710, mae: 2.405185, mean_q: 2.916810, mean_eps: 0.862427
 382966/500000: episode: 567, duration: 20.475s, episode steps: 373, steps per second:  18, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.013207, mae: 2.429048, mean_q: 2.944745, mean_eps: 0.862199
 383433/500000: episode: 568, duration: 25.530s, episode steps: 467, steps per second:  18, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014399, mae: 2.421210, mean_q: 2.931450, mean_eps: 0.862048
 384402/500000: episode: 569, duration: 53.649s, episode steps: 969, steps per second:  18, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.014081, mae: 2.418276, mean_q: 2.931997, mean_eps: 0.861790
 385023/500000: episode: 570, duration: 34.631s, episode steps: 621, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.015666, mae: 2.413708, mean_q: 2.925625, mean_eps: 0.861504
 385997/500000: episode: 571, duration: 54.133s, episode steps: 974, steps per second:  18, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.012834, mae: 2.417941, mean_q: 2.930192, mean_eps: 0.861216
 386588/500000: episode: 572, duration: 33.317s, episode steps: 591, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.014094, mae: 2.411537, mean_q: 2.923760, mean_eps: 0.860935
 387311/500000: episode: 573, duration: 40.395s, episode steps: 723, steps per second:  18, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.014625, mae: 2.407118, mean_q: 2.916319, mean_eps: 0.860699
 387958/500000: episode: 574, duration: 36.409s, episode steps: 647, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.013525, mae: 2.416840, mean_q: 2.931069, mean_eps: 0.860452
 388615/500000: episode: 575, duration: 36.527s, episode steps: 657, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.013381, mae: 2.424482, mean_q: 2.940533, mean_eps: 0.860217
 389461/500000: episode: 576, duration: 47.425s, episode steps: 846, steps per second:  18, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.014026, mae: 2.423344, mean_q: 2.938012, mean_eps: 0.859946
 389831/500000: episode: 577, duration: 20.003s, episode steps: 370, steps per second:  18, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013980, mae: 2.430788, mean_q: 2.948862, mean_eps: 0.859727
 390688/500000: episode: 578, duration: 48.225s, episode steps: 857, steps per second:  18, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012381, mae: 2.401026, mean_q: 2.909689, mean_eps: 0.859507
 391483/500000: episode: 579, duration: 44.892s, episode steps: 795, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.013916, mae: 2.424443, mean_q: 2.940685, mean_eps: 0.859210
 391933/500000: episode: 580, duration: 25.038s, episode steps: 450, steps per second:  18, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.013390, mae: 2.427896, mean_q: 2.941980, mean_eps: 0.858985
 392468/500000: episode: 581, duration: 30.394s, episode steps: 535, steps per second:  18, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013067, mae: 2.435141, mean_q: 2.952075, mean_eps: 0.858808
 393303/500000: episode: 582, duration: 48.920s, episode steps: 835, steps per second:  17, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.013839, mae: 2.394171, mean_q: 2.903600, mean_eps: 0.858562
 393882/500000: episode: 583, duration: 32.457s, episode steps: 579, steps per second:  18, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.013349, mae: 2.418077, mean_q: 2.930257, mean_eps: 0.858307
 395019/500000: episode: 584, duration: 57.617s, episode steps: 1137, steps per second:  20, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013581, mae: 2.422648, mean_q: 2.936364, mean_eps: 0.857998
 395754/500000: episode: 585, duration: 41.553s, episode steps: 735, steps per second:  18, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.016476, mae: 2.412573, mean_q: 2.922795, mean_eps: 0.857661
 396140/500000: episode: 586, duration: 21.432s, episode steps: 386, steps per second:  18, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.015228, mae: 2.412036, mean_q: 2.924345, mean_eps: 0.857459
 396669/500000: episode: 587, duration: 28.433s, episode steps: 529, steps per second:  19, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.016221, mae: 2.418596, mean_q: 2.931676, mean_eps: 0.857295
 397284/500000: episode: 588, duration: 30.057s, episode steps: 615, steps per second:  20, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.014351, mae: 2.412499, mean_q: 2.924966, mean_eps: 0.857089
 397762/500000: episode: 589, duration: 26.497s, episode steps: 478, steps per second:  18, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.012632, mae: 2.404585, mean_q: 2.914564, mean_eps: 0.856892
 398223/500000: episode: 590, duration: 25.781s, episode steps: 461, steps per second:  18, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.012983, mae: 2.399673, mean_q: 2.907800, mean_eps: 0.856723
 398768/500000: episode: 591, duration: 29.902s, episode steps: 545, steps per second:  18, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013009, mae: 2.398570, mean_q: 2.908458, mean_eps: 0.856542
 399382/500000: episode: 592, duration: 34.147s, episode steps: 614, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.015841, mae: 2.425894, mean_q: 2.939219, mean_eps: 0.856333
 400405/500000: episode: 593, duration: 57.808s, episode steps: 1023, steps per second:  18, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.013448, mae: 2.427504, mean_q: 2.941400, mean_eps: 0.856038
 401232/500000: episode: 594, duration: 46.515s, episode steps: 827, steps per second:  18, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.014895, mae: 2.425115, mean_q: 2.936922, mean_eps: 0.855706
 402117/500000: episode: 595, duration: 49.447s, episode steps: 885, steps per second:  18, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014388, mae: 2.413083, mean_q: 2.923884, mean_eps: 0.855397
 402515/500000: episode: 596, duration: 21.899s, episode steps: 398, steps per second:  18, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.014020, mae: 2.446757, mean_q: 2.964284, mean_eps: 0.855166
 403276/500000: episode: 597, duration: 42.864s, episode steps: 761, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014424, mae: 2.427511, mean_q: 2.940319, mean_eps: 0.854958
 404132/500000: episode: 598, duration: 48.154s, episode steps: 856, steps per second:  18, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.014141, mae: 2.420908, mean_q: 2.933463, mean_eps: 0.854667
 405036/500000: episode: 599, duration: 50.811s, episode steps: 904, steps per second:  18, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.015502, mae: 2.421328, mean_q: 2.932945, mean_eps: 0.854350
 405564/500000: episode: 600, duration: 30.126s, episode steps: 528, steps per second:  18, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.013444, mae: 2.429986, mean_q: 2.943415, mean_eps: 0.854093
 406194/500000: episode: 601, duration: 34.749s, episode steps: 630, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.014651, mae: 2.423023, mean_q: 2.934970, mean_eps: 0.853884
 406752/500000: episode: 602, duration: 31.940s, episode steps: 558, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.013038, mae: 2.409997, mean_q: 2.918218, mean_eps: 0.853670
 407492/500000: episode: 603, duration: 42.566s, episode steps: 740, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.016131, mae: 2.428250, mean_q: 2.940630, mean_eps: 0.853437
 408242/500000: episode: 604, duration: 43.726s, episode steps: 750, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.014559, mae: 2.442623, mean_q: 2.961018, mean_eps: 0.853168
 408709/500000: episode: 605, duration: 27.640s, episode steps: 467, steps per second:  17, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014153, mae: 2.438866, mean_q: 2.955405, mean_eps: 0.852949
 409297/500000: episode: 606, duration: 28.301s, episode steps: 588, steps per second:  21, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.014656, mae: 2.419290, mean_q: 2.930777, mean_eps: 0.852759
 409988/500000: episode: 607, duration: 33.021s, episode steps: 691, steps per second:  21, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.013342, mae: 2.420223, mean_q: 2.930842, mean_eps: 0.852529
 411081/500000: episode: 608, duration: 50.912s, episode steps: 1093, steps per second:  21, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.014135, mae: 2.438829, mean_q: 2.954284, mean_eps: 0.852208
 412176/500000: episode: 609, duration: 52.697s, episode steps: 1095, steps per second:  21, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.014283, mae: 2.425351, mean_q: 2.938064, mean_eps: 0.851814
 412862/500000: episode: 610, duration: 32.916s, episode steps: 686, steps per second:  21, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014781, mae: 2.433113, mean_q: 2.947530, mean_eps: 0.851494
 413498/500000: episode: 611, duration: 29.531s, episode steps: 636, steps per second:  22, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.014291, mae: 2.422558, mean_q: 2.937380, mean_eps: 0.851255
 414203/500000: episode: 612, duration: 34.059s, episode steps: 705, steps per second:  21, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013702, mae: 2.406068, mean_q: 2.915725, mean_eps: 0.851014
 415102/500000: episode: 613, duration: 41.779s, episode steps: 899, steps per second:  22, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.013995, mae: 2.427726, mean_q: 2.940250, mean_eps: 0.850725
 415815/500000: episode: 614, duration: 35.415s, episode steps: 713, steps per second:  20, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.015540, mae: 2.414845, mean_q: 2.924298, mean_eps: 0.850435
 416212/500000: episode: 615, duration: 21.399s, episode steps: 397, steps per second:  19, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.013434, mae: 2.421238, mean_q: 2.933368, mean_eps: 0.850236
 416597/500000: episode: 616, duration: 20.920s, episode steps: 385, steps per second:  18, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.013815, mae: 2.407131, mean_q: 2.916721, mean_eps: 0.850095
 417145/500000: episode: 617, duration: 31.386s, episode steps: 548, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.014002, mae: 2.432301, mean_q: 2.945508, mean_eps: 0.849926
 417544/500000: episode: 618, duration: 21.606s, episode steps: 399, steps per second:  18, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013095, mae: 2.436649, mean_q: 2.955201, mean_eps: 0.849756
 418361/500000: episode: 619, duration: 45.956s, episode steps: 817, steps per second:  18, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012322, mae: 2.442536, mean_q: 2.961220, mean_eps: 0.849537
 418919/500000: episode: 620, duration: 31.520s, episode steps: 558, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.014107, mae: 2.414565, mean_q: 2.925968, mean_eps: 0.849290
 419841/500000: episode: 621, duration: 51.198s, episode steps: 922, steps per second:  18, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012948, mae: 2.419483, mean_q: 2.931401, mean_eps: 0.849023
 420299/500000: episode: 622, duration: 26.372s, episode steps: 458, steps per second:  17, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.012819, mae: 2.410660, mean_q: 2.920244, mean_eps: 0.848775
 420938/500000: episode: 623, duration: 35.567s, episode steps: 639, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.014529, mae: 2.419815, mean_q: 2.930070, mean_eps: 0.848578
 421587/500000: episode: 624, duration: 37.781s, episode steps: 649, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.014563, mae: 2.440597, mean_q: 2.956643, mean_eps: 0.848346
 422647/500000: episode: 625, duration: 58.930s, episode steps: 1060, steps per second:  18, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.013991, mae: 2.426007, mean_q: 2.938132, mean_eps: 0.848038
 423043/500000: episode: 626, duration: 23.384s, episode steps: 396, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.013524, mae: 2.417088, mean_q: 2.926563, mean_eps: 0.847776
 423817/500000: episode: 627, duration: 43.505s, episode steps: 774, steps per second:  18, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.014546, mae: 2.428125, mean_q: 2.940300, mean_eps: 0.847565
 424189/500000: episode: 628, duration: 21.146s, episode steps: 372, steps per second:  18, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.012614, mae: 2.422378, mean_q: 2.932559, mean_eps: 0.847359
 425010/500000: episode: 629, duration: 46.964s, episode steps: 821, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012879, mae: 2.424696, mean_q: 2.937238, mean_eps: 0.847144
 425732/500000: episode: 630, duration: 40.695s, episode steps: 722, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.013796, mae: 2.436398, mean_q: 2.951588, mean_eps: 0.846867
 426292/500000: episode: 631, duration: 32.303s, episode steps: 560, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.016552, mae: 2.415996, mean_q: 2.925491, mean_eps: 0.846636
 426866/500000: episode: 632, duration: 32.397s, episode steps: 574, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.015156, mae: 2.448932, mean_q: 2.967704, mean_eps: 0.846432
 427674/500000: episode: 633, duration: 46.112s, episode steps: 808, steps per second:  18, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.014831, mae: 2.407301, mean_q: 2.915054, mean_eps: 0.846183
 428090/500000: episode: 634, duration: 23.562s, episode steps: 416, steps per second:  18, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.013702, mae: 2.432962, mean_q: 2.946424, mean_eps: 0.845962
 428698/500000: episode: 635, duration: 35.129s, episode steps: 608, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.015291, mae: 2.445544, mean_q: 2.962287, mean_eps: 0.845778
 429273/500000: episode: 636, duration: 32.026s, episode steps: 575, steps per second:  18, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.014694, mae: 2.411479, mean_q: 2.922075, mean_eps: 0.845565
 429719/500000: episode: 637, duration: 25.520s, episode steps: 446, steps per second:  17, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.013563, mae: 2.416987, mean_q: 2.929002, mean_eps: 0.845381
 430228/500000: episode: 638, duration: 28.889s, episode steps: 509, steps per second:  18, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.013590, mae: 2.424048, mean_q: 2.937072, mean_eps: 0.845210
 431815/500000: episode: 639, duration: 86.872s, episode steps: 1587, steps per second:  18, episode reward: 14.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.012313, mae: 2.430345, mean_q: 2.945747, mean_eps: 0.844833
 432692/500000: episode: 640, duration: 49.443s, episode steps: 877, steps per second:  18, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.014193, mae: 2.424704, mean_q: 2.936156, mean_eps: 0.844389
 433204/500000: episode: 641, duration: 29.546s, episode steps: 512, steps per second:  17, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013496, mae: 2.418009, mean_q: 2.928936, mean_eps: 0.844139
 433612/500000: episode: 642, duration: 22.750s, episode steps: 408, steps per second:  18, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.013532, mae: 2.428687, mean_q: 2.943440, mean_eps: 0.843974
 434342/500000: episode: 643, duration: 42.077s, episode steps: 730, steps per second:  17, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.012278, mae: 2.426581, mean_q: 2.939834, mean_eps: 0.843769
 434859/500000: episode: 644, duration: 28.788s, episode steps: 517, steps per second:  18, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.013980, mae: 2.437602, mean_q: 2.956994, mean_eps: 0.843544
 435373/500000: episode: 645, duration: 29.325s, episode steps: 514, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.013219, mae: 2.427880, mean_q: 2.943411, mean_eps: 0.843358
 436082/500000: episode: 646, duration: 40.421s, episode steps: 709, steps per second:  18, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013015, mae: 2.433342, mean_q: 2.950874, mean_eps: 0.843138
 436845/500000: episode: 647, duration: 44.243s, episode steps: 763, steps per second:  17, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.013444, mae: 2.407948, mean_q: 2.918200, mean_eps: 0.842873
 437342/500000: episode: 648, duration: 27.531s, episode steps: 497, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.013289, mae: 2.437439, mean_q: 2.952653, mean_eps: 0.842646
 438145/500000: episode: 649, duration: 45.407s, episode steps: 803, steps per second:  18, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.015121, mae: 2.414200, mean_q: 2.925949, mean_eps: 0.842412
 438822/500000: episode: 650, duration: 38.874s, episode steps: 677, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.015147, mae: 2.423180, mean_q: 2.934156, mean_eps: 0.842146
 439680/500000: episode: 651, duration: 49.434s, episode steps: 858, steps per second:  17, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.015951, mae: 2.427173, mean_q: 2.940649, mean_eps: 0.841870
 440172/500000: episode: 652, duration: 27.563s, episode steps: 492, steps per second:  18, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.014282, mae: 2.430017, mean_q: 2.945910, mean_eps: 0.841627
 440975/500000: episode: 653, duration: 45.842s, episode steps: 803, steps per second:  18, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.015217, mae: 2.425961, mean_q: 2.938661, mean_eps: 0.841394
 441411/500000: episode: 654, duration: 25.520s, episode steps: 436, steps per second:  17, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.013135, mae: 2.413680, mean_q: 2.924082, mean_eps: 0.841171
 442363/500000: episode: 655, duration: 67.398s, episode steps: 952, steps per second:  14, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.014206, mae: 2.432726, mean_q: 2.946988, mean_eps: 0.840921
 442815/500000: episode: 656, duration: 21.193s, episode steps: 452, steps per second:  21, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.011446, mae: 2.432833, mean_q: 2.949054, mean_eps: 0.840668
 443409/500000: episode: 657, duration: 29.113s, episode steps: 594, steps per second:  20, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.012835, mae: 2.414510, mean_q: 2.926224, mean_eps: 0.840480
 444042/500000: episode: 658, duration: 30.178s, episode steps: 633, steps per second:  21, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.012280, mae: 2.443801, mean_q: 2.958932, mean_eps: 0.840259
 444832/500000: episode: 659, duration: 38.516s, episode steps: 790, steps per second:  21, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013138, mae: 2.428981, mean_q: 2.941339, mean_eps: 0.840003
 445266/500000: episode: 660, duration: 22.227s, episode steps: 434, steps per second:  20, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.011217, mae: 2.396664, mean_q: 2.900662, mean_eps: 0.839783
 445788/500000: episode: 661, duration: 30.623s, episode steps: 522, steps per second:  17, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.014861, mae: 2.413097, mean_q: 2.922882, mean_eps: 0.839611
 446372/500000: episode: 662, duration: 33.495s, episode steps: 584, steps per second:  17, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.011274, mae: 2.416592, mean_q: 2.927792, mean_eps: 0.839412
 447139/500000: episode: 663, duration: 51.477s, episode steps: 767, steps per second:  15, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014145, mae: 2.415969, mean_q: 2.926258, mean_eps: 0.839169
 447701/500000: episode: 664, duration: 43.513s, episode steps: 562, steps per second:  13, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.014632, mae: 2.453689, mean_q: 2.974947, mean_eps: 0.838929
 448461/500000: episode: 665, duration: 57.554s, episode steps: 760, steps per second:  13, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013441, mae: 2.423717, mean_q: 2.934937, mean_eps: 0.838690
 449439/500000: episode: 666, duration: 53.084s, episode steps: 978, steps per second:  18, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.015516, mae: 2.413371, mean_q: 2.922006, mean_eps: 0.838378
 450068/500000: episode: 667, duration: 30.881s, episode steps: 629, steps per second:  20, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.014576, mae: 2.436696, mean_q: 2.949699, mean_eps: 0.838089
 450572/500000: episode: 668, duration: 24.068s, episode steps: 504, steps per second:  21, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.013753, mae: 2.434463, mean_q: 2.949781, mean_eps: 0.837886
 451155/500000: episode: 669, duration: 29.280s, episode steps: 583, steps per second:  20, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.013116, mae: 2.449080, mean_q: 2.968097, mean_eps: 0.837690
 452299/500000: episode: 670, duration: 63.511s, episode steps: 1144, steps per second:  18, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013439, mae: 2.448387, mean_q: 2.965284, mean_eps: 0.837379
 452776/500000: episode: 671, duration: 29.245s, episode steps: 477, steps per second:  16, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.016077, mae: 2.457651, mean_q: 2.975717, mean_eps: 0.837087
 453342/500000: episode: 672, duration: 32.855s, episode steps: 566, steps per second:  17, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.013401, mae: 2.457205, mean_q: 2.978378, mean_eps: 0.836899
 453717/500000: episode: 673, duration: 22.367s, episode steps: 375, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014867, mae: 2.428385, mean_q: 2.942215, mean_eps: 0.836729
 454342/500000: episode: 674, duration: 36.795s, episode steps: 625, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.015335, mae: 2.453772, mean_q: 2.972487, mean_eps: 0.836549
 455020/500000: episode: 675, duration: 39.651s, episode steps: 678, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.014597, mae: 2.455707, mean_q: 2.975860, mean_eps: 0.836315
 455367/500000: episode: 676, duration: 18.891s, episode steps: 347, steps per second:  18, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.014618, mae: 2.463006, mean_q: 2.985031, mean_eps: 0.836131
 455715/500000: episode: 677, duration: 16.723s, episode steps: 348, steps per second:  21, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.012703, mae: 2.438646, mean_q: 2.953043, mean_eps: 0.836006
 456228/500000: episode: 678, duration: 29.914s, episode steps: 513, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.015000, mae: 2.423413, mean_q: 2.934484, mean_eps: 0.835851
 457206/500000: episode: 679, duration: 76.730s, episode steps: 978, steps per second:  13, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013578, mae: 2.436668, mean_q: 2.953831, mean_eps: 0.835582
 458012/500000: episode: 680, duration: 59.413s, episode steps: 806, steps per second:  14, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.012384, mae: 2.430399, mean_q: 2.943445, mean_eps: 0.835261
 458697/500000: episode: 681, duration: 48.800s, episode steps: 685, steps per second:  14, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.014678, mae: 2.425001, mean_q: 2.937546, mean_eps: 0.834993
 459201/500000: episode: 682, duration: 35.898s, episode steps: 504, steps per second:  14, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.015929, mae: 2.457676, mean_q: 2.977691, mean_eps: 0.834778
 460036/500000: episode: 683, duration: 59.210s, episode steps: 835, steps per second:  14, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.015363, mae: 2.434565, mean_q: 2.948831, mean_eps: 0.834538
 461417/500000: episode: 684, duration: 100.816s, episode steps: 1381, steps per second:  14, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.012843, mae: 2.417673, mean_q: 2.930377, mean_eps: 0.834139
 462433/500000: episode: 685, duration: 66.303s, episode steps: 1016, steps per second:  15, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.013462, mae: 2.426506, mean_q: 2.941630, mean_eps: 0.833707
 463102/500000: episode: 686, duration: 40.262s, episode steps: 669, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.014886, mae: 2.448027, mean_q: 2.964058, mean_eps: 0.833404
 463591/500000: episode: 687, duration: 23.955s, episode steps: 489, steps per second:  20, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.013927, mae: 2.461000, mean_q: 2.980709, mean_eps: 0.833195
 464445/500000: episode: 688, duration: 45.241s, episode steps: 854, steps per second:  19, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013228, mae: 2.435556, mean_q: 2.949436, mean_eps: 0.832954
 465234/500000: episode: 689, duration: 45.713s, episode steps: 789, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.013403, mae: 2.437901, mean_q: 2.951095, mean_eps: 0.832658
 466205/500000: episode: 690, duration: 50.044s, episode steps: 971, steps per second:  19, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.014718, mae: 2.440589, mean_q: 2.956325, mean_eps: 0.832341
 466762/500000: episode: 691, duration: 28.165s, episode steps: 557, steps per second:  20, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.013571, mae: 2.445620, mean_q: 2.962366, mean_eps: 0.832066
 467266/500000: episode: 692, duration: 25.003s, episode steps: 504, steps per second:  20, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.014080, mae: 2.443611, mean_q: 2.960236, mean_eps: 0.831875
 467792/500000: episode: 693, duration: 26.376s, episode steps: 526, steps per second:  20, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013402, mae: 2.410823, mean_q: 2.919717, mean_eps: 0.831690
 468276/500000: episode: 694, duration: 23.524s, episode steps: 484, steps per second:  21, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.014848, mae: 2.450266, mean_q: 2.969231, mean_eps: 0.831508
 469496/500000: episode: 695, duration: 59.614s, episode steps: 1220, steps per second:  20, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.014232, mae: 2.425148, mean_q: 2.936058, mean_eps: 0.831202
 470319/500000: episode: 696, duration: 41.376s, episode steps: 823, steps per second:  20, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.013817, mae: 2.428778, mean_q: 2.940487, mean_eps: 0.830834
 470982/500000: episode: 697, duration: 37.504s, episode steps: 663, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.014801, mae: 2.420261, mean_q: 2.930633, mean_eps: 0.830566
 471648/500000: episode: 698, duration: 34.026s, episode steps: 666, steps per second:  20, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.012693, mae: 2.437614, mean_q: 2.953072, mean_eps: 0.830327
 472026/500000: episode: 699, duration: 18.521s, episode steps: 378, steps per second:  20, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.015745, mae: 2.452886, mean_q: 2.971078, mean_eps: 0.830139
 472745/500000: episode: 700, duration: 38.598s, episode steps: 719, steps per second:  19, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012447, mae: 2.439107, mean_q: 2.953931, mean_eps: 0.829941
 473126/500000: episode: 701, duration: 22.047s, episode steps: 381, steps per second:  17, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.014587, mae: 2.447732, mean_q: 2.962571, mean_eps: 0.829743
 473595/500000: episode: 702, duration: 23.365s, episode steps: 469, steps per second:  20, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.014869, mae: 2.445236, mean_q: 2.961936, mean_eps: 0.829590
 474552/500000: episode: 703, duration: 52.922s, episode steps: 957, steps per second:  18, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014798, mae: 2.431970, mean_q: 2.946900, mean_eps: 0.829334
 474985/500000: episode: 704, duration: 25.555s, episode steps: 433, steps per second:  17, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.015289, mae: 2.406671, mean_q: 2.914908, mean_eps: 0.829084
 475726/500000: episode: 705, duration: 43.394s, episode steps: 741, steps per second:  17, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.015034, mae: 2.426619, mean_q: 2.937989, mean_eps: 0.828872
 476431/500000: episode: 706, duration: 41.794s, episode steps: 705, steps per second:  17, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.014514, mae: 2.451970, mean_q: 2.969830, mean_eps: 0.828612
 476895/500000: episode: 707, duration: 30.694s, episode steps: 464, steps per second:  15, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.014873, mae: 2.467453, mean_q: 2.986612, mean_eps: 0.828402
 477937/500000: episode: 708, duration: 62.317s, episode steps: 1042, steps per second:  17, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.014053, mae: 2.457215, mean_q: 2.974854, mean_eps: 0.828130
 478520/500000: episode: 709, duration: 34.891s, episode steps: 583, steps per second:  17, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.014221, mae: 2.410613, mean_q: 2.921524, mean_eps: 0.827838
 479135/500000: episode: 710, duration: 35.298s, episode steps: 615, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.015001, mae: 2.453570, mean_q: 2.970143, mean_eps: 0.827623
 479852/500000: episode: 711, duration: 42.727s, episode steps: 717, steps per second:  17, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.012785, mae: 2.442073, mean_q: 2.957771, mean_eps: 0.827383
 480483/500000: episode: 712, duration: 36.503s, episode steps: 631, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.012457, mae: 2.426248, mean_q: 2.940743, mean_eps: 0.827140
 480962/500000: episode: 713, duration: 28.773s, episode steps: 479, steps per second:  17, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.012988, mae: 2.426915, mean_q: 2.940283, mean_eps: 0.826940
 481826/500000: episode: 714, duration: 44.251s, episode steps: 864, steps per second:  20, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013615, mae: 2.432931, mean_q: 2.946919, mean_eps: 0.826698
 482658/500000: episode: 715, duration: 40.448s, episode steps: 832, steps per second:  21, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.013652, mae: 2.437053, mean_q: 2.950930, mean_eps: 0.826393
 483690/500000: episode: 716, duration: 50.889s, episode steps: 1032, steps per second:  20, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.013553, mae: 2.436313, mean_q: 2.949511, mean_eps: 0.826057
 484186/500000: episode: 717, duration: 24.826s, episode steps: 496, steps per second:  20, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014805, mae: 2.421887, mean_q: 2.930946, mean_eps: 0.825782
 484717/500000: episode: 718, duration: 26.765s, episode steps: 531, steps per second:  20, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.014834, mae: 2.435942, mean_q: 2.949733, mean_eps: 0.825597
 485708/500000: episode: 719, duration: 49.607s, episode steps: 991, steps per second:  20, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015971, mae: 2.437823, mean_q: 2.952727, mean_eps: 0.825324
 486316/500000: episode: 720, duration: 32.581s, episode steps: 608, steps per second:  19, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.015202, mae: 2.439193, mean_q: 2.953880, mean_eps: 0.825036
 486817/500000: episode: 721, duration: 27.553s, episode steps: 501, steps per second:  18, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014332, mae: 2.426039, mean_q: 2.938494, mean_eps: 0.824836
 487518/500000: episode: 722, duration: 39.036s, episode steps: 701, steps per second:  18, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.017714, mae: 2.458783, mean_q: 2.977408, mean_eps: 0.824620
 488414/500000: episode: 723, duration: 45.326s, episode steps: 896, steps per second:  20, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014296, mae: 2.430417, mean_q: 2.944302, mean_eps: 0.824332
 488989/500000: episode: 724, duration: 27.460s, episode steps: 575, steps per second:  21, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.014927, mae: 2.443411, mean_q: 2.959869, mean_eps: 0.824067
 489340/500000: episode: 725, duration: 18.202s, episode steps: 351, steps per second:  19, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.803 [0.000, 5.000],  loss: 0.013575, mae: 2.423632, mean_q: 2.935439, mean_eps: 0.823901
 489803/500000: episode: 726, duration: 23.392s, episode steps: 463, steps per second:  20, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.013353, mae: 2.443292, mean_q: 2.958815, mean_eps: 0.823755
 490473/500000: episode: 727, duration: 32.050s, episode steps: 670, steps per second:  21, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.012213, mae: 2.424085, mean_q: 2.934523, mean_eps: 0.823550
 490872/500000: episode: 728, duration: 19.948s, episode steps: 399, steps per second:  20, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.015325, mae: 2.434715, mean_q: 2.948458, mean_eps: 0.823358
 491902/500000: episode: 729, duration: 61.595s, episode steps: 1030, steps per second:  17, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.014347, mae: 2.444383, mean_q: 2.959580, mean_eps: 0.823101
 492624/500000: episode: 730, duration: 45.181s, episode steps: 722, steps per second:  16, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.011939, mae: 2.420867, mean_q: 2.933138, mean_eps: 0.822786
 493342/500000: episode: 731, duration: 41.652s, episode steps: 718, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.015731, mae: 2.424000, mean_q: 2.933564, mean_eps: 0.822526
 494566/500000: episode: 732, duration: 71.571s, episode steps: 1224, steps per second:  17, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014588, mae: 2.444971, mean_q: 2.960340, mean_eps: 0.822177
 495249/500000: episode: 733, duration: 40.547s, episode steps: 683, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.013966, mae: 2.435260, mean_q: 2.949923, mean_eps: 0.821833
 495879/500000: episode: 734, duration: 36.999s, episode steps: 630, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014594, mae: 2.414338, mean_q: 2.922006, mean_eps: 0.821597
 497058/500000: episode: 735, duration: 68.394s, episode steps: 1179, steps per second:  17, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.015701, mae: 2.445070, mean_q: 2.959086, mean_eps: 0.821272
 497786/500000: episode: 736, duration: 43.283s, episode steps: 728, steps per second:  17, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.013815, mae: 2.429191, mean_q: 2.943066, mean_eps: 0.820928
 498446/500000: episode: 737, duration: 37.086s, episode steps: 660, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.013869, mae: 2.416050, mean_q: 2.924153, mean_eps: 0.820678
 498858/500000: episode: 738, duration: 20.493s, episode steps: 412, steps per second:  20, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.014408, mae: 2.432655, mean_q: 2.945240, mean_eps: 0.820485
 499574/500000: episode: 739, duration: 37.582s, episode steps: 716, steps per second:  19, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.014806, mae: 2.443388, mean_q: 2.958858, mean_eps: 0.820282
done, took 24242.692 seconds
########################################################
PROCESO TERMINADO
########################################################