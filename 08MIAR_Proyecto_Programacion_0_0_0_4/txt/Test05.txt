	['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_250000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
    420/250000: episode: 1, duration: 3.178s, episode steps: 420, steps per second: 132, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 5.579s, episode steps: 711, steps per second: 127, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 6.851s, episode steps: 810, steps per second: 118, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2809/250000: episode: 4, duration: 7.206s, episode steps: 868, steps per second: 120, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3622/250000: episode: 5, duration: 7.309s, episode steps: 813, steps per second: 111, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4463/250000: episode: 6, duration: 7.204s, episode steps: 841, steps per second: 117, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4961/250000: episode: 7, duration: 6.498s, episode steps: 498, steps per second:  77, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5639/250000: episode: 8, duration: 7.356s, episode steps: 678, steps per second:  92, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6219/250000: episode: 9, duration: 4.958s, episode steps: 580, steps per second: 117, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6620/250000: episode: 10, duration: 3.750s, episode steps: 401, steps per second: 107, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7250/250000: episode: 11, duration: 5.440s, episode steps: 630, steps per second: 116, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8012/250000: episode: 12, duration: 6.289s, episode steps: 762, steps per second: 121, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8968/250000: episode: 13, duration: 7.656s, episode steps: 956, steps per second: 125, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10239/250000: episode: 14, duration: 20.516s, episode steps: 1271, steps per second:  62, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.012112, mae: 0.911273, mean_q: 1.134502, mean_eps: 0.990892
  10867/250000: episode: 15, duration: 29.860s, episode steps: 628, steps per second:  21, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.010422, mae: 0.895247, mean_q: 1.116578, mean_eps: 0.990503
  11751/250000: episode: 16, duration: 39.865s, episode steps: 884, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.009105, mae: 0.902024, mean_q: 1.124333, mean_eps: 0.989823
  13350/250000: episode: 17, duration: 75.170s, episode steps: 1599, steps per second:  21, episode reward: 17.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.008205, mae: 0.908616, mean_q: 1.133788, mean_eps: 0.988705
  13992/250000: episode: 18, duration: 164.501s, episode steps: 642, steps per second:   4, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.007634, mae: 0.895629, mean_q: 1.118369, mean_eps: 0.987697
  14535/250000: episode: 19, duration: 39.487s, episode steps: 543, steps per second:  14, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.006860, mae: 0.882750, mean_q: 1.103730, mean_eps: 0.987164
  15132/250000: episode: 20, duration: 30.400s, episode steps: 597, steps per second:  20, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.007770, mae: 0.890735, mean_q: 1.115500, mean_eps: 0.986651
  15794/250000: episode: 21, duration: 30.748s, episode steps: 662, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.007035, mae: 0.889785, mean_q: 1.114219, mean_eps: 0.986084
  17037/250000: episode: 22, duration: 61.295s, episode steps: 1243, steps per second:  20, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.007300, mae: 0.892126, mean_q: 1.117223, mean_eps: 0.985226
  17676/250000: episode: 23, duration: 32.901s, episode steps: 639, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.006978, mae: 0.895732, mean_q: 1.124414, mean_eps: 0.984380
  18607/250000: episode: 24, duration: 40.943s, episode steps: 931, steps per second:  23, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.007167, mae: 0.885188, mean_q: 1.111305, mean_eps: 0.983674
  19099/250000: episode: 25, duration: 23.225s, episode steps: 492, steps per second:  21, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.009076, mae: 0.884387, mean_q: 1.109819, mean_eps: 0.983033
  19754/250000: episode: 26, duration: 30.089s, episode steps: 655, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.007863, mae: 0.879903, mean_q: 1.105093, mean_eps: 0.982517
  20739/250000: episode: 27, duration: 47.531s, episode steps: 985, steps per second:  21, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.006981, mae: 0.896636, mean_q: 1.128751, mean_eps: 0.981779
  21314/250000: episode: 28, duration: 26.950s, episode steps: 575, steps per second:  21, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.006120, mae: 0.884369, mean_q: 1.111820, mean_eps: 0.981077
  22300/250000: episode: 29, duration: 53.869s, episode steps: 986, steps per second:  18, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.007512, mae: 0.881753, mean_q: 1.108405, mean_eps: 0.980375
  22712/250000: episode: 30, duration: 19.826s, episode steps: 412, steps per second:  21, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.006683, mae: 0.898518, mean_q: 1.128463, mean_eps: 0.979746
  23386/250000: episode: 31, duration: 34.601s, episode steps: 674, steps per second:  19, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.007024, mae: 0.898642, mean_q: 1.130166, mean_eps: 0.979257
  23948/250000: episode: 32, duration: 27.251s, episode steps: 562, steps per second:  21, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.006633, mae: 0.894261, mean_q: 1.125146, mean_eps: 0.978701
  24736/250000: episode: 33, duration: 44.008s, episode steps: 788, steps per second:  18, episode reward:  4.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.007425, mae: 0.900732, mean_q: 1.132224, mean_eps: 0.978094
  25473/250000: episode: 34, duration: 52.643s, episode steps: 737, steps per second:  14, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.006622, mae: 0.897559, mean_q: 1.127395, mean_eps: 0.977406
  26127/250000: episode: 35, duration: 33.920s, episode steps: 654, steps per second:  19, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.007036, mae: 0.891349, mean_q: 1.122102, mean_eps: 0.976780
  27133/250000: episode: 36, duration: 54.461s, episode steps: 1006, steps per second:  18, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.006343, mae: 0.898115, mean_q: 1.129718, mean_eps: 0.976033
  28029/250000: episode: 37, duration: 51.363s, episode steps: 896, steps per second:  17, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.006812, mae: 0.893450, mean_q: 1.123648, mean_eps: 0.975176
  28575/250000: episode: 38, duration: 25.561s, episode steps: 546, steps per second:  21, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.007609, mae: 0.896734, mean_q: 1.128101, mean_eps: 0.974528
  29204/250000: episode: 39, duration: 28.680s, episode steps: 629, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.007662, mae: 0.897964, mean_q: 1.130212, mean_eps: 0.974001
  29862/250000: episode: 40, duration: 29.068s, episode steps: 658, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.006911, mae: 0.888362, mean_q: 1.119449, mean_eps: 0.973421
  30797/250000: episode: 41, duration: 41.980s, episode steps: 935, steps per second:  22, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.006977, mae: 0.904024, mean_q: 1.137991, mean_eps: 0.972703
  31486/250000: episode: 42, duration: 29.638s, episode steps: 689, steps per second:  23, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.006909, mae: 0.884040, mean_q: 1.113004, mean_eps: 0.971972
  32025/250000: episode: 43, duration: 25.294s, episode steps: 539, steps per second:  21, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.007638, mae: 0.909771, mean_q: 1.146906, mean_eps: 0.971420
  32653/250000: episode: 44, duration: 27.593s, episode steps: 628, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.006624, mae: 0.903991, mean_q: 1.138579, mean_eps: 0.970894
  33613/250000: episode: 45, duration: 43.305s, episode steps: 960, steps per second:  22, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.007400, mae: 0.908708, mean_q: 1.143457, mean_eps: 0.970179
  34322/250000: episode: 46, duration: 32.511s, episode steps: 709, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.007699, mae: 0.910197, mean_q: 1.145232, mean_eps: 0.969429
  35064/250000: episode: 47, duration: 36.737s, episode steps: 742, steps per second:  20, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.007455, mae: 0.902485, mean_q: 1.136398, mean_eps: 0.968777
  35526/250000: episode: 48, duration: 21.332s, episode steps: 462, steps per second:  22, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.007494, mae: 0.894102, mean_q: 1.124180, mean_eps: 0.968235
  35885/250000: episode: 49, duration: 16.049s, episode steps: 359, steps per second:  22, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.006133, mae: 0.898797, mean_q: 1.132039, mean_eps: 0.967865
  36339/250000: episode: 50, duration: 19.897s, episode steps: 454, steps per second:  23, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.008685, mae: 0.921431, mean_q: 1.159765, mean_eps: 0.967499
  37051/250000: episode: 51, duration: 30.718s, episode steps: 712, steps per second:  23, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.007832, mae: 0.917215, mean_q: 1.152061, mean_eps: 0.966975
  37731/250000: episode: 52, duration: 31.337s, episode steps: 680, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.007514, mae: 0.897723, mean_q: 1.128998, mean_eps: 0.966349
  38255/250000: episode: 53, duration: 22.359s, episode steps: 524, steps per second:  23, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.006933, mae: 0.899232, mean_q: 1.129633, mean_eps: 0.965807
  39029/250000: episode: 54, duration: 34.455s, episode steps: 774, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.007621, mae: 0.907282, mean_q: 1.141365, mean_eps: 0.965222
  39632/250000: episode: 55, duration: 26.403s, episode steps: 603, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.006159, mae: 0.919775, mean_q: 1.159457, mean_eps: 0.964603
  40266/250000: episode: 56, duration: 28.140s, episode steps: 634, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.006748, mae: 0.899202, mean_q: 1.133154, mean_eps: 0.964047
  40854/250000: episode: 57, duration: 26.876s, episode steps: 588, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.006958, mae: 0.901152, mean_q: 1.134400, mean_eps: 0.963496
  41448/250000: episode: 58, duration: 25.645s, episode steps: 594, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.006924, mae: 0.909946, mean_q: 1.143979, mean_eps: 0.962965
  42870/250000: episode: 59, duration: 66.104s, episode steps: 1422, steps per second:  22, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.007197, mae: 0.899533, mean_q: 1.131604, mean_eps: 0.962058
  43652/250000: episode: 60, duration: 36.965s, episode steps: 782, steps per second:  21, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.007125, mae: 0.895222, mean_q: 1.128274, mean_eps: 0.961066
  44579/250000: episode: 61, duration: 40.700s, episode steps: 927, steps per second:  23, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007296, mae: 0.898392, mean_q: 1.130394, mean_eps: 0.960297
  45202/250000: episode: 62, duration: 28.841s, episode steps: 623, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.007607, mae: 0.898182, mean_q: 1.128025, mean_eps: 0.959599
  46198/250000: episode: 63, duration: 44.491s, episode steps: 996, steps per second:  22, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007661, mae: 0.895456, mean_q: 1.126560, mean_eps: 0.958870
  46815/250000: episode: 64, duration: 27.219s, episode steps: 617, steps per second:  23, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.007744, mae: 0.901362, mean_q: 1.134500, mean_eps: 0.958145
  47534/250000: episode: 65, duration: 31.179s, episode steps: 719, steps per second:  23, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.007397, mae: 0.895270, mean_q: 1.126765, mean_eps: 0.957543
  48187/250000: episode: 66, duration: 29.635s, episode steps: 653, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007330, mae: 0.888024, mean_q: 1.117340, mean_eps: 0.956926
  48777/250000: episode: 67, duration: 26.052s, episode steps: 590, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.007166, mae: 0.908200, mean_q: 1.142851, mean_eps: 0.956366
  49441/250000: episode: 68, duration: 30.171s, episode steps: 664, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.006924, mae: 0.891024, mean_q: 1.120489, mean_eps: 0.955801
  50305/250000: episode: 69, duration: 37.851s, episode steps: 864, steps per second:  23, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.007907, mae: 0.915827, mean_q: 1.150356, mean_eps: 0.955113
  51202/250000: episode: 70, duration: 40.868s, episode steps: 897, steps per second:  22, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.010411, mae: 0.942851, mean_q: 1.183860, mean_eps: 0.954321
  51587/250000: episode: 71, duration: 16.574s, episode steps: 385, steps per second:  23, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.010921, mae: 0.944342, mean_q: 1.184744, mean_eps: 0.953745
  52082/250000: episode: 72, duration: 21.540s, episode steps: 495, steps per second:  23, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.008896, mae: 0.930330, mean_q: 1.169344, mean_eps: 0.953349
  53157/250000: episode: 73, duration: 48.602s, episode steps: 1075, steps per second:  22, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.007956, mae: 0.944440, mean_q: 1.184162, mean_eps: 0.952642
  53827/250000: episode: 74, duration: 30.229s, episode steps: 670, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.008409, mae: 0.954087, mean_q: 1.198230, mean_eps: 0.951857
  54515/250000: episode: 75, duration: 30.182s, episode steps: 688, steps per second:  23, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.008417, mae: 0.932888, mean_q: 1.170700, mean_eps: 0.951247
  54951/250000: episode: 76, duration: 21.687s, episode steps: 436, steps per second:  20, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.007735, mae: 0.948552, mean_q: 1.190548, mean_eps: 0.950741
  55472/250000: episode: 77, duration: 26.309s, episode steps: 521, steps per second:  20, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.008217, mae: 0.956329, mean_q: 1.199924, mean_eps: 0.950311
  55957/250000: episode: 78, duration: 20.822s, episode steps: 485, steps per second:  23, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.008640, mae: 0.944180, mean_q: 1.184162, mean_eps: 0.949857
  56494/250000: episode: 79, duration: 24.555s, episode steps: 537, steps per second:  22, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.008687, mae: 0.946373, mean_q: 1.187878, mean_eps: 0.949397
  56927/250000: episode: 80, duration: 19.365s, episode steps: 433, steps per second:  22, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.008589, mae: 0.932421, mean_q: 1.169384, mean_eps: 0.948961
  57531/250000: episode: 81, duration: 26.509s, episode steps: 604, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.007187, mae: 0.956997, mean_q: 1.201489, mean_eps: 0.948495
  58118/250000: episode: 82, duration: 26.600s, episode steps: 587, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.007142, mae: 0.926524, mean_q: 1.163242, mean_eps: 0.947958
  58822/250000: episode: 83, duration: 30.926s, episode steps: 704, steps per second:  23, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.008907, mae: 0.940290, mean_q: 1.178090, mean_eps: 0.947377
  59601/250000: episode: 84, duration: 35.495s, episode steps: 779, steps per second:  22, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.008585, mae: 0.944723, mean_q: 1.185533, mean_eps: 0.946709
  60142/250000: episode: 85, duration: 24.196s, episode steps: 541, steps per second:  22, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.008480, mae: 0.942967, mean_q: 1.182974, mean_eps: 0.946115
  60587/250000: episode: 86, duration: 19.308s, episode steps: 445, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.007758, mae: 0.953976, mean_q: 1.198001, mean_eps: 0.945672
  61096/250000: episode: 87, duration: 23.882s, episode steps: 509, steps per second:  21, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.008667, mae: 0.941183, mean_q: 1.180237, mean_eps: 0.945244
  61694/250000: episode: 88, duration: 26.263s, episode steps: 598, steps per second:  23, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.007809, mae: 0.948691, mean_q: 1.190670, mean_eps: 0.944745
  62681/250000: episode: 89, duration: 44.683s, episode steps: 987, steps per second:  22, episode reward:  8.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007323, mae: 0.940053, mean_q: 1.180082, mean_eps: 0.944031
  63176/250000: episode: 90, duration: 21.412s, episode steps: 495, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.008194, mae: 0.936767, mean_q: 1.175583, mean_eps: 0.943365
  63794/250000: episode: 91, duration: 27.332s, episode steps: 618, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.007643, mae: 0.943653, mean_q: 1.183815, mean_eps: 0.942864
  64217/250000: episode: 92, duration: 19.043s, episode steps: 423, steps per second:  22, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.006704, mae: 0.939181, mean_q: 1.177522, mean_eps: 0.942395
  64695/250000: episode: 93, duration: 21.343s, episode steps: 478, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.006423, mae: 0.922565, mean_q: 1.157893, mean_eps: 0.941990
  65187/250000: episode: 94, duration: 21.720s, episode steps: 492, steps per second:  23, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.009050, mae: 0.939496, mean_q: 1.177061, mean_eps: 0.941554
  65804/250000: episode: 95, duration: 28.318s, episode steps: 617, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.007119, mae: 0.934104, mean_q: 1.172157, mean_eps: 0.941055
  66367/250000: episode: 96, duration: 23.850s, episode steps: 563, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.008755, mae: 0.938752, mean_q: 1.177370, mean_eps: 0.940524
  67169/250000: episode: 97, duration: 36.055s, episode steps: 802, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.006980, mae: 0.935142, mean_q: 1.173020, mean_eps: 0.939909
  67574/250000: episode: 98, duration: 18.259s, episode steps: 405, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.008730, mae: 0.940041, mean_q: 1.180083, mean_eps: 0.939365
  68219/250000: episode: 99, duration: 28.586s, episode steps: 645, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.007791, mae: 0.925002, mean_q: 1.161100, mean_eps: 0.938894
  69419/250000: episode: 100, duration: 57.003s, episode steps: 1200, steps per second:  21, episode reward: 10.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.007086, mae: 0.928148, mean_q: 1.163564, mean_eps: 0.938064
  70093/250000: episode: 101, duration: 30.875s, episode steps: 674, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.007370, mae: 0.943041, mean_q: 1.182865, mean_eps: 0.937220
  70811/250000: episode: 102, duration: 31.321s, episode steps: 718, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007743, mae: 0.943378, mean_q: 1.184477, mean_eps: 0.936593
  71411/250000: episode: 103, duration: 27.621s, episode steps: 600, steps per second:  22, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.008333, mae: 0.947663, mean_q: 1.189206, mean_eps: 0.936001
  72467/250000: episode: 104, duration: 46.381s, episode steps: 1056, steps per second:  23, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.007837, mae: 0.941983, mean_q: 1.182850, mean_eps: 0.935256
  73396/250000: episode: 105, duration: 41.491s, episode steps: 929, steps per second:  22, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.007405, mae: 0.943201, mean_q: 1.184178, mean_eps: 0.934363
  74310/250000: episode: 106, duration: 42.015s, episode steps: 914, steps per second:  22, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.007947, mae: 0.942654, mean_q: 1.182320, mean_eps: 0.933533
  74678/250000: episode: 107, duration: 16.206s, episode steps: 368, steps per second:  23, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.007014, mae: 0.945111, mean_q: 1.186504, mean_eps: 0.932955
  75190/250000: episode: 108, duration: 21.828s, episode steps: 512, steps per second:  23, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.008114, mae: 0.947141, mean_q: 1.188284, mean_eps: 0.932559
  75853/250000: episode: 109, duration: 29.979s, episode steps: 663, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.007645, mae: 0.929835, mean_q: 1.164918, mean_eps: 0.932030
  76903/250000: episode: 110, duration: 46.103s, episode steps: 1050, steps per second:  23, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.008221, mae: 0.950910, mean_q: 1.191720, mean_eps: 0.931260
  77555/250000: episode: 111, duration: 29.268s, episode steps: 652, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.007482, mae: 0.946833, mean_q: 1.188464, mean_eps: 0.930495
  78249/250000: episode: 112, duration: 30.567s, episode steps: 694, steps per second:  23, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.007830, mae: 0.943450, mean_q: 1.184356, mean_eps: 0.929888
  78779/250000: episode: 113, duration: 23.587s, episode steps: 530, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.007747, mae: 0.944716, mean_q: 1.184430, mean_eps: 0.929337
  79303/250000: episode: 114, duration: 23.063s, episode steps: 524, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.007255, mae: 0.936409, mean_q: 1.174593, mean_eps: 0.928864
  79848/250000: episode: 115, duration: 24.435s, episode steps: 545, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.008677, mae: 0.939321, mean_q: 1.177447, mean_eps: 0.928383
  80685/250000: episode: 116, duration: 37.579s, episode steps: 837, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.006531, mae: 0.946235, mean_q: 1.187017, mean_eps: 0.927761
  81431/250000: episode: 117, duration: 33.744s, episode steps: 746, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.006873, mae: 0.928257, mean_q: 1.165036, mean_eps: 0.927048
  82257/250000: episode: 118, duration: 40.251s, episode steps: 826, steps per second:  21, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.007432, mae: 0.936133, mean_q: 1.174705, mean_eps: 0.926340
  83295/250000: episode: 119, duration: 46.410s, episode steps: 1038, steps per second:  22, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.007461, mae: 0.939231, mean_q: 1.179856, mean_eps: 0.925502
  83804/250000: episode: 120, duration: 22.018s, episode steps: 509, steps per second:  23, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.008025, mae: 0.940426, mean_q: 1.178918, mean_eps: 0.924807
  84332/250000: episode: 121, duration: 24.100s, episode steps: 528, steps per second:  22, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.006658, mae: 0.940662, mean_q: 1.179928, mean_eps: 0.924341
  84722/250000: episode: 122, duration: 18.245s, episode steps: 390, steps per second:  21, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.009262, mae: 0.943370, mean_q: 1.182918, mean_eps: 0.923927
  85351/250000: episode: 123, duration: 27.902s, episode steps: 629, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.007514, mae: 0.934410, mean_q: 1.173384, mean_eps: 0.923468
  85845/250000: episode: 124, duration: 22.142s, episode steps: 494, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.007667, mae: 0.940829, mean_q: 1.180882, mean_eps: 0.922962
  86569/250000: episode: 125, duration: 32.564s, episode steps: 724, steps per second:  22, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.007679, mae: 0.945309, mean_q: 1.184983, mean_eps: 0.922413
  86966/250000: episode: 126, duration: 17.174s, episode steps: 397, steps per second:  23, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.007635, mae: 0.913700, mean_q: 1.145864, mean_eps: 0.921909
  88217/250000: episode: 127, duration: 56.662s, episode steps: 1251, steps per second:  22, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.008166, mae: 0.933053, mean_q: 1.169407, mean_eps: 0.921167
  89156/250000: episode: 128, duration: 41.432s, episode steps: 939, steps per second:  23, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.007120, mae: 0.926533, mean_q: 1.163557, mean_eps: 0.920183
  89935/250000: episode: 129, duration: 34.488s, episode steps: 779, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.007666, mae: 0.932297, mean_q: 1.169200, mean_eps: 0.919410
  90928/250000: episode: 130, duration: 45.267s, episode steps: 993, steps per second:  22, episode reward:  8.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.007823, mae: 0.938877, mean_q: 1.178502, mean_eps: 0.918613
  91377/250000: episode: 131, duration: 20.371s, episode steps: 449, steps per second:  22, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.008356, mae: 0.944496, mean_q: 1.183543, mean_eps: 0.917963
  92066/250000: episode: 132, duration: 30.902s, episode steps: 689, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.006806, mae: 0.941075, mean_q: 1.181037, mean_eps: 0.917450
  92664/250000: episode: 133, duration: 26.192s, episode steps: 598, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007362, mae: 0.941887, mean_q: 1.182312, mean_eps: 0.916872
  93374/250000: episode: 134, duration: 32.239s, episode steps: 710, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.007685, mae: 0.950802, mean_q: 1.193577, mean_eps: 0.916284
  94020/250000: episode: 135, duration: 28.850s, episode steps: 646, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.007194, mae: 0.934244, mean_q: 1.172062, mean_eps: 0.915674
  94759/250000: episode: 136, duration: 34.646s, episode steps: 739, steps per second:  21, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.006876, mae: 0.939023, mean_q: 1.178918, mean_eps: 0.915051
  95238/250000: episode: 137, duration: 24.855s, episode steps: 479, steps per second:  19, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.006724, mae: 0.929223, mean_q: 1.165445, mean_eps: 0.914502
  96103/250000: episode: 138, duration: 38.583s, episode steps: 865, steps per second:  22, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.007547, mae: 0.930419, mean_q: 1.168033, mean_eps: 0.913897
  96768/250000: episode: 139, duration: 29.554s, episode steps: 665, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.006819, mae: 0.938215, mean_q: 1.176703, mean_eps: 0.913209
  97299/250000: episode: 140, duration: 22.971s, episode steps: 531, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.008200, mae: 0.939707, mean_q: 1.178921, mean_eps: 0.912671
  98535/250000: episode: 141, duration: 56.004s, episode steps: 1236, steps per second:  22, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.007399, mae: 0.928553, mean_q: 1.165351, mean_eps: 0.911876
  99246/250000: episode: 142, duration: 32.102s, episode steps: 711, steps per second:  22, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.007898, mae: 0.945703, mean_q: 1.185731, mean_eps: 0.910999
  99847/250000: episode: 143, duration: 26.583s, episode steps: 601, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.007589, mae: 0.922320, mean_q: 1.157428, mean_eps: 0.910409
 100531/250000: episode: 144, duration: 31.504s, episode steps: 684, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.008718, mae: 0.979546, mean_q: 1.226019, mean_eps: 0.909831
 101207/250000: episode: 145, duration: 29.735s, episode steps: 676, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.009074, mae: 0.985999, mean_q: 1.234360, mean_eps: 0.909219
 101718/250000: episode: 146, duration: 23.232s, episode steps: 511, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.008876, mae: 0.984405, mean_q: 1.232830, mean_eps: 0.908684
 102168/250000: episode: 147, duration: 20.556s, episode steps: 450, steps per second:  22, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.008312, mae: 0.995600, mean_q: 1.246942, mean_eps: 0.908252
 102837/250000: episode: 148, duration: 29.713s, episode steps: 669, steps per second:  23, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.008826, mae: 0.984496, mean_q: 1.232728, mean_eps: 0.907748
 103230/250000: episode: 149, duration: 17.567s, episode steps: 393, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.008289, mae: 0.978664, mean_q: 1.225781, mean_eps: 0.907269
 104311/250000: episode: 150, duration: 48.233s, episode steps: 1081, steps per second:  22, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009584, mae: 0.988779, mean_q: 1.237736, mean_eps: 0.906607
 105018/250000: episode: 151, duration: 32.245s, episode steps: 707, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.009035, mae: 0.997400, mean_q: 1.247706, mean_eps: 0.905802
 105516/250000: episode: 152, duration: 21.656s, episode steps: 498, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.008719, mae: 0.999028, mean_q: 1.251510, mean_eps: 0.905261
 106052/250000: episode: 153, duration: 23.800s, episode steps: 536, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.008658, mae: 0.978074, mean_q: 1.226122, mean_eps: 0.904796
 106532/250000: episode: 154, duration: 22.520s, episode steps: 480, steps per second:  21, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008918, mae: 0.983739, mean_q: 1.230303, mean_eps: 0.904339
 107190/250000: episode: 155, duration: 29.459s, episode steps: 658, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.008447, mae: 0.988016, mean_q: 1.236153, mean_eps: 0.903826
 107669/250000: episode: 156, duration: 21.952s, episode steps: 479, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.009874, mae: 0.990725, mean_q: 1.242630, mean_eps: 0.903313
 108769/250000: episode: 157, duration: 54.895s, episode steps: 1100, steps per second:  20, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.008572, mae: 0.986274, mean_q: 1.235365, mean_eps: 0.902602
 109322/250000: episode: 158, duration: 26.247s, episode steps: 553, steps per second:  21, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.009624, mae: 0.983862, mean_q: 1.230987, mean_eps: 0.901859
 109778/250000: episode: 159, duration: 19.759s, episode steps: 456, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.008923, mae: 0.985887, mean_q: 1.233311, mean_eps: 0.901405
 110421/250000: episode: 160, duration: 29.432s, episode steps: 643, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.009113, mae: 0.996509, mean_q: 1.246718, mean_eps: 0.900910
 111174/250000: episode: 161, duration: 33.699s, episode steps: 753, steps per second:  22, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.008876, mae: 0.990742, mean_q: 1.239311, mean_eps: 0.900282
 111773/250000: episode: 162, duration: 26.041s, episode steps: 599, steps per second:  23, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.008714, mae: 0.987422, mean_q: 1.235663, mean_eps: 0.899673
 112447/250000: episode: 163, duration: 30.652s, episode steps: 674, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.008624, mae: 0.989163, mean_q: 1.239037, mean_eps: 0.899101
 113240/250000: episode: 164, duration: 35.040s, episode steps: 793, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.008573, mae: 0.981932, mean_q: 1.230379, mean_eps: 0.898442
 113974/250000: episode: 165, duration: 33.446s, episode steps: 734, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.008713, mae: 0.995448, mean_q: 1.243637, mean_eps: 0.897755
 114642/250000: episode: 166, duration: 29.681s, episode steps: 668, steps per second:  23, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.007977, mae: 0.983762, mean_q: 1.230949, mean_eps: 0.897123
 115184/250000: episode: 167, duration: 25.104s, episode steps: 542, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.010003, mae: 0.978011, mean_q: 1.225502, mean_eps: 0.896579
 116108/250000: episode: 168, duration: 41.229s, episode steps: 924, steps per second:  22, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.007999, mae: 0.996507, mean_q: 1.249526, mean_eps: 0.895920
 116725/250000: episode: 169, duration: 28.666s, episode steps: 617, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.008525, mae: 0.979149, mean_q: 1.224485, mean_eps: 0.895226
 117214/250000: episode: 170, duration: 21.892s, episode steps: 489, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.008496, mae: 0.992296, mean_q: 1.240181, mean_eps: 0.894727
 117724/250000: episode: 171, duration: 22.888s, episode steps: 510, steps per second:  22, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.008535, mae: 0.980862, mean_q: 1.228204, mean_eps: 0.894279
 118373/250000: episode: 172, duration: 29.823s, episode steps: 649, steps per second:  22, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.008116, mae: 0.990700, mean_q: 1.241794, mean_eps: 0.893757
 119253/250000: episode: 173, duration: 39.740s, episode steps: 880, steps per second:  22, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.008431, mae: 0.983788, mean_q: 1.231995, mean_eps: 0.893067
 120224/250000: episode: 174, duration: 44.025s, episode steps: 971, steps per second:  22, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.008486, mae: 0.984881, mean_q: 1.231584, mean_eps: 0.892236
 120854/250000: episode: 175, duration: 29.549s, episode steps: 630, steps per second:  21, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.007608, mae: 1.006979, mean_q: 1.259576, mean_eps: 0.891516
 121535/250000: episode: 176, duration: 32.501s, episode steps: 681, steps per second:  21, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.008584, mae: 0.990905, mean_q: 1.239982, mean_eps: 0.890925
 123053/250000: episode: 177, duration: 73.300s, episode steps: 1518, steps per second:  21, episode reward: 15.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.008585, mae: 0.991090, mean_q: 1.238822, mean_eps: 0.889935
 123650/250000: episode: 178, duration: 28.145s, episode steps: 597, steps per second:  21, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.009214, mae: 0.991412, mean_q: 1.240275, mean_eps: 0.888983
 124477/250000: episode: 179, duration: 37.749s, episode steps: 827, steps per second:  22, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.008645, mae: 0.985281, mean_q: 1.233000, mean_eps: 0.888342
 125286/250000: episode: 180, duration: 36.906s, episode steps: 809, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.008953, mae: 0.987055, mean_q: 1.233908, mean_eps: 0.887606
 125912/250000: episode: 181, duration: 28.793s, episode steps: 626, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.008040, mae: 0.965139, mean_q: 1.208858, mean_eps: 0.886962
 126294/250000: episode: 182, duration: 17.163s, episode steps: 382, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.007963, mae: 0.987952, mean_q: 1.236555, mean_eps: 0.886508
 127198/250000: episode: 183, duration: 41.176s, episode steps: 904, steps per second:  22, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.008832, mae: 0.991124, mean_q: 1.239572, mean_eps: 0.885929
 127638/250000: episode: 184, duration: 19.571s, episode steps: 440, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.007874, mae: 0.981213, mean_q: 1.228147, mean_eps: 0.885324
 128329/250000: episode: 185, duration: 31.820s, episode steps: 691, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.008053, mae: 0.980727, mean_q: 1.227791, mean_eps: 0.884814
 128880/250000: episode: 186, duration: 24.454s, episode steps: 551, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.007734, mae: 0.969511, mean_q: 1.212318, mean_eps: 0.884256
 129554/250000: episode: 187, duration: 31.011s, episode steps: 674, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.009143, mae: 0.983237, mean_q: 1.228679, mean_eps: 0.883706
 130135/250000: episode: 188, duration: 26.388s, episode steps: 581, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.008125, mae: 1.007512, mean_q: 1.260471, mean_eps: 0.883140
 130751/250000: episode: 189, duration: 28.351s, episode steps: 616, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.008820, mae: 0.990028, mean_q: 1.239462, mean_eps: 0.882602
 131457/250000: episode: 190, duration: 31.901s, episode steps: 706, steps per second:  22, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.008831, mae: 0.988181, mean_q: 1.235358, mean_eps: 0.882006
 132096/250000: episode: 191, duration: 28.897s, episode steps: 639, steps per second:  22, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.009471, mae: 0.995282, mean_q: 1.244765, mean_eps: 0.881402
 133287/250000: episode: 192, duration: 54.069s, episode steps: 1191, steps per second:  22, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.007999, mae: 1.000359, mean_q: 1.251557, mean_eps: 0.880579
 134566/250000: episode: 193, duration: 59.972s, episode steps: 1279, steps per second:  21, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.008437, mae: 0.991288, mean_q: 1.240268, mean_eps: 0.879467
 135090/250000: episode: 194, duration: 23.588s, episode steps: 524, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.008593, mae: 0.989093, mean_q: 1.238036, mean_eps: 0.878655
 136027/250000: episode: 195, duration: 45.664s, episode steps: 937, steps per second:  21, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.009309, mae: 0.999881, mean_q: 1.250670, mean_eps: 0.877998
 136804/250000: episode: 196, duration: 35.781s, episode steps: 777, steps per second:  22, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.008391, mae: 0.987986, mean_q: 1.236158, mean_eps: 0.877227
 137265/250000: episode: 197, duration: 20.797s, episode steps: 461, steps per second:  22, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.007807, mae: 0.984048, mean_q: 1.232670, mean_eps: 0.876669
 137842/250000: episode: 198, duration: 25.858s, episode steps: 577, steps per second:  22, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.008523, mae: 0.996366, mean_q: 1.246372, mean_eps: 0.876201
 138493/250000: episode: 199, duration: 29.960s, episode steps: 651, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.008421, mae: 0.982484, mean_q: 1.228855, mean_eps: 0.875649
 138840/250000: episode: 200, duration: 15.320s, episode steps: 347, steps per second:  23, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.008693, mae: 0.974796, mean_q: 1.216876, mean_eps: 0.875201
 139569/250000: episode: 201, duration: 34.210s, episode steps: 729, steps per second:  21, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.008814, mae: 0.989838, mean_q: 1.237810, mean_eps: 0.874716
 140886/250000: episode: 202, duration: 74.373s, episode steps: 1317, steps per second:  18, episode reward: 15.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.008804, mae: 0.996568, mean_q: 1.250061, mean_eps: 0.873795
 141472/250000: episode: 203, duration: 32.942s, episode steps: 586, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.007299, mae: 0.984423, mean_q: 1.234145, mean_eps: 0.872940
 142087/250000: episode: 204, duration: 33.287s, episode steps: 615, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.009534, mae: 0.990602, mean_q: 1.241896, mean_eps: 0.872400
 142848/250000: episode: 205, duration: 40.992s, episode steps: 761, steps per second:  19, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009218, mae: 0.985694, mean_q: 1.233383, mean_eps: 0.871781
 143751/250000: episode: 206, duration: 48.392s, episode steps: 903, steps per second:  19, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.009277, mae: 0.984103, mean_q: 1.232380, mean_eps: 0.871032
 144221/250000: episode: 207, duration: 25.212s, episode steps: 470, steps per second:  19, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.007086, mae: 0.995083, mean_q: 1.248222, mean_eps: 0.870413
 144607/250000: episode: 208, duration: 20.794s, episode steps: 386, steps per second:  19, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.008172, mae: 0.988407, mean_q: 1.237175, mean_eps: 0.870027
 145239/250000: episode: 209, duration: 35.700s, episode steps: 632, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.009246, mae: 0.992379, mean_q: 1.241539, mean_eps: 0.869570
 145857/250000: episode: 210, duration: 35.125s, episode steps: 618, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.008265, mae: 0.989120, mean_q: 1.237123, mean_eps: 0.869007
 146967/250000: episode: 211, duration: 51.350s, episode steps: 1110, steps per second:  22, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.008524, mae: 0.987688, mean_q: 1.235984, mean_eps: 0.868229
 147933/250000: episode: 212, duration: 46.859s, episode steps: 966, steps per second:  21, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.008206, mae: 0.988004, mean_q: 1.234530, mean_eps: 0.867295
 149093/250000: episode: 213, duration: 55.355s, episode steps: 1160, steps per second:  21, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.007950, mae: 0.986212, mean_q: 1.233503, mean_eps: 0.866337
 149798/250000: episode: 214, duration: 33.881s, episode steps: 705, steps per second:  21, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.007805, mae: 0.987546, mean_q: 1.236171, mean_eps: 0.865499
 150467/250000: episode: 215, duration: 30.349s, episode steps: 669, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.009474, mae: 1.022158, mean_q: 1.276205, mean_eps: 0.864881
 151359/250000: episode: 216, duration: 41.399s, episode steps: 892, steps per second:  22, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.010136, mae: 1.028139, mean_q: 1.283876, mean_eps: 0.864179
 151951/250000: episode: 217, duration: 27.030s, episode steps: 592, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.009711, mae: 1.030488, mean_q: 1.286578, mean_eps: 0.863511
 152730/250000: episode: 218, duration: 36.102s, episode steps: 779, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.009604, mae: 1.031894, mean_q: 1.288330, mean_eps: 0.862894
 153271/250000: episode: 219, duration: 23.889s, episode steps: 541, steps per second:  23, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.009442, mae: 1.038104, mean_q: 1.295743, mean_eps: 0.862300
 153874/250000: episode: 220, duration: 27.683s, episode steps: 603, steps per second:  22, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.008933, mae: 1.026641, mean_q: 1.280315, mean_eps: 0.861785
 154401/250000: episode: 221, duration: 24.388s, episode steps: 527, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.010190, mae: 1.014405, mean_q: 1.265556, mean_eps: 0.861276
 155298/250000: episode: 222, duration: 40.063s, episode steps: 897, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.009329, mae: 1.020099, mean_q: 1.272501, mean_eps: 0.860635
 155677/250000: episode: 223, duration: 17.341s, episode steps: 379, steps per second:  22, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.009946, mae: 1.028219, mean_q: 1.284358, mean_eps: 0.860061
 157065/250000: episode: 224, duration: 64.770s, episode steps: 1388, steps per second:  21, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.008777, mae: 1.029579, mean_q: 1.286738, mean_eps: 0.859265
 157613/250000: episode: 225, duration: 24.903s, episode steps: 548, steps per second:  22, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.008935, mae: 1.030797, mean_q: 1.287188, mean_eps: 0.858394
 158289/250000: episode: 226, duration: 31.622s, episode steps: 676, steps per second:  21, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.009507, mae: 1.029109, mean_q: 1.283618, mean_eps: 0.857843
 159542/250000: episode: 227, duration: 56.757s, episode steps: 1253, steps per second:  22, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.009243, mae: 1.033338, mean_q: 1.290297, mean_eps: 0.856976
 160034/250000: episode: 228, duration: 23.061s, episode steps: 492, steps per second:  21, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.009045, mae: 1.038687, mean_q: 1.297421, mean_eps: 0.856191
 160768/250000: episode: 229, duration: 36.833s, episode steps: 734, steps per second:  20, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.009593, mae: 1.031954, mean_q: 1.290816, mean_eps: 0.855640
 161227/250000: episode: 230, duration: 21.751s, episode steps: 459, steps per second:  21, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.009191, mae: 1.048669, mean_q: 1.310128, mean_eps: 0.855104
 162088/250000: episode: 231, duration: 38.773s, episode steps: 861, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.010133, mae: 1.042692, mean_q: 1.302884, mean_eps: 0.854510
 163370/250000: episode: 232, duration: 59.370s, episode steps: 1282, steps per second:  22, episode reward: 14.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.008385, mae: 1.025429, mean_q: 1.281987, mean_eps: 0.853545
 163755/250000: episode: 233, duration: 17.917s, episode steps: 385, steps per second:  21, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.009393, mae: 1.033592, mean_q: 1.290093, mean_eps: 0.852794
 164450/250000: episode: 234, duration: 33.249s, episode steps: 695, steps per second:  21, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.009045, mae: 1.033735, mean_q: 1.291798, mean_eps: 0.852308
 165199/250000: episode: 235, duration: 34.222s, episode steps: 749, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008460, mae: 1.030717, mean_q: 1.287771, mean_eps: 0.851658
 166159/250000: episode: 236, duration: 44.249s, episode steps: 960, steps per second:  22, episode reward:  7.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.009071, mae: 1.040451, mean_q: 1.299036, mean_eps: 0.850890
 166709/250000: episode: 237, duration: 25.936s, episode steps: 550, steps per second:  21, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.009222, mae: 1.026073, mean_q: 1.279702, mean_eps: 0.850209
 167100/250000: episode: 238, duration: 18.046s, episode steps: 391, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.008965, mae: 1.035269, mean_q: 1.292144, mean_eps: 0.849786
 167888/250000: episode: 239, duration: 35.869s, episode steps: 788, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.009565, mae: 1.029984, mean_q: 1.286080, mean_eps: 0.849257
 168727/250000: episode: 240, duration: 38.367s, episode steps: 839, steps per second:  22, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010029, mae: 1.024592, mean_q: 1.280258, mean_eps: 0.848525
 169333/250000: episode: 241, duration: 27.869s, episode steps: 606, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.008792, mae: 1.018419, mean_q: 1.271858, mean_eps: 0.847873
 169714/250000: episode: 242, duration: 17.785s, episode steps: 381, steps per second:  21, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.009356, mae: 1.035255, mean_q: 1.293486, mean_eps: 0.847428
 170103/250000: episode: 243, duration: 17.748s, episode steps: 389, steps per second:  22, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.011301, mae: 1.032508, mean_q: 1.288747, mean_eps: 0.847083
 171064/250000: episode: 244, duration: 45.128s, episode steps: 961, steps per second:  21, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.009249, mae: 1.032473, mean_q: 1.290326, mean_eps: 0.846476
 171564/250000: episode: 245, duration: 22.389s, episode steps: 500, steps per second:  22, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.009112, mae: 1.032687, mean_q: 1.290264, mean_eps: 0.845819
 172338/250000: episode: 246, duration: 40.131s, episode steps: 774, steps per second:  19, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.007850, mae: 1.021466, mean_q: 1.278992, mean_eps: 0.845245
 172983/250000: episode: 247, duration: 29.563s, episode steps: 645, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.008328, mae: 1.029402, mean_q: 1.285453, mean_eps: 0.844606
 174023/250000: episode: 248, duration: 51.288s, episode steps: 1040, steps per second:  20, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.008973, mae: 1.018179, mean_q: 1.271190, mean_eps: 0.843848
 174591/250000: episode: 249, duration: 25.441s, episode steps: 568, steps per second:  22, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.009948, mae: 1.013940, mean_q: 1.264445, mean_eps: 0.843125
 175242/250000: episode: 250, duration: 30.540s, episode steps: 651, steps per second:  21, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.009020, mae: 1.030757, mean_q: 1.285055, mean_eps: 0.842576
 176452/250000: episode: 251, duration: 55.596s, episode steps: 1210, steps per second:  22, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.009024, mae: 1.022458, mean_q: 1.274992, mean_eps: 0.841739
 177091/250000: episode: 252, duration: 29.783s, episode steps: 639, steps per second:  21, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.009698, mae: 1.041230, mean_q: 1.298522, mean_eps: 0.840907
 177599/250000: episode: 253, duration: 22.821s, episode steps: 508, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.009549, mae: 1.039404, mean_q: 1.296721, mean_eps: 0.840390
 178049/250000: episode: 254, duration: 21.788s, episode steps: 450, steps per second:  21, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.008389, mae: 1.025805, mean_q: 1.279588, mean_eps: 0.839958
 178850/250000: episode: 255, duration: 36.720s, episode steps: 801, steps per second:  22, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.009002, mae: 1.031061, mean_q: 1.285871, mean_eps: 0.839395
 179348/250000: episode: 256, duration: 22.913s, episode steps: 498, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.008938, mae: 1.025367, mean_q: 1.279329, mean_eps: 0.838812
 179897/250000: episode: 257, duration: 27.400s, episode steps: 549, steps per second:  20, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.009349, mae: 1.042803, mean_q: 1.300974, mean_eps: 0.838340
 180797/250000: episode: 258, duration: 41.403s, episode steps: 900, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.009657, mae: 1.036324, mean_q: 1.294173, mean_eps: 0.837687
 181287/250000: episode: 259, duration: 22.864s, episode steps: 490, steps per second:  21, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010574, mae: 1.044587, mean_q: 1.303994, mean_eps: 0.837062
 182191/250000: episode: 260, duration: 42.065s, episode steps: 904, steps per second:  21, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.008778, mae: 1.019721, mean_q: 1.272663, mean_eps: 0.836436
 182535/250000: episode: 261, duration: 16.435s, episode steps: 344, steps per second:  21, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.008999, mae: 1.019381, mean_q: 1.273289, mean_eps: 0.835874
 183161/250000: episode: 262, duration: 29.628s, episode steps: 626, steps per second:  21, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.008193, mae: 1.028560, mean_q: 1.284332, mean_eps: 0.835437
 184266/250000: episode: 263, duration: 51.867s, episode steps: 1105, steps per second:  21, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.008811, mae: 1.021804, mean_q: 1.275720, mean_eps: 0.834657
 184994/250000: episode: 264, duration: 33.935s, episode steps: 728, steps per second:  21, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.009912, mae: 1.022798, mean_q: 1.275364, mean_eps: 0.833833
 185584/250000: episode: 265, duration: 27.881s, episode steps: 590, steps per second:  21, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.008705, mae: 1.037228, mean_q: 1.294200, mean_eps: 0.833241
 186348/250000: episode: 266, duration: 42.233s, episode steps: 764, steps per second:  18, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.008684, mae: 1.018445, mean_q: 1.271310, mean_eps: 0.832632
 186881/250000: episode: 267, duration: 28.575s, episode steps: 533, steps per second:  19, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.007911, mae: 1.033906, mean_q: 1.288687, mean_eps: 0.832047
 187715/250000: episode: 268, duration: 46.640s, episode steps: 834, steps per second:  18, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.008848, mae: 1.034119, mean_q: 1.290405, mean_eps: 0.831432
 188510/250000: episode: 269, duration: 43.242s, episode steps: 795, steps per second:  18, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009340, mae: 1.041425, mean_q: 1.300902, mean_eps: 0.830699
 189293/250000: episode: 270, duration: 42.676s, episode steps: 783, steps per second:  18, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.008477, mae: 1.025347, mean_q: 1.278717, mean_eps: 0.829988
 189686/250000: episode: 271, duration: 22.210s, episode steps: 393, steps per second:  18, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.009578, mae: 1.032517, mean_q: 1.288925, mean_eps: 0.829459
 190364/250000: episode: 272, duration: 36.805s, episode steps: 678, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.009251, mae: 1.026978, mean_q: 1.281711, mean_eps: 0.828978
 191209/250000: episode: 273, duration: 40.647s, episode steps: 845, steps per second:  21, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.007775, mae: 1.027864, mean_q: 1.283239, mean_eps: 0.828293
 191644/250000: episode: 274, duration: 21.693s, episode steps: 435, steps per second:  20, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.009606, mae: 1.030925, mean_q: 1.286181, mean_eps: 0.827717
 192504/250000: episode: 275, duration: 40.709s, episode steps: 860, steps per second:  21, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.008342, mae: 1.022291, mean_q: 1.275782, mean_eps: 0.827135
 193403/250000: episode: 276, duration: 43.803s, episode steps: 899, steps per second:  21, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.008227, mae: 1.019880, mean_q: 1.273138, mean_eps: 0.826343
 194502/250000: episode: 277, duration: 59.653s, episode steps: 1099, steps per second:  18, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.009812, mae: 1.034453, mean_q: 1.289457, mean_eps: 0.825443
 195429/250000: episode: 278, duration: 51.160s, episode steps: 927, steps per second:  18, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.008446, mae: 1.030282, mean_q: 1.284948, mean_eps: 0.824531
 195987/250000: episode: 279, duration: 31.826s, episode steps: 558, steps per second:  18, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.008679, mae: 1.022889, mean_q: 1.276190, mean_eps: 0.823863
 196912/250000: episode: 280, duration: 45.738s, episode steps: 925, steps per second:  20, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.009309, mae: 1.037414, mean_q: 1.293522, mean_eps: 0.823197
 197309/250000: episode: 281, duration: 18.695s, episode steps: 397, steps per second:  21, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.009782, mae: 1.023764, mean_q: 1.277408, mean_eps: 0.822601
 197696/250000: episode: 282, duration: 22.753s, episode steps: 387, steps per second:  17, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.009284, mae: 1.046131, mean_q: 1.306745, mean_eps: 0.822248
 198176/250000: episode: 283, duration: 22.305s, episode steps: 480, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.008783, mae: 1.039460, mean_q: 1.297602, mean_eps: 0.821859
 198681/250000: episode: 284, duration: 23.117s, episode steps: 505, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.009079, mae: 1.023752, mean_q: 1.278826, mean_eps: 0.821415
 199886/250000: episode: 285, duration: 56.982s, episode steps: 1205, steps per second:  21, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.009795, mae: 1.027063, mean_q: 1.281624, mean_eps: 0.820644
 200511/250000: episode: 286, duration: 31.077s, episode steps: 625, steps per second:  20, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.009801, mae: 1.089705, mean_q: 1.357037, mean_eps: 0.819822
 201497/250000: episode: 287, duration: 55.533s, episode steps: 986, steps per second:  18, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.009278, mae: 1.091302, mean_q: 1.360536, mean_eps: 0.819096
 201895/250000: episode: 288, duration: 22.515s, episode steps: 398, steps per second:  18, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.011007, mae: 1.091806, mean_q: 1.359271, mean_eps: 0.818474
 202365/250000: episode: 289, duration: 25.179s, episode steps: 470, steps per second:  19, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.010383, mae: 1.093396, mean_q: 1.361445, mean_eps: 0.818083
 202709/250000: episode: 290, duration: 19.265s, episode steps: 344, steps per second:  18, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.009077, mae: 1.094163, mean_q: 1.364797, mean_eps: 0.817716
 203452/250000: episode: 291, duration: 40.684s, episode steps: 743, steps per second:  18, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.009053, mae: 1.086938, mean_q: 1.354570, mean_eps: 0.817228
 203798/250000: episode: 292, duration: 22.645s, episode steps: 346, steps per second:  15, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.009992, mae: 1.100028, mean_q: 1.370107, mean_eps: 0.816738
 204893/250000: episode: 293, duration: 65.503s, episode steps: 1095, steps per second:  17, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.010246, mae: 1.096513, mean_q: 1.366946, mean_eps: 0.816089
 205443/250000: episode: 294, duration: 31.541s, episode steps: 550, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.008972, mae: 1.083183, mean_q: 1.350207, mean_eps: 0.815349
 205955/250000: episode: 295, duration: 33.021s, episode steps: 512, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.009935, mae: 1.093201, mean_q: 1.362366, mean_eps: 0.814872
 206688/250000: episode: 296, duration: 36.130s, episode steps: 733, steps per second:  20, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.010131, mae: 1.089286, mean_q: 1.357493, mean_eps: 0.814312
 207082/250000: episode: 297, duration: 23.983s, episode steps: 394, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010512, mae: 1.067101, mean_q: 1.329284, mean_eps: 0.813804
 207481/250000: episode: 298, duration: 23.773s, episode steps: 399, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.010109, mae: 1.099893, mean_q: 1.371765, mean_eps: 0.813446
 207984/250000: episode: 299, duration: 28.207s, episode steps: 503, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.009103, mae: 1.108043, mean_q: 1.380752, mean_eps: 0.813041
 208399/250000: episode: 300, duration: 24.154s, episode steps: 415, steps per second:  17, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.010511, mae: 1.105213, mean_q: 1.375820, mean_eps: 0.812629
 208761/250000: episode: 301, duration: 27.851s, episode steps: 362, steps per second:  13, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.009387, mae: 1.077481, mean_q: 1.340188, mean_eps: 0.812278
 209362/250000: episode: 302, duration: 34.753s, episode steps: 601, steps per second:  17, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.009892, mae: 1.087345, mean_q: 1.352941, mean_eps: 0.811844
 209994/250000: episode: 303, duration: 38.804s, episode steps: 632, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.010522, mae: 1.102324, mean_q: 1.372255, mean_eps: 0.811290
 210615/250000: episode: 304, duration: 31.816s, episode steps: 621, steps per second:  20, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.009266, mae: 1.096701, mean_q: 1.365815, mean_eps: 0.810726
 211385/250000: episode: 305, duration: 38.446s, episode steps: 770, steps per second:  20, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.009878, mae: 1.102452, mean_q: 1.373678, mean_eps: 0.810100
 212010/250000: episode: 306, duration: 31.697s, episode steps: 625, steps per second:  20, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.009614, mae: 1.095770, mean_q: 1.364531, mean_eps: 0.809472
 213397/250000: episode: 307, duration: 68.947s, episode steps: 1387, steps per second:  20, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.008543, mae: 1.097953, mean_q: 1.366323, mean_eps: 0.808566
 213795/250000: episode: 308, duration: 22.625s, episode steps: 398, steps per second:  18, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.008630, mae: 1.079172, mean_q: 1.343674, mean_eps: 0.807764
 214345/250000: episode: 309, duration: 31.944s, episode steps: 550, steps per second:  17, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.009299, mae: 1.096936, mean_q: 1.366068, mean_eps: 0.807337
 214887/250000: episode: 310, duration: 27.960s, episode steps: 542, steps per second:  19, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.009796, mae: 1.096867, mean_q: 1.365532, mean_eps: 0.806846
 215292/250000: episode: 311, duration: 22.772s, episode steps: 405, steps per second:  18, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.009202, mae: 1.084687, mean_q: 1.348643, mean_eps: 0.806421
 215799/250000: episode: 312, duration: 28.537s, episode steps: 507, steps per second:  18, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009103, mae: 1.096867, mean_q: 1.365412, mean_eps: 0.806010
 216512/250000: episode: 313, duration: 36.408s, episode steps: 713, steps per second:  20, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.009038, mae: 1.104300, mean_q: 1.375883, mean_eps: 0.805461
 217326/250000: episode: 314, duration: 41.127s, episode steps: 814, steps per second:  20, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.009428, mae: 1.093129, mean_q: 1.359786, mean_eps: 0.804774
 218040/250000: episode: 315, duration: 83.492s, episode steps: 714, steps per second:   9, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010185, mae: 1.098861, mean_q: 1.368006, mean_eps: 0.804086
 218817/250000: episode: 316, duration: 71.432s, episode steps: 777, steps per second:  11, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.009325, mae: 1.090911, mean_q: 1.357290, mean_eps: 0.803415
 219494/250000: episode: 317, duration: 40.861s, episode steps: 677, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010296, mae: 1.101358, mean_q: 1.371189, mean_eps: 0.802760
 220143/250000: episode: 318, duration: 35.817s, episode steps: 649, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.009359, mae: 1.102557, mean_q: 1.370837, mean_eps: 0.802164
 220644/250000: episode: 319, duration: 33.124s, episode steps: 501, steps per second:  15, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.009494, mae: 1.095673, mean_q: 1.363372, mean_eps: 0.801647
 221375/250000: episode: 320, duration: 41.420s, episode steps: 731, steps per second:  18, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.009603, mae: 1.095165, mean_q: 1.364085, mean_eps: 0.801093
 222171/250000: episode: 321, duration: 42.161s, episode steps: 796, steps per second:  19, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.009056, mae: 1.086932, mean_q: 1.353902, mean_eps: 0.800405
 222850/250000: episode: 322, duration: 35.306s, episode steps: 679, steps per second:  19, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.009368, mae: 1.100210, mean_q: 1.367822, mean_eps: 0.799741
 223388/250000: episode: 323, duration: 25.509s, episode steps: 538, steps per second:  21, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.009710, mae: 1.083908, mean_q: 1.346983, mean_eps: 0.799194
 223941/250000: episode: 324, duration: 26.643s, episode steps: 553, steps per second:  21, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010234, mae: 1.103992, mean_q: 1.373744, mean_eps: 0.798702
 224842/250000: episode: 325, duration: 43.799s, episode steps: 901, steps per second:  21, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.009218, mae: 1.091270, mean_q: 1.357704, mean_eps: 0.798047
 225942/250000: episode: 326, duration: 55.044s, episode steps: 1100, steps per second:  20, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.008755, mae: 1.090389, mean_q: 1.355365, mean_eps: 0.797147
 226613/250000: episode: 327, duration: 32.207s, episode steps: 671, steps per second:  21, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010506, mae: 1.102262, mean_q: 1.371701, mean_eps: 0.796350
 227210/250000: episode: 328, duration: 29.416s, episode steps: 597, steps per second:  20, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.008451, mae: 1.103333, mean_q: 1.372496, mean_eps: 0.795779
 227875/250000: episode: 329, duration: 32.157s, episode steps: 665, steps per second:  21, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.009073, mae: 1.082188, mean_q: 1.346360, mean_eps: 0.795212
 228568/250000: episode: 330, duration: 35.890s, episode steps: 693, steps per second:  19, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008100, mae: 1.091253, mean_q: 1.358726, mean_eps: 0.794602
 229087/250000: episode: 331, duration: 24.155s, episode steps: 519, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.009083, mae: 1.080634, mean_q: 1.344285, mean_eps: 0.794057
 230072/250000: episode: 332, duration: 47.581s, episode steps: 985, steps per second:  21, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.009544, mae: 1.092520, mean_q: 1.359660, mean_eps: 0.793380
 230872/250000: episode: 333, duration: 41.757s, episode steps: 800, steps per second:  19, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.009469, mae: 1.097883, mean_q: 1.365527, mean_eps: 0.792577
 231452/250000: episode: 334, duration: 28.333s, episode steps: 580, steps per second:  20, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.009368, mae: 1.095248, mean_q: 1.363446, mean_eps: 0.791956
 232280/250000: episode: 335, duration: 42.219s, episode steps: 828, steps per second:  20, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009122, mae: 1.102939, mean_q: 1.370883, mean_eps: 0.791322
 233067/250000: episode: 336, duration: 38.256s, episode steps: 787, steps per second:  21, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.009211, mae: 1.107203, mean_q: 1.378442, mean_eps: 0.790595
 233938/250000: episode: 337, duration: 41.699s, episode steps: 871, steps per second:  21, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.009634, mae: 1.106018, mean_q: 1.375916, mean_eps: 0.789848
 234336/250000: episode: 338, duration: 19.162s, episode steps: 398, steps per second:  21, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.009766, mae: 1.119037, mean_q: 1.392908, mean_eps: 0.789278
 235572/250000: episode: 339, duration: 59.357s, episode steps: 1236, steps per second:  21, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.009086, mae: 1.100152, mean_q: 1.367770, mean_eps: 0.788543
 235962/250000: episode: 340, duration: 20.564s, episode steps: 390, steps per second:  19, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.010325, mae: 1.098992, mean_q: 1.367912, mean_eps: 0.787811
 236357/250000: episode: 341, duration: 19.383s, episode steps: 395, steps per second:  20, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.009510, mae: 1.095081, mean_q: 1.363325, mean_eps: 0.787456
 237061/250000: episode: 342, duration: 32.734s, episode steps: 704, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008810, mae: 1.094430, mean_q: 1.362499, mean_eps: 0.786961
 237584/250000: episode: 343, duration: 25.395s, episode steps: 523, steps per second:  21, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.008389, mae: 1.094262, mean_q: 1.360669, mean_eps: 0.786410
 238575/250000: episode: 344, duration: 47.434s, episode steps: 991, steps per second:  21, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.009533, mae: 1.101859, mean_q: 1.371241, mean_eps: 0.785730
 239286/250000: episode: 345, duration: 34.664s, episode steps: 711, steps per second:  21, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.010022, mae: 1.085957, mean_q: 1.350139, mean_eps: 0.784963
 239735/250000: episode: 346, duration: 21.041s, episode steps: 449, steps per second:  21, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.008800, mae: 1.087946, mean_q: 1.354349, mean_eps: 0.784441
 240484/250000: episode: 347, duration: 43.854s, episode steps: 749, steps per second:  17, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.009487, mae: 1.080230, mean_q: 1.344849, mean_eps: 0.783903
 241715/250000: episode: 348, duration: 77.553s, episode steps: 1231, steps per second:  16, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.009284, mae: 1.086751, mean_q: 1.351537, mean_eps: 0.783012
 242225/250000: episode: 349, duration: 37.417s, episode steps: 510, steps per second:  14, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.009117, mae: 1.111319, mean_q: 1.384091, mean_eps: 0.782227
 243113/250000: episode: 350, duration: 60.857s, episode steps: 888, steps per second:  15, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.008625, mae: 1.085279, mean_q: 1.349363, mean_eps: 0.781597
 244194/250000: episode: 351, duration: 68.909s, episode steps: 1081, steps per second:  16, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008899, mae: 1.099959, mean_q: 1.366616, mean_eps: 0.780711
 244717/250000: episode: 352, duration: 39.284s, episode steps: 523, steps per second:  13, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009464, mae: 1.096256, mean_q: 1.363191, mean_eps: 0.779990
 245364/250000: episode: 353, duration: 41.484s, episode steps: 647, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.009332, mae: 1.090785, mean_q: 1.356228, mean_eps: 0.779464
 245713/250000: episode: 354, duration: 22.400s, episode steps: 349, steps per second:  16, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.008628, mae: 1.116316, mean_q: 1.389593, mean_eps: 0.779016
 246374/250000: episode: 355, duration: 42.910s, episode steps: 661, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.009021, mae: 1.081867, mean_q: 1.345143, mean_eps: 0.778560
 247121/250000: episode: 356, duration: 51.895s, episode steps: 747, steps per second:  14, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.009840, mae: 1.096243, mean_q: 1.362554, mean_eps: 0.777927
 248083/250000: episode: 357, duration: 66.437s, episode steps: 962, steps per second:  14, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009013, mae: 1.104145, mean_q: 1.374817, mean_eps: 0.777158
 248582/250000: episode: 358, duration: 29.849s, episode steps: 499, steps per second:  17, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.008699, mae: 1.076640, mean_q: 1.339146, mean_eps: 0.776501
 249381/250000: episode: 359, duration: 41.865s, episode steps: 799, steps per second:  19, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.008750, mae: 1.093423, mean_q: 1.359195, mean_eps: 0.775916
 249773/250000: episode: 360, duration: 20.512s, episode steps: 392, steps per second:  19, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.009957, mae: 1.092962, mean_q: 1.360821, mean_eps: 0.775380
done, took 11969.398 seconds
########################################################
PROCESO TERMINADO
########################################################