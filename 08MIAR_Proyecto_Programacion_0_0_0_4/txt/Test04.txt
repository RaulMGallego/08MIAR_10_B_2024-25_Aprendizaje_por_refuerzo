['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_50000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
    420/250000: episode: 1, duration: 3.191s, episode steps: 420, steps per second: 132, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 4.232s, episode steps: 711, steps per second: 168, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 5.809s, episode steps: 810, steps per second: 139, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2809/250000: episode: 4, duration: 5.937s, episode steps: 868, steps per second: 146, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3786/250000: episode: 5, duration: 6.015s, episode steps: 977, steps per second: 162, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4436/250000: episode: 6, duration: 4.008s, episode steps: 650, steps per second: 162, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5349/250000: episode: 7, duration: 6.504s, episode steps: 913, steps per second: 140, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5825/250000: episode: 8, duration: 2.982s, episode steps: 476, steps per second: 160, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6765/250000: episode: 9, duration: 5.584s, episode steps: 940, steps per second: 168, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8002/250000: episode: 10, duration: 8.034s, episode steps: 1237, steps per second: 154, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8415/250000: episode: 11, duration: 2.540s, episode steps: 413, steps per second: 163, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8913/250000: episode: 12, duration: 2.972s, episode steps: 498, steps per second: 168, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9506/250000: episode: 13, duration: 3.621s, episode steps: 593, steps per second: 164, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10398/250000: episode: 14, duration: 22.064s, episode steps: 892, steps per second:  40, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.010522, mae: 0.671316, mean_q: 0.843932, mean_eps: 0.990820
  10862/250000: episode: 15, duration: 19.812s, episode steps: 464, steps per second:  23, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010414, mae: 0.666835, mean_q: 0.836122, mean_eps: 0.990433
  11905/250000: episode: 16, duration: 46.435s, episode steps: 1043, steps per second:  22, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.010894, mae: 0.671758, mean_q: 0.838955, mean_eps: 0.989754
  12272/250000: episode: 17, duration: 15.749s, episode steps: 367, steps per second:  23, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.010543, mae: 0.654771, mean_q: 0.815849, mean_eps: 0.989121
  13195/250000: episode: 18, duration: 39.411s, episode steps: 923, steps per second:  23, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.009723, mae: 0.661978, mean_q: 0.827037, mean_eps: 0.988541
  13615/250000: episode: 19, duration: 18.953s, episode steps: 420, steps per second:  22, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.009182, mae: 0.669751, mean_q: 0.836944, mean_eps: 0.987936
  14447/250000: episode: 20, duration: 35.444s, episode steps: 832, steps per second:  23, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.008739, mae: 0.663339, mean_q: 0.830006, mean_eps: 0.987373
  14962/250000: episode: 21, duration: 22.466s, episode steps: 515, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.008803, mae: 0.658691, mean_q: 0.825596, mean_eps: 0.986766
  15659/250000: episode: 22, duration: 33.562s, episode steps: 697, steps per second:  21, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.007758, mae: 0.662245, mean_q: 0.829668, mean_eps: 0.986221
  16675/250000: episode: 23, duration: 48.974s, episode steps: 1016, steps per second:  21, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.007899, mae: 0.656618, mean_q: 0.823684, mean_eps: 0.985451
  17472/250000: episode: 24, duration: 40.416s, episode steps: 797, steps per second:  20, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.008113, mae: 0.659747, mean_q: 0.827822, mean_eps: 0.984635
  18073/250000: episode: 25, duration: 29.744s, episode steps: 601, steps per second:  20, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.007400, mae: 0.650240, mean_q: 0.816291, mean_eps: 0.984005
  18724/250000: episode: 26, duration: 28.317s, episode steps: 651, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.006926, mae: 0.654866, mean_q: 0.822435, mean_eps: 0.983442
  19683/250000: episode: 27, duration: 43.570s, episode steps: 959, steps per second:  22, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.008274, mae: 0.658273, mean_q: 0.828494, mean_eps: 0.982718
  20119/250000: episode: 28, duration: 19.047s, episode steps: 436, steps per second:  23, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.006661, mae: 0.640484, mean_q: 0.806155, mean_eps: 0.982090
  21053/250000: episode: 29, duration: 42.784s, episode steps: 934, steps per second:  22, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.007094, mae: 0.644284, mean_q: 0.811350, mean_eps: 0.981473
  21732/250000: episode: 30, duration: 30.307s, episode steps: 679, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.007210, mae: 0.651022, mean_q: 0.818646, mean_eps: 0.980747
  22373/250000: episode: 31, duration: 29.108s, episode steps: 641, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.007652, mae: 0.645680, mean_q: 0.813223, mean_eps: 0.980153
  23095/250000: episode: 32, duration: 31.150s, episode steps: 722, steps per second:  23, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.007541, mae: 0.643681, mean_q: 0.809342, mean_eps: 0.979539
  23490/250000: episode: 33, duration: 17.070s, episode steps: 395, steps per second:  23, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.006867, mae: 0.648542, mean_q: 0.816006, mean_eps: 0.979037
  24664/250000: episode: 34, duration: 51.583s, episode steps: 1174, steps per second:  23, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.007293, mae: 0.648503, mean_q: 0.816803, mean_eps: 0.978332
  25398/250000: episode: 35, duration: 35.969s, episode steps: 734, steps per second:  20, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.007354, mae: 0.652524, mean_q: 0.822786, mean_eps: 0.977473
  26102/250000: episode: 36, duration: 30.312s, episode steps: 704, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.007285, mae: 0.653541, mean_q: 0.826735, mean_eps: 0.976825
  26499/250000: episode: 37, duration: 17.457s, episode steps: 397, steps per second:  23, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.007938, mae: 0.650641, mean_q: 0.823621, mean_eps: 0.976330
  27210/250000: episode: 38, duration: 29.969s, episode steps: 711, steps per second:  24, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.006620, mae: 0.649768, mean_q: 0.820286, mean_eps: 0.975831
  28418/250000: episode: 39, duration: 53.437s, episode steps: 1208, steps per second:  23, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.006440, mae: 0.649155, mean_q: 0.819219, mean_eps: 0.974967
  29066/250000: episode: 40, duration: 28.483s, episode steps: 648, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.007117, mae: 0.652734, mean_q: 0.824157, mean_eps: 0.974132
  29645/250000: episode: 41, duration: 27.448s, episode steps: 579, steps per second:  21, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.006579, mae: 0.651803, mean_q: 0.820999, mean_eps: 0.973580
  30270/250000: episode: 42, duration: 27.010s, episode steps: 625, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.006532, mae: 0.656894, mean_q: 0.829526, mean_eps: 0.973038
  30647/250000: episode: 43, duration: 17.206s, episode steps: 377, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.006757, mae: 0.651333, mean_q: 0.822195, mean_eps: 0.972588
  31245/250000: episode: 44, duration: 27.858s, episode steps: 598, steps per second:  21, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.006567, mae: 0.647543, mean_q: 0.818388, mean_eps: 0.972149
  31753/250000: episode: 45, duration: 22.551s, episode steps: 508, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.006741, mae: 0.644652, mean_q: 0.813445, mean_eps: 0.971650
  32137/250000: episode: 46, duration: 18.171s, episode steps: 384, steps per second:  21, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.007684, mae: 0.647894, mean_q: 0.818346, mean_eps: 0.971249
  32632/250000: episode: 47, duration: 22.868s, episode steps: 495, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.006823, mae: 0.649719, mean_q: 0.820245, mean_eps: 0.970854
  33343/250000: episode: 48, duration: 30.829s, episode steps: 711, steps per second:  23, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.006346, mae: 0.642322, mean_q: 0.811141, mean_eps: 0.970313
  33875/250000: episode: 49, duration: 24.354s, episode steps: 532, steps per second:  22, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.007048, mae: 0.648230, mean_q: 0.817419, mean_eps: 0.969753
  34802/250000: episode: 50, duration: 39.265s, episode steps: 927, steps per second:  24, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.006687, mae: 0.642986, mean_q: 0.811787, mean_eps: 0.969096
  35192/250000: episode: 51, duration: 17.862s, episode steps: 390, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.007387, mae: 0.651105, mean_q: 0.821533, mean_eps: 0.968504
  36133/250000: episode: 52, duration: 40.978s, episode steps: 941, steps per second:  23, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.007211, mae: 0.644571, mean_q: 0.814344, mean_eps: 0.967904
  36668/250000: episode: 53, duration: 24.278s, episode steps: 535, steps per second:  22, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.007740, mae: 0.650461, mean_q: 0.822143, mean_eps: 0.967240
  37611/250000: episode: 54, duration: 42.099s, episode steps: 943, steps per second:  22, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.006424, mae: 0.644249, mean_q: 0.815337, mean_eps: 0.966576
  38475/250000: episode: 55, duration: 40.582s, episode steps: 864, steps per second:  21, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.007250, mae: 0.646285, mean_q: 0.816606, mean_eps: 0.965762
  39559/250000: episode: 56, duration: 48.214s, episode steps: 1084, steps per second:  22, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.007346, mae: 0.646282, mean_q: 0.816071, mean_eps: 0.964886
  40076/250000: episode: 57, duration: 22.696s, episode steps: 517, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.008125, mae: 0.653398, mean_q: 0.823823, mean_eps: 0.964166
  41008/250000: episode: 58, duration: 41.130s, episode steps: 932, steps per second:  23, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.006838, mae: 0.650977, mean_q: 0.822562, mean_eps: 0.963514
  41388/250000: episode: 59, duration: 17.754s, episode steps: 380, steps per second:  21, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.006855, mae: 0.657950, mean_q: 0.832168, mean_eps: 0.962924
  42175/250000: episode: 60, duration: 34.038s, episode steps: 787, steps per second:  23, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.006615, mae: 0.655379, mean_q: 0.827432, mean_eps: 0.962398
  42812/250000: episode: 61, duration: 29.608s, episode steps: 637, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.007170, mae: 0.654862, mean_q: 0.827068, mean_eps: 0.961757
  43320/250000: episode: 62, duration: 22.455s, episode steps: 508, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.006577, mae: 0.643780, mean_q: 0.812982, mean_eps: 0.961242
  44019/250000: episode: 63, duration: 36.654s, episode steps: 699, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.006245, mae: 0.639610, mean_q: 0.807927, mean_eps: 0.960699
  44849/250000: episode: 64, duration: 41.619s, episode steps: 830, steps per second:  20, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.006787, mae: 0.653640, mean_q: 0.825170, mean_eps: 0.960009
  45604/250000: episode: 65, duration: 37.030s, episode steps: 755, steps per second:  20, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.006879, mae: 0.651352, mean_q: 0.822159, mean_eps: 0.959297
  46300/250000: episode: 66, duration: 32.061s, episode steps: 696, steps per second:  22, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.007869, mae: 0.652666, mean_q: 0.823645, mean_eps: 0.958645
  46943/250000: episode: 67, duration: 35.859s, episode steps: 643, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.006879, mae: 0.654211, mean_q: 0.826765, mean_eps: 0.958042
  47908/250000: episode: 68, duration: 60.767s, episode steps: 965, steps per second:  16, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.006933, mae: 0.653584, mean_q: 0.825519, mean_eps: 0.957318
  48599/250000: episode: 69, duration: 37.094s, episode steps: 691, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.007619, mae: 0.662122, mean_q: 0.835788, mean_eps: 0.956573
  49166/250000: episode: 70, duration: 32.601s, episode steps: 567, steps per second:  17, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.007614, mae: 0.655355, mean_q: 0.828476, mean_eps: 0.956006
  49712/250000: episode: 71, duration: 30.433s, episode steps: 546, steps per second:  18, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007269, mae: 0.656248, mean_q: 0.830142, mean_eps: 0.955506
  50252/250000: episode: 72, duration: 29.910s, episode steps: 540, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.008149, mae: 0.658720, mean_q: 0.830148, mean_eps: 0.955018
  50876/250000: episode: 73, duration: 31.322s, episode steps: 624, steps per second:  20, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.009174, mae: 0.676817, mean_q: 0.853117, mean_eps: 0.954494
  51387/250000: episode: 74, duration: 24.435s, episode steps: 511, steps per second:  21, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.008685, mae: 0.681624, mean_q: 0.858107, mean_eps: 0.953983
  51868/250000: episode: 75, duration: 22.084s, episode steps: 481, steps per second:  22, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.007222, mae: 0.667043, mean_q: 0.839616, mean_eps: 0.953537
  52370/250000: episode: 76, duration: 24.960s, episode steps: 502, steps per second:  20, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.008313, mae: 0.669853, mean_q: 0.844743, mean_eps: 0.953094
  53004/250000: episode: 77, duration: 30.237s, episode steps: 634, steps per second:  21, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008619, mae: 0.667784, mean_q: 0.841115, mean_eps: 0.952583
  53886/250000: episode: 78, duration: 40.728s, episode steps: 882, steps per second:  22, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.008461, mae: 0.672862, mean_q: 0.848627, mean_eps: 0.951900
  54565/250000: episode: 79, duration: 45.799s, episode steps: 679, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.008095, mae: 0.666847, mean_q: 0.840633, mean_eps: 0.951197
  55398/250000: episode: 80, duration: 47.992s, episode steps: 833, steps per second:  17, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.008325, mae: 0.680454, mean_q: 0.858103, mean_eps: 0.950516
  56057/250000: episode: 81, duration: 30.062s, episode steps: 659, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.008228, mae: 0.681931, mean_q: 0.861502, mean_eps: 0.949845
  56636/250000: episode: 82, duration: 28.948s, episode steps: 579, steps per second:  20, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.008200, mae: 0.673189, mean_q: 0.849455, mean_eps: 0.949289
  57372/250000: episode: 83, duration: 32.439s, episode steps: 736, steps per second:  23, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.007629, mae: 0.666853, mean_q: 0.842286, mean_eps: 0.948698
  57973/250000: episode: 84, duration: 27.123s, episode steps: 601, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.007518, mae: 0.665936, mean_q: 0.840648, mean_eps: 0.948095
  58605/250000: episode: 85, duration: 27.976s, episode steps: 632, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007443, mae: 0.678426, mean_q: 0.856854, mean_eps: 0.947539
  59202/250000: episode: 86, duration: 26.483s, episode steps: 597, steps per second:  23, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.007387, mae: 0.676017, mean_q: 0.853985, mean_eps: 0.946986
  59818/250000: episode: 87, duration: 27.605s, episode steps: 616, steps per second:  22, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007078, mae: 0.670639, mean_q: 0.846979, mean_eps: 0.946441
  60470/250000: episode: 88, duration: 28.493s, episode steps: 652, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.008074, mae: 0.671382, mean_q: 0.847144, mean_eps: 0.945870
  61041/250000: episode: 89, duration: 29.663s, episode steps: 571, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.008172, mae: 0.682342, mean_q: 0.859244, mean_eps: 0.945320
  61706/250000: episode: 90, duration: 29.165s, episode steps: 665, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.006879, mae: 0.677923, mean_q: 0.855989, mean_eps: 0.944763
  62258/250000: episode: 91, duration: 25.948s, episode steps: 552, steps per second:  21, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.007365, mae: 0.668316, mean_q: 0.843453, mean_eps: 0.944216
  62949/250000: episode: 92, duration: 31.323s, episode steps: 691, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.008182, mae: 0.675696, mean_q: 0.852647, mean_eps: 0.943656
  63295/250000: episode: 93, duration: 15.985s, episode steps: 346, steps per second:  22, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007710, mae: 0.686020, mean_q: 0.867659, mean_eps: 0.943190
  63793/250000: episode: 94, duration: 23.101s, episode steps: 498, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.006799, mae: 0.678446, mean_q: 0.856373, mean_eps: 0.942810
  64341/250000: episode: 95, duration: 24.669s, episode steps: 548, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.007568, mae: 0.674873, mean_q: 0.851789, mean_eps: 0.942339
  64968/250000: episode: 96, duration: 27.394s, episode steps: 627, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.007404, mae: 0.679630, mean_q: 0.858383, mean_eps: 0.941811
  65324/250000: episode: 97, duration: 17.050s, episode steps: 356, steps per second:  21, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.006715, mae: 0.685565, mean_q: 0.866953, mean_eps: 0.941370
  66064/250000: episode: 98, duration: 35.886s, episode steps: 740, steps per second:  21, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.007454, mae: 0.676892, mean_q: 0.854200, mean_eps: 0.940877
  66692/250000: episode: 99, duration: 29.846s, episode steps: 628, steps per second:  21, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.006613, mae: 0.681363, mean_q: 0.860440, mean_eps: 0.940262
  67423/250000: episode: 100, duration: 34.432s, episode steps: 731, steps per second:  21, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.007606, mae: 0.686707, mean_q: 0.866694, mean_eps: 0.939650
  68129/250000: episode: 101, duration: 32.575s, episode steps: 706, steps per second:  22, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.007448, mae: 0.680419, mean_q: 0.859555, mean_eps: 0.939002
  69111/250000: episode: 102, duration: 44.846s, episode steps: 982, steps per second:  22, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.007207, mae: 0.685126, mean_q: 0.866002, mean_eps: 0.938242
  69528/250000: episode: 103, duration: 19.243s, episode steps: 417, steps per second:  22, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.006458, mae: 0.666602, mean_q: 0.842415, mean_eps: 0.937614
  70028/250000: episode: 104, duration: 23.978s, episode steps: 500, steps per second:  21, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.007077, mae: 0.677119, mean_q: 0.855277, mean_eps: 0.937202
  70920/250000: episode: 105, duration: 41.313s, episode steps: 892, steps per second:  22, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.007287, mae: 0.681073, mean_q: 0.859328, mean_eps: 0.936575
  72074/250000: episode: 106, duration: 52.430s, episode steps: 1154, steps per second:  22, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.007343, mae: 0.681411, mean_q: 0.861456, mean_eps: 0.935654
  72853/250000: episode: 107, duration: 34.602s, episode steps: 779, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.007555, mae: 0.682528, mean_q: 0.861945, mean_eps: 0.934782
  73560/250000: episode: 108, duration: 35.996s, episode steps: 707, steps per second:  20, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.007914, mae: 0.695626, mean_q: 0.878214, mean_eps: 0.934115
  74256/250000: episode: 109, duration: 35.053s, episode steps: 696, steps per second:  20, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.007893, mae: 0.691492, mean_q: 0.872365, mean_eps: 0.933485
  74884/250000: episode: 110, duration: 35.211s, episode steps: 628, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.006761, mae: 0.678910, mean_q: 0.857025, mean_eps: 0.932889
  75468/250000: episode: 111, duration: 34.633s, episode steps: 584, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.007649, mae: 0.677271, mean_q: 0.854648, mean_eps: 0.932343
  75982/250000: episode: 112, duration: 27.816s, episode steps: 514, steps per second:  18, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.007145, mae: 0.687468, mean_q: 0.868609, mean_eps: 0.931848
  76316/250000: episode: 113, duration: 17.511s, episode steps: 334, steps per second:  19, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.006349, mae: 0.679744, mean_q: 0.857773, mean_eps: 0.931467
  76891/250000: episode: 114, duration: 29.589s, episode steps: 575, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.006681, mae: 0.675113, mean_q: 0.850581, mean_eps: 0.931058
  78353/250000: episode: 115, duration: 73.847s, episode steps: 1462, steps per second:  20, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007962, mae: 0.684611, mean_q: 0.863528, mean_eps: 0.930140
  79287/250000: episode: 116, duration: 49.937s, episode steps: 934, steps per second:  19, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007298, mae: 0.669475, mean_q: 0.844867, mean_eps: 0.929062
  79790/250000: episode: 117, duration: 29.081s, episode steps: 503, steps per second:  17, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.007173, mae: 0.678132, mean_q: 0.855822, mean_eps: 0.928416
  80666/250000: episode: 118, duration: 47.989s, episode steps: 876, steps per second:  18, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.006420, mae: 0.679118, mean_q: 0.858066, mean_eps: 0.927795
  81498/250000: episode: 119, duration: 55.811s, episode steps: 832, steps per second:  15, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.006743, mae: 0.684159, mean_q: 0.862969, mean_eps: 0.927026
  82030/250000: episode: 120, duration: 25.818s, episode steps: 532, steps per second:  21, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.007367, mae: 0.677939, mean_q: 0.855599, mean_eps: 0.926412
  82638/250000: episode: 121, duration: 26.881s, episode steps: 608, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.007080, mae: 0.679556, mean_q: 0.856633, mean_eps: 0.925899
  83314/250000: episode: 122, duration: 29.298s, episode steps: 676, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.006657, mae: 0.685239, mean_q: 0.865357, mean_eps: 0.925322
  83703/250000: episode: 123, duration: 17.742s, episode steps: 389, steps per second:  22, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.007667, mae: 0.686384, mean_q: 0.866988, mean_eps: 0.924843
  84287/250000: episode: 124, duration: 25.945s, episode steps: 584, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.007339, mae: 0.681977, mean_q: 0.862406, mean_eps: 0.924405
  84957/250000: episode: 125, duration: 29.248s, episode steps: 670, steps per second:  23, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.007746, mae: 0.672713, mean_q: 0.848542, mean_eps: 0.923840
  85725/250000: episode: 126, duration: 34.953s, episode steps: 768, steps per second:  22, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.007171, mae: 0.678407, mean_q: 0.856778, mean_eps: 0.923192
  86470/250000: episode: 127, duration: 32.703s, episode steps: 745, steps per second:  23, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.007076, mae: 0.676780, mean_q: 0.854052, mean_eps: 0.922512
  87105/250000: episode: 128, duration: 30.335s, episode steps: 635, steps per second:  21, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.007124, mae: 0.673103, mean_q: 0.850039, mean_eps: 0.921891
  88071/250000: episode: 129, duration: 42.403s, episode steps: 966, steps per second:  23, episode reward:  8.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007368, mae: 0.678855, mean_q: 0.856647, mean_eps: 0.921171
  89110/250000: episode: 130, duration: 46.092s, episode steps: 1039, steps per second:  23, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.006972, mae: 0.676677, mean_q: 0.853435, mean_eps: 0.920269
  89750/250000: episode: 131, duration: 28.924s, episode steps: 640, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.006653, mae: 0.680101, mean_q: 0.858196, mean_eps: 0.919513
  90724/250000: episode: 132, duration: 40.713s, episode steps: 974, steps per second:  24, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.007331, mae: 0.683833, mean_q: 0.861804, mean_eps: 0.918788
  91285/250000: episode: 133, duration: 24.199s, episode steps: 561, steps per second:  23, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.006729, mae: 0.679799, mean_q: 0.857712, mean_eps: 0.918096
  91907/250000: episode: 134, duration: 26.528s, episode steps: 622, steps per second:  23, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.006691, mae: 0.674413, mean_q: 0.852277, mean_eps: 0.917564
  92797/250000: episode: 135, duration: 38.070s, episode steps: 890, steps per second:  23, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.007055, mae: 0.675612, mean_q: 0.852306, mean_eps: 0.916883
  93192/250000: episode: 136, duration: 16.917s, episode steps: 395, steps per second:  23, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.006220, mae: 0.682396, mean_q: 0.859972, mean_eps: 0.916305
  93717/250000: episode: 137, duration: 23.783s, episode steps: 525, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.007753, mae: 0.681905, mean_q: 0.860127, mean_eps: 0.915891
  94171/250000: episode: 138, duration: 19.100s, episode steps: 454, steps per second:  24, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.006580, mae: 0.681375, mean_q: 0.859146, mean_eps: 0.915450
  94678/250000: episode: 139, duration: 21.618s, episode steps: 507, steps per second:  23, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.007383, mae: 0.685882, mean_q: 0.864392, mean_eps: 0.915018
  95064/250000: episode: 140, duration: 16.123s, episode steps: 386, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.007473, mae: 0.674211, mean_q: 0.851854, mean_eps: 0.914617
  95444/250000: episode: 141, duration: 15.867s, episode steps: 380, steps per second:  24, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.007523, mae: 0.686707, mean_q: 0.865796, mean_eps: 0.914273
  96222/250000: episode: 142, duration: 34.782s, episode steps: 778, steps per second:  22, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.007632, mae: 0.689583, mean_q: 0.869559, mean_eps: 0.913751
  96606/250000: episode: 143, duration: 15.976s, episode steps: 384, steps per second:  24, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.006714, mae: 0.685422, mean_q: 0.865101, mean_eps: 0.913227
  97522/250000: episode: 144, duration: 41.335s, episode steps: 916, steps per second:  22, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.008079, mae: 0.688177, mean_q: 0.867902, mean_eps: 0.912642
  97863/250000: episode: 145, duration: 14.390s, episode steps: 341, steps per second:  24, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.007491, mae: 0.688000, mean_q: 0.868272, mean_eps: 0.912077
  98969/250000: episode: 146, duration: 51.637s, episode steps: 1106, steps per second:  21, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.007063, mae: 0.674310, mean_q: 0.851535, mean_eps: 0.911426
  99404/250000: episode: 147, duration: 18.906s, episode steps: 435, steps per second:  23, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.007606, mae: 0.687277, mean_q: 0.867976, mean_eps: 0.910733
 100136/250000: episode: 148, duration: 32.625s, episode steps: 732, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.007244, mae: 0.687683, mean_q: 0.867367, mean_eps: 0.910209
 100762/250000: episode: 149, duration: 27.358s, episode steps: 626, steps per second:  23, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.008522, mae: 0.728396, mean_q: 0.914706, mean_eps: 0.909597
 101167/250000: episode: 150, duration: 16.620s, episode steps: 405, steps per second:  24, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.007627, mae: 0.734490, mean_q: 0.925301, mean_eps: 0.909132
 101765/250000: episode: 151, duration: 26.434s, episode steps: 598, steps per second:  23, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.008635, mae: 0.736233, mean_q: 0.928651, mean_eps: 0.908681
 102224/250000: episode: 152, duration: 20.005s, episode steps: 459, steps per second:  23, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.008528, mae: 0.733291, mean_q: 0.925359, mean_eps: 0.908205
 102632/250000: episode: 153, duration: 17.247s, episode steps: 408, steps per second:  24, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.009585, mae: 0.718637, mean_q: 0.903213, mean_eps: 0.907817
 103309/250000: episode: 154, duration: 30.418s, episode steps: 677, steps per second:  22, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.007677, mae: 0.734178, mean_q: 0.923571, mean_eps: 0.907327
 103944/250000: episode: 155, duration: 27.907s, episode steps: 635, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.008643, mae: 0.734192, mean_q: 0.922986, mean_eps: 0.906737
 104829/250000: episode: 156, duration: 38.881s, episode steps: 885, steps per second:  23, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.007913, mae: 0.727863, mean_q: 0.915481, mean_eps: 0.906053
 105932/250000: episode: 157, duration: 50.505s, episode steps: 1103, steps per second:  22, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.007814, mae: 0.727001, mean_q: 0.914467, mean_eps: 0.905158
 106794/250000: episode: 158, duration: 38.689s, episode steps: 862, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.007661, mae: 0.735305, mean_q: 0.926293, mean_eps: 0.904274
 107321/250000: episode: 159, duration: 22.762s, episode steps: 527, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.007777, mae: 0.726241, mean_q: 0.912733, mean_eps: 0.903648
 108500/250000: episode: 160, duration: 52.105s, episode steps: 1179, steps per second:  23, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.007861, mae: 0.736963, mean_q: 0.926929, mean_eps: 0.902881
 109027/250000: episode: 161, duration: 23.350s, episode steps: 527, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.007510, mae: 0.740566, mean_q: 0.932972, mean_eps: 0.902114
 109814/250000: episode: 162, duration: 35.461s, episode steps: 787, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.007756, mae: 0.733417, mean_q: 0.922381, mean_eps: 0.901522
 110364/250000: episode: 163, duration: 23.465s, episode steps: 550, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.006712, mae: 0.729059, mean_q: 0.917878, mean_eps: 0.900921
 110906/250000: episode: 164, duration: 23.932s, episode steps: 542, steps per second:  23, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.007962, mae: 0.731855, mean_q: 0.921662, mean_eps: 0.900429
 111920/250000: episode: 165, duration: 43.629s, episode steps: 1014, steps per second:  23, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.008902, mae: 0.737471, mean_q: 0.927759, mean_eps: 0.899729
 112443/250000: episode: 166, duration: 23.027s, episode steps: 523, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.007392, mae: 0.717915, mean_q: 0.901581, mean_eps: 0.899038
 113008/250000: episode: 167, duration: 24.367s, episode steps: 565, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.008694, mae: 0.730927, mean_q: 0.919958, mean_eps: 0.898548
 113644/250000: episode: 168, duration: 29.537s, episode steps: 636, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.008336, mae: 0.725546, mean_q: 0.912727, mean_eps: 0.898008
 114373/250000: episode: 169, duration: 32.521s, episode steps: 729, steps per second:  22, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.006363, mae: 0.718209, mean_q: 0.903944, mean_eps: 0.897393
 115141/250000: episode: 170, duration: 33.609s, episode steps: 768, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.007377, mae: 0.719589, mean_q: 0.904583, mean_eps: 0.896718
 115678/250000: episode: 171, duration: 23.684s, episode steps: 537, steps per second:  23, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.008559, mae: 0.728242, mean_q: 0.916497, mean_eps: 0.896131
 116125/250000: episode: 172, duration: 19.437s, episode steps: 447, steps per second:  23, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.007327, mae: 0.722790, mean_q: 0.911039, mean_eps: 0.895688
 116741/250000: episode: 173, duration: 27.207s, episode steps: 616, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.008252, mae: 0.731008, mean_q: 0.921966, mean_eps: 0.895209
 117947/250000: episode: 174, duration: 54.745s, episode steps: 1206, steps per second:  22, episode reward: 11.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.007726, mae: 0.722050, mean_q: 0.909895, mean_eps: 0.894390
 118396/250000: episode: 175, duration: 26.058s, episode steps: 449, steps per second:  17, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.007131, mae: 0.726815, mean_q: 0.916491, mean_eps: 0.893647
 118889/250000: episode: 176, duration: 24.874s, episode steps: 493, steps per second:  20, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.008373, mae: 0.730760, mean_q: 0.920733, mean_eps: 0.893222
 119336/250000: episode: 177, duration: 22.883s, episode steps: 447, steps per second:  20, episode reward: 11.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.008063, mae: 0.721553, mean_q: 0.910431, mean_eps: 0.892799
 119954/250000: episode: 178, duration: 30.987s, episode steps: 618, steps per second:  20, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.008182, mae: 0.732225, mean_q: 0.922966, mean_eps: 0.892320
 121476/250000: episode: 179, duration: 76.194s, episode steps: 1522, steps per second:  20, episode reward: 17.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.007599, mae: 0.735760, mean_q: 0.928030, mean_eps: 0.891357
 121961/250000: episode: 180, duration: 21.833s, episode steps: 485, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.008041, mae: 0.738829, mean_q: 0.931561, mean_eps: 0.890454
 122653/250000: episode: 181, duration: 29.301s, episode steps: 692, steps per second:  24, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.007362, mae: 0.725841, mean_q: 0.913340, mean_eps: 0.889923
 123928/250000: episode: 182, duration: 56.094s, episode steps: 1275, steps per second:  23, episode reward: 13.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.007285, mae: 0.727998, mean_q: 0.916614, mean_eps: 0.889039
 124610/250000: episode: 183, duration: 29.359s, episode steps: 682, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.007296, mae: 0.725168, mean_q: 0.912844, mean_eps: 0.888159
 125186/250000: episode: 184, duration: 25.569s, episode steps: 576, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.007038, mae: 0.728844, mean_q: 0.916851, mean_eps: 0.887592
 125971/250000: episode: 185, duration: 33.673s, episode steps: 785, steps per second:  23, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.006600, mae: 0.731375, mean_q: 0.921314, mean_eps: 0.886980
 126766/250000: episode: 186, duration: 34.488s, episode steps: 795, steps per second:  23, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.007353, mae: 0.729500, mean_q: 0.917381, mean_eps: 0.886269
 127408/250000: episode: 187, duration: 28.434s, episode steps: 642, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.007757, mae: 0.727419, mean_q: 0.916202, mean_eps: 0.885623
 127888/250000: episode: 188, duration: 20.594s, episode steps: 480, steps per second:  23, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.007837, mae: 0.729655, mean_q: 0.919204, mean_eps: 0.885119
 128851/250000: episode: 189, duration: 43.503s, episode steps: 963, steps per second:  22, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.007333, mae: 0.732211, mean_q: 0.921744, mean_eps: 0.884469
 129977/250000: episode: 190, duration: 51.024s, episode steps: 1126, steps per second:  22, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.007696, mae: 0.723164, mean_q: 0.910130, mean_eps: 0.883527
 130783/250000: episode: 191, duration: 35.869s, episode steps: 806, steps per second:  22, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.008107, mae: 0.723159, mean_q: 0.910877, mean_eps: 0.882658
 131590/250000: episode: 192, duration: 35.170s, episode steps: 807, steps per second:  23, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008076, mae: 0.729306, mean_q: 0.920312, mean_eps: 0.881933
 132014/250000: episode: 193, duration: 17.818s, episode steps: 424, steps per second:  24, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.007492, mae: 0.729313, mean_q: 0.918889, mean_eps: 0.881378
 132360/250000: episode: 194, duration: 15.400s, episode steps: 346, steps per second:  22, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.007599, mae: 0.728748, mean_q: 0.919189, mean_eps: 0.881033
 133151/250000: episode: 195, duration: 33.020s, episode steps: 791, steps per second:  24, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.007780, mae: 0.732482, mean_q: 0.922199, mean_eps: 0.880521
 133548/250000: episode: 196, duration: 17.044s, episode steps: 397, steps per second:  23, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.007782, mae: 0.718372, mean_q: 0.904665, mean_eps: 0.879987
 134124/250000: episode: 197, duration: 25.773s, episode steps: 576, steps per second:  22, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.007903, mae: 0.730829, mean_q: 0.921831, mean_eps: 0.879549
 135020/250000: episode: 198, duration: 38.094s, episode steps: 896, steps per second:  24, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.007605, mae: 0.728185, mean_q: 0.916384, mean_eps: 0.878887
 135746/250000: episode: 199, duration: 32.219s, episode steps: 726, steps per second:  23, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.006764, mae: 0.722252, mean_q: 0.908718, mean_eps: 0.878156
 136688/250000: episode: 200, duration: 41.011s, episode steps: 942, steps per second:  23, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.007660, mae: 0.720858, mean_q: 0.908742, mean_eps: 0.877406
 137217/250000: episode: 201, duration: 23.852s, episode steps: 529, steps per second:  22, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.007227, mae: 0.712639, mean_q: 0.897542, mean_eps: 0.876743
 137714/250000: episode: 202, duration: 20.727s, episode steps: 497, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.007034, mae: 0.727171, mean_q: 0.915440, mean_eps: 0.876281
 138186/250000: episode: 203, duration: 20.701s, episode steps: 472, steps per second:  23, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.007437, mae: 0.724222, mean_q: 0.910910, mean_eps: 0.875845
 138684/250000: episode: 204, duration: 22.279s, episode steps: 498, steps per second:  22, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.007360, mae: 0.721240, mean_q: 0.906842, mean_eps: 0.875409
 139194/250000: episode: 205, duration: 22.204s, episode steps: 510, steps per second:  23, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.008124, mae: 0.743359, mean_q: 0.935745, mean_eps: 0.874956
 140284/250000: episode: 206, duration: 47.349s, episode steps: 1090, steps per second:  23, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.006524, mae: 0.715410, mean_q: 0.900854, mean_eps: 0.874236
 141085/250000: episode: 207, duration: 34.243s, episode steps: 801, steps per second:  23, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.006456, mae: 0.718558, mean_q: 0.903370, mean_eps: 0.873384
 141490/250000: episode: 208, duration: 18.091s, episode steps: 405, steps per second:  22, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.006576, mae: 0.720544, mean_q: 0.906236, mean_eps: 0.872841
 141941/250000: episode: 209, duration: 19.667s, episode steps: 451, steps per second:  23, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.008426, mae: 0.724862, mean_q: 0.911086, mean_eps: 0.872456
 142360/250000: episode: 210, duration: 17.591s, episode steps: 419, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.007654, mae: 0.728928, mean_q: 0.918111, mean_eps: 0.872065
 143224/250000: episode: 211, duration: 37.266s, episode steps: 864, steps per second:  23, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.007108, mae: 0.717282, mean_q: 0.903292, mean_eps: 0.871489
 143829/250000: episode: 212, duration: 25.929s, episode steps: 605, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.007249, mae: 0.721648, mean_q: 0.907609, mean_eps: 0.870827
 144729/250000: episode: 213, duration: 40.456s, episode steps: 900, steps per second:  22, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.007849, mae: 0.724883, mean_q: 0.911826, mean_eps: 0.870148
 145341/250000: episode: 214, duration: 26.508s, episode steps: 612, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.008022, mae: 0.718375, mean_q: 0.903905, mean_eps: 0.869468
 145834/250000: episode: 215, duration: 20.915s, episode steps: 493, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.008159, mae: 0.723041, mean_q: 0.909597, mean_eps: 0.868971
 146733/250000: episode: 216, duration: 40.051s, episode steps: 899, steps per second:  22, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.007418, mae: 0.733545, mean_q: 0.922333, mean_eps: 0.868344
 147383/250000: episode: 217, duration: 27.408s, episode steps: 650, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.007323, mae: 0.724913, mean_q: 0.912506, mean_eps: 0.867648
 147815/250000: episode: 218, duration: 19.443s, episode steps: 432, steps per second:  22, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.006331, mae: 0.733497, mean_q: 0.921857, mean_eps: 0.867162
 148179/250000: episode: 219, duration: 15.341s, episode steps: 364, steps per second:  24, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.007908, mae: 0.736272, mean_q: 0.924023, mean_eps: 0.866804
 148804/250000: episode: 220, duration: 26.111s, episode steps: 625, steps per second:  24, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.006428, mae: 0.731428, mean_q: 0.919923, mean_eps: 0.866359
 149460/250000: episode: 221, duration: 29.517s, episode steps: 656, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.007016, mae: 0.722648, mean_q: 0.907757, mean_eps: 0.865783
 150178/250000: episode: 222, duration: 30.588s, episode steps: 718, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.007168, mae: 0.733779, mean_q: 0.921688, mean_eps: 0.865164
 150955/250000: episode: 223, duration: 34.262s, episode steps: 777, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.009069, mae: 0.775589, mean_q: 0.974086, mean_eps: 0.864491
 151682/250000: episode: 224, duration: 31.490s, episode steps: 727, steps per second:  23, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.008388, mae: 0.777919, mean_q: 0.976878, mean_eps: 0.863814
 152348/250000: episode: 225, duration: 30.020s, episode steps: 666, steps per second:  22, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.008822, mae: 0.776880, mean_q: 0.973468, mean_eps: 0.863187
 153041/250000: episode: 226, duration: 31.682s, episode steps: 693, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.008564, mae: 0.768820, mean_q: 0.965299, mean_eps: 0.862575
 153772/250000: episode: 227, duration: 33.852s, episode steps: 731, steps per second:  22, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.007636, mae: 0.762214, mean_q: 0.956359, mean_eps: 0.861935
 154542/250000: episode: 228, duration: 32.563s, episode steps: 770, steps per second:  24, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.008351, mae: 0.774596, mean_q: 0.972560, mean_eps: 0.861260
 155173/250000: episode: 229, duration: 28.057s, episode steps: 631, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.007896, mae: 0.770162, mean_q: 0.966695, mean_eps: 0.860628
 156049/250000: episode: 230, duration: 37.066s, episode steps: 876, steps per second:  24, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.009293, mae: 0.778783, mean_q: 0.976290, mean_eps: 0.859949
 156560/250000: episode: 231, duration: 23.229s, episode steps: 511, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.007870, mae: 0.764071, mean_q: 0.958391, mean_eps: 0.859326
 157541/250000: episode: 232, duration: 41.506s, episode steps: 981, steps per second:  24, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.007989, mae: 0.770807, mean_q: 0.967111, mean_eps: 0.858655
 158258/250000: episode: 233, duration: 32.612s, episode steps: 717, steps per second:  22, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.009124, mae: 0.772658, mean_q: 0.969004, mean_eps: 0.857890
 158641/250000: episode: 234, duration: 17.500s, episode steps: 383, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.007554, mae: 0.772773, mean_q: 0.972445, mean_eps: 0.857395
 159305/250000: episode: 235, duration: 29.133s, episode steps: 664, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.007791, mae: 0.775208, mean_q: 0.973226, mean_eps: 0.856923
 160219/250000: episode: 236, duration: 41.392s, episode steps: 914, steps per second:  22, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.007477, mae: 0.776790, mean_q: 0.975316, mean_eps: 0.856214
 160903/250000: episode: 237, duration: 30.721s, episode steps: 684, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.007363, mae: 0.775910, mean_q: 0.972084, mean_eps: 0.855496
 161652/250000: episode: 238, duration: 35.630s, episode steps: 749, steps per second:  21, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.007618, mae: 0.766743, mean_q: 0.963109, mean_eps: 0.854852
 162723/250000: episode: 239, duration: 49.130s, episode steps: 1071, steps per second:  22, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.007934, mae: 0.773821, mean_q: 0.971399, mean_eps: 0.854033
 163115/250000: episode: 240, duration: 17.315s, episode steps: 392, steps per second:  23, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.008665, mae: 0.783450, mean_q: 0.983464, mean_eps: 0.853374
 163718/250000: episode: 241, duration: 27.364s, episode steps: 603, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.007763, mae: 0.769844, mean_q: 0.966425, mean_eps: 0.852926
 164693/250000: episode: 242, duration: 44.226s, episode steps: 975, steps per second:  22, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007677, mae: 0.768943, mean_q: 0.963939, mean_eps: 0.852215
 165790/250000: episode: 243, duration: 55.485s, episode steps: 1097, steps per second:  20, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.007858, mae: 0.776976, mean_q: 0.974037, mean_eps: 0.851282
 166581/250000: episode: 244, duration: 38.091s, episode steps: 791, steps per second:  21, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.008192, mae: 0.779356, mean_q: 0.978305, mean_eps: 0.850433
 167122/250000: episode: 245, duration: 23.891s, episode steps: 541, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.008290, mae: 0.773890, mean_q: 0.972196, mean_eps: 0.849833
 167562/250000: episode: 246, duration: 22.233s, episode steps: 440, steps per second:  20, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.007871, mae: 0.777295, mean_q: 0.974676, mean_eps: 0.849392
 169016/250000: episode: 247, duration: 83.130s, episode steps: 1454, steps per second:  17, episode reward: 22.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.007964, mae: 0.778506, mean_q: 0.976827, mean_eps: 0.848541
 169660/250000: episode: 248, duration: 34.207s, episode steps: 644, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.008081, mae: 0.773424, mean_q: 0.970833, mean_eps: 0.847598
 170179/250000: episode: 249, duration: 27.126s, episode steps: 519, steps per second:  19, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.007820, mae: 0.766335, mean_q: 0.961207, mean_eps: 0.847074
 170689/250000: episode: 250, duration: 26.829s, episode steps: 510, steps per second:  19, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.008117, mae: 0.775883, mean_q: 0.973848, mean_eps: 0.846609
 171121/250000: episode: 251, duration: 22.453s, episode steps: 432, steps per second:  19, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.008736, mae: 0.775491, mean_q: 0.971442, mean_eps: 0.846185
 171698/250000: episode: 252, duration: 30.436s, episode steps: 577, steps per second:  19, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.007865, mae: 0.780389, mean_q: 0.977940, mean_eps: 0.845731
 172882/250000: episode: 253, duration: 61.177s, episode steps: 1184, steps per second:  19, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.007234, mae: 0.775464, mean_q: 0.972637, mean_eps: 0.844939
 173417/250000: episode: 254, duration: 27.899s, episode steps: 535, steps per second:  19, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.007963, mae: 0.766103, mean_q: 0.960509, mean_eps: 0.844165
 173789/250000: episode: 255, duration: 19.244s, episode steps: 372, steps per second:  19, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.007176, mae: 0.772712, mean_q: 0.969557, mean_eps: 0.843756
 174330/250000: episode: 256, duration: 28.381s, episode steps: 541, steps per second:  19, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.007243, mae: 0.779537, mean_q: 0.979164, mean_eps: 0.843346
 174995/250000: episode: 257, duration: 33.382s, episode steps: 665, steps per second:  20, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.007949, mae: 0.783348, mean_q: 0.982056, mean_eps: 0.842804
 175620/250000: episode: 258, duration: 34.456s, episode steps: 625, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.007284, mae: 0.770652, mean_q: 0.966121, mean_eps: 0.842225
 176149/250000: episode: 259, duration: 27.579s, episode steps: 529, steps per second:  19, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.008303, mae: 0.774292, mean_q: 0.970463, mean_eps: 0.841704
 176546/250000: episode: 260, duration: 20.412s, episode steps: 397, steps per second:  19, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.008353, mae: 0.773354, mean_q: 0.969232, mean_eps: 0.841287
 177185/250000: episode: 261, duration: 33.551s, episode steps: 639, steps per second:  19, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.007673, mae: 0.773105, mean_q: 0.968467, mean_eps: 0.840821
 177838/250000: episode: 262, duration: 31.570s, episode steps: 653, steps per second:  21, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.008491, mae: 0.780396, mean_q: 0.978599, mean_eps: 0.840239
 178358/250000: episode: 263, duration: 24.002s, episode steps: 520, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.007313, mae: 0.775346, mean_q: 0.973360, mean_eps: 0.839712
 178955/250000: episode: 264, duration: 25.361s, episode steps: 597, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.008464, mae: 0.777028, mean_q: 0.974188, mean_eps: 0.839210
 179616/250000: episode: 265, duration: 29.989s, episode steps: 661, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.008668, mae: 0.782443, mean_q: 0.982538, mean_eps: 0.838644
 180326/250000: episode: 266, duration: 30.486s, episode steps: 710, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.008875, mae: 0.782473, mean_q: 0.981994, mean_eps: 0.838027
 181048/250000: episode: 267, duration: 32.633s, episode steps: 722, steps per second:  22, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.007332, mae: 0.770572, mean_q: 0.966124, mean_eps: 0.837383
 181545/250000: episode: 268, duration: 21.525s, episode steps: 497, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.007656, mae: 0.779845, mean_q: 0.978186, mean_eps: 0.836834
 182252/250000: episode: 269, duration: 30.311s, episode steps: 707, steps per second:  23, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007495, mae: 0.773204, mean_q: 0.968444, mean_eps: 0.836292
 182840/250000: episode: 270, duration: 26.895s, episode steps: 588, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.008349, mae: 0.766923, mean_q: 0.961021, mean_eps: 0.835710
 183356/250000: episode: 271, duration: 24.348s, episode steps: 516, steps per second:  21, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.007878, mae: 0.785828, mean_q: 0.985453, mean_eps: 0.835214
 184187/250000: episode: 272, duration: 37.022s, episode steps: 831, steps per second:  22, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.008447, mae: 0.778771, mean_q: 0.979417, mean_eps: 0.834607
 185214/250000: episode: 273, duration: 44.486s, episode steps: 1027, steps per second:  23, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.008441, mae: 0.782549, mean_q: 0.982614, mean_eps: 0.833770
 186162/250000: episode: 274, duration: 41.656s, episode steps: 948, steps per second:  23, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.006956, mae: 0.778176, mean_q: 0.976256, mean_eps: 0.832881
 186971/250000: episode: 275, duration: 36.719s, episode steps: 809, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.007936, mae: 0.773255, mean_q: 0.970021, mean_eps: 0.832091
 187688/250000: episode: 276, duration: 31.157s, episode steps: 717, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.007796, mae: 0.772604, mean_q: 0.970282, mean_eps: 0.831405
 188046/250000: episode: 277, duration: 15.772s, episode steps: 358, steps per second:  23, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.007153, mae: 0.779585, mean_q: 0.979281, mean_eps: 0.830921
 188715/250000: episode: 278, duration: 30.338s, episode steps: 669, steps per second:  22, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.007349, mae: 0.780922, mean_q: 0.978689, mean_eps: 0.830458
 189545/250000: episode: 279, duration: 35.694s, episode steps: 830, steps per second:  23, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.007955, mae: 0.776677, mean_q: 0.974418, mean_eps: 0.829783
 190197/250000: episode: 280, duration: 28.698s, episode steps: 652, steps per second:  23, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.007025, mae: 0.769591, mean_q: 0.965331, mean_eps: 0.829115
 190847/250000: episode: 281, duration: 27.627s, episode steps: 650, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.008579, mae: 0.773109, mean_q: 0.968978, mean_eps: 0.828530
 191606/250000: episode: 282, duration: 37.018s, episode steps: 759, steps per second:  21, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.008962, mae: 0.772560, mean_q: 0.969622, mean_eps: 0.827897
 191973/250000: episode: 283, duration: 15.759s, episode steps: 367, steps per second:  23, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.007606, mae: 0.784094, mean_q: 0.982945, mean_eps: 0.827389
 192516/250000: episode: 284, duration: 23.538s, episode steps: 543, steps per second:  23, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.008297, mae: 0.781463, mean_q: 0.979253, mean_eps: 0.826980
 193494/250000: episode: 285, duration: 44.728s, episode steps: 978, steps per second:  22, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.008023, mae: 0.771845, mean_q: 0.969983, mean_eps: 0.826296
 194018/250000: episode: 286, duration: 22.199s, episode steps: 524, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.007961, mae: 0.780393, mean_q: 0.978604, mean_eps: 0.825620
 194696/250000: episode: 287, duration: 31.788s, episode steps: 678, steps per second:  21, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.006828, mae: 0.766412, mean_q: 0.962941, mean_eps: 0.825080
 195249/250000: episode: 288, duration: 25.403s, episode steps: 553, steps per second:  22, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.007557, mae: 0.770091, mean_q: 0.964995, mean_eps: 0.824525
 196069/250000: episode: 289, duration: 38.885s, episode steps: 820, steps per second:  21, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.007643, mae: 0.766804, mean_q: 0.962834, mean_eps: 0.823906
 196683/250000: episode: 290, duration: 28.665s, episode steps: 614, steps per second:  21, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.008413, mae: 0.769962, mean_q: 0.967196, mean_eps: 0.823262
 197631/250000: episode: 291, duration: 46.642s, episode steps: 948, steps per second:  20, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.007612, mae: 0.774500, mean_q: 0.972904, mean_eps: 0.822560
 198168/250000: episode: 292, duration: 27.899s, episode steps: 537, steps per second:  19, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.007293, mae: 0.780979, mean_q: 0.978801, mean_eps: 0.821892
 198666/250000: episode: 293, duration: 25.720s, episode steps: 498, steps per second:  19, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.006776, mae: 0.765117, mean_q: 0.960419, mean_eps: 0.821426
 199182/250000: episode: 294, duration: 26.721s, episode steps: 516, steps per second:  19, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.006929, mae: 0.772642, mean_q: 0.969321, mean_eps: 0.820968
 199829/250000: episode: 295, duration: 34.067s, episode steps: 647, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.007984, mae: 0.781149, mean_q: 0.980554, mean_eps: 0.820445
 200491/250000: episode: 296, duration: 33.554s, episode steps: 662, steps per second:  20, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.008846, mae: 0.811277, mean_q: 1.014739, mean_eps: 0.819856
 201560/250000: episode: 297, duration: 59.453s, episode steps: 1069, steps per second:  18, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.008817, mae: 0.829177, mean_q: 1.040227, mean_eps: 0.819078
 201955/250000: episode: 298, duration: 17.903s, episode steps: 395, steps per second:  22, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.008563, mae: 0.822850, mean_q: 1.033517, mean_eps: 0.818420
 202551/250000: episode: 299, duration: 29.801s, episode steps: 596, steps per second:  20, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.008466, mae: 0.827469, mean_q: 1.036097, mean_eps: 0.817973
 203051/250000: episode: 300, duration: 25.356s, episode steps: 500, steps per second:  20, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.007937, mae: 0.826871, mean_q: 1.034544, mean_eps: 0.817480
 203669/250000: episode: 301, duration: 32.154s, episode steps: 618, steps per second:  19, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.009302, mae: 0.824137, mean_q: 1.032109, mean_eps: 0.816976
 204311/250000: episode: 302, duration: 28.055s, episode steps: 642, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.008598, mae: 0.826625, mean_q: 1.035742, mean_eps: 0.816409
 204710/250000: episode: 303, duration: 17.243s, episode steps: 399, steps per second:  23, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.009176, mae: 0.839375, mean_q: 1.053122, mean_eps: 0.815941
 205143/250000: episode: 304, duration: 20.335s, episode steps: 433, steps per second:  21, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.009104, mae: 0.830590, mean_q: 1.040538, mean_eps: 0.815567
 206140/250000: episode: 305, duration: 44.631s, episode steps: 997, steps per second:  22, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.009180, mae: 0.827526, mean_q: 1.037143, mean_eps: 0.814924
 206867/250000: episode: 306, duration: 36.730s, episode steps: 727, steps per second:  20, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.008857, mae: 0.821443, mean_q: 1.029496, mean_eps: 0.814148
 207548/250000: episode: 307, duration: 34.294s, episode steps: 681, steps per second:  20, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009800, mae: 0.821971, mean_q: 1.029857, mean_eps: 0.813515
 208641/250000: episode: 308, duration: 52.644s, episode steps: 1093, steps per second:  21, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.009177, mae: 0.829257, mean_q: 1.037813, mean_eps: 0.812715
 209374/250000: episode: 309, duration: 34.515s, episode steps: 733, steps per second:  21, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.007267, mae: 0.825870, mean_q: 1.033346, mean_eps: 0.811893
 209995/250000: episode: 310, duration: 29.817s, episode steps: 621, steps per second:  21, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.008958, mae: 0.820237, mean_q: 1.025224, mean_eps: 0.811284
 210767/250000: episode: 311, duration: 37.488s, episode steps: 772, steps per second:  21, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.009473, mae: 0.837316, mean_q: 1.047550, mean_eps: 0.810658
 211531/250000: episode: 312, duration: 40.446s, episode steps: 764, steps per second:  19, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.009891, mae: 0.832505, mean_q: 1.042249, mean_eps: 0.809967
 212076/250000: episode: 313, duration: 28.666s, episode steps: 545, steps per second:  19, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.009785, mae: 0.837131, mean_q: 1.047983, mean_eps: 0.809378
 212596/250000: episode: 314, duration: 24.674s, episode steps: 520, steps per second:  21, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.009139, mae: 0.823473, mean_q: 1.030012, mean_eps: 0.808899
 213147/250000: episode: 315, duration: 24.780s, episode steps: 551, steps per second:  22, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009165, mae: 0.821645, mean_q: 1.029358, mean_eps: 0.808417
 213490/250000: episode: 316, duration: 16.270s, episode steps: 343, steps per second:  21, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.008674, mae: 0.837124, mean_q: 1.047621, mean_eps: 0.808014
 214321/250000: episode: 317, duration: 36.474s, episode steps: 831, steps per second:  23, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.008803, mae: 0.827404, mean_q: 1.036326, mean_eps: 0.807485
 214953/250000: episode: 318, duration: 29.476s, episode steps: 632, steps per second:  21, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.009176, mae: 0.830548, mean_q: 1.038308, mean_eps: 0.806826
 215534/250000: episode: 319, duration: 26.079s, episode steps: 581, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009426, mae: 0.827461, mean_q: 1.036085, mean_eps: 0.806280
 216199/250000: episode: 320, duration: 30.971s, episode steps: 665, steps per second:  21, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.008299, mae: 0.832898, mean_q: 1.043056, mean_eps: 0.805721
 216854/250000: episode: 321, duration: 29.309s, episode steps: 655, steps per second:  22, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.008314, mae: 0.825588, mean_q: 1.033161, mean_eps: 0.805127
 217359/250000: episode: 322, duration: 22.543s, episode steps: 505, steps per second:  22, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.008180, mae: 0.807792, mean_q: 1.009678, mean_eps: 0.804605
 218484/250000: episode: 323, duration: 50.473s, episode steps: 1125, steps per second:  22, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.008596, mae: 0.825755, mean_q: 1.034265, mean_eps: 0.803872
 219216/250000: episode: 324, duration: 33.390s, episode steps: 732, steps per second:  22, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.008491, mae: 0.833248, mean_q: 1.042199, mean_eps: 0.803037
 219933/250000: episode: 325, duration: 31.630s, episode steps: 717, steps per second:  23, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.008019, mae: 0.837006, mean_q: 1.048158, mean_eps: 0.802383
 220603/250000: episode: 326, duration: 30.282s, episode steps: 670, steps per second:  22, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.009436, mae: 0.837135, mean_q: 1.045758, mean_eps: 0.801759
 221194/250000: episode: 327, duration: 26.482s, episode steps: 591, steps per second:  22, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.008483, mae: 0.830967, mean_q: 1.041334, mean_eps: 0.801192
 221906/250000: episode: 328, duration: 30.837s, episode steps: 712, steps per second:  23, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.008456, mae: 0.826898, mean_q: 1.035820, mean_eps: 0.800605
 222701/250000: episode: 329, duration: 36.816s, episode steps: 795, steps per second:  22, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.009028, mae: 0.834610, mean_q: 1.044932, mean_eps: 0.799926
 223632/250000: episode: 330, duration: 41.162s, episode steps: 931, steps per second:  23, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.009018, mae: 0.834083, mean_q: 1.044722, mean_eps: 0.799151
 224248/250000: episode: 331, duration: 27.582s, episode steps: 616, steps per second:  22, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.007474, mae: 0.831063, mean_q: 1.042439, mean_eps: 0.798456
 224930/250000: episode: 332, duration: 30.155s, episode steps: 682, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.008035, mae: 0.827530, mean_q: 1.036577, mean_eps: 0.797871
 225999/250000: episode: 333, duration: 48.256s, episode steps: 1069, steps per second:  22, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.008535, mae: 0.832777, mean_q: 1.042746, mean_eps: 0.797082
 226800/250000: episode: 334, duration: 37.591s, episode steps: 801, steps per second:  21, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.008474, mae: 0.828780, mean_q: 1.038888, mean_eps: 0.796242
 227352/250000: episode: 335, duration: 25.228s, episode steps: 552, steps per second:  22, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.007985, mae: 0.817125, mean_q: 1.023929, mean_eps: 0.795633
 228039/250000: episode: 336, duration: 34.962s, episode steps: 687, steps per second:  20, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.007645, mae: 0.821121, mean_q: 1.026906, mean_eps: 0.795075
 228922/250000: episode: 337, duration: 46.283s, episode steps: 883, steps per second:  19, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.008896, mae: 0.834180, mean_q: 1.042056, mean_eps: 0.794368
 229355/250000: episode: 338, duration: 20.816s, episode steps: 433, steps per second:  21, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.008426, mae: 0.825389, mean_q: 1.029806, mean_eps: 0.793776
 229886/250000: episode: 339, duration: 24.387s, episode steps: 531, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.009506, mae: 0.830029, mean_q: 1.036240, mean_eps: 0.793342
 231408/250000: episode: 340, duration: 68.240s, episode steps: 1522, steps per second:  22, episode reward: 16.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.008675, mae: 0.823525, mean_q: 1.030804, mean_eps: 0.792419
 232323/250000: episode: 341, duration: 49.914s, episode steps: 915, steps per second:  18, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.008164, mae: 0.830671, mean_q: 1.038697, mean_eps: 0.791322
 232870/250000: episode: 342, duration: 28.615s, episode steps: 547, steps per second:  19, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.008440, mae: 0.839222, mean_q: 1.051119, mean_eps: 0.790664
 233261/250000: episode: 343, duration: 20.431s, episode steps: 391, steps per second:  19, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.010495, mae: 0.823819, mean_q: 1.031133, mean_eps: 0.790241
 233899/250000: episode: 344, duration: 33.574s, episode steps: 638, steps per second:  19, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.009353, mae: 0.826079, mean_q: 1.032546, mean_eps: 0.789778
 234751/250000: episode: 345, duration: 44.689s, episode steps: 852, steps per second:  19, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.008613, mae: 0.839301, mean_q: 1.049183, mean_eps: 0.789108
 235395/250000: episode: 346, duration: 33.493s, episode steps: 644, steps per second:  19, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.008459, mae: 0.827130, mean_q: 1.035435, mean_eps: 0.788435
 235899/250000: episode: 347, duration: 26.726s, episode steps: 504, steps per second:  19, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.008319, mae: 0.836963, mean_q: 1.049191, mean_eps: 0.787919
 236407/250000: episode: 348, duration: 28.024s, episode steps: 508, steps per second:  18, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.009248, mae: 0.834525, mean_q: 1.044284, mean_eps: 0.787463
 236811/250000: episode: 349, duration: 21.805s, episode steps: 404, steps per second:  19, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.007135, mae: 0.835232, mean_q: 1.045255, mean_eps: 0.787053
 237463/250000: episode: 350, duration: 35.367s, episode steps: 652, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.008419, mae: 0.825562, mean_q: 1.032537, mean_eps: 0.786578
 237939/250000: episode: 351, duration: 22.566s, episode steps: 476, steps per second:  21, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.009418, mae: 0.835695, mean_q: 1.045103, mean_eps: 0.786070
 238781/250000: episode: 352, duration: 39.210s, episode steps: 842, steps per second:  21, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.008434, mae: 0.831755, mean_q: 1.040816, mean_eps: 0.785476
 239549/250000: episode: 353, duration: 34.812s, episode steps: 768, steps per second:  22, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.008388, mae: 0.827244, mean_q: 1.034700, mean_eps: 0.784751
 240604/250000: episode: 354, duration: 47.776s, episode steps: 1055, steps per second:  22, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.007824, mae: 0.819879, mean_q: 1.025056, mean_eps: 0.783932
 240994/250000: episode: 355, duration: 18.617s, episode steps: 390, steps per second:  21, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.008406, mae: 0.837580, mean_q: 1.046839, mean_eps: 0.783282
 241749/250000: episode: 356, duration: 36.685s, episode steps: 755, steps per second:  21, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.008742, mae: 0.828872, mean_q: 1.034462, mean_eps: 0.782765
 242370/250000: episode: 357, duration: 28.199s, episode steps: 621, steps per second:  22, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.008503, mae: 0.829343, mean_q: 1.035192, mean_eps: 0.782146
 242834/250000: episode: 358, duration: 22.816s, episode steps: 464, steps per second:  20, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.007684, mae: 0.816756, mean_q: 1.022023, mean_eps: 0.781658
 243210/250000: episode: 359, duration: 17.481s, episode steps: 376, steps per second:  22, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.009142, mae: 0.830241, mean_q: 1.038743, mean_eps: 0.781280
 243592/250000: episode: 360, duration: 17.916s, episode steps: 382, steps per second:  21, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.008738, mae: 0.824412, mean_q: 1.032316, mean_eps: 0.780940
 244238/250000: episode: 361, duration: 32.961s, episode steps: 646, steps per second:  20, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.008424, mae: 0.838900, mean_q: 1.050703, mean_eps: 0.780477
 244935/250000: episode: 362, duration: 31.354s, episode steps: 697, steps per second:  22, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.008088, mae: 0.842027, mean_q: 1.051353, mean_eps: 0.779873
 245473/250000: episode: 363, duration: 25.884s, episode steps: 538, steps per second:  21, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.009093, mae: 0.832585, mean_q: 1.040623, mean_eps: 0.779316
 246052/250000: episode: 364, duration: 25.081s, episode steps: 579, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.008011, mae: 0.818859, mean_q: 1.024437, mean_eps: 0.778814
 246548/250000: episode: 365, duration: 22.858s, episode steps: 496, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.008347, mae: 0.817326, mean_q: 1.022024, mean_eps: 0.778332
 247148/250000: episode: 366, duration: 27.901s, episode steps: 600, steps per second:  22, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.007342, mae: 0.826077, mean_q: 1.031117, mean_eps: 0.777839
 247639/250000: episode: 367, duration: 22.215s, episode steps: 491, steps per second:  22, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.008417, mae: 0.828938, mean_q: 1.034552, mean_eps: 0.777347
 248033/250000: episode: 368, duration: 17.932s, episode steps: 394, steps per second:  22, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.009977, mae: 0.837654, mean_q: 1.045949, mean_eps: 0.776948
 248579/250000: episode: 369, duration: 25.493s, episode steps: 546, steps per second:  21, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.008559, mae: 0.836554, mean_q: 1.045831, mean_eps: 0.776525
 249091/250000: episode: 370, duration: 22.428s, episode steps: 512, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.009395, mae: 0.838712, mean_q: 1.046381, mean_eps: 0.776049
 249872/250000: episode: 371, duration: 35.764s, episode steps: 781, steps per second:  22, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.008087, mae: 0.824342, mean_q: 1.028576, mean_eps: 0.775468
done, took 11250.257 seconds
########################################################
PROCESO TERMINADO
########################################################