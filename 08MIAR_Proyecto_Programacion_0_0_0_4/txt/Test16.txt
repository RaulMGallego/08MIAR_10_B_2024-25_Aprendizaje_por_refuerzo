['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_1000000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_260000.h5f.index', './models\\dqn_weights_270000.h5f.index', './models\\dqn_weights_280000.h5f.index', './models\\dqn_weights_290000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_300000.h5f.index', './models\\dqn_weights_310000.h5f.index', './models\\dqn_weights_320000.h5f.index', './models\\dqn_weights_330000.h5f.index', './models\\dqn_weights_340000.h5f.index', './models\\dqn_weights_350000.h5f.index', './models\\dqn_weights_360000.h5f.index', './models\\dqn_weights_370000.h5f.index', './models\\dqn_weights_380000.h5f.index', './models\\dqn_weights_390000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_400000.h5f.index', './models\\dqn_weights_410000.h5f.index', './models\\dqn_weights_420000.h5f.index', './models\\dqn_weights_430000.h5f.index', './models\\dqn_weights_440000.h5f.index', './models\\dqn_weights_450000.h5f.index', './models\\dqn_weights_460000.h5f.index', './models\\dqn_weights_470000.h5f.index', './models\\dqn_weights_480000.h5f.index', './models\\dqn_weights_490000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_500000.h5f.index', './models\\dqn_weights_510000.h5f.index', './models\\dqn_weights_520000.h5f.index', './models\\dqn_weights_530000.h5f.index', './models\\dqn_weights_540000.h5f.index', './models\\dqn_weights_550000.h5f.index', './models\\dqn_weights_560000.h5f.index', './models\\dqn_weights_570000.h5f.index', './models\\dqn_weights_580000.h5f.index', './models\\dqn_weights_590000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_600000.h5f.index', './models\\dqn_weights_610000.h5f.index', './models\\dqn_weights_620000.h5f.index', './models\\dqn_weights_630000.h5f.index', './models\\dqn_weights_640000.h5f.index', './models\\dqn_weights_650000.h5f.index', './models\\dqn_weights_660000.h5f.index', './models\\dqn_weights_670000.h5f.index', './models\\dqn_weights_680000.h5f.index', './models\\dqn_weights_690000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_700000.h5f.index', './models\\dqn_weights_710000.h5f.index', './models\\dqn_weights_720000.h5f.index', './models\\dqn_weights_730000.h5f.index', './models\\dqn_weights_740000.h5f.index', './models\\dqn_weights_750000.h5f.index', './models\\dqn_weights_760000.h5f.index', './models\\dqn_weights_770000.h5f.index', './models\\dqn_weights_780000.h5f.index', './models\\dqn_weights_790000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_800000.h5f.index', './models\\dqn_weights_810000.h5f.index', './models\\dqn_weights_820000.h5f.index', './models\\dqn_weights_830000.h5f.index', './models\\dqn_weights_840000.h5f.index', './models\\dqn_weights_850000.h5f.index', './models\\dqn_weights_860000.h5f.index', './models\\dqn_weights_870000.h5f.index', './models\\dqn_weights_880000.h5f.index', './models\\dqn_weights_890000.h5f.index', './models\\dqn_weights_90000.h5f.index', './models\\dqn_weights_900000.h5f.index', './models\\dqn_weights_910000.h5f.index', './models\\dqn_weights_920000.h5f.index', './models\\dqn_weights_930000.h5f.index', './models\\dqn_weights_940000.h5f.index', './models\\dqn_weights_950000.h5f.index', './models\\dqn_weights_960000.h5f.index', './models\\dqn_weights_970000.h5f.index', './models\\dqn_weights_980000.h5f.index', './models\\dqn_weights_990000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_1000000.h5f
Hay pesos anteriores y se van a cargar
Training for 500000 steps ...
C:\Users\Rociniel\Anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    420/500000: episode: 1, duration: 3.861s, episode steps: 420, steps per second: 109, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/500000: episode: 2, duration: 7.583s, episode steps: 711, steps per second:  94, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/500000: episode: 3, duration: 8.430s, episode steps: 810, steps per second:  96, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2831/500000: episode: 4, duration: 8.252s, episode steps: 890, steps per second: 108, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3206/500000: episode: 5, duration: 3.735s, episode steps: 375, steps per second: 100, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3738/500000: episode: 6, duration: 4.537s, episode steps: 532, steps per second: 117, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
C:\Users\Rociniel\Anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
   5225/500000: episode: 7, duration: 97.224s, episode steps: 1487, steps per second:  15, episode reward: 15.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.028505, mae: 3.428311, mean_q: 4.164987, mean_eps: 0.990797
   5733/500000: episode: 8, duration: 178.646s, episode steps: 508, steps per second:   3, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.019459, mae: 3.381360, mean_q: 4.107146, mean_eps: 0.990139
   6563/500000: episode: 9, duration: 299.689s, episode steps: 830, steps per second:   3, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.016136, mae: 3.332034, mean_q: 4.047805, mean_eps: 0.988935
   7203/500000: episode: 10, duration: 240.420s, episode steps: 640, steps per second:   3, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.013411, mae: 3.305442, mean_q: 4.017231, mean_eps: 0.987611
   7857/500000: episode: 11, duration: 240.141s, episode steps: 654, steps per second:   3, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.010890, mae: 3.227737, mean_q: 3.923305, mean_eps: 0.986447
   8691/500000: episode: 12, duration: 304.251s, episode steps: 834, steps per second:   3, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.011841, mae: 3.207275, mean_q: 3.897296, mean_eps: 0.985108
   9075/500000: episode: 13, duration: 141.154s, episode steps: 384, steps per second:   3, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012920, mae: 3.187530, mean_q: 3.872776, mean_eps: 0.984011
   9924/500000: episode: 14, duration: 296.010s, episode steps: 849, steps per second:   3, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.011315, mae: 3.152360, mean_q: 3.830314, mean_eps: 0.982902
  10833/500000: episode: 15, duration: 355.619s, episode steps: 909, steps per second:   3, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.011036, mae: 3.137400, mean_q: 3.812704, mean_eps: 0.981320
  11263/500000: episode: 16, duration: 150.914s, episode steps: 430, steps per second:   3, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.011546, mae: 3.145909, mean_q: 3.824157, mean_eps: 0.980115
  11643/500000: episode: 17, duration: 126.628s, episode steps: 380, steps per second:   3, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.013000, mae: 3.096005, mean_q: 3.760811, mean_eps: 0.979386
  12035/500000: episode: 18, duration: 131.313s, episode steps: 392, steps per second:   3, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013101, mae: 3.079491, mean_q: 3.741730, mean_eps: 0.978691
  12667/500000: episode: 19, duration: 212.079s, episode steps: 632, steps per second:   3, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.011151, mae: 3.104060, mean_q: 3.772097, mean_eps: 0.977769
  13140/500000: episode: 20, duration: 158.659s, episode steps: 473, steps per second:   3, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.010781, mae: 3.075262, mean_q: 3.738032, mean_eps: 0.976775
  13800/500000: episode: 21, duration: 220.339s, episode steps: 660, steps per second:   3, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.009778, mae: 3.068919, mean_q: 3.730691, mean_eps: 0.975755
  14246/500000: episode: 22, duration: 150.533s, episode steps: 446, steps per second:   3, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010362, mae: 3.046438, mean_q: 3.702237, mean_eps: 0.974760
  14861/500000: episode: 23, duration: 204.543s, episode steps: 615, steps per second:   3, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.011370, mae: 3.045616, mean_q: 3.699699, mean_eps: 0.973805
  15768/500000: episode: 24, duration: 301.490s, episode steps: 907, steps per second:   3, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.010213, mae: 3.044950, mean_q: 3.701329, mean_eps: 0.972435
  16283/500000: episode: 25, duration: 173.107s, episode steps: 515, steps per second:   3, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.010022, mae: 3.035294, mean_q: 3.689540, mean_eps: 0.971155
  16991/500000: episode: 26, duration: 235.632s, episode steps: 708, steps per second:   3, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011728, mae: 3.033378, mean_q: 3.687603, mean_eps: 0.970054
  17721/500000: episode: 27, duration: 243.947s, episode steps: 730, steps per second:   3, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010378, mae: 2.988657, mean_q: 3.633171, mean_eps: 0.968760
  18336/500000: episode: 28, duration: 206.208s, episode steps: 615, steps per second:   3, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.009627, mae: 2.979447, mean_q: 3.620633, mean_eps: 0.967550
  19012/500000: episode: 29, duration: 226.493s, episode steps: 676, steps per second:   3, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.009558, mae: 2.974212, mean_q: 3.614830, mean_eps: 0.966388
  19818/500000: episode: 30, duration: 267.146s, episode steps: 806, steps per second:   3, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.010451, mae: 2.962208, mean_q: 3.598958, mean_eps: 0.965054
  20803/500000: episode: 31, duration: 328.982s, episode steps: 985, steps per second:   3, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011071, mae: 2.917662, mean_q: 3.544942, mean_eps: 0.963442
  21782/500000: episode: 32, duration: 330.534s, episode steps: 979, steps per second:   3, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.009804, mae: 2.915175, mean_q: 3.543597, mean_eps: 0.961674
  22588/500000: episode: 33, duration: 268.119s, episode steps: 806, steps per second:   3, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.009990, mae: 2.903470, mean_q: 3.529120, mean_eps: 0.960068
  23172/500000: episode: 34, duration: 195.375s, episode steps: 584, steps per second:   3, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010033, mae: 2.878240, mean_q: 3.499372, mean_eps: 0.958817
  23791/500000: episode: 35, duration: 205.656s, episode steps: 619, steps per second:   3, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009421, mae: 2.895634, mean_q: 3.520431, mean_eps: 0.957734
  24899/500000: episode: 36, duration: 366.956s, episode steps: 1108, steps per second:   3, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.009773, mae: 2.883270, mean_q: 3.505955, mean_eps: 0.956180
  25532/500000: episode: 37, duration: 210.588s, episode steps: 633, steps per second:   3, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.010085, mae: 2.859780, mean_q: 3.477684, mean_eps: 0.954613
  26308/500000: episode: 38, duration: 257.450s, episode steps: 776, steps per second:   3, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.010921, mae: 2.870308, mean_q: 3.489931, mean_eps: 0.953345
  27485/500000: episode: 39, duration: 390.055s, episode steps: 1177, steps per second:   3, episode reward:  7.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.009275, mae: 2.851004, mean_q: 3.467687, mean_eps: 0.951587
  28045/500000: episode: 40, duration: 184.995s, episode steps: 560, steps per second:   3, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.010583, mae: 2.855029, mean_q: 3.474149, mean_eps: 0.950024
  28651/500000: episode: 41, duration: 199.389s, episode steps: 606, steps per second:   3, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.009982, mae: 2.863712, mean_q: 3.484959, mean_eps: 0.948975
  29281/500000: episode: 42, duration: 209.140s, episode steps: 630, steps per second:   3, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.009649, mae: 2.831601, mean_q: 3.445103, mean_eps: 0.947862
  29694/500000: episode: 43, duration: 138.508s, episode steps: 413, steps per second:   3, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.009575, mae: 2.800578, mean_q: 3.407803, mean_eps: 0.946923
  30292/500000: episode: 44, duration: 198.109s, episode steps: 598, steps per second:   3, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.009583, mae: 2.796759, mean_q: 3.401670, mean_eps: 0.946013
  30937/500000: episode: 45, duration: 214.809s, episode steps: 645, steps per second:   3, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.010123, mae: 2.804372, mean_q: 3.410765, mean_eps: 0.944895
  32009/500000: episode: 46, duration: 374.708s, episode steps: 1072, steps per second:   3, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.009318, mae: 2.818752, mean_q: 3.428991, mean_eps: 0.943350
  33041/500000: episode: 47, duration: 344.879s, episode steps: 1032, steps per second:   3, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.009791, mae: 2.824581, mean_q: 3.436307, mean_eps: 0.941456
  33650/500000: episode: 48, duration: 201.169s, episode steps: 609, steps per second:   3, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.010351, mae: 2.825701, mean_q: 3.436523, mean_eps: 0.939979
  34293/500000: episode: 49, duration: 213.376s, episode steps: 643, steps per second:   3, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.011143, mae: 2.812250, mean_q: 3.420876, mean_eps: 0.938852
  34709/500000: episode: 50, duration: 138.721s, episode steps: 416, steps per second:   3, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.008974, mae: 2.817881, mean_q: 3.428717, mean_eps: 0.937899
  35663/500000: episode: 51, duration: 320.182s, episode steps: 954, steps per second:   3, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.008918, mae: 2.816724, mean_q: 3.425379, mean_eps: 0.936666
  36269/500000: episode: 52, duration: 201.647s, episode steps: 606, steps per second:   3, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.009943, mae: 2.821489, mean_q: 3.431872, mean_eps: 0.935262
  37147/500000: episode: 53, duration: 291.368s, episode steps: 878, steps per second:   3, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.009447, mae: 2.802524, mean_q: 3.408316, mean_eps: 0.933926
  37708/500000: episode: 54, duration: 187.071s, episode steps: 561, steps per second:   3, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.009413, mae: 2.791301, mean_q: 3.394801, mean_eps: 0.932631
  38416/500000: episode: 55, duration: 233.257s, episode steps: 708, steps per second:   3, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.010312, mae: 2.806644, mean_q: 3.413871, mean_eps: 0.931489
  38801/500000: episode: 56, duration: 127.666s, episode steps: 385, steps per second:   3, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.010290, mae: 2.793995, mean_q: 3.400524, mean_eps: 0.930506
  39360/500000: episode: 57, duration: 187.681s, episode steps: 559, steps per second:   3, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.008914, mae: 2.799572, mean_q: 3.406691, mean_eps: 0.929656
  39982/500000: episode: 58, duration: 205.916s, episode steps: 622, steps per second:   3, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.009475, mae: 2.800141, mean_q: 3.406674, mean_eps: 0.928593
  40922/500000: episode: 59, duration: 313.563s, episode steps: 940, steps per second:   3, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.009240, mae: 2.791830, mean_q: 3.396181, mean_eps: 0.927187
  41528/500000: episode: 60, duration: 202.255s, episode steps: 606, steps per second:   3, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.008541, mae: 2.791026, mean_q: 3.395181, mean_eps: 0.925796
  42291/500000: episode: 61, duration: 264.652s, episode steps: 763, steps per second:   3, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.009658, mae: 2.780322, mean_q: 3.381131, mean_eps: 0.924564
  42897/500000: episode: 62, duration: 201.314s, episode steps: 606, steps per second:   3, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.009094, mae: 2.787528, mean_q: 3.391154, mean_eps: 0.923332
  43381/500000: episode: 63, duration: 161.025s, episode steps: 484, steps per second:   3, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.010302, mae: 2.772724, mean_q: 3.374360, mean_eps: 0.922351
  44141/500000: episode: 64, duration: 251.913s, episode steps: 760, steps per second:   3, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.009288, mae: 2.785228, mean_q: 3.388374, mean_eps: 0.921231
  45331/500000: episode: 65, duration: 395.244s, episode steps: 1190, steps per second:   3, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.009510, mae: 2.776634, mean_q: 3.376761, mean_eps: 0.919476
  46409/500000: episode: 66, duration: 356.969s, episode steps: 1078, steps per second:   3, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.008643, mae: 2.767742, mean_q: 3.367820, mean_eps: 0.917435
  47595/500000: episode: 67, duration: 391.035s, episode steps: 1186, steps per second:   3, episode reward:  9.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.009170, mae: 2.770270, mean_q: 3.370745, mean_eps: 0.915397
  48560/500000: episode: 68, duration: 320.104s, episode steps: 965, steps per second:   3, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.009481, mae: 2.753530, mean_q: 3.349626, mean_eps: 0.913461
  49361/500000: episode: 69, duration: 266.912s, episode steps: 801, steps per second:   3, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.010136, mae: 2.752005, mean_q: 3.348998, mean_eps: 0.911872
  50219/500000: episode: 70, duration: 285.279s, episode steps: 858, steps per second:   3, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.010197, mae: 2.773566, mean_q: 3.373095, mean_eps: 0.910379
  50800/500000: episode: 71, duration: 192.509s, episode steps: 581, steps per second:   3, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.014097, mae: 2.797461, mean_q: 3.395436, mean_eps: 0.909084
  51442/500000: episode: 72, duration: 212.283s, episode steps: 642, steps per second:   3, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.014033, mae: 2.804850, mean_q: 3.403069, mean_eps: 0.907983
  52512/500000: episode: 73, duration: 354.539s, episode steps: 1070, steps per second:   3, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013055, mae: 2.799178, mean_q: 3.397065, mean_eps: 0.906442
  53141/500000: episode: 74, duration: 209.345s, episode steps: 629, steps per second:   3, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.012386, mae: 2.793929, mean_q: 3.391670, mean_eps: 0.904913
  53634/500000: episode: 75, duration: 163.177s, episode steps: 493, steps per second:   3, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.011485, mae: 2.784530, mean_q: 3.381642, mean_eps: 0.903903
  54149/500000: episode: 76, duration: 172.262s, episode steps: 515, steps per second:   3, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.013244, mae: 2.801499, mean_q: 3.401638, mean_eps: 0.902996
  54760/500000: episode: 77, duration: 203.015s, episode steps: 611, steps per second:   3, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011319, mae: 2.792552, mean_q: 3.390456, mean_eps: 0.901983
  55306/500000: episode: 78, duration: 181.207s, episode steps: 546, steps per second:   3, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.010004, mae: 2.782828, mean_q: 3.378153, mean_eps: 0.900941
  56175/500000: episode: 79, duration: 287.658s, episode steps: 869, steps per second:   3, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.010883, mae: 2.774979, mean_q: 3.368906, mean_eps: 0.899668
  56792/500000: episode: 80, duration: 205.591s, episode steps: 617, steps per second:   3, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.011274, mae: 2.793672, mean_q: 3.391643, mean_eps: 0.898331
  57588/500000: episode: 81, duration: 265.501s, episode steps: 796, steps per second:   3, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.011060, mae: 2.787878, mean_q: 3.384962, mean_eps: 0.897059
  58248/500000: episode: 82, duration: 219.624s, episode steps: 660, steps per second:   3, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010429, mae: 2.788527, mean_q: 3.386138, mean_eps: 0.895749
  58684/500000: episode: 83, duration: 145.737s, episode steps: 436, steps per second:   3, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.011542, mae: 2.788098, mean_q: 3.385388, mean_eps: 0.894762
  59318/500000: episode: 84, duration: 210.773s, episode steps: 634, steps per second:   3, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.010807, mae: 2.790020, mean_q: 3.387754, mean_eps: 0.893799
  59820/500000: episode: 85, duration: 165.118s, episode steps: 502, steps per second:   3, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.011017, mae: 2.791649, mean_q: 3.389879, mean_eps: 0.892777
  60329/500000: episode: 86, duration: 171.133s, episode steps: 509, steps per second:   3, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.010863, mae: 2.786407, mean_q: 3.383429, mean_eps: 0.891867
  60720/500000: episode: 87, duration: 131.218s, episode steps: 391, steps per second:   3, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.011197, mae: 2.774099, mean_q: 3.368847, mean_eps: 0.891057
  61416/500000: episode: 88, duration: 230.241s, episode steps: 696, steps per second:   3, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.010342, mae: 2.778936, mean_q: 3.373612, mean_eps: 0.890078
  62092/500000: episode: 89, duration: 225.386s, episode steps: 676, steps per second:   3, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.011676, mae: 2.773386, mean_q: 3.367080, mean_eps: 0.888844
  62765/500000: episode: 90, duration: 224.734s, episode steps: 673, steps per second:   3, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.009915, mae: 2.774930, mean_q: 3.369164, mean_eps: 0.887630
  63268/500000: episode: 91, duration: 168.163s, episode steps: 503, steps per second:   3, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.009372, mae: 2.764586, mean_q: 3.356554, mean_eps: 0.886571
  63817/500000: episode: 92, duration: 184.695s, episode steps: 549, steps per second:   3, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.011011, mae: 2.783022, mean_q: 3.379388, mean_eps: 0.885624
  64319/500000: episode: 93, duration: 167.484s, episode steps: 502, steps per second:   3, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.010392, mae: 2.757242, mean_q: 3.347980, mean_eps: 0.884679
  64816/500000: episode: 94, duration: 170.412s, episode steps: 497, steps per second:   3, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.010818, mae: 2.782029, mean_q: 3.377239, mean_eps: 0.883779
  65977/500000: episode: 95, duration: 387.899s, episode steps: 1161, steps per second:   3, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.009582, mae: 2.751853, mean_q: 3.342075, mean_eps: 0.882287
  66633/500000: episode: 96, duration: 216.701s, episode steps: 656, steps per second:   3, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.009838, mae: 2.752461, mean_q: 3.342382, mean_eps: 0.880652
  67206/500000: episode: 97, duration: 190.724s, episode steps: 573, steps per second:   3, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011798, mae: 2.755852, mean_q: 3.345672, mean_eps: 0.879546
  67966/500000: episode: 98, duration: 254.054s, episode steps: 760, steps per second:   3, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.010147, mae: 2.766265, mean_q: 3.359606, mean_eps: 0.878346
  68639/500000: episode: 99, duration: 224.041s, episode steps: 673, steps per second:   3, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.009554, mae: 2.772308, mean_q: 3.367062, mean_eps: 0.877056
  69284/500000: episode: 100, duration: 214.351s, episode steps: 645, steps per second:   3, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.009619, mae: 2.749227, mean_q: 3.338670, mean_eps: 0.875870
  70061/500000: episode: 101, duration: 257.978s, episode steps: 777, steps per second:   3, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.008798, mae: 2.742057, mean_q: 3.329691, mean_eps: 0.874590
  70619/500000: episode: 102, duration: 185.204s, episode steps: 558, steps per second:   3, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010245, mae: 2.751985, mean_q: 3.342704, mean_eps: 0.873389
  71540/500000: episode: 103, duration: 308.110s, episode steps: 921, steps per second:   3, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.010187, mae: 2.739650, mean_q: 3.326512, mean_eps: 0.872058
  71924/500000: episode: 104, duration: 129.981s, episode steps: 384, steps per second:   3, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.008602, mae: 2.754890, mean_q: 3.346233, mean_eps: 0.870883
  72606/500000: episode: 105, duration: 228.511s, episode steps: 682, steps per second:   3, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.010629, mae: 2.749842, mean_q: 3.340061, mean_eps: 0.869924
  73286/500000: episode: 106, duration: 227.808s, episode steps: 680, steps per second:   3, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.009422, mae: 2.745200, mean_q: 3.334150, mean_eps: 0.868698
  74148/500000: episode: 107, duration: 288.038s, episode steps: 862, steps per second:   3, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.009335, mae: 2.745186, mean_q: 3.335344, mean_eps: 0.867310
  74679/500000: episode: 108, duration: 177.580s, episode steps: 531, steps per second:   3, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010906, mae: 2.754588, mean_q: 3.345987, mean_eps: 0.866057
  75136/500000: episode: 109, duration: 152.189s, episode steps: 457, steps per second:   3, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.010636, mae: 2.764711, mean_q: 3.358176, mean_eps: 0.865167
  75791/500000: episode: 110, duration: 217.118s, episode steps: 655, steps per second:   3, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010554, mae: 2.756756, mean_q: 3.348371, mean_eps: 0.864167
  76393/500000: episode: 111, duration: 201.238s, episode steps: 602, steps per second:   3, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.011056, mae: 2.742190, mean_q: 3.330136, mean_eps: 0.863035
  76966/500000: episode: 112, duration: 190.520s, episode steps: 573, steps per second:   3, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.009718, mae: 2.729925, mean_q: 3.315549, mean_eps: 0.861978
  77361/500000: episode: 113, duration: 131.815s, episode steps: 395, steps per second:   3, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.009669, mae: 2.750099, mean_q: 3.339528, mean_eps: 0.861107
  78218/500000: episode: 114, duration: 285.631s, episode steps: 857, steps per second:   3, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.009456, mae: 2.738501, mean_q: 3.326757, mean_eps: 0.859980
  78819/500000: episode: 115, duration: 198.757s, episode steps: 601, steps per second:   3, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.008938, mae: 2.747100, mean_q: 3.336304, mean_eps: 0.858668
  79520/500000: episode: 116, duration: 233.603s, episode steps: 701, steps per second:   3, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.010690, mae: 2.753078, mean_q: 3.343284, mean_eps: 0.857496
  80324/500000: episode: 117, duration: 266.626s, episode steps: 804, steps per second:   3, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.010282, mae: 2.741961, mean_q: 3.329579, mean_eps: 0.856141
  81010/500000: episode: 118, duration: 229.182s, episode steps: 686, steps per second:   3, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.010369, mae: 2.730681, mean_q: 3.316492, mean_eps: 0.854800
  81835/500000: episode: 119, duration: 274.052s, episode steps: 825, steps per second:   3, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.010162, mae: 2.725473, mean_q: 3.310360, mean_eps: 0.853440
  82429/500000: episode: 120, duration: 196.955s, episode steps: 594, steps per second:   3, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.009629, mae: 2.739850, mean_q: 3.328313, mean_eps: 0.852163
  83277/500000: episode: 121, duration: 284.001s, episode steps: 848, steps per second:   3, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.009180, mae: 2.716243, mean_q: 3.298806, mean_eps: 0.850866
  83662/500000: episode: 122, duration: 127.842s, episode steps: 385, steps per second:   3, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.010090, mae: 2.726474, mean_q: 3.311112, mean_eps: 0.849756
  84912/500000: episode: 123, duration: 417.275s, episode steps: 1250, steps per second:   3, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.009030, mae: 2.734360, mean_q: 3.321307, mean_eps: 0.848284
  85793/500000: episode: 124, duration: 293.496s, episode steps: 881, steps per second:   3, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.010395, mae: 2.726214, mean_q: 3.311358, mean_eps: 0.846366
  86180/500000: episode: 125, duration: 129.577s, episode steps: 387, steps per second:   3, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.009983, mae: 2.723678, mean_q: 3.308781, mean_eps: 0.845225
  86994/500000: episode: 126, duration: 272.111s, episode steps: 814, steps per second:   3, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.011489, mae: 2.718758, mean_q: 3.301769, mean_eps: 0.844144
  87747/500000: episode: 127, duration: 252.889s, episode steps: 753, steps per second:   3, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010628, mae: 2.719443, mean_q: 3.302256, mean_eps: 0.842734
  88193/500000: episode: 128, duration: 150.313s, episode steps: 446, steps per second:   3, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.009426, mae: 2.750970, mean_q: 3.342215, mean_eps: 0.841655
  89271/500000: episode: 129, duration: 361.186s, episode steps: 1078, steps per second:   3, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.011289, mae: 2.728251, mean_q: 3.314257, mean_eps: 0.840283
  89938/500000: episode: 130, duration: 223.279s, episode steps: 667, steps per second:   3, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.009614, mae: 2.732076, mean_q: 3.319812, mean_eps: 0.838713
  90338/500000: episode: 131, duration: 133.536s, episode steps: 400, steps per second:   3, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.009570, mae: 2.732688, mean_q: 3.319087, mean_eps: 0.837753
  91609/500000: episode: 132, duration: 427.110s, episode steps: 1271, steps per second:   3, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.009540, mae: 2.723865, mean_q: 3.308754, mean_eps: 0.836249
  92766/500000: episode: 133, duration: 387.220s, episode steps: 1157, steps per second:   3, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.010503, mae: 2.739239, mean_q: 3.326854, mean_eps: 0.834063
  93387/500000: episode: 134, duration: 209.775s, episode steps: 621, steps per second:   3, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.012305, mae: 2.727403, mean_q: 3.312881, mean_eps: 0.832463
  93764/500000: episode: 135, duration: 126.134s, episode steps: 377, steps per second:   3, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.009593, mae: 2.742517, mean_q: 3.331852, mean_eps: 0.831565
  94482/500000: episode: 136, duration: 242.059s, episode steps: 718, steps per second:   3, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.010520, mae: 2.728879, mean_q: 3.314207, mean_eps: 0.830580
  94979/500000: episode: 137, duration: 166.745s, episode steps: 497, steps per second:   3, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.009255, mae: 2.729472, mean_q: 3.316699, mean_eps: 0.829486
  95581/500000: episode: 138, duration: 201.651s, episode steps: 602, steps per second:   3, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010056, mae: 2.734009, mean_q: 3.320104, mean_eps: 0.828497
  96367/500000: episode: 139, duration: 265.118s, episode steps: 786, steps per second:   3, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.010744, mae: 2.727418, mean_q: 3.312682, mean_eps: 0.827248
  96765/500000: episode: 140, duration: 133.292s, episode steps: 398, steps per second:   3, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.009748, mae: 2.714999, mean_q: 3.297206, mean_eps: 0.826182
  97175/500000: episode: 141, duration: 138.759s, episode steps: 410, steps per second:   3, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.009994, mae: 2.724011, mean_q: 3.309066, mean_eps: 0.825455
  97986/500000: episode: 142, duration: 273.234s, episode steps: 811, steps per second:   3, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.010765, mae: 2.721227, mean_q: 3.305321, mean_eps: 0.824356
  98938/500000: episode: 143, duration: 320.535s, episode steps: 952, steps per second:   3, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.010121, mae: 2.717676, mean_q: 3.301624, mean_eps: 0.822769
  99426/500000: episode: 144, duration: 165.045s, episode steps: 488, steps per second:   3, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.009733, mae: 2.729832, mean_q: 3.316314, mean_eps: 0.821473
 100046/500000: episode: 145, duration: 215.345s, episode steps: 620, steps per second:   3, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.010228, mae: 2.731728, mean_q: 3.317959, mean_eps: 0.820476
 100658/500000: episode: 146, duration: 206.306s, episode steps: 612, steps per second:   3, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014389, mae: 2.760923, mean_q: 3.349593, mean_eps: 0.819367
 101201/500000: episode: 147, duration: 181.816s, episode steps: 543, steps per second:   3, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.014453, mae: 2.738043, mean_q: 3.320286, mean_eps: 0.818328
 101956/500000: episode: 148, duration: 255.203s, episode steps: 755, steps per second:   3, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012563, mae: 2.757035, mean_q: 3.344043, mean_eps: 0.817160
 102817/500000: episode: 149, duration: 290.201s, episode steps: 861, steps per second:   3, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.013155, mae: 2.757083, mean_q: 3.345024, mean_eps: 0.815705
 103466/500000: episode: 150, duration: 219.816s, episode steps: 649, steps per second:   3, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011926, mae: 2.769701, mean_q: 3.359387, mean_eps: 0.814346
 103984/500000: episode: 151, duration: 174.286s, episode steps: 518, steps per second:   3, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.011778, mae: 2.744806, mean_q: 3.331020, mean_eps: 0.813296
 104358/500000: episode: 152, duration: 126.632s, episode steps: 374, steps per second:   3, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.012126, mae: 2.754627, mean_q: 3.342372, mean_eps: 0.812493
 104811/500000: episode: 153, duration: 152.639s, episode steps: 453, steps per second:   3, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.013388, mae: 2.754432, mean_q: 3.341330, mean_eps: 0.811749
 105320/500000: episode: 154, duration: 173.103s, episode steps: 509, steps per second:   3, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.012402, mae: 2.758476, mean_q: 3.346682, mean_eps: 0.810883
 105954/500000: episode: 155, duration: 212.353s, episode steps: 634, steps per second:   3, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011731, mae: 2.755320, mean_q: 3.342786, mean_eps: 0.809854
 106657/500000: episode: 156, duration: 236.633s, episode steps: 703, steps per second:   3, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.012640, mae: 2.749065, mean_q: 3.335782, mean_eps: 0.808651
 107191/500000: episode: 157, duration: 181.800s, episode steps: 534, steps per second:   3, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010902, mae: 2.742717, mean_q: 3.327791, mean_eps: 0.807538
 108120/500000: episode: 158, duration: 313.611s, episode steps: 929, steps per second:   3, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.011256, mae: 2.748402, mean_q: 3.335106, mean_eps: 0.806221
 109153/500000: episode: 159, duration: 348.131s, episode steps: 1033, steps per second:   3, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.011738, mae: 2.739339, mean_q: 3.324084, mean_eps: 0.804455
 110076/500000: episode: 160, duration: 310.657s, episode steps: 923, steps per second:   3, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.010474, mae: 2.746844, mean_q: 3.332525, mean_eps: 0.802695
 110878/500000: episode: 161, duration: 271.965s, episode steps: 802, steps per second:   3, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.012100, mae: 2.748344, mean_q: 3.334356, mean_eps: 0.801142
 111218/500000: episode: 162, duration: 115.827s, episode steps: 340, steps per second:   3, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.010392, mae: 2.747558, mean_q: 3.333862, mean_eps: 0.800114
 111947/500000: episode: 163, duration: 247.663s, episode steps: 729, steps per second:   3, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.010870, mae: 2.740211, mean_q: 3.324039, mean_eps: 0.799152
 112795/500000: episode: 164, duration: 285.986s, episode steps: 848, steps per second:   3, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.010760, mae: 2.745674, mean_q: 3.331785, mean_eps: 0.797733
 113544/500000: episode: 165, duration: 254.419s, episode steps: 749, steps per second:   3, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.009834, mae: 2.746640, mean_q: 3.333438, mean_eps: 0.796296
 114767/500000: episode: 166, duration: 414.039s, episode steps: 1223, steps per second:   3, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.010778, mae: 2.746300, mean_q: 3.332397, mean_eps: 0.794521
 115299/500000: episode: 167, duration: 180.703s, episode steps: 532, steps per second:   3, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.011559, mae: 2.738302, mean_q: 3.322135, mean_eps: 0.792941
 116014/500000: episode: 168, duration: 243.209s, episode steps: 715, steps per second:   3, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.011426, mae: 2.750856, mean_q: 3.337829, mean_eps: 0.791819
 116669/500000: episode: 169, duration: 221.499s, episode steps: 655, steps per second:   3, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.011842, mae: 2.745278, mean_q: 3.329564, mean_eps: 0.790586
 117927/500000: episode: 170, duration: 430.922s, episode steps: 1258, steps per second:   3, episode reward: 13.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.011178, mae: 2.747439, mean_q: 3.333347, mean_eps: 0.788864
 118316/500000: episode: 171, duration: 132.301s, episode steps: 389, steps per second:   3, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.851 [0.000, 5.000],  loss: 0.011705, mae: 2.739276, mean_q: 3.323910, mean_eps: 0.787382
 119158/500000: episode: 172, duration: 286.260s, episode steps: 842, steps per second:   3, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.010302, mae: 2.739816, mean_q: 3.325682, mean_eps: 0.786274
 119767/500000: episode: 173, duration: 209.074s, episode steps: 609, steps per second:   3, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.011067, mae: 2.742330, mean_q: 3.326892, mean_eps: 0.784968
 120347/500000: episode: 174, duration: 197.500s, episode steps: 580, steps per second:   3, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.010991, mae: 2.743872, mean_q: 3.328476, mean_eps: 0.783898
 121052/500000: episode: 175, duration: 241.394s, episode steps: 705, steps per second:   3, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011241, mae: 2.754267, mean_q: 3.342085, mean_eps: 0.782742
 121681/500000: episode: 176, duration: 215.064s, episode steps: 629, steps per second:   3, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.010821, mae: 2.748156, mean_q: 3.334802, mean_eps: 0.781541
 122135/500000: episode: 177, duration: 155.301s, episode steps: 454, steps per second:   3, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.011738, mae: 2.747595, mean_q: 3.334661, mean_eps: 0.780566
 122819/500000: episode: 178, duration: 234.666s, episode steps: 684, steps per second:   3, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.009379, mae: 2.755442, mean_q: 3.343979, mean_eps: 0.779542
 123211/500000: episode: 179, duration: 135.636s, episode steps: 392, steps per second:   3, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.010560, mae: 2.749011, mean_q: 3.334744, mean_eps: 0.778574
 123862/500000: episode: 180, duration: 222.279s, episode steps: 651, steps per second:   3, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.009847, mae: 2.730531, mean_q: 3.314112, mean_eps: 0.777635
 124521/500000: episode: 181, duration: 226.514s, episode steps: 659, steps per second:   3, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.011037, mae: 2.742539, mean_q: 3.328236, mean_eps: 0.776456
 125358/500000: episode: 182, duration: 285.925s, episode steps: 837, steps per second:   3, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.010411, mae: 2.742905, mean_q: 3.328240, mean_eps: 0.775110
 126481/500000: episode: 183, duration: 384.891s, episode steps: 1123, steps per second:   3, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.009992, mae: 2.738820, mean_q: 3.324338, mean_eps: 0.773346
 126862/500000: episode: 184, duration: 131.427s, episode steps: 381, steps per second:   3, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.009999, mae: 2.744855, mean_q: 3.331637, mean_eps: 0.771992
 127420/500000: episode: 185, duration: 189.941s, episode steps: 558, steps per second:   3, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.011571, mae: 2.730716, mean_q: 3.313330, mean_eps: 0.771147
 128188/500000: episode: 186, duration: 262.230s, episode steps: 768, steps per second:   3, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.010071, mae: 2.749946, mean_q: 3.336799, mean_eps: 0.769954
 128574/500000: episode: 187, duration: 131.554s, episode steps: 386, steps per second:   3, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011348, mae: 2.745541, mean_q: 3.330463, mean_eps: 0.768915
 129449/500000: episode: 188, duration: 302.647s, episode steps: 875, steps per second:   3, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.009669, mae: 2.745911, mean_q: 3.331412, mean_eps: 0.767780
 129880/500000: episode: 189, duration: 147.676s, episode steps: 431, steps per second:   3, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.008743, mae: 2.736097, mean_q: 3.320658, mean_eps: 0.766605
 130352/500000: episode: 190, duration: 162.754s, episode steps: 472, steps per second:   3, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.009817, mae: 2.743157, mean_q: 3.328887, mean_eps: 0.765792
 131121/500000: episode: 191, duration: 264.377s, episode steps: 769, steps per second:   3, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.010570, mae: 2.734551, mean_q: 3.318203, mean_eps: 0.764675
 132244/500000: episode: 192, duration: 408.146s, episode steps: 1123, steps per second:   3, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010384, mae: 2.750259, mean_q: 3.338367, mean_eps: 0.762972
 132615/500000: episode: 193, duration: 128.853s, episode steps: 371, steps per second:   3, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011636, mae: 2.757373, mean_q: 3.344596, mean_eps: 0.761628
 133096/500000: episode: 194, duration: 164.649s, episode steps: 481, steps per second:   3, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.010082, mae: 2.744639, mean_q: 3.329812, mean_eps: 0.760861
 133887/500000: episode: 195, duration: 271.980s, episode steps: 791, steps per second:   3, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.010202, mae: 2.750480, mean_q: 3.337941, mean_eps: 0.759716
 134271/500000: episode: 196, duration: 131.585s, episode steps: 384, steps per second:   3, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.010734, mae: 2.748974, mean_q: 3.334212, mean_eps: 0.758659
 134848/500000: episode: 197, duration: 198.821s, episode steps: 577, steps per second:   3, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011203, mae: 2.743679, mean_q: 3.329093, mean_eps: 0.757794
 135450/500000: episode: 198, duration: 207.957s, episode steps: 602, steps per second:   3, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.010934, mae: 2.750242, mean_q: 3.336820, mean_eps: 0.756733
 136233/500000: episode: 199, duration: 270.174s, episode steps: 783, steps per second:   3, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.009875, mae: 2.726416, mean_q: 3.306960, mean_eps: 0.755486
 136668/500000: episode: 200, duration: 151.200s, episode steps: 435, steps per second:   3, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.011115, mae: 2.750146, mean_q: 3.336879, mean_eps: 0.754390
 137304/500000: episode: 201, duration: 218.553s, episode steps: 636, steps per second:   3, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.010863, mae: 2.747075, mean_q: 3.332731, mean_eps: 0.753426
 137894/500000: episode: 202, duration: 204.486s, episode steps: 590, steps per second:   3, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.011861, mae: 2.745556, mean_q: 3.332107, mean_eps: 0.752323
 138404/500000: episode: 203, duration: 176.628s, episode steps: 510, steps per second:   3, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.009703, mae: 2.728179, mean_q: 3.310146, mean_eps: 0.751333
 138922/500000: episode: 204, duration: 177.958s, episode steps: 518, steps per second:   3, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.011010, mae: 2.741881, mean_q: 3.327698, mean_eps: 0.750408
 139480/500000: episode: 205, duration: 192.850s, episode steps: 558, steps per second:   3, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.010805, mae: 2.741811, mean_q: 3.327175, mean_eps: 0.749439
 140247/500000: episode: 206, duration: 264.915s, episode steps: 767, steps per second:   3, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010719, mae: 2.749541, mean_q: 3.336894, mean_eps: 0.748247
 140825/500000: episode: 207, duration: 197.872s, episode steps: 578, steps per second:   3, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.010769, mae: 2.733269, mean_q: 3.317058, mean_eps: 0.747036
 141406/500000: episode: 208, duration: 201.191s, episode steps: 581, steps per second:   3, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.010085, mae: 2.739663, mean_q: 3.324412, mean_eps: 0.745993
 142196/500000: episode: 209, duration: 270.564s, episode steps: 790, steps per second:   3, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.010414, mae: 2.741687, mean_q: 3.327138, mean_eps: 0.744759
 143068/500000: episode: 210, duration: 297.616s, episode steps: 872, steps per second:   3, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011675, mae: 2.740412, mean_q: 3.324810, mean_eps: 0.743263
 143572/500000: episode: 211, duration: 173.636s, episode steps: 504, steps per second:   3, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.009987, mae: 2.731793, mean_q: 3.315175, mean_eps: 0.742025
 144015/500000: episode: 212, duration: 151.336s, episode steps: 443, steps per second:   3, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010409, mae: 2.756120, mean_q: 3.344953, mean_eps: 0.741173
 144671/500000: episode: 213, duration: 226.048s, episode steps: 656, steps per second:   3, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.010959, mae: 2.747525, mean_q: 3.334496, mean_eps: 0.740183
 145659/500000: episode: 214, duration: 337.072s, episode steps: 988, steps per second:   3, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011145, mae: 2.738585, mean_q: 3.323435, mean_eps: 0.738704
 146282/500000: episode: 215, duration: 212.577s, episode steps: 623, steps per second:   3, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.010172, mae: 2.726516, mean_q: 3.307578, mean_eps: 0.737254
 146767/500000: episode: 216, duration: 165.952s, episode steps: 485, steps per second:   3, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.010707, mae: 2.744509, mean_q: 3.329973, mean_eps: 0.736257
 147370/500000: episode: 217, duration: 205.674s, episode steps: 603, steps per second:   3, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.010935, mae: 2.723576, mean_q: 3.304994, mean_eps: 0.735278
 147881/500000: episode: 218, duration: 180.096s, episode steps: 511, steps per second:   3, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.011385, mae: 2.740644, mean_q: 3.326339, mean_eps: 0.734275
 148697/500000: episode: 219, duration: 317.509s, episode steps: 816, steps per second:   3, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.010719, mae: 2.740621, mean_q: 3.326441, mean_eps: 0.733081
 149387/500000: episode: 220, duration: 254.766s, episode steps: 690, steps per second:   3, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.010714, mae: 2.747033, mean_q: 3.333784, mean_eps: 0.731725
 150008/500000: episode: 221, duration: 212.444s, episode steps: 621, steps per second:   3, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.008948, mae: 2.738364, mean_q: 3.323590, mean_eps: 0.730545
 150441/500000: episode: 222, duration: 147.418s, episode steps: 433, steps per second:   3, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.013868, mae: 2.757450, mean_q: 3.344789, mean_eps: 0.729597
 150988/500000: episode: 223, duration: 187.648s, episode steps: 547, steps per second:   3, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.012764, mae: 2.775634, mean_q: 3.366284, mean_eps: 0.728715
 151418/500000: episode: 224, duration: 146.333s, episode steps: 430, steps per second:   3, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.013586, mae: 2.781189, mean_q: 3.371520, mean_eps: 0.727835
 151997/500000: episode: 225, duration: 195.799s, episode steps: 579, steps per second:   3, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.012510, mae: 2.769296, mean_q: 3.358040, mean_eps: 0.726927
 152767/500000: episode: 226, duration: 263.398s, episode steps: 770, steps per second:   3, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.012433, mae: 2.762867, mean_q: 3.350040, mean_eps: 0.725713
 153481/500000: episode: 227, duration: 242.923s, episode steps: 714, steps per second:   3, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012567, mae: 2.758129, mean_q: 3.343496, mean_eps: 0.724378
 154177/500000: episode: 228, duration: 236.610s, episode steps: 696, steps per second:   3, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.013038, mae: 2.767920, mean_q: 3.355904, mean_eps: 0.723109
 155145/500000: episode: 229, duration: 328.537s, episode steps: 968, steps per second:   3, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.011500, mae: 2.761462, mean_q: 3.348257, mean_eps: 0.721611
 156182/500000: episode: 230, duration: 351.220s, episode steps: 1037, steps per second:   3, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.011871, mae: 2.766929, mean_q: 3.354542, mean_eps: 0.719807
 157085/500000: episode: 231, duration: 308.218s, episode steps: 903, steps per second:   3, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.011454, mae: 2.751752, mean_q: 3.336111, mean_eps: 0.718061
 157551/500000: episode: 232, duration: 158.331s, episode steps: 466, steps per second:   3, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.011716, mae: 2.753207, mean_q: 3.338171, mean_eps: 0.716828
 158245/500000: episode: 233, duration: 237.292s, episode steps: 694, steps per second:   3, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.011780, mae: 2.758935, mean_q: 3.345278, mean_eps: 0.715785
 159004/500000: episode: 234, duration: 259.223s, episode steps: 759, steps per second:   3, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.011761, mae: 2.749923, mean_q: 3.335027, mean_eps: 0.714477
 159400/500000: episode: 235, duration: 137.370s, episode steps: 396, steps per second:   3, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.011571, mae: 2.757420, mean_q: 3.344105, mean_eps: 0.713437
 160083/500000: episode: 236, duration: 232.042s, episode steps: 683, steps per second:   3, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.011166, mae: 2.758066, mean_q: 3.343725, mean_eps: 0.712466
 160644/500000: episode: 237, duration: 191.504s, episode steps: 561, steps per second:   3, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.011811, mae: 2.760038, mean_q: 3.347450, mean_eps: 0.711347
 161183/500000: episode: 238, duration: 185.246s, episode steps: 539, steps per second:   3, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.011798, mae: 2.768211, mean_q: 3.357356, mean_eps: 0.710357
 161758/500000: episode: 239, duration: 196.566s, episode steps: 575, steps per second:   3, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011308, mae: 2.757142, mean_q: 3.343582, mean_eps: 0.709354
 162416/500000: episode: 240, duration: 224.812s, episode steps: 658, steps per second:   3, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.010992, mae: 2.756774, mean_q: 3.342326, mean_eps: 0.708244
 163086/500000: episode: 241, duration: 226.856s, episode steps: 670, steps per second:   3, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.010730, mae: 2.758960, mean_q: 3.345304, mean_eps: 0.707049
 163946/500000: episode: 242, duration: 293.717s, episode steps: 860, steps per second:   3, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.012230, mae: 2.751830, mean_q: 3.335895, mean_eps: 0.705672
 164343/500000: episode: 243, duration: 135.486s, episode steps: 397, steps per second:   3, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.012535, mae: 2.771305, mean_q: 3.358573, mean_eps: 0.704541
 164852/500000: episode: 244, duration: 173.144s, episode steps: 509, steps per second:   3, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.011912, mae: 2.749046, mean_q: 3.333406, mean_eps: 0.703725
 165543/500000: episode: 245, duration: 235.886s, episode steps: 691, steps per second:   3, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010835, mae: 2.749481, mean_q: 3.333303, mean_eps: 0.702645
 166036/500000: episode: 246, duration: 166.808s, episode steps: 493, steps per second:   3, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.011508, mae: 2.745363, mean_q: 3.329356, mean_eps: 0.701580
 166573/500000: episode: 247, duration: 182.531s, episode steps: 537, steps per second:   3, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.012336, mae: 2.758755, mean_q: 3.344629, mean_eps: 0.700653
 167858/500000: episode: 248, duration: 438.970s, episode steps: 1285, steps per second:   3, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.011166, mae: 2.757121, mean_q: 3.343235, mean_eps: 0.699013
 168551/500000: episode: 249, duration: 234.396s, episode steps: 693, steps per second:   3, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.011067, mae: 2.752718, mean_q: 3.337487, mean_eps: 0.697233
 169033/500000: episode: 250, duration: 165.801s, episode steps: 482, steps per second:   3, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.011123, mae: 2.756928, mean_q: 3.343009, mean_eps: 0.696175
 169899/500000: episode: 251, duration: 293.496s, episode steps: 866, steps per second:   3, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.011239, mae: 2.752193, mean_q: 3.337323, mean_eps: 0.694962
 170495/500000: episode: 252, duration: 203.994s, episode steps: 596, steps per second:   3, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.011261, mae: 2.741054, mean_q: 3.323795, mean_eps: 0.693646
 170948/500000: episode: 253, duration: 154.816s, episode steps: 453, steps per second:   3, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.012444, mae: 2.747078, mean_q: 3.330169, mean_eps: 0.692702
 171353/500000: episode: 254, duration: 138.299s, episode steps: 405, steps per second:   3, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.009816, mae: 2.748588, mean_q: 3.333109, mean_eps: 0.691930
 171729/500000: episode: 255, duration: 129.158s, episode steps: 376, steps per second:   3, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.010149, mae: 2.756122, mean_q: 3.341957, mean_eps: 0.691227
 172741/500000: episode: 256, duration: 346.593s, episode steps: 1012, steps per second:   3, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.010295, mae: 2.760763, mean_q: 3.347299, mean_eps: 0.689978
 173315/500000: episode: 257, duration: 207.125s, episode steps: 574, steps per second:   3, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.011118, mae: 2.743417, mean_q: 3.325027, mean_eps: 0.688550
 174078/500000: episode: 258, duration: 264.553s, episode steps: 763, steps per second:   3, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.010803, mae: 2.751820, mean_q: 3.336478, mean_eps: 0.687347
 174457/500000: episode: 259, duration: 130.947s, episode steps: 379, steps per second:   3, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.010247, mae: 2.741471, mean_q: 3.322711, mean_eps: 0.686319
 175052/500000: episode: 260, duration: 204.460s, episode steps: 595, steps per second:   3, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.011481, mae: 2.747525, mean_q: 3.331839, mean_eps: 0.685443
 176241/500000: episode: 261, duration: 406.955s, episode steps: 1189, steps per second:   3, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011247, mae: 2.751925, mean_q: 3.336582, mean_eps: 0.683837
 176767/500000: episode: 262, duration: 180.912s, episode steps: 526, steps per second:   3, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011970, mae: 2.760574, mean_q: 3.346973, mean_eps: 0.682294
 177403/500000: episode: 263, duration: 221.225s, episode steps: 636, steps per second:   3, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010821, mae: 2.764343, mean_q: 3.352954, mean_eps: 0.681248
 178327/500000: episode: 264, duration: 318.886s, episode steps: 924, steps per second:   3, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.010733, mae: 2.747172, mean_q: 3.331166, mean_eps: 0.679844
 178934/500000: episode: 265, duration: 209.301s, episode steps: 607, steps per second:   3, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.011772, mae: 2.738896, mean_q: 3.320588, mean_eps: 0.678466
 179716/500000: episode: 266, duration: 270.498s, episode steps: 782, steps per second:   3, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.010174, mae: 2.747973, mean_q: 3.331518, mean_eps: 0.677216
 180186/500000: episode: 267, duration: 162.993s, episode steps: 470, steps per second:   3, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.009874, mae: 2.742217, mean_q: 3.326017, mean_eps: 0.676089
 180841/500000: episode: 268, duration: 226.560s, episode steps: 655, steps per second:   3, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.011119, mae: 2.740973, mean_q: 3.324996, mean_eps: 0.675077
 181294/500000: episode: 269, duration: 155.585s, episode steps: 453, steps per second:   3, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.011197, mae: 2.749029, mean_q: 3.333431, mean_eps: 0.674079
 181987/500000: episode: 270, duration: 240.747s, episode steps: 693, steps per second:   3, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.010394, mae: 2.747396, mean_q: 3.331462, mean_eps: 0.673048
 182533/500000: episode: 271, duration: 188.604s, episode steps: 546, steps per second:   3, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.010522, mae: 2.737043, mean_q: 3.319553, mean_eps: 0.671933
 183099/500000: episode: 272, duration: 195.324s, episode steps: 566, steps per second:   3, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.011376, mae: 2.753493, mean_q: 3.339547, mean_eps: 0.670932
 184045/500000: episode: 273, duration: 328.564s, episode steps: 946, steps per second:   3, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010743, mae: 2.743016, mean_q: 3.326222, mean_eps: 0.669571
 185142/500000: episode: 274, duration: 378.705s, episode steps: 1097, steps per second:   3, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.011048, mae: 2.750854, mean_q: 3.335419, mean_eps: 0.667733
 186041/500000: episode: 275, duration: 311.960s, episode steps: 899, steps per second:   3, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.010473, mae: 2.736745, mean_q: 3.318307, mean_eps: 0.665936
 186765/500000: episode: 276, duration: 270.200s, episode steps: 724, steps per second:   3, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.010526, mae: 2.740950, mean_q: 3.323636, mean_eps: 0.664475
 187642/500000: episode: 277, duration: 311.818s, episode steps: 877, steps per second:   3, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.012083, mae: 2.746888, mean_q: 3.330406, mean_eps: 0.663035
 188174/500000: episode: 278, duration: 185.985s, episode steps: 532, steps per second:   3, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.010206, mae: 2.733778, mean_q: 3.315908, mean_eps: 0.661766
 189052/500000: episode: 279, duration: 303.933s, episode steps: 878, steps per second:   3, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.010897, mae: 2.734518, mean_q: 3.315274, mean_eps: 0.660498
 190229/500000: episode: 280, duration: 404.088s, episode steps: 1177, steps per second:   3, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.010894, mae: 2.738537, mean_q: 3.320356, mean_eps: 0.658648
 190728/500000: episode: 281, duration: 170.972s, episode steps: 499, steps per second:   3, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.011984, mae: 2.747841, mean_q: 3.332044, mean_eps: 0.657140
 191452/500000: episode: 282, duration: 249.863s, episode steps: 724, steps per second:   3, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.011037, mae: 2.740460, mean_q: 3.322435, mean_eps: 0.656039
 192632/500000: episode: 283, duration: 403.435s, episode steps: 1180, steps per second:   3, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.010890, mae: 2.734874, mean_q: 3.316847, mean_eps: 0.654325
 193406/500000: episode: 284, duration: 265.922s, episode steps: 774, steps per second:   3, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.010886, mae: 2.754559, mean_q: 3.340443, mean_eps: 0.652567