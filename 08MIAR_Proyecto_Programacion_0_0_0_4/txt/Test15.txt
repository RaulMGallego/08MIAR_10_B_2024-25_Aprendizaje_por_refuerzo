['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_260000.h5f.index', './models\\dqn_weights_270000.h5f.index', './models\\dqn_weights_280000.h5f.index', './models\\dqn_weights_290000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_300000.h5f.index', './models\\dqn_weights_310000.h5f.index', './models\\dqn_weights_320000.h5f.index', './models\\dqn_weights_330000.h5f.index', './models\\dqn_weights_340000.h5f.index', './models\\dqn_weights_350000.h5f.index', './models\\dqn_weights_360000.h5f.index', './models\\dqn_weights_370000.h5f.index', './models\\dqn_weights_380000.h5f.index', './models\\dqn_weights_390000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_400000.h5f.index', './models\\dqn_weights_410000.h5f.index', './models\\dqn_weights_420000.h5f.index', './models\\dqn_weights_430000.h5f.index', './models\\dqn_weights_440000.h5f.index', './models\\dqn_weights_450000.h5f.index', './models\\dqn_weights_460000.h5f.index', './models\\dqn_weights_470000.h5f.index', './models\\dqn_weights_480000.h5f.index', './models\\dqn_weights_490000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_500000.h5f.index', './models\\dqn_weights_510000.h5f.index', './models\\dqn_weights_520000.h5f.index', './models\\dqn_weights_530000.h5f.index', './models\\dqn_weights_540000.h5f.index', './models\\dqn_weights_550000.h5f.index', './models\\dqn_weights_560000.h5f.index', './models\\dqn_weights_570000.h5f.index', './models\\dqn_weights_580000.h5f.index', './models\\dqn_weights_590000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_600000.h5f.index', './models\\dqn_weights_610000.h5f.index', './models\\dqn_weights_620000.h5f.index', './models\\dqn_weights_630000.h5f.index', './models\\dqn_weights_640000.h5f.index', './models\\dqn_weights_650000.h5f.index', './models\\dqn_weights_660000.h5f.index', './models\\dqn_weights_670000.h5f.index', './models\\dqn_weights_680000.h5f.index', './models\\dqn_weights_690000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_700000.h5f.index', './models\\dqn_weights_710000.h5f.index', './models\\dqn_weights_720000.h5f.index', './models\\dqn_weights_730000.h5f.index', './models\\dqn_weights_740000.h5f.index', './models\\dqn_weights_750000.h5f.index', './models\\dqn_weights_760000.h5f.index', './models\\dqn_weights_770000.h5f.index', './models\\dqn_weights_780000.h5f.index', './models\\dqn_weights_790000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_800000.h5f.index', './models\\dqn_weights_810000.h5f.index', './models\\dqn_weights_820000.h5f.index', './models\\dqn_weights_830000.h5f.index', './models\\dqn_weights_840000.h5f.index', './models\\dqn_weights_850000.h5f.index', './models\\dqn_weights_860000.h5f.index', './models\\dqn_weights_870000.h5f.index', './models\\dqn_weights_880000.h5f.index', './models\\dqn_weights_890000.h5f.index', './models\\dqn_weights_90000.h5f.index', './models\\dqn_weights_900000.h5f.index', './models\\dqn_weights_910000.h5f.index', './models\\dqn_weights_920000.h5f.index', './models\\dqn_weights_930000.h5f.index', './models\\dqn_weights_940000.h5f.index', './models\\dqn_weights_950000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_950000.h5f
Hay pesos anteriores y se van a cargar
Training for 1000000 steps ...
C:\Users\Rociniel\Anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    420/1000000: episode: 1, duration: 2.666s, episode steps: 420, steps per second: 158, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/1000000: episode: 2, duration: 5.304s, episode steps: 711, steps per second: 134, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/1000000: episode: 3, duration: 7.055s, episode steps: 810, steps per second: 115, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2827/1000000: episode: 4, duration: 4.783s, episode steps: 886, steps per second: 185, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3345/1000000: episode: 5, duration: 3.104s, episode steps: 518, steps per second: 167, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4055/1000000: episode: 6, duration: 5.760s, episode steps: 710, steps per second: 123, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4544/1000000: episode: 7, duration: 3.397s, episode steps: 489, steps per second: 144, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5078/1000000: episode: 8, duration: 3.143s, episode steps: 534, steps per second: 170, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5870/1000000: episode: 9, duration: 5.523s, episode steps: 792, steps per second: 143, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6266/1000000: episode: 10, duration: 3.155s, episode steps: 396, steps per second: 126, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7630/1000000: episode: 11, duration: 8.526s, episode steps: 1364, steps per second: 160, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8278/1000000: episode: 12, duration: 5.069s, episode steps: 648, steps per second: 128, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8940/1000000: episode: 13, duration: 4.154s, episode steps: 662, steps per second: 159, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9470/1000000: episode: 14, duration: 3.038s, episode steps: 530, steps per second: 174, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
C:\Users\Rociniel\Anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
  10292/1000000: episode: 15, duration: 22.817s, episode steps: 822, steps per second:  36, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.015815, mae: 2.833363, mean_q: 3.432609, mean_eps: 0.979911
  11484/1000000: episode: 16, duration: 68.467s, episode steps: 1192, steps per second:  17, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.019212, mae: 2.850413, mean_q: 3.451623, mean_eps: 0.978446
  12742/1000000: episode: 17, duration: 82.573s, episode steps: 1258, steps per second:  15, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.015363, mae: 2.869210, mean_q: 3.473272, mean_eps: 0.976018
  13145/1000000: episode: 18, duration: 22.315s, episode steps: 403, steps per second:  18, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.012099, mae: 2.853730, mean_q: 3.455754, mean_eps: 0.974371
  14055/1000000: episode: 19, duration: 55.348s, episode steps: 910, steps per second:  16, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.016051, mae: 2.830189, mean_q: 3.421927, mean_eps: 0.973072
  14713/1000000: episode: 20, duration: 39.695s, episode steps: 658, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.014833, mae: 2.804422, mean_q: 3.393891, mean_eps: 0.971520
  15453/1000000: episode: 21, duration: 42.860s, episode steps: 740, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.015314, mae: 2.787139, mean_q: 3.372354, mean_eps: 0.970134
  16214/1000000: episode: 22, duration: 47.218s, episode steps: 761, steps per second:  16, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.014063, mae: 2.800947, mean_q: 3.391129, mean_eps: 0.968649
  17054/1000000: episode: 23, duration: 49.688s, episode steps: 840, steps per second:  17, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.014540, mae: 2.783050, mean_q: 3.368630, mean_eps: 0.967065
  17761/1000000: episode: 24, duration: 44.717s, episode steps: 707, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.015082, mae: 2.811141, mean_q: 3.401337, mean_eps: 0.965532
  18838/1000000: episode: 25, duration: 61.320s, episode steps: 1077, steps per second:  18, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.014488, mae: 2.818421, mean_q: 3.411568, mean_eps: 0.963766
  19238/1000000: episode: 26, duration: 22.647s, episode steps: 400, steps per second:  18, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.017758, mae: 2.779325, mean_q: 3.361263, mean_eps: 0.962305
  20010/1000000: episode: 27, duration: 42.906s, episode steps: 772, steps per second:  18, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012586, mae: 2.813266, mean_q: 3.406486, mean_eps: 0.961144
  20660/1000000: episode: 28, duration: 36.247s, episode steps: 650, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.013941, mae: 2.789367, mean_q: 3.375216, mean_eps: 0.959739
  21197/1000000: episode: 29, duration: 29.458s, episode steps: 537, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.013129, mae: 2.807437, mean_q: 3.396683, mean_eps: 0.958563
  22203/1000000: episode: 30, duration: 54.854s, episode steps: 1006, steps per second:  18, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.014690, mae: 2.787512, mean_q: 3.371955, mean_eps: 0.957034
  22609/1000000: episode: 31, duration: 23.345s, episode steps: 406, steps per second:  17, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.013559, mae: 2.788468, mean_q: 3.375331, mean_eps: 0.955636
  23224/1000000: episode: 32, duration: 34.492s, episode steps: 615, steps per second:  18, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.013765, mae: 2.784050, mean_q: 3.371539, mean_eps: 0.954626
  23812/1000000: episode: 33, duration: 32.952s, episode steps: 588, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.013893, mae: 2.790019, mean_q: 3.374640, mean_eps: 0.953438
  24206/1000000: episode: 34, duration: 20.930s, episode steps: 394, steps per second:  19, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013031, mae: 2.751721, mean_q: 3.329715, mean_eps: 0.952464
  25005/1000000: episode: 35, duration: 42.819s, episode steps: 799, steps per second:  19, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012522, mae: 2.764740, mean_q: 3.346012, mean_eps: 0.951280
  25955/1000000: episode: 36, duration: 48.729s, episode steps: 950, steps per second:  19, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012370, mae: 2.745731, mean_q: 3.321640, mean_eps: 0.949550
  26700/1000000: episode: 37, duration: 38.990s, episode steps: 745, steps per second:  19, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.012020, mae: 2.745937, mean_q: 3.319729, mean_eps: 0.947875
  27209/1000000: episode: 38, duration: 27.634s, episode steps: 509, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.010314, mae: 2.748688, mean_q: 3.327714, mean_eps: 0.946631
  27718/1000000: episode: 39, duration: 27.711s, episode steps: 509, steps per second:  18, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013898, mae: 2.778112, mean_q: 3.359682, mean_eps: 0.945621
  28362/1000000: episode: 40, duration: 33.197s, episode steps: 644, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.012189, mae: 2.764131, mean_q: 3.341156, mean_eps: 0.944481
  28983/1000000: episode: 41, duration: 33.278s, episode steps: 621, steps per second:  19, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.010987, mae: 2.746007, mean_q: 3.321138, mean_eps: 0.943229
  29838/1000000: episode: 42, duration: 44.689s, episode steps: 855, steps per second:  19, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.013390, mae: 2.740807, mean_q: 3.313691, mean_eps: 0.941768
  30750/1000000: episode: 43, duration: 47.346s, episode steps: 912, steps per second:  19, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.014467, mae: 2.738634, mean_q: 3.312538, mean_eps: 0.940018
  31634/1000000: episode: 44, duration: 45.917s, episode steps: 884, steps per second:  19, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.013056, mae: 2.725206, mean_q: 3.295708, mean_eps: 0.938240
  32329/1000000: episode: 45, duration: 36.980s, episode steps: 695, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011102, mae: 2.720560, mean_q: 3.290338, mean_eps: 0.936676
  33011/1000000: episode: 46, duration: 34.650s, episode steps: 682, steps per second:  20, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.013944, mae: 2.690019, mean_q: 3.251486, mean_eps: 0.935313
  33799/1000000: episode: 47, duration: 41.251s, episode steps: 788, steps per second:  19, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.013651, mae: 2.710486, mean_q: 3.277660, mean_eps: 0.933860
  34560/1000000: episode: 48, duration: 39.445s, episode steps: 761, steps per second:  19, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.011744, mae: 2.713591, mean_q: 3.279646, mean_eps: 0.932328
  35077/1000000: episode: 49, duration: 28.085s, episode steps: 517, steps per second:  18, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.013091, mae: 2.701907, mean_q: 3.264707, mean_eps: 0.931060
  35624/1000000: episode: 50, duration: 29.280s, episode steps: 547, steps per second:  19, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012829, mae: 2.676969, mean_q: 3.234512, mean_eps: 0.930007
  36214/1000000: episode: 51, duration: 31.232s, episode steps: 590, steps per second:  19, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012344, mae: 2.689911, mean_q: 3.249481, mean_eps: 0.928882
  37070/1000000: episode: 52, duration: 43.927s, episode steps: 856, steps per second:  19, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.014127, mae: 2.682300, mean_q: 3.240128, mean_eps: 0.927449
  37553/1000000: episode: 53, duration: 25.693s, episode steps: 483, steps per second:  19, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.015862, mae: 2.704540, mean_q: 3.268471, mean_eps: 0.926122
  38179/1000000: episode: 54, duration: 32.841s, episode steps: 626, steps per second:  19, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.010648, mae: 2.669748, mean_q: 3.228466, mean_eps: 0.925025
  38874/1000000: episode: 55, duration: 39.683s, episode steps: 695, steps per second:  18, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.010439, mae: 2.665734, mean_q: 3.224640, mean_eps: 0.923719
  39531/1000000: episode: 56, duration: 34.925s, episode steps: 657, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.013816, mae: 2.675852, mean_q: 3.231623, mean_eps: 0.922380
  40332/1000000: episode: 57, duration: 42.542s, episode steps: 801, steps per second:  19, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.014649, mae: 2.695582, mean_q: 3.258564, mean_eps: 0.920939
  41006/1000000: episode: 58, duration: 37.030s, episode steps: 674, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.011960, mae: 2.684492, mean_q: 3.243331, mean_eps: 0.919477
  41629/1000000: episode: 59, duration: 33.541s, episode steps: 623, steps per second:  19, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011738, mae: 2.685326, mean_q: 3.245031, mean_eps: 0.918190
  42439/1000000: episode: 60, duration: 41.838s, episode steps: 810, steps per second:  19, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.014652, mae: 2.664619, mean_q: 3.219844, mean_eps: 0.916773
  43134/1000000: episode: 61, duration: 37.486s, episode steps: 695, steps per second:  19, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.012893, mae: 2.685969, mean_q: 3.245227, mean_eps: 0.915284
  43868/1000000: episode: 62, duration: 39.094s, episode steps: 734, steps per second:  19, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.014284, mae: 2.704867, mean_q: 3.269649, mean_eps: 0.913870
  44566/1000000: episode: 63, duration: 36.354s, episode steps: 698, steps per second:  19, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.013749, mae: 2.701601, mean_q: 3.266167, mean_eps: 0.912452
  44954/1000000: episode: 64, duration: 21.800s, episode steps: 388, steps per second:  18, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.013194, mae: 2.684347, mean_q: 3.244809, mean_eps: 0.911375
  45464/1000000: episode: 65, duration: 26.347s, episode steps: 510, steps per second:  19, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013401, mae: 2.679254, mean_q: 3.240456, mean_eps: 0.910488
  45828/1000000: episode: 66, duration: 19.993s, episode steps: 364, steps per second:  18, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.013695, mae: 2.672036, mean_q: 3.228625, mean_eps: 0.909625
  46410/1000000: episode: 67, duration: 33.747s, episode steps: 582, steps per second:  17, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010980, mae: 2.674569, mean_q: 3.231612, mean_eps: 0.908686
  47047/1000000: episode: 68, duration: 34.826s, episode steps: 637, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012697, mae: 2.677286, mean_q: 3.237598, mean_eps: 0.907479
  47647/1000000: episode: 69, duration: 32.485s, episode steps: 600, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012280, mae: 2.662164, mean_q: 3.218820, mean_eps: 0.906255
  48195/1000000: episode: 70, duration: 30.923s, episode steps: 548, steps per second:  18, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.013690, mae: 2.669090, mean_q: 3.224773, mean_eps: 0.905118
  48589/1000000: episode: 71, duration: 21.964s, episode steps: 394, steps per second:  18, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.011594, mae: 2.653029, mean_q: 3.206163, mean_eps: 0.904184
  49272/1000000: episode: 72, duration: 38.173s, episode steps: 683, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.012962, mae: 2.620828, mean_q: 3.167333, mean_eps: 0.903119
  49909/1000000: episode: 73, duration: 37.158s, episode steps: 637, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.011423, mae: 2.663024, mean_q: 3.218704, mean_eps: 0.901812
  50617/1000000: episode: 74, duration: 39.289s, episode steps: 708, steps per second:  18, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.013792, mae: 2.671948, mean_q: 3.229883, mean_eps: 0.900477
  51678/1000000: episode: 75, duration: 57.419s, episode steps: 1061, steps per second:  18, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.015052, mae: 2.679265, mean_q: 3.237476, mean_eps: 0.898727
  52201/1000000: episode: 76, duration: 31.711s, episode steps: 523, steps per second:  16, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.016937, mae: 2.677660, mean_q: 3.238237, mean_eps: 0.897159
  52742/1000000: episode: 77, duration: 29.370s, episode steps: 541, steps per second:  18, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.015622, mae: 2.687778, mean_q: 3.248653, mean_eps: 0.896105
  54005/1000000: episode: 78, duration: 69.105s, episode steps: 1263, steps per second:  18, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.012587, mae: 2.687265, mean_q: 3.249465, mean_eps: 0.894319
  54438/1000000: episode: 79, duration: 26.786s, episode steps: 433, steps per second:  16, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.011923, mae: 2.658672, mean_q: 3.215222, mean_eps: 0.892640
  55285/1000000: episode: 80, duration: 46.729s, episode steps: 847, steps per second:  18, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.014604, mae: 2.677798, mean_q: 3.236778, mean_eps: 0.891373
  55784/1000000: episode: 81, duration: 28.508s, episode steps: 499, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014641, mae: 2.673669, mean_q: 3.230306, mean_eps: 0.890043
  56287/1000000: episode: 82, duration: 26.870s, episode steps: 503, steps per second:  19, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012497, mae: 2.692428, mean_q: 3.254751, mean_eps: 0.889053
  57564/1000000: episode: 83, duration: 72.941s, episode steps: 1277, steps per second:  18, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.013570, mae: 2.665416, mean_q: 3.222131, mean_eps: 0.887290
  58238/1000000: episode: 84, duration: 38.031s, episode steps: 674, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.014968, mae: 2.654035, mean_q: 3.208921, mean_eps: 0.885358
  58721/1000000: episode: 85, duration: 27.254s, episode steps: 483, steps per second:  18, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.012725, mae: 2.662755, mean_q: 3.220552, mean_eps: 0.884210
  59335/1000000: episode: 86, duration: 34.427s, episode steps: 614, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.011780, mae: 2.673360, mean_q: 3.231763, mean_eps: 0.883125
  59855/1000000: episode: 87, duration: 28.959s, episode steps: 520, steps per second:  18, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.013440, mae: 2.670672, mean_q: 3.229133, mean_eps: 0.882004
  60364/1000000: episode: 88, duration: 28.824s, episode steps: 509, steps per second:  18, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.013805, mae: 2.682511, mean_q: 3.241885, mean_eps: 0.880986
  60856/1000000: episode: 89, duration: 27.316s, episode steps: 492, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.016125, mae: 2.635224, mean_q: 3.183806, mean_eps: 0.879996
  61317/1000000: episode: 90, duration: 26.194s, episode steps: 461, steps per second:  18, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.012515, mae: 2.654285, mean_q: 3.207658, mean_eps: 0.879050
  62159/1000000: episode: 91, duration: 46.888s, episode steps: 842, steps per second:  18, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.013939, mae: 2.643590, mean_q: 3.193721, mean_eps: 0.877759
  63198/1000000: episode: 92, duration: 57.936s, episode steps: 1039, steps per second:  18, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.013757, mae: 2.633096, mean_q: 3.181940, mean_eps: 0.875898
  63748/1000000: episode: 93, duration: 31.356s, episode steps: 550, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.012534, mae: 2.655080, mean_q: 3.207186, mean_eps: 0.874325
  64599/1000000: episode: 94, duration: 46.651s, episode steps: 851, steps per second:  18, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012930, mae: 2.639873, mean_q: 3.190787, mean_eps: 0.872939
  65179/1000000: episode: 95, duration: 33.958s, episode steps: 580, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.016008, mae: 2.650363, mean_q: 3.203495, mean_eps: 0.871522
  66104/1000000: episode: 96, duration: 51.456s, episode steps: 925, steps per second:  18, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.010787, mae: 2.651850, mean_q: 3.206413, mean_eps: 0.870033
  66728/1000000: episode: 97, duration: 35.109s, episode steps: 624, steps per second:  18, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.014535, mae: 2.635905, mean_q: 3.185569, mean_eps: 0.868500
  67393/1000000: episode: 98, duration: 37.674s, episode steps: 665, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.015823, mae: 2.655837, mean_q: 3.208375, mean_eps: 0.867221
  67791/1000000: episode: 99, duration: 22.742s, episode steps: 398, steps per second:  18, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.012661, mae: 2.656994, mean_q: 3.212878, mean_eps: 0.866168
  68778/1000000: episode: 100, duration: 54.985s, episode steps: 987, steps per second:  18, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.013744, mae: 2.654435, mean_q: 3.208939, mean_eps: 0.864798
  69401/1000000: episode: 101, duration: 34.892s, episode steps: 623, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.014544, mae: 2.638586, mean_q: 3.188158, mean_eps: 0.863202
  70020/1000000: episode: 102, duration: 34.594s, episode steps: 619, steps per second:  18, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.015073, mae: 2.621838, mean_q: 3.167130, mean_eps: 0.861974
  70426/1000000: episode: 103, duration: 23.304s, episode steps: 406, steps per second:  17, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.013241, mae: 2.643777, mean_q: 3.195698, mean_eps: 0.860960
  71199/1000000: episode: 104, duration: 44.774s, episode steps: 773, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.012636, mae: 2.633390, mean_q: 3.181553, mean_eps: 0.859792
  71649/1000000: episode: 105, duration: 23.680s, episode steps: 450, steps per second:  19, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.014145, mae: 2.630126, mean_q: 3.177458, mean_eps: 0.858580
  72145/1000000: episode: 106, duration: 29.047s, episode steps: 496, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.011694, mae: 2.617752, mean_q: 3.163955, mean_eps: 0.857642
  72641/1000000: episode: 107, duration: 27.338s, episode steps: 496, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.011259, mae: 2.647226, mean_q: 3.198191, mean_eps: 0.856660
  73298/1000000: episode: 108, duration: 37.678s, episode steps: 657, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.013209, mae: 2.664249, mean_q: 3.218726, mean_eps: 0.855519
  73932/1000000: episode: 109, duration: 34.336s, episode steps: 634, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.014831, mae: 2.626485, mean_q: 3.176062, mean_eps: 0.854244
  74448/1000000: episode: 110, duration: 30.362s, episode steps: 516, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.013598, mae: 2.648512, mean_q: 3.199436, mean_eps: 0.853108
  75157/1000000: episode: 111, duration: 39.971s, episode steps: 709, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.011230, mae: 2.639240, mean_q: 3.188801, mean_eps: 0.851892
  75745/1000000: episode: 112, duration: 34.017s, episode steps: 588, steps per second:  17, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.012006, mae: 2.647758, mean_q: 3.200104, mean_eps: 0.850605
  76615/1000000: episode: 113, duration: 48.255s, episode steps: 870, steps per second:  18, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012822, mae: 2.626075, mean_q: 3.172614, mean_eps: 0.849164
  77699/1000000: episode: 114, duration: 60.799s, episode steps: 1084, steps per second:  18, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013320, mae: 2.648674, mean_q: 3.200263, mean_eps: 0.847231
  78272/1000000: episode: 115, duration: 31.651s, episode steps: 573, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.015552, mae: 2.639116, mean_q: 3.188361, mean_eps: 0.845592
  79264/1000000: episode: 116, duration: 56.792s, episode steps: 992, steps per second:  17, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.012147, mae: 2.657847, mean_q: 3.211567, mean_eps: 0.844043
  79770/1000000: episode: 117, duration: 28.477s, episode steps: 506, steps per second:  18, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.015171, mae: 2.653135, mean_q: 3.206858, mean_eps: 0.842558
  81090/1000000: episode: 118, duration: 75.053s, episode steps: 1320, steps per second:  18, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.015398, mae: 2.648397, mean_q: 3.199744, mean_eps: 0.840749
  81726/1000000: episode: 119, duration: 35.780s, episode steps: 636, steps per second:  18, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.014035, mae: 2.651214, mean_q: 3.202557, mean_eps: 0.838812
  82505/1000000: episode: 120, duration: 43.748s, episode steps: 779, steps per second:  18, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.014030, mae: 2.666079, mean_q: 3.222718, mean_eps: 0.837410
  83109/1000000: episode: 121, duration: 33.325s, episode steps: 604, steps per second:  18, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.010868, mae: 2.635741, mean_q: 3.185664, mean_eps: 0.836040
  83881/1000000: episode: 122, duration: 43.627s, episode steps: 772, steps per second:  18, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.013009, mae: 2.650597, mean_q: 3.202288, mean_eps: 0.834678
  84941/1000000: episode: 123, duration: 59.564s, episode steps: 1060, steps per second:  18, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.012054, mae: 2.652982, mean_q: 3.205616, mean_eps: 0.832864
  85746/1000000: episode: 124, duration: 45.504s, episode steps: 805, steps per second:  18, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.012545, mae: 2.655235, mean_q: 3.207786, mean_eps: 0.831019
  86599/1000000: episode: 125, duration: 47.759s, episode steps: 853, steps per second:  18, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.013849, mae: 2.626038, mean_q: 3.172648, mean_eps: 0.829379
  87217/1000000: episode: 126, duration: 34.982s, episode steps: 618, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.013107, mae: 2.653960, mean_q: 3.205948, mean_eps: 0.827922
  88267/1000000: episode: 127, duration: 60.169s, episode steps: 1050, steps per second:  17, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.011928, mae: 2.646148, mean_q: 3.198857, mean_eps: 0.826271
  88976/1000000: episode: 128, duration: 39.085s, episode steps: 709, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.015411, mae: 2.649947, mean_q: 3.201952, mean_eps: 0.824532
  89456/1000000: episode: 129, duration: 28.878s, episode steps: 480, steps per second:  17, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.013616, mae: 2.629232, mean_q: 3.177756, mean_eps: 0.823356
  89986/1000000: episode: 130, duration: 29.270s, episode steps: 530, steps per second:  18, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012826, mae: 2.669315, mean_q: 3.224552, mean_eps: 0.822354
  90588/1000000: episode: 131, duration: 35.075s, episode steps: 602, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013859, mae: 2.629005, mean_q: 3.176159, mean_eps: 0.821234
  91387/1000000: episode: 132, duration: 44.239s, episode steps: 799, steps per second:  18, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.011974, mae: 2.637726, mean_q: 3.187381, mean_eps: 0.819848
  92004/1000000: episode: 133, duration: 35.751s, episode steps: 617, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.014076, mae: 2.616802, mean_q: 3.161594, mean_eps: 0.818446
  92664/1000000: episode: 134, duration: 37.270s, episode steps: 660, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.012564, mae: 2.644903, mean_q: 3.197410, mean_eps: 0.817183
  93084/1000000: episode: 135, duration: 25.086s, episode steps: 420, steps per second:  17, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.011630, mae: 2.623008, mean_q: 3.171152, mean_eps: 0.816113
  93595/1000000: episode: 136, duration: 27.854s, episode steps: 511, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012730, mae: 2.621428, mean_q: 3.168350, mean_eps: 0.815191
  94413/1000000: episode: 137, duration: 47.038s, episode steps: 818, steps per second:  17, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.012825, mae: 2.645636, mean_q: 3.195803, mean_eps: 0.813872
  95022/1000000: episode: 138, duration: 35.095s, episode steps: 609, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012617, mae: 2.646998, mean_q: 3.200532, mean_eps: 0.812458
  95707/1000000: episode: 139, duration: 37.723s, episode steps: 685, steps per second:  18, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012271, mae: 2.641524, mean_q: 3.192302, mean_eps: 0.811179
  96728/1000000: episode: 140, duration: 58.703s, episode steps: 1021, steps per second:  17, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013919, mae: 2.645481, mean_q: 3.195921, mean_eps: 0.809492
  97541/1000000: episode: 141, duration: 46.357s, episode steps: 813, steps per second:  18, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.013284, mae: 2.656660, mean_q: 3.210962, mean_eps: 0.807675
  98242/1000000: episode: 142, duration: 38.368s, episode steps: 701, steps per second:  18, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013194, mae: 2.628369, mean_q: 3.176425, mean_eps: 0.806174
  98918/1000000: episode: 143, duration: 40.236s, episode steps: 676, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.012226, mae: 2.615973, mean_q: 3.160504, mean_eps: 0.804812
  99454/1000000: episode: 144, duration: 28.924s, episode steps: 536, steps per second:  19, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.013294, mae: 2.640492, mean_q: 3.187719, mean_eps: 0.803612
 100138/1000000: episode: 145, duration: 40.074s, episode steps: 684, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.012541, mae: 2.645977, mean_q: 3.196248, mean_eps: 0.802404
 100534/1000000: episode: 146, duration: 21.554s, episode steps: 396, steps per second:  18, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.015113, mae: 2.639065, mean_q: 3.187256, mean_eps: 0.801335
 101057/1000000: episode: 147, duration: 30.618s, episode steps: 523, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013279, mae: 2.631977, mean_q: 3.178598, mean_eps: 0.800424
 101451/1000000: episode: 148, duration: 22.723s, episode steps: 394, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.015015, mae: 2.652523, mean_q: 3.203302, mean_eps: 0.799517
 102018/1000000: episode: 149, duration: 32.700s, episode steps: 567, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.015650, mae: 2.628946, mean_q: 3.176016, mean_eps: 0.798567
 102644/1000000: episode: 150, duration: 36.258s, episode steps: 626, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013265, mae: 2.635043, mean_q: 3.183671, mean_eps: 0.797387
 103248/1000000: episode: 151, duration: 36.304s, episode steps: 604, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.012031, mae: 2.662778, mean_q: 3.218311, mean_eps: 0.796171
 103649/1000000: episode: 152, duration: 23.401s, episode steps: 401, steps per second:  17, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.015502, mae: 2.664891, mean_q: 3.216828, mean_eps: 0.795173
 104484/1000000: episode: 153, duration: 47.947s, episode steps: 835, steps per second:  17, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.013271, mae: 2.655556, mean_q: 3.205947, mean_eps: 0.793949
 105099/1000000: episode: 154, duration: 34.427s, episode steps: 615, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.012740, mae: 2.633131, mean_q: 3.179479, mean_eps: 0.792516
 105636/1000000: episode: 155, duration: 31.931s, episode steps: 537, steps per second:  17, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011451, mae: 2.626714, mean_q: 3.170704, mean_eps: 0.791375
 106180/1000000: episode: 156, duration: 30.142s, episode steps: 544, steps per second:  18, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.014575, mae: 2.629466, mean_q: 3.173972, mean_eps: 0.790306
 106907/1000000: episode: 157, duration: 41.068s, episode steps: 727, steps per second:  18, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.014236, mae: 2.628826, mean_q: 3.173794, mean_eps: 0.789047
 107302/1000000: episode: 158, duration: 22.762s, episode steps: 395, steps per second:  17, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.013124, mae: 2.623681, mean_q: 3.168493, mean_eps: 0.787934
 107811/1000000: episode: 159, duration: 29.670s, episode steps: 509, steps per second:  17, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.016493, mae: 2.659437, mean_q: 3.212837, mean_eps: 0.787039
 108636/1000000: episode: 160, duration: 45.829s, episode steps: 825, steps per second:  18, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.013414, mae: 2.617649, mean_q: 3.161476, mean_eps: 0.785720
 109147/1000000: episode: 161, duration: 30.703s, episode steps: 511, steps per second:  17, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.012024, mae: 2.643407, mean_q: 3.193239, mean_eps: 0.784398
 110199/1000000: episode: 162, duration: 58.892s, episode steps: 1052, steps per second:  18, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.012271, mae: 2.629948, mean_q: 3.176564, mean_eps: 0.782849
 111100/1000000: episode: 163, duration: 50.660s, episode steps: 901, steps per second:  18, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.013776, mae: 2.639768, mean_q: 3.186531, mean_eps: 0.780917
 111594/1000000: episode: 164, duration: 28.947s, episode steps: 494, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012419, mae: 2.629768, mean_q: 3.178300, mean_eps: 0.779535
 112398/1000000: episode: 165, duration: 46.052s, episode steps: 804, steps per second:  17, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.014175, mae: 2.652578, mean_q: 3.203470, mean_eps: 0.778248
 113249/1000000: episode: 166, duration: 47.617s, episode steps: 851, steps per second:  18, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.013807, mae: 2.643135, mean_q: 3.192683, mean_eps: 0.776608
 113680/1000000: episode: 167, duration: 24.995s, episode steps: 431, steps per second:  17, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.014572, mae: 2.654514, mean_q: 3.209295, mean_eps: 0.775341
 114264/1000000: episode: 168, duration: 33.642s, episode steps: 584, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.015610, mae: 2.624225, mean_q: 3.168423, mean_eps: 0.774339
 114918/1000000: episode: 169, duration: 38.125s, episode steps: 654, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.011771, mae: 2.602503, mean_q: 3.146275, mean_eps: 0.773112
 115569/1000000: episode: 170, duration: 36.132s, episode steps: 651, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.011139, mae: 2.626257, mean_q: 3.175603, mean_eps: 0.771817
 116091/1000000: episode: 171, duration: 30.854s, episode steps: 522, steps per second:  17, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.016643, mae: 2.647032, mean_q: 3.197614, mean_eps: 0.770657
 116445/1000000: episode: 172, duration: 19.912s, episode steps: 354, steps per second:  18, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.015394, mae: 2.613485, mean_q: 3.156251, mean_eps: 0.769789
 117083/1000000: episode: 173, duration: 36.703s, episode steps: 638, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.012951, mae: 2.642867, mean_q: 3.192034, mean_eps: 0.768807
 117772/1000000: episode: 174, duration: 38.760s, episode steps: 689, steps per second:  18, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.014462, mae: 2.610407, mean_q: 3.150936, mean_eps: 0.767497
 118409/1000000: episode: 175, duration: 37.085s, episode steps: 637, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.012422, mae: 2.636042, mean_q: 3.184913, mean_eps: 0.766182
 118827/1000000: episode: 176, duration: 23.922s, episode steps: 418, steps per second:  17, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.013886, mae: 2.618525, mean_q: 3.161096, mean_eps: 0.765136
 119353/1000000: episode: 177, duration: 30.052s, episode steps: 526, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012718, mae: 2.617478, mean_q: 3.159571, mean_eps: 0.764202
 120108/1000000: episode: 178, duration: 42.015s, episode steps: 755, steps per second:  18, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.012188, mae: 2.621494, mean_q: 3.166139, mean_eps: 0.762935
 120768/1000000: episode: 179, duration: 39.520s, episode steps: 660, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.013748, mae: 2.621045, mean_q: 3.165861, mean_eps: 0.761537
 121292/1000000: episode: 180, duration: 28.926s, episode steps: 524, steps per second:  18, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.014106, mae: 2.635985, mean_q: 3.181879, mean_eps: 0.760365
 122109/1000000: episode: 181, duration: 47.143s, episode steps: 817, steps per second:  17, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.013158, mae: 2.637777, mean_q: 3.186457, mean_eps: 0.759034
 122726/1000000: episode: 182, duration: 36.538s, episode steps: 617, steps per second:  17, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.014412, mae: 2.648077, mean_q: 3.199133, mean_eps: 0.757612
 123524/1000000: episode: 183, duration: 45.285s, episode steps: 798, steps per second:  18, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.012933, mae: 2.654725, mean_q: 3.206700, mean_eps: 0.756214
 124901/1000000: episode: 184, duration: 78.777s, episode steps: 1377, steps per second:  17, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.013487, mae: 2.639437, mean_q: 3.187711, mean_eps: 0.754060
 125592/1000000: episode: 185, duration: 39.015s, episode steps: 691, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012004, mae: 2.616775, mean_q: 3.161202, mean_eps: 0.752013
 126217/1000000: episode: 186, duration: 35.544s, episode steps: 625, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.015004, mae: 2.637743, mean_q: 3.186330, mean_eps: 0.750710
 126862/1000000: episode: 187, duration: 36.953s, episode steps: 645, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013032, mae: 2.624417, mean_q: 3.170003, mean_eps: 0.749451
 127272/1000000: episode: 188, duration: 23.015s, episode steps: 410, steps per second:  18, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.014550, mae: 2.668406, mean_q: 3.222487, mean_eps: 0.748409
 127927/1000000: episode: 189, duration: 37.546s, episode steps: 655, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011952, mae: 2.645353, mean_q: 3.193884, mean_eps: 0.747356
 128429/1000000: episode: 190, duration: 28.736s, episode steps: 502, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.012308, mae: 2.607794, mean_q: 3.151962, mean_eps: 0.746208
 129552/1000000: episode: 191, duration: 64.009s, episode steps: 1123, steps per second:  18, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.011774, mae: 2.630831, mean_q: 3.176869, mean_eps: 0.744600
 130216/1000000: episode: 192, duration: 38.873s, episode steps: 664, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.013491, mae: 2.633567, mean_q: 3.179826, mean_eps: 0.742834
 130937/1000000: episode: 193, duration: 40.699s, episode steps: 721, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.012195, mae: 2.633562, mean_q: 3.180430, mean_eps: 0.741460
 131584/1000000: episode: 194, duration: 36.861s, episode steps: 647, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.013549, mae: 2.616996, mean_q: 3.162109, mean_eps: 0.740105
 132247/1000000: episode: 195, duration: 39.779s, episode steps: 663, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.012820, mae: 2.631478, mean_q: 3.177565, mean_eps: 0.738810
 132911/1000000: episode: 196, duration: 35.793s, episode steps: 664, steps per second:  19, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.011702, mae: 2.638361, mean_q: 3.186293, mean_eps: 0.737496
 133827/1000000: episode: 197, duration: 53.277s, episode steps: 916, steps per second:  17, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.012056, mae: 2.624172, mean_q: 3.166795, mean_eps: 0.735931
 134334/1000000: episode: 198, duration: 29.858s, episode steps: 507, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.012091, mae: 2.617783, mean_q: 3.162918, mean_eps: 0.734522
 134839/1000000: episode: 199, duration: 28.578s, episode steps: 505, steps per second:  18, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013670, mae: 2.612053, mean_q: 3.155273, mean_eps: 0.733520
 135670/1000000: episode: 200, duration: 48.298s, episode steps: 831, steps per second:  17, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.012095, mae: 2.619476, mean_q: 3.163365, mean_eps: 0.732197
 136225/1000000: episode: 201, duration: 30.625s, episode steps: 555, steps per second:  18, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.013548, mae: 2.625401, mean_q: 3.169208, mean_eps: 0.730823
 136930/1000000: episode: 202, duration: 42.250s, episode steps: 705, steps per second:  17, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.012947, mae: 2.632232, mean_q: 3.179702, mean_eps: 0.729576
 137505/1000000: episode: 203, duration: 31.537s, episode steps: 575, steps per second:  18, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.014838, mae: 2.628315, mean_q: 3.172732, mean_eps: 0.728308
 138373/1000000: episode: 204, duration: 50.437s, episode steps: 868, steps per second:  17, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.012629, mae: 2.628221, mean_q: 3.173424, mean_eps: 0.726879
 139011/1000000: episode: 205, duration: 36.607s, episode steps: 638, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.013976, mae: 2.635947, mean_q: 3.183743, mean_eps: 0.725390
 139713/1000000: episode: 206, duration: 40.316s, episode steps: 702, steps per second:  17, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.013076, mae: 2.612231, mean_q: 3.153152, mean_eps: 0.724063
 140332/1000000: episode: 207, duration: 36.491s, episode steps: 619, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.013547, mae: 2.644676, mean_q: 3.193308, mean_eps: 0.722756
 140824/1000000: episode: 208, duration: 27.695s, episode steps: 492, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012153, mae: 2.641442, mean_q: 3.187764, mean_eps: 0.721660
 141810/1000000: episode: 209, duration: 56.294s, episode steps: 986, steps per second:  18, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.012681, mae: 2.619259, mean_q: 3.164147, mean_eps: 0.720194
 142806/1000000: episode: 210, duration: 59.413s, episode steps: 996, steps per second:  17, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011836, mae: 2.615727, mean_q: 3.156476, mean_eps: 0.718230
 143811/1000000: episode: 211, duration: 57.702s, episode steps: 1005, steps per second:  17, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.012047, mae: 2.623801, mean_q: 3.167364, mean_eps: 0.716250
 144459/1000000: episode: 212, duration: 37.598s, episode steps: 648, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.012812, mae: 2.635262, mean_q: 3.182695, mean_eps: 0.714615
 145267/1000000: episode: 213, duration: 46.047s, episode steps: 808, steps per second:  18, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.012592, mae: 2.647079, mean_q: 3.198348, mean_eps: 0.713173
 146070/1000000: episode: 214, duration: 47.256s, episode steps: 803, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.011463, mae: 2.612182, mean_q: 3.151389, mean_eps: 0.711577
 146696/1000000: episode: 215, duration: 35.775s, episode steps: 626, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.014250, mae: 2.603179, mean_q: 3.141234, mean_eps: 0.710164
 147104/1000000: episode: 216, duration: 23.115s, episode steps: 408, steps per second:  18, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.015968, mae: 2.630831, mean_q: 3.176875, mean_eps: 0.709142
 147606/1000000: episode: 217, duration: 28.905s, episode steps: 502, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.012521, mae: 2.633364, mean_q: 3.176003, mean_eps: 0.708239
 148418/1000000: episode: 218, duration: 47.180s, episode steps: 812, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.012392, mae: 2.634528, mean_q: 3.180144, mean_eps: 0.706936
 149471/1000000: episode: 219, duration: 61.288s, episode steps: 1053, steps per second:  17, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.012201, mae: 2.624852, mean_q: 3.168819, mean_eps: 0.705091
 150566/1000000: episode: 220, duration: 63.665s, episode steps: 1095, steps per second:  17, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013161, mae: 2.636076, mean_q: 3.180889, mean_eps: 0.702964
 151225/1000000: episode: 221, duration: 36.763s, episode steps: 659, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.014211, mae: 2.651825, mean_q: 3.199927, mean_eps: 0.701226
 151843/1000000: episode: 222, duration: 37.592s, episode steps: 618, steps per second:  16, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.013405, mae: 2.630897, mean_q: 3.174481, mean_eps: 0.699963
 152383/1000000: episode: 223, duration: 30.743s, episode steps: 540, steps per second:  18, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.014620, mae: 2.614453, mean_q: 3.157764, mean_eps: 0.698818
 153070/1000000: episode: 224, duration: 41.117s, episode steps: 687, steps per second:  17, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.014797, mae: 2.650383, mean_q: 3.198843, mean_eps: 0.697603
 153887/1000000: episode: 225, duration: 47.733s, episode steps: 817, steps per second:  17, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.013426, mae: 2.655547, mean_q: 3.205925, mean_eps: 0.696114
 154400/1000000: episode: 226, duration: 29.699s, episode steps: 513, steps per second:  17, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.011783, mae: 2.647882, mean_q: 3.196295, mean_eps: 0.694799
 155157/1000000: episode: 227, duration: 44.321s, episode steps: 757, steps per second:  17, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.013568, mae: 2.619777, mean_q: 3.160463, mean_eps: 0.693540
 155838/1000000: episode: 228, duration: 37.874s, episode steps: 681, steps per second:  18, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.014055, mae: 2.656106, mean_q: 3.204646, mean_eps: 0.692114
 156493/1000000: episode: 229, duration: 40.303s, episode steps: 655, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.011820, mae: 2.627402, mean_q: 3.169925, mean_eps: 0.690791
 157167/1000000: episode: 230, duration: 39.403s, episode steps: 674, steps per second:  17, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.012701, mae: 2.628289, mean_q: 3.174268, mean_eps: 0.689477
 158067/1000000: episode: 231, duration: 50.722s, episode steps: 900, steps per second:  18, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.014335, mae: 2.628146, mean_q: 3.170282, mean_eps: 0.687920
 158749/1000000: episode: 232, duration: 41.018s, episode steps: 682, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.013364, mae: 2.637478, mean_q: 3.183430, mean_eps: 0.686352
 159173/1000000: episode: 233, duration: 24.140s, episode steps: 424, steps per second:  18, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.814 [0.000, 5.000],  loss: 0.013402, mae: 2.669096, mean_q: 3.221571, mean_eps: 0.685255
 159999/1000000: episode: 234, duration: 48.035s, episode steps: 826, steps per second:  17, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.014064, mae: 2.637290, mean_q: 3.183330, mean_eps: 0.684020
 160655/1000000: episode: 235, duration: 38.898s, episode steps: 656, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.011986, mae: 2.634978, mean_q: 3.180371, mean_eps: 0.682555
 161837/1000000: episode: 236, duration: 69.573s, episode steps: 1182, steps per second:  17, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.014237, mae: 2.645544, mean_q: 3.192673, mean_eps: 0.680733
 162789/1000000: episode: 237, duration: 55.480s, episode steps: 952, steps per second:  17, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013022, mae: 2.631528, mean_q: 3.175919, mean_eps: 0.678618
 163424/1000000: episode: 238, duration: 38.461s, episode steps: 635, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.012613, mae: 2.640118, mean_q: 3.184989, mean_eps: 0.677050
 164067/1000000: episode: 239, duration: 37.498s, episode steps: 643, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.013276, mae: 2.649956, mean_q: 3.199031, mean_eps: 0.675787
 164656/1000000: episode: 240, duration: 34.343s, episode steps: 589, steps per second:  17, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.013233, mae: 2.631194, mean_q: 3.174217, mean_eps: 0.674567
 165467/1000000: episode: 241, duration: 47.437s, episode steps: 811, steps per second:  17, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.012667, mae: 2.676361, mean_q: 3.228999, mean_eps: 0.673181
 166252/1000000: episode: 242, duration: 46.135s, episode steps: 785, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.015995, mae: 2.643358, mean_q: 3.190365, mean_eps: 0.671601
 166735/1000000: episode: 243, duration: 28.743s, episode steps: 483, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.012071, mae: 2.641811, mean_q: 3.188243, mean_eps: 0.670346
 167439/1000000: episode: 244, duration: 41.664s, episode steps: 704, steps per second:  17, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.011614, mae: 2.643423, mean_q: 3.190050, mean_eps: 0.669170
 168707/1000000: episode: 245, duration: 71.985s, episode steps: 1268, steps per second:  18, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.013291, mae: 2.619548, mean_q: 3.161748, mean_eps: 0.667217
 169062/1000000: episode: 246, duration: 21.177s, episode steps: 355, steps per second:  17, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013335, mae: 2.650433, mean_q: 3.200681, mean_eps: 0.665610
 169733/1000000: episode: 247, duration: 39.177s, episode steps: 671, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012617, mae: 2.644035, mean_q: 3.189358, mean_eps: 0.664592
 171089/1000000: episode: 248, duration: 78.516s, episode steps: 1356, steps per second:  17, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.013402, mae: 2.634648, mean_q: 3.180244, mean_eps: 0.662584
 171759/1000000: episode: 249, duration: 37.325s, episode steps: 670, steps per second:  18, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.013047, mae: 2.653733, mean_q: 3.202701, mean_eps: 0.660580
 172321/1000000: episode: 250, duration: 33.339s, episode steps: 562, steps per second:  17, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.013285, mae: 2.634986, mean_q: 3.180561, mean_eps: 0.659361
 173178/1000000: episode: 251, duration: 49.665s, episode steps: 857, steps per second:  17, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.013930, mae: 2.636499, mean_q: 3.180854, mean_eps: 0.657955
 173925/1000000: episode: 252, duration: 42.785s, episode steps: 747, steps per second:  17, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.014046, mae: 2.643225, mean_q: 3.188892, mean_eps: 0.656367
 174614/1000000: episode: 253, duration: 42.228s, episode steps: 689, steps per second:  16, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013880, mae: 2.651086, mean_q: 3.198377, mean_eps: 0.654945
 175111/1000000: episode: 254, duration: 27.326s, episode steps: 497, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.012885, mae: 2.643444, mean_q: 3.189779, mean_eps: 0.653773
 175810/1000000: episode: 255, duration: 43.377s, episode steps: 699, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.012904, mae: 2.653930, mean_q: 3.203905, mean_eps: 0.652589
 176643/1000000: episode: 256, duration: 48.270s, episode steps: 833, steps per second:  17, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013850, mae: 2.632333, mean_q: 3.175786, mean_eps: 0.651073
 177266/1000000: episode: 257, duration: 34.844s, episode steps: 623, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.014471, mae: 2.604523, mean_q: 3.144623, mean_eps: 0.649631
 177797/1000000: episode: 258, duration: 31.710s, episode steps: 531, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.013357, mae: 2.612646, mean_q: 3.153008, mean_eps: 0.648487
 178293/1000000: episode: 259, duration: 28.805s, episode steps: 496, steps per second:  17, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.014177, mae: 2.650575, mean_q: 3.199029, mean_eps: 0.647469
 178952/1000000: episode: 260, duration: 39.027s, episode steps: 659, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.014610, mae: 2.635399, mean_q: 3.179696, mean_eps: 0.646328
 179863/1000000: episode: 261, duration: 53.857s, episode steps: 911, steps per second:  17, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.013904, mae: 2.628710, mean_q: 3.171417, mean_eps: 0.644776
 180622/1000000: episode: 262, duration: 42.534s, episode steps: 759, steps per second:  18, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012697, mae: 2.615165, mean_q: 3.155302, mean_eps: 0.643121
 181012/1000000: episode: 263, duration: 23.842s, episode steps: 390, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.015187, mae: 2.619450, mean_q: 3.161411, mean_eps: 0.641984
 181618/1000000: episode: 264, duration: 34.872s, episode steps: 606, steps per second:  17, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.012839, mae: 2.626467, mean_q: 3.171499, mean_eps: 0.640998
 182268/1000000: episode: 265, duration: 37.259s, episode steps: 650, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013691, mae: 2.608489, mean_q: 3.148735, mean_eps: 0.639755
 182962/1000000: episode: 266, duration: 40.976s, episode steps: 694, steps per second:  17, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.012935, mae: 2.617386, mean_q: 3.159181, mean_eps: 0.638424
 183502/1000000: episode: 267, duration: 32.141s, episode steps: 540, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.012540, mae: 2.633270, mean_q: 3.178296, mean_eps: 0.637201
 184311/1000000: episode: 268, duration: 47.163s, episode steps: 809, steps per second:  17, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.012757, mae: 2.643626, mean_q: 3.191294, mean_eps: 0.635866
 184803/1000000: episode: 269, duration: 29.927s, episode steps: 492, steps per second:  16, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012867, mae: 2.642255, mean_q: 3.190233, mean_eps: 0.634579
 185540/1000000: episode: 270, duration: 42.947s, episode steps: 737, steps per second:  17, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.012777, mae: 2.623141, mean_q: 3.165650, mean_eps: 0.633363
 186728/1000000: episode: 271, duration: 68.679s, episode steps: 1188, steps per second:  17, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.013960, mae: 2.620130, mean_q: 3.161853, mean_eps: 0.631459
 187557/1000000: episode: 272, duration: 48.254s, episode steps: 829, steps per second:  17, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012791, mae: 2.632330, mean_q: 3.176763, mean_eps: 0.629459
 188623/1000000: episode: 273, duration: 61.715s, episode steps: 1066, steps per second:  17, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012753, mae: 2.612107, mean_q: 3.151443, mean_eps: 0.627582
 189169/1000000: episode: 274, duration: 32.972s, episode steps: 546, steps per second:  17, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.013988, mae: 2.650257, mean_q: 3.197012, mean_eps: 0.625986
 189800/1000000: episode: 275, duration: 35.324s, episode steps: 631, steps per second:  18, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.012180, mae: 2.636184, mean_q: 3.181715, mean_eps: 0.624822
 190454/1000000: episode: 276, duration: 38.871s, episode steps: 654, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.010921, mae: 2.646682, mean_q: 3.194058, mean_eps: 0.623551
 191382/1000000: episode: 277, duration: 54.386s, episode steps: 928, steps per second:  17, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.012640, mae: 2.624176, mean_q: 3.165865, mean_eps: 0.621982
 192044/1000000: episode: 278, duration: 37.757s, episode steps: 662, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.015841, mae: 2.630931, mean_q: 3.176614, mean_eps: 0.620410
 192625/1000000: episode: 279, duration: 34.660s, episode steps: 581, steps per second:  17, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.013856, mae: 2.601991, mean_q: 3.137577, mean_eps: 0.619179
 193322/1000000: episode: 280, duration: 41.466s, episode steps: 697, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012954, mae: 2.643329, mean_q: 3.188951, mean_eps: 0.617911
 193825/1000000: episode: 281, duration: 29.855s, episode steps: 503, steps per second:  17, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.012552, mae: 2.628766, mean_q: 3.170049, mean_eps: 0.616723
 194666/1000000: episode: 282, duration: 48.365s, episode steps: 841, steps per second:  17, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.012517, mae: 2.637903, mean_q: 3.180721, mean_eps: 0.615393
 195344/1000000: episode: 283, duration: 38.694s, episode steps: 678, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.012459, mae: 2.620741, mean_q: 3.162619, mean_eps: 0.613892
 196557/1000000: episode: 284, duration: 72.699s, episode steps: 1213, steps per second:  17, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.013330, mae: 2.623600, mean_q: 3.166158, mean_eps: 0.612019
 196929/1000000: episode: 285, duration: 21.925s, episode steps: 372, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.013640, mae: 2.611345, mean_q: 3.150499, mean_eps: 0.610447
 197516/1000000: episode: 286, duration: 33.950s, episode steps: 587, steps per second:  17, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.014817, mae: 2.622226, mean_q: 3.165938, mean_eps: 0.609500
 197905/1000000: episode: 287, duration: 25.567s, episode steps: 389, steps per second:  15, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.015431, mae: 2.632464, mean_q: 3.175905, mean_eps: 0.608534
 198662/1000000: episode: 288, duration: 43.680s, episode steps: 757, steps per second:  17, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.015459, mae: 2.634336, mean_q: 3.176649, mean_eps: 0.607398
 199566/1000000: episode: 289, duration: 53.877s, episode steps: 904, steps per second:  17, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.013977, mae: 2.626209, mean_q: 3.167315, mean_eps: 0.605754
 200510/1000000: episode: 290, duration: 54.591s, episode steps: 944, steps per second:  17, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.013532, mae: 2.635179, mean_q: 3.179644, mean_eps: 0.603925
 201071/1000000: episode: 291, duration: 32.067s, episode steps: 561, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.014380, mae: 2.636597, mean_q: 3.181656, mean_eps: 0.602436
 201616/1000000: episode: 292, duration: 32.896s, episode steps: 545, steps per second:  17, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.012030, mae: 2.670731, mean_q: 3.224049, mean_eps: 0.601343
 202672/1000000: episode: 293, duration: 62.041s, episode steps: 1056, steps per second:  17, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.013886, mae: 2.637552, mean_q: 3.182772, mean_eps: 0.599759
 203076/1000000: episode: 294, duration: 25.385s, episode steps: 404, steps per second:  16, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.016939, mae: 2.645804, mean_q: 3.191677, mean_eps: 0.598313
 203659/1000000: episode: 295, duration: 33.555s, episode steps: 583, steps per second:  17, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.011529, mae: 2.631608, mean_q: 3.176022, mean_eps: 0.597335
 204734/1000000: episode: 296, duration: 63.321s, episode steps: 1075, steps per second:  17, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.014559, mae: 2.637726, mean_q: 3.182469, mean_eps: 0.595692
 205687/1000000: episode: 297, duration: 55.806s, episode steps: 953, steps per second:  17, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.013470, mae: 2.632852, mean_q: 3.176382, mean_eps: 0.593684
 206286/1000000: episode: 298, duration: 36.191s, episode steps: 599, steps per second:  17, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.012043, mae: 2.634106, mean_q: 3.177271, mean_eps: 0.592148
 207042/1000000: episode: 299, duration: 44.380s, episode steps: 756, steps per second:  17, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.013183, mae: 2.633572, mean_q: 3.177010, mean_eps: 0.590805
 207619/1000000: episode: 300, duration: 33.657s, episode steps: 577, steps per second:  17, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.014624, mae: 2.657556, mean_q: 3.205769, mean_eps: 0.589487
 208111/1000000: episode: 301, duration: 29.720s, episode steps: 492, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.017754, mae: 2.621421, mean_q: 3.159550, mean_eps: 0.588429
 208945/1000000: episode: 302, duration: 49.130s, episode steps: 834, steps per second:  17, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.012166, mae: 2.631756, mean_q: 3.172610, mean_eps: 0.587115
 209608/1000000: episode: 303, duration: 41.452s, episode steps: 663, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.013198, mae: 2.639807, mean_q: 3.184279, mean_eps: 0.585634
 210170/1000000: episode: 304, duration: 32.559s, episode steps: 562, steps per second:  17, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.013855, mae: 2.625867, mean_q: 3.167030, mean_eps: 0.584422
 210958/1000000: episode: 305, duration: 47.446s, episode steps: 788, steps per second:  17, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013798, mae: 2.637012, mean_q: 3.181450, mean_eps: 0.583083
 211529/1000000: episode: 306, duration: 34.425s, episode steps: 571, steps per second:  17, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.946 [0.000, 5.000],  loss: 0.012064, mae: 2.638994, mean_q: 3.184042, mean_eps: 0.581737
 212171/1000000: episode: 307, duration: 36.909s, episode steps: 642, steps per second:  17, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.011941, mae: 2.639694, mean_q: 3.184881, mean_eps: 0.580537
 213040/1000000: episode: 308, duration: 53.615s, episode steps: 869, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.014355, mae: 2.634094, mean_q: 3.176340, mean_eps: 0.579044
 213754/1000000: episode: 309, duration: 42.299s, episode steps: 714, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.016085, mae: 2.638850, mean_q: 3.181233, mean_eps: 0.577476
 214718/1000000: episode: 310, duration: 56.981s, episode steps: 964, steps per second:  17, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.016433, mae: 2.637053, mean_q: 3.180414, mean_eps: 0.575813
 215236/1000000: episode: 311, duration: 30.432s, episode steps: 518, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.015126, mae: 2.631171, mean_q: 3.174193, mean_eps: 0.574348
 216315/1000000: episode: 312, duration: 64.534s, episode steps: 1079, steps per second:  17, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.014703, mae: 2.633394, mean_q: 3.177240, mean_eps: 0.572767
 218021/1000000: episode: 313, duration: 100.885s, episode steps: 1706, steps per second:  17, episode reward: 32.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.011961, mae: 2.631596, mean_q: 3.175574, mean_eps: 0.570007
 218557/1000000: episode: 314, duration: 31.556s, episode steps: 536, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.013097, mae: 2.626295, mean_q: 3.169109, mean_eps: 0.567786
 219290/1000000: episode: 315, duration: 43.350s, episode steps: 733, steps per second:  17, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013463, mae: 2.622889, mean_q: 3.163735, mean_eps: 0.566530
 220067/1000000: episode: 316, duration: 48.975s, episode steps: 777, steps per second:  16, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.013639, mae: 2.620109, mean_q: 3.160960, mean_eps: 0.565038
 220800/1000000: episode: 317, duration: 47.719s, episode steps: 733, steps per second:  15, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.013537, mae: 2.646584, mean_q: 3.192354, mean_eps: 0.563545
 221644/1000000: episode: 318, duration: 54.424s, episode steps: 844, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.013502, mae: 2.632991, mean_q: 3.178435, mean_eps: 0.561984
 222639/1000000: episode: 319, duration: 59.068s, episode steps: 995, steps per second:  17, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.014608, mae: 2.639306, mean_q: 3.183323, mean_eps: 0.560163
 223314/1000000: episode: 320, duration: 38.987s, episode steps: 675, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.017215, mae: 2.643058, mean_q: 3.189751, mean_eps: 0.558508
 224150/1000000: episode: 321, duration: 50.198s, episode steps: 836, steps per second:  17, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.014210, mae: 2.637683, mean_q: 3.180614, mean_eps: 0.557011
 224667/1000000: episode: 322, duration: 31.084s, episode steps: 517, steps per second:  17, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.012830, mae: 2.624408, mean_q: 3.168020, mean_eps: 0.555672
 225275/1000000: episode: 323, duration: 35.770s, episode steps: 608, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014387, mae: 2.624130, mean_q: 3.163955, mean_eps: 0.554559
 225870/1000000: episode: 324, duration: 35.106s, episode steps: 595, steps per second:  17, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.803 [0.000, 5.000],  loss: 0.013726, mae: 2.637009, mean_q: 3.182000, mean_eps: 0.553367
 226297/1000000: episode: 325, duration: 26.572s, episode steps: 427, steps per second:  16, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.013081, mae: 2.602003, mean_q: 3.139682, mean_eps: 0.552354
 227106/1000000: episode: 326, duration: 51.700s, episode steps: 809, steps per second:  16, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.014050, mae: 2.625652, mean_q: 3.167351, mean_eps: 0.551130
 227878/1000000: episode: 327, duration: 63.337s, episode steps: 772, steps per second:  12, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014288, mae: 2.633155, mean_q: 3.177041, mean_eps: 0.549566
 228530/1000000: episode: 328, duration: 47.128s, episode steps: 652, steps per second:  14, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.015667, mae: 2.638825, mean_q: 3.181854, mean_eps: 0.548156
 229028/1000000: episode: 329, duration: 30.960s, episode steps: 498, steps per second:  16, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.012659, mae: 2.634826, mean_q: 3.179238, mean_eps: 0.547020
 230415/1000000: episode: 330, duration: 88.187s, episode steps: 1387, steps per second:  16, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.013435, mae: 2.615565, mean_q: 3.154620, mean_eps: 0.545154
 230947/1000000: episode: 331, duration: 35.846s, episode steps: 532, steps per second:  15, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.012611, mae: 2.622652, mean_q: 3.164657, mean_eps: 0.543254
 231433/1000000: episode: 332, duration: 30.929s, episode steps: 486, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.014660, mae: 2.650687, mean_q: 3.198981, mean_eps: 0.542244
 232228/1000000: episode: 333, duration: 52.444s, episode steps: 795, steps per second:  15, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.014185, mae: 2.629840, mean_q: 3.172719, mean_eps: 0.540977
 232947/1000000: episode: 334, duration: 47.178s, episode steps: 719, steps per second:  15, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.014370, mae: 2.627742, mean_q: 3.169798, mean_eps: 0.539480
 233640/1000000: episode: 335, duration: 41.584s, episode steps: 693, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013109, mae: 2.643431, mean_q: 3.188285, mean_eps: 0.538082
 234996/1000000: episode: 336, duration: 85.292s, episode steps: 1356, steps per second:  16, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.013653, mae: 2.613181, mean_q: 3.151619, mean_eps: 0.536054
 235420/1000000: episode: 337, duration: 26.295s, episode steps: 424, steps per second:  16, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014079, mae: 2.634866, mean_q: 3.177983, mean_eps: 0.534292
 235999/1000000: episode: 338, duration: 36.366s, episode steps: 579, steps per second:  16, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.014757, mae: 2.610385, mean_q: 3.147366, mean_eps: 0.533298
 236947/1000000: episode: 339, duration: 58.129s, episode steps: 948, steps per second:  16, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.013406, mae: 2.639458, mean_q: 3.185199, mean_eps: 0.531785
 237865/1000000: episode: 340, duration: 55.943s, episode steps: 918, steps per second:  16, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.014218, mae: 2.633960, mean_q: 3.178538, mean_eps: 0.529936
 238470/1000000: episode: 341, duration: 37.289s, episode steps: 605, steps per second:  16, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.836 [0.000, 5.000],  loss: 0.015692, mae: 2.634461, mean_q: 3.175641, mean_eps: 0.528427
 239155/1000000: episode: 342, duration: 43.262s, episode steps: 685, steps per second:  16, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.013505, mae: 2.635491, mean_q: 3.177777, mean_eps: 0.527152
 239776/1000000: episode: 343, duration: 37.783s, episode steps: 621, steps per second:  16, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.012455, mae: 2.658366, mean_q: 3.206703, mean_eps: 0.525861
 240155/1000000: episode: 344, duration: 21.776s, episode steps: 379, steps per second:  17, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.765 [0.000, 5.000],  loss: 0.015791, mae: 2.607002, mean_q: 3.144972, mean_eps: 0.524871
 241017/1000000: episode: 345, duration: 56.741s, episode steps: 862, steps per second:  15, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.014665, mae: 2.618061, mean_q: 3.157737, mean_eps: 0.523640
 241641/1000000: episode: 346, duration: 38.386s, episode steps: 624, steps per second:  16, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.017408, mae: 2.620530, mean_q: 3.160862, mean_eps: 0.522167
 242540/1000000: episode: 347, duration: 54.751s, episode steps: 899, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.013759, mae: 2.654086, mean_q: 3.201247, mean_eps: 0.520662
 243445/1000000: episode: 348, duration: 55.046s, episode steps: 905, steps per second:  16, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.012333, mae: 2.629761, mean_q: 3.172086, mean_eps: 0.518876
 243818/1000000: episode: 349, duration: 25.536s, episode steps: 373, steps per second:  15, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.016485, mae: 2.632201, mean_q: 3.176133, mean_eps: 0.517609
 244438/1000000: episode: 350, duration: 35.344s, episode steps: 620, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.013206, mae: 2.661675, mean_q: 3.211226, mean_eps: 0.516627
 245478/1000000: episode: 351, duration: 63.761s, episode steps: 1040, steps per second:  16, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.014727, mae: 2.619641, mean_q: 3.159399, mean_eps: 0.514983
 246316/1000000: episode: 352, duration: 52.421s, episode steps: 838, steps per second:  16, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.014151, mae: 2.620102, mean_q: 3.159794, mean_eps: 0.513126
 247346/1000000: episode: 353, duration: 63.567s, episode steps: 1030, steps per second:  16, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.014377, mae: 2.624798, mean_q: 3.165578, mean_eps: 0.511277
 248377/1000000: episode: 354, duration: 64.250s, episode steps: 1031, steps per second:  16, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.012753, mae: 2.629561, mean_q: 3.170950, mean_eps: 0.509233
 249138/1000000: episode: 355, duration: 47.923s, episode steps: 761, steps per second:  16, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.740 [0.000, 5.000],  loss: 0.011987, mae: 2.626586, mean_q: 3.168160, mean_eps: 0.507459
 249864/1000000: episode: 356, duration: 44.317s, episode steps: 726, steps per second:  16, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.014188, mae: 2.617927, mean_q: 3.158108, mean_eps: 0.505990
 250229/1000000: episode: 357, duration: 21.813s, episode steps: 365, steps per second:  17, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.219 [0.000, 5.000],  loss: 0.013073, mae: 2.615015, mean_q: 3.154686, mean_eps: 0.504909
 250879/1000000: episode: 358, duration: 40.581s, episode steps: 650, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.014251, mae: 2.625913, mean_q: 3.168937, mean_eps: 0.503903
 251595/1000000: episode: 359, duration: 45.087s, episode steps: 716, steps per second:  16, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.014320, mae: 2.598196, mean_q: 3.134398, mean_eps: 0.502553
 252244/1000000: episode: 360, duration: 39.365s, episode steps: 649, steps per second:  16, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.012863, mae: 2.616197, mean_q: 3.155305, mean_eps: 0.501202
 253332/1000000: episode: 361, duration: 68.922s, episode steps: 1088, steps per second:  16, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.013153, mae: 2.616535, mean_q: 3.156379, mean_eps: 0.499484
 253942/1000000: episode: 362, duration: 38.911s, episode steps: 610, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.013206, mae: 2.605750, mean_q: 3.143876, mean_eps: 0.497801
 255557/1000000: episode: 363, duration: 100.824s, episode steps: 1615, steps per second:  16, episode reward: 25.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.013547, mae: 2.622957, mean_q: 3.164428, mean_eps: 0.495595
 256131/1000000: episode: 364, duration: 36.756s, episode steps: 574, steps per second:  16, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.014590, mae: 2.611401, mean_q: 3.148461, mean_eps: 0.493429
 256851/1000000: episode: 365, duration: 46.967s, episode steps: 720, steps per second:  15, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.013852, mae: 2.622240, mean_q: 3.162770, mean_eps: 0.492150
 257908/1000000: episode: 366, duration: 68.006s, episode steps: 1057, steps per second:  16, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.014239, mae: 2.611490, mean_q: 3.150519, mean_eps: 0.490392
 258667/1000000: episode: 367, duration: 45.786s, episode steps: 759, steps per second:  17, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.014412, mae: 2.630643, mean_q: 3.172282, mean_eps: 0.488594
 259214/1000000: episode: 368, duration: 35.520s, episode steps: 547, steps per second:  15, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.013076, mae: 2.624646, mean_q: 3.166770, mean_eps: 0.487299
 259608/1000000: episode: 369, duration: 25.262s, episode steps: 394, steps per second:  16, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.014774, mae: 2.621928, mean_q: 3.163892, mean_eps: 0.486368
 260490/1000000: episode: 370, duration: 56.206s, episode steps: 882, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.014024, mae: 2.606637, mean_q: 3.143742, mean_eps: 0.485105
 261359/1000000: episode: 371, duration: 57.262s, episode steps: 869, steps per second:  15, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.014832, mae: 2.624913, mean_q: 3.166159, mean_eps: 0.483370
 262481/1000000: episode: 372, duration: 78.616s, episode steps: 1122, steps per second:  14, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.013568, mae: 2.614677, mean_q: 3.153913, mean_eps: 0.481398
 263025/1000000: episode: 373, duration: 38.128s, episode steps: 544, steps per second:  14, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.976 [0.000, 5.000],  loss: 0.014944, mae: 2.594593, mean_q: 3.128337, mean_eps: 0.479747
 264035/1000000: episode: 374, duration: 70.042s, episode steps: 1010, steps per second:  14, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.013897, mae: 2.626378, mean_q: 3.168733, mean_eps: 0.478211
 264787/1000000: episode: 375, duration: 52.871s, episode steps: 752, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.014838, mae: 2.621399, mean_q: 3.162104, mean_eps: 0.476468
 265449/1000000: episode: 376, duration: 44.855s, episode steps: 662, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.014915, mae: 2.612846, mean_q: 3.151918, mean_eps: 0.475066
 266423/1000000: episode: 377, duration: 65.500s, episode steps: 974, steps per second:  15, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.014388, mae: 2.617683, mean_q: 3.158429, mean_eps: 0.473447
 267161/1000000: episode: 378, duration: 50.621s, episode steps: 738, steps per second:  15, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.011598, mae: 2.629003, mean_q: 3.171910, mean_eps: 0.471752
 267826/1000000: episode: 379, duration: 43.270s, episode steps: 665, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.014312, mae: 2.609751, mean_q: 3.150187, mean_eps: 0.470362
 268860/1000000: episode: 380, duration: 71.310s, episode steps: 1034, steps per second:  14, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.013550, mae: 2.604780, mean_q: 3.142409, mean_eps: 0.468683
 269317/1000000: episode: 381, duration: 33.348s, episode steps: 457, steps per second:  14, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.012842, mae: 2.603978, mean_q: 3.143887, mean_eps: 0.467206
 270014/1000000: episode: 382, duration: 47.329s, episode steps: 697, steps per second:  15, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.012022, mae: 2.627799, mean_q: 3.170690, mean_eps: 0.466061
 270995/1000000: episode: 383, duration: 68.083s, episode steps: 981, steps per second:  14, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.014339, mae: 2.625892, mean_q: 3.165768, mean_eps: 0.464402
 271751/1000000: episode: 384, duration: 47.728s, episode steps: 756, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.016538, mae: 2.640890, mean_q: 3.182959, mean_eps: 0.462683
 272765/1000000: episode: 385, duration: 67.814s, episode steps: 1014, steps per second:  15, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.014561, mae: 2.632378, mean_q: 3.174039, mean_eps: 0.460929
 273273/1000000: episode: 386, duration: 33.753s, episode steps: 508, steps per second:  15, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.014956, mae: 2.602782, mean_q: 3.140389, mean_eps: 0.459420
 273851/1000000: episode: 387, duration: 37.978s, episode steps: 578, steps per second:  15, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.015296, mae: 2.608970, mean_q: 3.147759, mean_eps: 0.458347
 274540/1000000: episode: 388, duration: 48.167s, episode steps: 689, steps per second:  14, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.015684, mae: 2.622063, mean_q: 3.163323, mean_eps: 0.457096
 275418/1000000: episode: 389, duration: 59.026s, episode steps: 878, steps per second:  15, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.012520, mae: 2.619799, mean_q: 3.160536, mean_eps: 0.455544
 275897/1000000: episode: 390, duration: 32.500s, episode steps: 479, steps per second:  15, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.017723, mae: 2.617516, mean_q: 3.153835, mean_eps: 0.454197
 276502/1000000: episode: 391, duration: 42.162s, episode steps: 605, steps per second:  14, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.015276, mae: 2.609188, mean_q: 3.145359, mean_eps: 0.453124
 277478/1000000: episode: 392, duration: 66.967s, episode steps: 976, steps per second:  15, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.012974, mae: 2.622362, mean_q: 3.163043, mean_eps: 0.451560
 278025/1000000: episode: 393, duration: 37.338s, episode steps: 547, steps per second:  15, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.013232, mae: 2.614203, mean_q: 3.152969, mean_eps: 0.450051
 278624/1000000: episode: 394, duration: 41.038s, episode steps: 599, steps per second:  15, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.013586, mae: 2.638413, mean_q: 3.179815, mean_eps: 0.448918
 279590/1000000: episode: 395, duration: 66.291s, episode steps: 966, steps per second:  15, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.759 [0.000, 5.000],  loss: 0.015349, mae: 2.612976, mean_q: 3.150472, mean_eps: 0.447370
 280441/1000000: episode: 396, duration: 59.039s, episode steps: 851, steps per second:  14, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.918 [0.000, 5.000],  loss: 0.012815, mae: 2.617942, mean_q: 3.157075, mean_eps: 0.445568
 281243/1000000: episode: 397, duration: 54.195s, episode steps: 802, steps per second:  15, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013795, mae: 2.631296, mean_q: 3.174079, mean_eps: 0.443933
 282018/1000000: episode: 398, duration: 55.909s, episode steps: 775, steps per second:  14, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.015096, mae: 2.619868, mean_q: 3.159220, mean_eps: 0.442373
 282851/1000000: episode: 399, duration: 57.186s, episode steps: 833, steps per second:  15, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011532, mae: 2.597623, mean_q: 3.134259, mean_eps: 0.440781
 283479/1000000: episode: 400, duration: 44.592s, episode steps: 628, steps per second:  14, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.013504, mae: 2.636637, mean_q: 3.179468, mean_eps: 0.439335
 284302/1000000: episode: 401, duration: 57.708s, episode steps: 823, steps per second:  14, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.014123, mae: 2.616861, mean_q: 3.156064, mean_eps: 0.437898
 284899/1000000: episode: 402, duration: 40.807s, episode steps: 597, steps per second:  15, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013829, mae: 2.615425, mean_q: 3.154013, mean_eps: 0.436492
 285429/1000000: episode: 403, duration: 36.702s, episode steps: 530, steps per second:  14, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.015727, mae: 2.585496, mean_q: 3.116065, mean_eps: 0.435375
 286093/1000000: episode: 404, duration: 45.488s, episode steps: 664, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.015475, mae: 2.620908, mean_q: 3.159747, mean_eps: 0.434191
 286796/1000000: episode: 405, duration: 48.330s, episode steps: 703, steps per second:  15, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.011277, mae: 2.601657, mean_q: 3.137658, mean_eps: 0.432841
 287482/1000000: episode: 406, duration: 47.270s, episode steps: 686, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.012874, mae: 2.599965, mean_q: 3.133864, mean_eps: 0.431467
 288187/1000000: episode: 407, duration: 48.393s, episode steps: 705, steps per second:  15, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.014187, mae: 2.618846, mean_q: 3.158213, mean_eps: 0.430089
 289046/1000000: episode: 408, duration: 59.893s, episode steps: 859, steps per second:  14, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.013960, mae: 2.597192, mean_q: 3.131204, mean_eps: 0.428540
 289655/1000000: episode: 409, duration: 41.957s, episode steps: 609, steps per second:  15, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.015955, mae: 2.589912, mean_q: 3.122268, mean_eps: 0.427087
 290389/1000000: episode: 410, duration: 51.326s, episode steps: 734, steps per second:  14, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.015191, mae: 2.607071, mean_q: 3.141699, mean_eps: 0.425756
 291243/1000000: episode: 411, duration: 59.173s, episode steps: 854, steps per second:  14, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.013573, mae: 2.610477, mean_q: 3.146279, mean_eps: 0.424184
 292055/1000000: episode: 412, duration: 56.352s, episode steps: 812, steps per second:  14, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013526, mae: 2.628685, mean_q: 3.169783, mean_eps: 0.422537
 292777/1000000: episode: 413, duration: 49.971s, episode steps: 722, steps per second:  14, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014153, mae: 2.616911, mean_q: 3.157106, mean_eps: 0.421016
 293880/1000000: episode: 414, duration: 76.513s, episode steps: 1103, steps per second:  14, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.013100, mae: 2.629889, mean_q: 3.171204, mean_eps: 0.419211
 294615/1000000: episode: 415, duration: 50.296s, episode steps: 735, steps per second:  15, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.014245, mae: 2.614748, mean_q: 3.152328, mean_eps: 0.417393
 295593/1000000: episode: 416, duration: 68.283s, episode steps: 978, steps per second:  14, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.012987, mae: 2.629770, mean_q: 3.171023, mean_eps: 0.415694
 296280/1000000: episode: 417, duration: 47.886s, episode steps: 687, steps per second:  14, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.015013, mae: 2.575401, mean_q: 3.106024, mean_eps: 0.414047
 296773/1000000: episode: 418, duration: 35.255s, episode steps: 493, steps per second:  14, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.012467, mae: 2.604703, mean_q: 3.140231, mean_eps: 0.412879
 297638/1000000: episode: 419, duration: 61.870s, episode steps: 865, steps per second:  14, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.012823, mae: 2.603715, mean_q: 3.138769, mean_eps: 0.411532
 298331/1000000: episode: 420, duration: 46.770s, episode steps: 693, steps per second:  15, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.015076, mae: 2.609261, mean_q: 3.146532, mean_eps: 0.409992
 299303/1000000: episode: 421, duration: 66.915s, episode steps: 972, steps per second:  15, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.012858, mae: 2.615876, mean_q: 3.154563, mean_eps: 0.408344
 300089/1000000: episode: 422, duration: 54.192s, episode steps: 786, steps per second:  15, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.012964, mae: 2.622878, mean_q: 3.162712, mean_eps: 0.406602
 300799/1000000: episode: 423, duration: 49.327s, episode steps: 710, steps per second:  14, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.012142, mae: 2.616371, mean_q: 3.155798, mean_eps: 0.405121
 301342/1000000: episode: 424, duration: 38.544s, episode steps: 543, steps per second:  14, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.014410, mae: 2.625272, mean_q: 3.165032, mean_eps: 0.403881
 302094/1000000: episode: 425, duration: 52.647s, episode steps: 752, steps per second:  14, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.013649, mae: 2.640236, mean_q: 3.183167, mean_eps: 0.402598
 302915/1000000: episode: 426, duration: 56.518s, episode steps: 821, steps per second:  15, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.017004, mae: 2.620598, mean_q: 3.159814, mean_eps: 0.401042
 304120/1000000: episode: 427, duration: 85.019s, episode steps: 1205, steps per second:  14, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.012948, mae: 2.633523, mean_q: 3.175594, mean_eps: 0.399038
 304884/1000000: episode: 428, duration: 54.120s, episode steps: 764, steps per second:  14, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.014223, mae: 2.624904, mean_q: 3.165570, mean_eps: 0.397090
 305580/1000000: episode: 429, duration: 48.555s, episode steps: 696, steps per second:  14, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013204, mae: 2.636225, mean_q: 3.180101, mean_eps: 0.395645
 306120/1000000: episode: 430, duration: 37.803s, episode steps: 540, steps per second:  14, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.016026, mae: 2.605017, mean_q: 3.138712, mean_eps: 0.394421
 306733/1000000: episode: 431, duration: 42.827s, episode steps: 613, steps per second:  14, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013378, mae: 2.622365, mean_q: 3.161905, mean_eps: 0.393277
 307326/1000000: episode: 432, duration: 42.347s, episode steps: 593, steps per second:  14, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.015292, mae: 2.628539, mean_q: 3.169558, mean_eps: 0.392081
 308316/1000000: episode: 433, duration: 69.790s, episode steps: 990, steps per second:  14, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.014326, mae: 2.611936, mean_q: 3.149940, mean_eps: 0.390516
 308860/1000000: episode: 434, duration: 38.906s, episode steps: 544, steps per second:  14, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.015572, mae: 2.595321, mean_q: 3.128209, mean_eps: 0.389000
 309495/1000000: episode: 435, duration: 42.804s, episode steps: 635, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.013315, mae: 2.614902, mean_q: 3.156234, mean_eps: 0.387832
 310171/1000000: episode: 436, duration: 48.129s, episode steps: 676, steps per second:  14, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.012389, mae: 2.614764, mean_q: 3.152674, mean_eps: 0.386533
 310729/1000000: episode: 437, duration: 40.432s, episode steps: 558, steps per second:  14, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.014201, mae: 2.608807, mean_q: 3.146442, mean_eps: 0.385309
 311516/1000000: episode: 438, duration: 54.223s, episode steps: 787, steps per second:  15, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.014078, mae: 2.606419, mean_q: 3.142372, mean_eps: 0.383978
 312872/1000000: episode: 439, duration: 95.672s, episode steps: 1356, steps per second:  14, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.013268, mae: 2.622778, mean_q: 3.162397, mean_eps: 0.381860
 313750/1000000: episode: 440, duration: 62.875s, episode steps: 878, steps per second:  14, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.014451, mae: 2.608815, mean_q: 3.144832, mean_eps: 0.379646
 314546/1000000: episode: 441, duration: 55.769s, episode steps: 796, steps per second:  14, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.014387, mae: 2.632427, mean_q: 3.174837, mean_eps: 0.377987
 315446/1000000: episode: 442, duration: 63.730s, episode steps: 900, steps per second:  14, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.012291, mae: 2.641463, mean_q: 3.185010, mean_eps: 0.376308
 316355/1000000: episode: 443, duration: 63.705s, episode steps: 909, steps per second:  14, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.013719, mae: 2.623432, mean_q: 3.164292, mean_eps: 0.374518
 316904/1000000: episode: 444, duration: 38.673s, episode steps: 549, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.014597, mae: 2.609359, mean_q: 3.145035, mean_eps: 0.373077
 317513/1000000: episode: 445, duration: 43.645s, episode steps: 609, steps per second:  14, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013410, mae: 2.616351, mean_q: 3.153687, mean_eps: 0.371928
 318283/1000000: episode: 446, duration: 53.852s, episode steps: 770, steps per second:  14, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.013051, mae: 2.610428, mean_q: 3.148086, mean_eps: 0.370562
 318917/1000000: episode: 447, duration: 45.710s, episode steps: 634, steps per second:  14, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.874 [0.000, 5.000],  loss: 0.015519, mae: 2.611027, mean_q: 3.147701, mean_eps: 0.369172
 319558/1000000: episode: 448, duration: 44.740s, episode steps: 641, steps per second:  14, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.014890, mae: 2.637740, mean_q: 3.180750, mean_eps: 0.367909
 320418/1000000: episode: 449, duration: 59.762s, episode steps: 860, steps per second:  14, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.013530, mae: 2.622691, mean_q: 3.163197, mean_eps: 0.366424
 321159/1000000: episode: 450, duration: 54.069s, episode steps: 741, steps per second:  14, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.012426, mae: 2.615586, mean_q: 3.155430, mean_eps: 0.364840
 321783/1000000: episode: 451, duration: 43.452s, episode steps: 624, steps per second:  14, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.015047, mae: 2.608605, mean_q: 3.145374, mean_eps: 0.363489
 322428/1000000: episode: 452, duration: 46.020s, episode steps: 645, steps per second:  14, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.013792, mae: 2.619902, mean_q: 3.156825, mean_eps: 0.362234
 323053/1000000: episode: 453, duration: 46.732s, episode steps: 625, steps per second:  13, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.015124, mae: 2.591525, mean_q: 3.122985, mean_eps: 0.360975
 323859/1000000: episode: 454, duration: 57.852s, episode steps: 806, steps per second:  14, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.016803, mae: 2.626237, mean_q: 3.165465, mean_eps: 0.359557
 324645/1000000: episode: 455, duration: 56.419s, episode steps: 786, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.010 [0.000, 5.000],  loss: 0.014138, mae: 2.589904, mean_q: 3.120750, mean_eps: 0.357981
 325463/1000000: episode: 456, duration: 57.921s, episode steps: 818, steps per second:  14, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.014805, mae: 2.610046, mean_q: 3.144348, mean_eps: 0.356393
 326133/1000000: episode: 457, duration: 47.725s, episode steps: 670, steps per second:  14, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.715 [0.000, 5.000],  loss: 0.014733, mae: 2.614068, mean_q: 3.150076, mean_eps: 0.354920
 327136/1000000: episode: 458, duration: 71.205s, episode steps: 1003, steps per second:  14, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.013075, mae: 2.619588, mean_q: 3.159050, mean_eps: 0.353265
 327902/1000000: episode: 459, duration: 55.033s, episode steps: 766, steps per second:  14, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.013953, mae: 2.608537, mean_q: 3.144021, mean_eps: 0.351514
 329024/1000000: episode: 460, duration: 78.964s, episode steps: 1122, steps per second:  14, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.012855, mae: 2.612449, mean_q: 3.150224, mean_eps: 0.349645
 329496/1000000: episode: 461, duration: 33.762s, episode steps: 472, steps per second:  14, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.013362, mae: 2.597992, mean_q: 3.136787, mean_eps: 0.348069
 330123/1000000: episode: 462, duration: 44.479s, episode steps: 627, steps per second:  14, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.015468, mae: 2.624148, mean_q: 3.164556, mean_eps: 0.346980
 331095/1000000: episode: 463, duration: 68.467s, episode steps: 972, steps per second:  14, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.014288, mae: 2.615117, mean_q: 3.152135, mean_eps: 0.345396
 331735/1000000: episode: 464, duration: 45.614s, episode steps: 640, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.012813, mae: 2.620167, mean_q: 3.158783, mean_eps: 0.343800
 332398/1000000: episode: 465, duration: 49.479s, episode steps: 663, steps per second:  13, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.013207, mae: 2.609950, mean_q: 3.146758, mean_eps: 0.342509
 333075/1000000: episode: 466, duration: 49.154s, episode steps: 677, steps per second:  14, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.014300, mae: 2.618691, mean_q: 3.156312, mean_eps: 0.341183
 333720/1000000: episode: 467, duration: 43.950s, episode steps: 645, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.012585, mae: 2.599914, mean_q: 3.135665, mean_eps: 0.339876
 334627/1000000: episode: 468, duration: 58.726s, episode steps: 907, steps per second:  15, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.013862, mae: 2.602739, mean_q: 3.138372, mean_eps: 0.338339
 335835/1000000: episode: 469, duration: 82.941s, episode steps: 1208, steps per second:  15, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.013129, mae: 2.610807, mean_q: 3.147506, mean_eps: 0.336245
 336874/1000000: episode: 470, duration: 66.292s, episode steps: 1039, steps per second:  16, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.013191, mae: 2.602665, mean_q: 3.135907, mean_eps: 0.334019
 337885/1000000: episode: 471, duration: 66.605s, episode steps: 1011, steps per second:  15, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014474, mae: 2.608677, mean_q: 3.143867, mean_eps: 0.331988
 338782/1000000: episode: 472, duration: 57.506s, episode steps: 897, steps per second:  16, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.013483, mae: 2.607889, mean_q: 3.145005, mean_eps: 0.330099
 339557/1000000: episode: 473, duration: 51.690s, episode steps: 775, steps per second:  15, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.014164, mae: 2.604710, mean_q: 3.138675, mean_eps: 0.328443
 340509/1000000: episode: 474, duration: 62.283s, episode steps: 952, steps per second:  15, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.013266, mae: 2.616278, mean_q: 3.152562, mean_eps: 0.326733
 341136/1000000: episode: 475, duration: 43.490s, episode steps: 627, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.013555, mae: 2.594300, mean_q: 3.126544, mean_eps: 0.325172
 341462/1000000: episode: 476, duration: 21.967s, episode steps: 326, steps per second:  15, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.011860, mae: 2.582084, mean_q: 3.110642, mean_eps: 0.324230
 342204/1000000: episode: 477, duration: 48.208s, episode steps: 742, steps per second:  15, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.013673, mae: 2.623182, mean_q: 3.161803, mean_eps: 0.323173
 342666/1000000: episode: 478, duration: 31.550s, episode steps: 462, steps per second:  15, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.879 [0.000, 5.000],  loss: 0.017112, mae: 2.604976, mean_q: 3.137929, mean_eps: 0.321981
 343474/1000000: episode: 479, duration: 53.388s, episode steps: 808, steps per second:  15, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.886 [0.000, 5.000],  loss: 0.013825, mae: 2.610514, mean_q: 3.145796, mean_eps: 0.320721
 344185/1000000: episode: 480, duration: 48.602s, episode steps: 711, steps per second:  15, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.014960, mae: 2.618514, mean_q: 3.154090, mean_eps: 0.319217
 345116/1000000: episode: 481, duration: 65.343s, episode steps: 931, steps per second:  14, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.013416, mae: 2.581498, mean_q: 3.111433, mean_eps: 0.317593
 345821/1000000: episode: 482, duration: 48.943s, episode steps: 705, steps per second:  14, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.015464, mae: 2.616689, mean_q: 3.154514, mean_eps: 0.315973
 346565/1000000: episode: 483, duration: 53.810s, episode steps: 744, steps per second:  14, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.013721, mae: 2.609045, mean_q: 3.144951, mean_eps: 0.314536
 347671/1000000: episode: 484, duration: 77.948s, episode steps: 1106, steps per second:  14, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.013223, mae: 2.607516, mean_q: 3.143312, mean_eps: 0.312706
 348832/1000000: episode: 485, duration: 85.276s, episode steps: 1161, steps per second:  14, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014261, mae: 2.621595, mean_q: 3.161358, mean_eps: 0.310465
 349837/1000000: episode: 486, duration: 68.199s, episode steps: 1005, steps per second:  15, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.014494, mae: 2.605427, mean_q: 3.141236, mean_eps: 0.308319
 350541/1000000: episode: 487, duration: 47.945s, episode steps: 704, steps per second:  15, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.015331, mae: 2.593645, mean_q: 3.127052, mean_eps: 0.306624
 351183/1000000: episode: 488, duration: 43.046s, episode steps: 642, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.014125, mae: 2.599322, mean_q: 3.135221, mean_eps: 0.305293
 351963/1000000: episode: 489, duration: 55.843s, episode steps: 780, steps per second:  14, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.014435, mae: 2.613549, mean_q: 3.150136, mean_eps: 0.303887
 352515/1000000: episode: 490, duration: 37.788s, episode steps: 552, steps per second:  15, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.013075, mae: 2.627441, mean_q: 3.168089, mean_eps: 0.302569
 353032/1000000: episode: 491, duration: 38.133s, episode steps: 517, steps per second:  14, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.013586, mae: 2.597182, mean_q: 3.130022, mean_eps: 0.301511
 353838/1000000: episode: 492, duration: 58.201s, episode steps: 806, steps per second:  14, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.012694, mae: 2.612112, mean_q: 3.150229, mean_eps: 0.300201
 354842/1000000: episode: 493, duration: 71.233s, episode steps: 1004, steps per second:  14, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.012908, mae: 2.621686, mean_q: 3.159612, mean_eps: 0.298407
 355673/1000000: episode: 494, duration: 61.716s, episode steps: 831, steps per second:  13, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.014439, mae: 2.609155, mean_q: 3.144033, mean_eps: 0.296589
 356629/1000000: episode: 495, duration: 69.054s, episode steps: 956, steps per second:  14, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.013811, mae: 2.614747, mean_q: 3.151524, mean_eps: 0.294819
 357652/1000000: episode: 496, duration: 76.157s, episode steps: 1023, steps per second:  13, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.014913, mae: 2.612074, mean_q: 3.146914, mean_eps: 0.292863
 358143/1000000: episode: 497, duration: 37.918s, episode steps: 491, steps per second:  13, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.013229, mae: 2.627727, mean_q: 3.167458, mean_eps: 0.291366
 359107/1000000: episode: 498, duration: 66.365s, episode steps: 964, steps per second:  15, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.014283, mae: 2.585806, mean_q: 3.116812, mean_eps: 0.289924
 359626/1000000: episode: 499, duration: 34.687s, episode steps: 519, steps per second:  15, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.012882, mae: 2.611847, mean_q: 3.147515, mean_eps: 0.288455
 360588/1000000: episode: 500, duration: 64.541s, episode steps: 962, steps per second:  15, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.015488, mae: 2.603826, mean_q: 3.137539, mean_eps: 0.286990
 361253/1000000: episode: 501, duration: 41.421s, episode steps: 665, steps per second:  16, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.138 [0.000, 5.000],  loss: 0.012819, mae: 2.618136, mean_q: 3.157437, mean_eps: 0.285378
 361688/1000000: episode: 502, duration: 29.428s, episode steps: 435, steps per second:  15, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.015924, mae: 2.616491, mean_q: 3.152278, mean_eps: 0.284289
 362479/1000000: episode: 503, duration: 53.679s, episode steps: 791, steps per second:  15, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.013024, mae: 2.620620, mean_q: 3.159109, mean_eps: 0.283078
 363406/1000000: episode: 504, duration: 59.969s, episode steps: 927, steps per second:  15, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.012984, mae: 2.599990, mean_q: 3.134455, mean_eps: 0.281375
 364668/1000000: episode: 505, duration: 84.800s, episode steps: 1262, steps per second:  15, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.015205, mae: 2.614548, mean_q: 3.149545, mean_eps: 0.279209
 365821/1000000: episode: 506, duration: 75.498s, episode steps: 1153, steps per second:  15, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.849 [0.000, 5.000],  loss: 0.013136, mae: 2.616731, mean_q: 3.154526, mean_eps: 0.276817
 366881/1000000: episode: 507, duration: 72.388s, episode steps: 1060, steps per second:  15, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.013526, mae: 2.619929, mean_q: 3.157430, mean_eps: 0.274623
 368046/1000000: episode: 508, duration: 79.531s, episode steps: 1165, steps per second:  15, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.013063, mae: 2.624931, mean_q: 3.163464, mean_eps: 0.272421
 368740/1000000: episode: 509, duration: 45.125s, episode steps: 694, steps per second:  15, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.014045, mae: 2.611082, mean_q: 3.146271, mean_eps: 0.270584
 369457/1000000: episode: 510, duration: 46.929s, episode steps: 717, steps per second:  15, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.013321, mae: 2.606605, mean_q: 3.142626, mean_eps: 0.269186
 370653/1000000: episode: 511, duration: 80.623s, episode steps: 1196, steps per second:  15, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.014159, mae: 2.626513, mean_q: 3.165856, mean_eps: 0.267289
 371397/1000000: episode: 512, duration: 51.549s, episode steps: 744, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.014528, mae: 2.615022, mean_q: 3.152720, mean_eps: 0.265369
 372040/1000000: episode: 513, duration: 43.254s, episode steps: 643, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.135 [0.000, 5.000],  loss: 0.014731, mae: 2.633969, mean_q: 3.175003, mean_eps: 0.263998
 372821/1000000: episode: 514, duration: 51.747s, episode steps: 781, steps per second:  15, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.014156, mae: 2.616452, mean_q: 3.152792, mean_eps: 0.262589
 373612/1000000: episode: 515, duration: 56.121s, episode steps: 791, steps per second:  14, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.015950, mae: 2.630839, mean_q: 3.169411, mean_eps: 0.261032
 374086/1000000: episode: 516, duration: 31.092s, episode steps: 474, steps per second:  15, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.941 [0.000, 5.000],  loss: 0.014557, mae: 2.592572, mean_q: 3.122890, mean_eps: 0.259781
 374773/1000000: episode: 517, duration: 46.473s, episode steps: 687, steps per second:  15, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.014780, mae: 2.632626, mean_q: 3.172236, mean_eps: 0.258629
 375251/1000000: episode: 518, duration: 35.518s, episode steps: 478, steps per second:  13, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.611 [0.000, 5.000],  loss: 0.014159, mae: 2.621813, mean_q: 3.162005, mean_eps: 0.257476
 375892/1000000: episode: 519, duration: 41.389s, episode steps: 641, steps per second:  15, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012738, mae: 2.585670, mean_q: 3.116733, mean_eps: 0.256371
 376718/1000000: episode: 520, duration: 54.409s, episode steps: 826, steps per second:  15, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.016063, mae: 2.589852, mean_q: 3.120573, mean_eps: 0.254918
 377999/1000000: episode: 521, duration: 87.978s, episode steps: 1281, steps per second:  15, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.015531, mae: 2.601935, mean_q: 3.134849, mean_eps: 0.252831
 378997/1000000: episode: 522, duration: 68.247s, episode steps: 998, steps per second:  15, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.015542, mae: 2.597013, mean_q: 3.128964, mean_eps: 0.250574
 379614/1000000: episode: 523, duration: 42.088s, episode steps: 617, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.012555, mae: 2.597350, mean_q: 3.130213, mean_eps: 0.248974
 380198/1000000: episode: 524, duration: 39.075s, episode steps: 584, steps per second:  15, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.195 [0.000, 5.000],  loss: 0.012718, mae: 2.601278, mean_q: 3.135944, mean_eps: 0.247786
 381578/1000000: episode: 525, duration: 96.623s, episode steps: 1380, steps per second:  14, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.013553, mae: 2.604774, mean_q: 3.137757, mean_eps: 0.245842
 382205/1000000: episode: 526, duration: 44.048s, episode steps: 627, steps per second:  14, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.012866, mae: 2.592482, mean_q: 3.125188, mean_eps: 0.243854
 382847/1000000: episode: 527, duration: 46.893s, episode steps: 642, steps per second:  14, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.012567, mae: 2.586507, mean_q: 3.119106, mean_eps: 0.242599
 383607/1000000: episode: 528, duration: 52.657s, episode steps: 760, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.014071, mae: 2.619504, mean_q: 3.157612, mean_eps: 0.241213
 384600/1000000: episode: 529, duration: 73.919s, episode steps: 993, steps per second:  13, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.016058, mae: 2.608938, mean_q: 3.142811, mean_eps: 0.239478
 385347/1000000: episode: 530, duration: 53.143s, episode steps: 747, steps per second:  14, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.013969, mae: 2.621172, mean_q: 3.159813, mean_eps: 0.237755
 385957/1000000: episode: 531, duration: 46.174s, episode steps: 610, steps per second:  13, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.013444, mae: 2.593797, mean_q: 3.126198, mean_eps: 0.236409
 386745/1000000: episode: 532, duration: 59.194s, episode steps: 788, steps per second:  13, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.968 [0.000, 5.000],  loss: 0.016831, mae: 2.599535, mean_q: 3.133868, mean_eps: 0.235023
 387361/1000000: episode: 533, duration: 46.295s, episode steps: 616, steps per second:  13, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.015589, mae: 2.608624, mean_q: 3.142913, mean_eps: 0.233633
 388034/1000000: episode: 534, duration: 51.655s, episode steps: 673, steps per second:  13, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.351 [0.000, 5.000],  loss: 0.015397, mae: 2.611959, mean_q: 3.148524, mean_eps: 0.232358
 388740/1000000: episode: 535, duration: 50.698s, episode steps: 706, steps per second:  14, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.745 [0.000, 5.000],  loss: 0.015371, mae: 2.599168, mean_q: 3.131936, mean_eps: 0.230996
 389470/1000000: episode: 536, duration: 53.109s, episode steps: 730, steps per second:  14, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.014192, mae: 2.611464, mean_q: 3.146493, mean_eps: 0.229574
 390534/1000000: episode: 537, duration: 80.651s, episode steps: 1064, steps per second:  13, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.014898, mae: 2.599505, mean_q: 3.134284, mean_eps: 0.227796
 391165/1000000: episode: 538, duration: 47.919s, episode steps: 631, steps per second:  13, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.013998, mae: 2.605309, mean_q: 3.139137, mean_eps: 0.226117
 391552/1000000: episode: 539, duration: 29.127s, episode steps: 387, steps per second:  13, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.017007, mae: 2.610656, mean_q: 3.146517, mean_eps: 0.225111
 392609/1000000: episode: 540, duration: 80.046s, episode steps: 1057, steps per second:  13, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.014137, mae: 2.591776, mean_q: 3.124252, mean_eps: 0.223682
 393367/1000000: episode: 541, duration: 57.517s, episode steps: 758, steps per second:  13, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.015079, mae: 2.596862, mean_q: 3.129232, mean_eps: 0.221884
 393896/1000000: episode: 542, duration: 39.323s, episode steps: 529, steps per second:  13, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.013548, mae: 2.599689, mean_q: 3.133931, mean_eps: 0.220613
 394578/1000000: episode: 543, duration: 52.287s, episode steps: 682, steps per second:  13, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.012938, mae: 2.595393, mean_q: 3.128615, mean_eps: 0.219413
 395248/1000000: episode: 544, duration: 50.024s, episode steps: 670, steps per second:  13, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.372 [0.000, 5.000],  loss: 0.013064, mae: 2.597955, mean_q: 3.131034, mean_eps: 0.218074
 395892/1000000: episode: 545, duration: 48.908s, episode steps: 644, steps per second:  13, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.014720, mae: 2.614318, mean_q: 3.151396, mean_eps: 0.216775
 396390/1000000: episode: 546, duration: 38.092s, episode steps: 498, steps per second:  13, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.013294, mae: 2.600479, mean_q: 3.133305, mean_eps: 0.215643
 397009/1000000: episode: 547, duration: 46.214s, episode steps: 619, steps per second:  13, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.014910, mae: 2.614492, mean_q: 3.149288, mean_eps: 0.214534
 397709/1000000: episode: 548, duration: 52.592s, episode steps: 700, steps per second:  13, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.801 [0.000, 5.000],  loss: 0.013648, mae: 2.627159, mean_q: 3.166025, mean_eps: 0.213227
 398373/1000000: episode: 549, duration: 48.574s, episode steps: 664, steps per second:  14, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.014886, mae: 2.609402, mean_q: 3.144158, mean_eps: 0.211877
 399038/1000000: episode: 550, duration: 50.096s, episode steps: 665, steps per second:  13, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.014577, mae: 2.616440, mean_q: 3.152771, mean_eps: 0.210562
 399855/1000000: episode: 551, duration: 62.229s, episode steps: 817, steps per second:  13, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.012782, mae: 2.616391, mean_q: 3.152330, mean_eps: 0.209097
 400379/1000000: episode: 552, duration: 40.964s, episode steps: 524, steps per second:  13, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.012641, mae: 2.596117, mean_q: 3.129692, mean_eps: 0.207770
 401118/1000000: episode: 553, duration: 55.463s, episode steps: 739, steps per second:  13, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.013255, mae: 2.574978, mean_q: 3.104748, mean_eps: 0.206519
 401686/1000000: episode: 554, duration: 42.743s, episode steps: 568, steps per second:  13, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.013541, mae: 2.592225, mean_q: 3.125537, mean_eps: 0.205224
 402415/1000000: episode: 555, duration: 54.300s, episode steps: 729, steps per second:  13, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.015756, mae: 2.605285, mean_q: 3.139824, mean_eps: 0.203941
 403155/1000000: episode: 556, duration: 56.232s, episode steps: 740, steps per second:  13, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.015772, mae: 2.602571, mean_q: 3.135236, mean_eps: 0.202488
 404203/1000000: episode: 557, duration: 79.120s, episode steps: 1048, steps per second:  13, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.014251, mae: 2.597442, mean_q: 3.130519, mean_eps: 0.200718
 405279/1000000: episode: 558, duration: 81.051s, episode steps: 1076, steps per second:  13, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.016209, mae: 2.609393, mean_q: 3.144264, mean_eps: 0.198615
 406271/1000000: episode: 559, duration: 75.254s, episode steps: 992, steps per second:  13, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.015239, mae: 2.601648, mean_q: 3.135383, mean_eps: 0.196567
 406949/1000000: episode: 560, duration: 50.655s, episode steps: 678, steps per second:  13, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.014110, mae: 2.585085, mean_q: 3.114936, mean_eps: 0.194912
 407577/1000000: episode: 561, duration: 48.045s, episode steps: 628, steps per second:  13, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.013363, mae: 2.585982, mean_q: 3.114651, mean_eps: 0.193617
 408242/1000000: episode: 562, duration: 51.770s, episode steps: 665, steps per second:  13, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.013925, mae: 2.601505, mean_q: 3.133974, mean_eps: 0.192338
 408730/1000000: episode: 563, duration: 36.385s, episode steps: 488, steps per second:  13, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.014984, mae: 2.598735, mean_q: 3.132667, mean_eps: 0.191198
 409816/1000000: episode: 564, duration: 84.299s, episode steps: 1086, steps per second:  13, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.015405, mae: 2.607998, mean_q: 3.143135, mean_eps: 0.189641
 410635/1000000: episode: 565, duration: 62.123s, episode steps: 819, steps per second:  13, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.015048, mae: 2.606713, mean_q: 3.141198, mean_eps: 0.187756
 411338/1000000: episode: 566, duration: 55.975s, episode steps: 703, steps per second:  13, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.013187, mae: 2.601298, mean_q: 3.134060, mean_eps: 0.186248
 411882/1000000: episode: 567, duration: 41.683s, episode steps: 544, steps per second:  13, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.086 [0.000, 5.000],  loss: 0.016361, mae: 2.603356, mean_q: 3.135905, mean_eps: 0.185012
 412545/1000000: episode: 568, duration: 50.231s, episode steps: 663, steps per second:  13, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.802 [0.000, 5.000],  loss: 0.015144, mae: 2.602311, mean_q: 3.135601, mean_eps: 0.183816
 413312/1000000: episode: 569, duration: 58.860s, episode steps: 767, steps per second:  13, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.012465, mae: 2.586234, mean_q: 3.116273, mean_eps: 0.182403
 414356/1000000: episode: 570, duration: 79.078s, episode steps: 1044, steps per second:  13, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.013718, mae: 2.605788, mean_q: 3.139866, mean_eps: 0.180613
 415075/1000000: episode: 571, duration: 54.900s, episode steps: 719, steps per second:  13, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.013609, mae: 2.587204, mean_q: 3.118093, mean_eps: 0.178866
 415723/1000000: episode: 572, duration: 49.949s, episode steps: 648, steps per second:  13, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011882, mae: 2.602939, mean_q: 3.138596, mean_eps: 0.177512
 416902/1000000: episode: 573, duration: 90.019s, episode steps: 1179, steps per second:  13, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.014718, mae: 2.593914, mean_q: 3.124972, mean_eps: 0.175702
 417700/1000000: episode: 574, duration: 61.064s, episode steps: 798, steps per second:  13, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.012728, mae: 2.603792, mean_q: 3.137802, mean_eps: 0.173746
 418228/1000000: episode: 575, duration: 41.005s, episode steps: 528, steps per second:  13, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.015259, mae: 2.602204, mean_q: 3.136992, mean_eps: 0.172435
 418889/1000000: episode: 576, duration: 50.371s, episode steps: 661, steps per second:  13, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.013618, mae: 2.599177, mean_q: 3.132759, mean_eps: 0.171255
 419397/1000000: episode: 577, duration: 38.794s, episode steps: 508, steps per second:  13, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.531 [0.000, 5.000],  loss: 0.014565, mae: 2.599967, mean_q: 3.133844, mean_eps: 0.170095
 420145/1000000: episode: 578, duration: 57.162s, episode steps: 748, steps per second:  13, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013725, mae: 2.601478, mean_q: 3.132364, mean_eps: 0.168851
 420873/1000000: episode: 579, duration: 56.175s, episode steps: 728, steps per second:  13, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.014137, mae: 2.609070, mean_q: 3.145407, mean_eps: 0.167390
 421714/1000000: episode: 580, duration: 66.128s, episode steps: 841, steps per second:  13, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.013167, mae: 2.587970, mean_q: 3.119537, mean_eps: 0.165838
 422371/1000000: episode: 581, duration: 51.598s, episode steps: 657, steps per second:  13, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.012085, mae: 2.569516, mean_q: 3.096516, mean_eps: 0.164357
 423056/1000000: episode: 582, duration: 52.686s, episode steps: 685, steps per second:  13, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.014181, mae: 2.598517, mean_q: 3.131243, mean_eps: 0.163030
 424062/1000000: episode: 583, duration: 78.470s, episode steps: 1006, steps per second:  13, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.014429, mae: 2.595733, mean_q: 3.126921, mean_eps: 0.161355
 424665/1000000: episode: 584, duration: 45.381s, episode steps: 603, steps per second:  13, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.013760, mae: 2.602635, mean_q: 3.137075, mean_eps: 0.159759
 426062/1000000: episode: 585, duration: 106.374s, episode steps: 1397, steps per second:  13, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.014223, mae: 2.596695, mean_q: 3.128846, mean_eps: 0.157779
 427060/1000000: episode: 586, duration: 76.281s, episode steps: 998, steps per second:  13, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.014411, mae: 2.603758, mean_q: 3.138806, mean_eps: 0.155411
 427888/1000000: episode: 587, duration: 63.191s, episode steps: 828, steps per second:  13, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.014937, mae: 2.597951, mean_q: 3.130673, mean_eps: 0.153605
 428733/1000000: episode: 588, duration: 63.528s, episode steps: 845, steps per second:  13, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.014576, mae: 2.595813, mean_q: 3.127756, mean_eps: 0.151946
 429423/1000000: episode: 589, duration: 54.644s, episode steps: 690, steps per second:  13, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.013149, mae: 2.571101, mean_q: 3.096845, mean_eps: 0.150426
 430345/1000000: episode: 590, duration: 70.256s, episode steps: 922, steps per second:  13, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.016290, mae: 2.584101, mean_q: 3.111900, mean_eps: 0.148830
 431012/1000000: episode: 591, duration: 53.529s, episode steps: 667, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.014662, mae: 2.585882, mean_q: 3.115257, mean_eps: 0.147258
 431801/1000000: episode: 592, duration: 68.124s, episode steps: 789, steps per second:  12, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.013844, mae: 2.585084, mean_q: 3.114963, mean_eps: 0.145816
 432189/1000000: episode: 593, duration: 34.592s, episode steps: 388, steps per second:  11, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.637 [0.000, 5.000],  loss: 0.014599, mae: 2.584684, mean_q: 3.117823, mean_eps: 0.144648
 432872/1000000: episode: 594, duration: 59.763s, episode steps: 683, steps per second:  11, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.013208, mae: 2.560058, mean_q: 3.083797, mean_eps: 0.143591
 433303/1000000: episode: 595, duration: 37.444s, episode steps: 431, steps per second:  12, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.016586, mae: 2.566090, mean_q: 3.093467, mean_eps: 0.142490
 434351/1000000: episode: 596, duration: 91.116s, episode steps: 1048, steps per second:  12, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.014118, mae: 2.595671, mean_q: 3.127675, mean_eps: 0.141025
 435620/1000000: episode: 597, duration: 110.519s, episode steps: 1269, steps per second:  11, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.013496, mae: 2.601894, mean_q: 3.135084, mean_eps: 0.138732
 436546/1000000: episode: 598, duration: 78.897s, episode steps: 926, steps per second:  12, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.012430, mae: 2.587241, mean_q: 3.117813, mean_eps: 0.136558
 437402/1000000: episode: 599, duration: 74.528s, episode steps: 856, steps per second:  11, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.797 [0.000, 5.000],  loss: 0.015832, mae: 2.598454, mean_q: 3.130356, mean_eps: 0.134791
 438013/1000000: episode: 600, duration: 52.591s, episode steps: 611, steps per second:  12, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.014135, mae: 2.615311, mean_q: 3.150967, mean_eps: 0.133338
 438866/1000000: episode: 601, duration: 72.893s, episode steps: 853, steps per second:  12, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.707 [0.000, 5.000],  loss: 0.015190, mae: 2.598566, mean_q: 3.130114, mean_eps: 0.131889
 439516/1000000: episode: 602, duration: 58.368s, episode steps: 650, steps per second:  11, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.013955, mae: 2.561435, mean_q: 3.085424, mean_eps: 0.130404
 440225/1000000: episode: 603, duration: 61.519s, episode steps: 709, steps per second:  12, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.516 [0.000, 5.000],  loss: 0.013372, mae: 2.614281, mean_q: 3.150609, mean_eps: 0.129057
 441071/1000000: episode: 604, duration: 73.774s, episode steps: 846, steps per second:  11, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.012919, mae: 2.587215, mean_q: 3.118592, mean_eps: 0.127517
 441923/1000000: episode: 605, duration: 72.623s, episode steps: 852, steps per second:  12, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.015080, mae: 2.593720, mean_q: 3.124772, mean_eps: 0.125838
 442676/1000000: episode: 606, duration: 64.283s, episode steps: 753, steps per second:  12, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.013761, mae: 2.634742, mean_q: 3.174578, mean_eps: 0.124250
 443384/1000000: episode: 607, duration: 61.721s, episode steps: 708, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.014694, mae: 2.587552, mean_q: 3.117299, mean_eps: 0.122805
 443894/1000000: episode: 608, duration: 56.126s, episode steps: 510, steps per second:   9, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.014268, mae: 2.570500, mean_q: 3.096442, mean_eps: 0.121597
 444836/1000000: episode: 609, duration: 89.793s, episode steps: 942, steps per second:  10, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.873 [0.000, 5.000],  loss: 0.015423, mae: 2.584442, mean_q: 3.112154, mean_eps: 0.120159
 445632/1000000: episode: 610, duration: 74.539s, episode steps: 796, steps per second:  11, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.014473, mae: 2.612096, mean_q: 3.145370, mean_eps: 0.118441
 446295/1000000: episode: 611, duration: 60.578s, episode steps: 663, steps per second:  11, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.016053, mae: 2.566161, mean_q: 3.090605, mean_eps: 0.116995
 446873/1000000: episode: 612, duration: 53.674s, episode steps: 578, steps per second:  11, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.012077, mae: 2.577274, mean_q: 3.105205, mean_eps: 0.115764
 447806/1000000: episode: 613, duration: 78.119s, episode steps: 933, steps per second:  12, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.015366, mae: 2.589559, mean_q: 3.118935, mean_eps: 0.114267
 448329/1000000: episode: 614, duration: 41.444s, episode steps: 523, steps per second:  13, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.014405, mae: 2.581495, mean_q: 3.111655, mean_eps: 0.112825
 449119/1000000: episode: 615, duration: 60.041s, episode steps: 790, steps per second:  13, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.015416, mae: 2.589241, mean_q: 3.118587, mean_eps: 0.111526
 450142/1000000: episode: 616, duration: 77.963s, episode steps: 1023, steps per second:  13, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.015338, mae: 2.579959, mean_q: 3.106562, mean_eps: 0.109733
 450914/1000000: episode: 617, duration: 60.611s, episode steps: 772, steps per second:  13, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.014723, mae: 2.585603, mean_q: 3.114920, mean_eps: 0.107955
 451730/1000000: episode: 618, duration: 62.919s, episode steps: 816, steps per second:  13, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.016840, mae: 2.609997, mean_q: 3.143295, mean_eps: 0.106382
 452743/1000000: episode: 619, duration: 78.123s, episode steps: 1013, steps per second:  13, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.016432, mae: 2.591626, mean_q: 3.121177, mean_eps: 0.104573
 453434/1000000: episode: 620, duration: 52.994s, episode steps: 691, steps per second:  13, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013195, mae: 2.594998, mean_q: 3.127052, mean_eps: 0.102886
 454383/1000000: episode: 621, duration: 73.771s, episode steps: 949, steps per second:  13, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.014659, mae: 2.584214, mean_q: 3.113570, mean_eps: 0.101262
 455221/1000000: episode: 622, duration: 64.895s, episode steps: 838, steps per second:  13, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.014110, mae: 2.598205, mean_q: 3.130343, mean_eps: 0.099492
 455983/1000000: episode: 623, duration: 58.435s, episode steps: 762, steps per second:  13, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.013973, mae: 2.570671, mean_q: 3.096402, mean_eps: 0.097908
 456512/1000000: episode: 624, duration: 41.051s, episode steps: 529, steps per second:  13, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.945 [0.000, 5.000],  loss: 0.013412, mae: 2.583699, mean_q: 3.112040, mean_eps: 0.096633
 457447/1000000: episode: 625, duration: 72.482s, episode steps: 935, steps per second:  13, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.013942, mae: 2.607081, mean_q: 3.141178, mean_eps: 0.095184
 458441/1000000: episode: 626, duration: 76.349s, episode steps: 994, steps per second:  13, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.015241, mae: 2.589715, mean_q: 3.121211, mean_eps: 0.093271
 458919/1000000: episode: 627, duration: 37.532s, episode steps: 478, steps per second:  13, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.990 [0.000, 5.000],  loss: 0.012632, mae: 2.604986, mean_q: 3.140568, mean_eps: 0.091814
 459697/1000000: episode: 628, duration: 59.412s, episode steps: 778, steps per second:  13, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.929 [0.000, 5.000],  loss: 0.013560, mae: 2.591147, mean_q: 3.122610, mean_eps: 0.090570
 460372/1000000: episode: 629, duration: 52.702s, episode steps: 675, steps per second:  13, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.853 [0.000, 5.000],  loss: 0.013723, mae: 2.574501, mean_q: 3.101392, mean_eps: 0.089133
 460971/1000000: episode: 630, duration: 46.573s, episode steps: 599, steps per second:  13, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.013718, mae: 2.605124, mean_q: 3.139452, mean_eps: 0.087873
 461547/1000000: episode: 631, duration: 42.893s, episode steps: 576, steps per second:  13, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.013513, mae: 2.590201, mean_q: 3.119119, mean_eps: 0.086709
 462531/1000000: episode: 632, duration: 76.628s, episode steps: 984, steps per second:  13, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.016148, mae: 2.582778, mean_q: 3.110550, mean_eps: 0.085165
 463219/1000000: episode: 633, duration: 52.879s, episode steps: 688, steps per second:  13, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.013735, mae: 2.582356, mean_q: 3.111830, mean_eps: 0.083509
 464170/1000000: episode: 634, duration: 74.937s, episode steps: 951, steps per second:  13, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.014235, mae: 2.607754, mean_q: 3.142275, mean_eps: 0.081886
 464862/1000000: episode: 635, duration: 53.398s, episode steps: 692, steps per second:  13, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.012749, mae: 2.569284, mean_q: 3.097158, mean_eps: 0.080258
 465562/1000000: episode: 636, duration: 54.655s, episode steps: 700, steps per second:  13, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.013855, mae: 2.600643, mean_q: 3.133540, mean_eps: 0.078880
 466113/1000000: episode: 637, duration: 43.339s, episode steps: 551, steps per second:  13, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.020 [0.000, 5.000],  loss: 0.013149, mae: 2.601334, mean_q: 3.137207, mean_eps: 0.077641
 466692/1000000: episode: 638, duration: 44.427s, episode steps: 579, steps per second:  13, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.839 [0.000, 5.000],  loss: 0.016320, mae: 2.598496, mean_q: 3.130941, mean_eps: 0.076524
 467287/1000000: episode: 639, duration: 48.287s, episode steps: 595, steps per second:  12, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.013898, mae: 2.578457, mean_q: 3.106490, mean_eps: 0.075364
 468053/1000000: episode: 640, duration: 59.527s, episode steps: 766, steps per second:  13, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.715 [0.000, 5.000],  loss: 0.015159, mae: 2.597091, mean_q: 3.128269, mean_eps: 0.074013
 468620/1000000: episode: 641, duration: 46.685s, episode steps: 567, steps per second:  12, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014255, mae: 2.572038, mean_q: 3.098550, mean_eps: 0.072695
 469160/1000000: episode: 642, duration: 45.051s, episode steps: 540, steps per second:  12, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.014880, mae: 2.584893, mean_q: 3.113755, mean_eps: 0.071602
 469704/1000000: episode: 643, duration: 42.253s, episode steps: 544, steps per second:  13, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.013646, mae: 2.578132, mean_q: 3.105908, mean_eps: 0.070529
 470358/1000000: episode: 644, duration: 54.935s, episode steps: 654, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.933 [0.000, 5.000],  loss: 0.013694, mae: 2.575403, mean_q: 3.102938, mean_eps: 0.069341
 471327/1000000: episode: 645, duration: 74.583s, episode steps: 969, steps per second:  13, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.013164, mae: 2.583344, mean_q: 3.112231, mean_eps: 0.067733
 472392/1000000: episode: 646, duration: 89.288s, episode steps: 1065, steps per second:  12, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.012794, mae: 2.584240, mean_q: 3.115237, mean_eps: 0.065721
 472955/1000000: episode: 647, duration: 46.702s, episode steps: 563, steps per second:  12, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.015489, mae: 2.579107, mean_q: 3.104443, mean_eps: 0.064109
 473629/1000000: episode: 648, duration: 57.514s, episode steps: 674, steps per second:  12, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.014712, mae: 2.591064, mean_q: 3.120990, mean_eps: 0.062882
 474305/1000000: episode: 649, duration: 54.119s, episode steps: 676, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.013025, mae: 2.587153, mean_q: 3.117031, mean_eps: 0.061543
 475340/1000000: episode: 650, duration: 85.874s, episode steps: 1035, steps per second:  12, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.015927, mae: 2.597339, mean_q: 3.128232, mean_eps: 0.059852
 476040/1000000: episode: 651, duration: 58.847s, episode steps: 700, steps per second:  12, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.419 [0.000, 5.000],  loss: 0.016216, mae: 2.583548, mean_q: 3.113877, mean_eps: 0.058138
 476500/1000000: episode: 652, duration: 41.804s, episode steps: 460, steps per second:  11, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.014829, mae: 2.575791, mean_q: 3.103505, mean_eps: 0.056989
 477156/1000000: episode: 653, duration: 57.521s, episode steps: 656, steps per second:  11, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014746, mae: 2.558281, mean_q: 3.082796, mean_eps: 0.055885
 478268/1000000: episode: 654, duration: 97.595s, episode steps: 1112, steps per second:  11, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.014748, mae: 2.565902, mean_q: 3.091559, mean_eps: 0.054134
 479246/1000000: episode: 655, duration: 86.705s, episode steps: 978, steps per second:  11, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.015470, mae: 2.588791, mean_q: 3.118771, mean_eps: 0.052063
 480182/1000000: episode: 656, duration: 80.789s, episode steps: 936, steps per second:  12, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.014235, mae: 2.590723, mean_q: 3.121618, mean_eps: 0.050166
 481073/1000000: episode: 657, duration: 79.341s, episode steps: 891, steps per second:  11, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.759 [0.000, 5.000],  loss: 0.013861, mae: 2.566263, mean_q: 3.093079, mean_eps: 0.048357
 481633/1000000: episode: 658, duration: 48.720s, episode steps: 560, steps per second:  11, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.016220, mae: 2.588151, mean_q: 3.118100, mean_eps: 0.046919
 482479/1000000: episode: 659, duration: 73.397s, episode steps: 846, steps per second:  12, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.014073, mae: 2.578492, mean_q: 3.106586, mean_eps: 0.045529
 483727/1000000: episode: 660, duration: 108.265s, episode steps: 1248, steps per second:  12, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.014193, mae: 2.594671, mean_q: 3.125321, mean_eps: 0.043458
 484656/1000000: episode: 661, duration: 81.624s, episode steps: 929, steps per second:  11, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014219, mae: 2.580576, mean_q: 3.110930, mean_eps: 0.041304
 485229/1000000: episode: 662, duration: 50.903s, episode steps: 573, steps per second:  11, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.016977, mae: 2.572558, mean_q: 3.098169, mean_eps: 0.039815
 485907/1000000: episode: 663, duration: 60.291s, episode steps: 678, steps per second:  11, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.013793, mae: 2.574083, mean_q: 3.100167, mean_eps: 0.038575
 486653/1000000: episode: 664, duration: 68.699s, episode steps: 746, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.015243, mae: 2.573816, mean_q: 3.101890, mean_eps: 0.037166
 487557/1000000: episode: 665, duration: 71.632s, episode steps: 904, steps per second:  13, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.015653, mae: 2.576549, mean_q: 3.103557, mean_eps: 0.035530
 488501/1000000: episode: 666, duration: 76.711s, episode steps: 944, steps per second:  12, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.014131, mae: 2.580953, mean_q: 3.109237, mean_eps: 0.033701
 488883/1000000: episode: 667, duration: 30.463s, episode steps: 382, steps per second:  13, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.012995, mae: 2.591948, mean_q: 3.125543, mean_eps: 0.032390
 489663/1000000: episode: 668, duration: 67.414s, episode steps: 780, steps per second:  12, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.013154, mae: 2.596612, mean_q: 3.129260, mean_eps: 0.031241
 490208/1000000: episode: 669, duration: 46.558s, episode steps: 545, steps per second:  12, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.014191, mae: 2.598309, mean_q: 3.132040, mean_eps: 0.029931
 491142/1000000: episode: 670, duration: 82.816s, episode steps: 934, steps per second:  11, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.013394, mae: 2.581383, mean_q: 3.111597, mean_eps: 0.028465
 492112/1000000: episode: 671, duration: 84.193s, episode steps: 970, steps per second:  12, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.014008, mae: 2.584766, mean_q: 3.113576, mean_eps: 0.026581
 492787/1000000: episode: 672, duration: 60.220s, episode steps: 675, steps per second:  11, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.014125, mae: 2.586020, mean_q: 3.114806, mean_eps: 0.024953
 493890/1000000: episode: 673, duration: 97.445s, episode steps: 1103, steps per second:  11, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.013651, mae: 2.573545, mean_q: 3.100928, mean_eps: 0.023191
 495149/1000000: episode: 674, duration: 111.313s, episode steps: 1259, steps per second:  11, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.876 [0.000, 5.000],  loss: 0.013578, mae: 2.585033, mean_q: 3.114847, mean_eps: 0.020850
 495566/1000000: episode: 675, duration: 36.086s, episode steps: 417, steps per second:  12, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.014939, mae: 2.590848, mean_q: 3.120895, mean_eps: 0.019191
 496136/1000000: episode: 676, duration: 51.214s, episode steps: 570, steps per second:  11, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.012076, mae: 2.549163, mean_q: 3.071646, mean_eps: 0.018217
 497237/1000000: episode: 677, duration: 98.685s, episode steps: 1101, steps per second:  11, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.015291, mae: 2.596349, mean_q: 3.126832, mean_eps: 0.016562
 498153/1000000: episode: 678, duration: 81.092s, episode steps: 916, steps per second:  11, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.014100, mae: 2.568608, mean_q: 3.095269, mean_eps: 0.014562
 499078/1000000: episode: 679, duration: 80.442s, episode steps: 925, steps per second:  11, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.012902, mae: 2.570020, mean_q: 3.097837, mean_eps: 0.012740
 500002/1000000: episode: 680, duration: 82.943s, episode steps: 924, steps per second:  11, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.013807, mae: 2.581510, mean_q: 3.111532, mean_eps: 0.010911
 500563/1000000: episode: 681, duration: 49.438s, episode steps: 561, steps per second:  11, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.012551, mae: 2.605794, mean_q: 3.140326, mean_eps: 0.010000
 501263/1000000: episode: 682, duration: 59.562s, episode steps: 700, steps per second:  12, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.015099, mae: 2.559289, mean_q: 3.083107, mean_eps: 0.010000
 502277/1000000: episode: 683, duration: 84.746s, episode steps: 1014, steps per second:  12, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.013054, mae: 2.576592, mean_q: 3.105514, mean_eps: 0.010000
 503340/1000000: episode: 684, duration: 111.611s, episode steps: 1063, steps per second:  10, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.012736, mae: 2.583339, mean_q: 3.113334, mean_eps: 0.010000
 504072/1000000: episode: 685, duration: 70.164s, episode steps: 732, steps per second:  10, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.013610, mae: 2.564666, mean_q: 3.088216, mean_eps: 0.010000
 504884/1000000: episode: 686, duration: 77.323s, episode steps: 812, steps per second:  11, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.013198, mae: 2.581531, mean_q: 3.111020, mean_eps: 0.010000
 505408/1000000: episode: 687, duration: 54.300s, episode steps: 524, steps per second:  10, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.013701, mae: 2.556685, mean_q: 3.081635, mean_eps: 0.010000
 506414/1000000: episode: 688, duration: 91.117s, episode steps: 1006, steps per second:  11, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.015313, mae: 2.579782, mean_q: 3.107063, mean_eps: 0.010000
 507155/1000000: episode: 689, duration: 66.022s, episode steps: 741, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.012315, mae: 2.580085, mean_q: 3.108150, mean_eps: 0.010000
 507707/1000000: episode: 690, duration: 50.406s, episode steps: 552, steps per second:  11, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.015366, mae: 2.567774, mean_q: 3.092404, mean_eps: 0.010000
 508548/1000000: episode: 691, duration: 68.787s, episode steps: 841, steps per second:  12, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.618 [0.000, 5.000],  loss: 0.016307, mae: 2.589591, mean_q: 3.119608, mean_eps: 0.010000
 509549/1000000: episode: 692, duration: 81.162s, episode steps: 1001, steps per second:  12, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.015844, mae: 2.567716, mean_q: 3.092083, mean_eps: 0.010000
 510589/1000000: episode: 693, duration: 84.350s, episode steps: 1040, steps per second:  12, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.013242, mae: 2.588672, mean_q: 3.118915, mean_eps: 0.010000
 511349/1000000: episode: 694, duration: 60.153s, episode steps: 760, steps per second:  13, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.015329, mae: 2.595155, mean_q: 3.125285, mean_eps: 0.010000
 512004/1000000: episode: 695, duration: 52.701s, episode steps: 655, steps per second:  12, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.016191, mae: 2.584358, mean_q: 3.112852, mean_eps: 0.010000
 512521/1000000: episode: 696, duration: 42.780s, episode steps: 517, steps per second:  12, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.014297, mae: 2.567104, mean_q: 3.093832, mean_eps: 0.010000
 513031/1000000: episode: 697, duration: 40.117s, episode steps: 510, steps per second:  13, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012529, mae: 2.568884, mean_q: 3.096595, mean_eps: 0.010000
 513981/1000000: episode: 698, duration: 77.534s, episode steps: 950, steps per second:  12, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014335, mae: 2.581604, mean_q: 3.110976, mean_eps: 0.010000
 514653/1000000: episode: 699, duration: 54.789s, episode steps: 672, steps per second:  12, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013756, mae: 2.585177, mean_q: 3.116188, mean_eps: 0.010000
 515458/1000000: episode: 700, duration: 64.995s, episode steps: 805, steps per second:  12, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012397, mae: 2.571381, mean_q: 3.096954, mean_eps: 0.010000
 516537/1000000: episode: 701, duration: 86.855s, episode steps: 1079, steps per second:  12, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.015875, mae: 2.580075, mean_q: 3.107762, mean_eps: 0.010000
 517050/1000000: episode: 702, duration: 42.473s, episode steps: 513, steps per second:  12, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.013210, mae: 2.592869, mean_q: 3.124438, mean_eps: 0.010000
 517760/1000000: episode: 703, duration: 57.989s, episode steps: 710, steps per second:  12, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.014977, mae: 2.582703, mean_q: 3.112546, mean_eps: 0.010000
 518440/1000000: episode: 704, duration: 56.508s, episode steps: 680, steps per second:  12, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.078 [0.000, 5.000],  loss: 0.014564, mae: 2.544221, mean_q: 3.064411, mean_eps: 0.010000
 519651/1000000: episode: 705, duration: 100.534s, episode steps: 1211, steps per second:  12, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.015303, mae: 2.567919, mean_q: 3.093770, mean_eps: 0.010000
 520519/1000000: episode: 706, duration: 71.845s, episode steps: 868, steps per second:  12, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.813 [0.000, 5.000],  loss: 0.014087, mae: 2.582185, mean_q: 3.111991, mean_eps: 0.010000
 521081/1000000: episode: 707, duration: 49.536s, episode steps: 562, steps per second:  11, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.013601, mae: 2.587305, mean_q: 3.118869, mean_eps: 0.010000
 521906/1000000: episode: 708, duration: 74.174s, episode steps: 825, steps per second:  11, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.014037, mae: 2.569431, mean_q: 3.095536, mean_eps: 0.010000
 522571/1000000: episode: 709, duration: 58.475s, episode steps: 665, steps per second:  11, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.173 [0.000, 5.000],  loss: 0.014153, mae: 2.575039, mean_q: 3.102491, mean_eps: 0.010000
 523420/1000000: episode: 710, duration: 70.017s, episode steps: 849, steps per second:  12, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.755 [0.000, 5.000],  loss: 0.013239, mae: 2.580114, mean_q: 3.110370, mean_eps: 0.010000
 524104/1000000: episode: 711, duration: 60.902s, episode steps: 684, steps per second:  11, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.013917, mae: 2.574274, mean_q: 3.103323, mean_eps: 0.010000
 524774/1000000: episode: 712, duration: 60.612s, episode steps: 670, steps per second:  11, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.013050, mae: 2.561027, mean_q: 3.086119, mean_eps: 0.010000
 525435/1000000: episode: 713, duration: 60.960s, episode steps: 661, steps per second:  11, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.015021, mae: 2.583165, mean_q: 3.114683, mean_eps: 0.010000
 527096/1000000: episode: 714, duration: 151.090s, episode steps: 1661, steps per second:  11, episode reward: 35.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.014691, mae: 2.581897, mean_q: 3.111228, mean_eps: 0.010000
 528150/1000000: episode: 715, duration: 97.857s, episode steps: 1054, steps per second:  11, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.868 [0.000, 5.000],  loss: 0.013874, mae: 2.592663, mean_q: 3.123747, mean_eps: 0.010000
 528708/1000000: episode: 716, duration: 45.806s, episode steps: 558, steps per second:  12, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.014854, mae: 2.604663, mean_q: 3.139119, mean_eps: 0.010000
 529565/1000000: episode: 717, duration: 71.046s, episode steps: 857, steps per second:  12, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.188 [0.000, 5.000],  loss: 0.014115, mae: 2.581016, mean_q: 3.109691, mean_eps: 0.010000
 530462/1000000: episode: 718, duration: 72.998s, episode steps: 897, steps per second:  12, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.511 [0.000, 5.000],  loss: 0.015196, mae: 2.571603, mean_q: 3.098727, mean_eps: 0.010000
 531143/1000000: episode: 719, duration: 57.278s, episode steps: 681, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.969 [0.000, 5.000],  loss: 0.015131, mae: 2.580182, mean_q: 3.109047, mean_eps: 0.010000
 531793/1000000: episode: 720, duration: 52.395s, episode steps: 650, steps per second:  12, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.014947, mae: 2.580900, mean_q: 3.111925, mean_eps: 0.010000
 532136/1000000: episode: 721, duration: 28.321s, episode steps: 343, steps per second:  12, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.015388, mae: 2.596886, mean_q: 3.127217, mean_eps: 0.010000
 532939/1000000: episode: 722, duration: 66.939s, episode steps: 803, steps per second:  12, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.013148, mae: 2.578936, mean_q: 3.108676, mean_eps: 0.010000
 533600/1000000: episode: 723, duration: 55.691s, episode steps: 661, steps per second:  12, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.015405, mae: 2.590811, mean_q: 3.122552, mean_eps: 0.010000
 534268/1000000: episode: 724, duration: 58.072s, episode steps: 668, steps per second:  12, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.013361, mae: 2.580646, mean_q: 3.107836, mean_eps: 0.010000
 534993/1000000: episode: 725, duration: 61.409s, episode steps: 725, steps per second:  12, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.012136, mae: 2.607271, mean_q: 3.139800, mean_eps: 0.010000
 535952/1000000: episode: 726, duration: 85.091s, episode steps: 959, steps per second:  11, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.013695, mae: 2.562088, mean_q: 3.085897, mean_eps: 0.010000
 536640/1000000: episode: 727, duration: 63.420s, episode steps: 688, steps per second:  11, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.015971, mae: 2.567395, mean_q: 3.092920, mean_eps: 0.010000
 537248/1000000: episode: 728, duration: 56.287s, episode steps: 608, steps per second:  11, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.015513, mae: 2.594043, mean_q: 3.123045, mean_eps: 0.010000
 537886/1000000: episode: 729, duration: 59.975s, episode steps: 638, steps per second:  11, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.014488, mae: 2.580571, mean_q: 3.109277, mean_eps: 0.010000
 538865/1000000: episode: 730, duration: 97.756s, episode steps: 979, steps per second:  10, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.014456, mae: 2.561560, mean_q: 3.084825, mean_eps: 0.010000
 539560/1000000: episode: 731, duration: 68.608s, episode steps: 695, steps per second:  10, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.012458, mae: 2.586889, mean_q: 3.116844, mean_eps: 0.010000
 540094/1000000: episode: 732, duration: 56.649s, episode steps: 534, steps per second:   9, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.013800, mae: 2.592506, mean_q: 3.123208, mean_eps: 0.010000
 540803/1000000: episode: 733, duration: 67.219s, episode steps: 709, steps per second:  11, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.014447, mae: 2.572756, mean_q: 3.101952, mean_eps: 0.010000
 541515/1000000: episode: 734, duration: 63.191s, episode steps: 712, steps per second:  11, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.830 [0.000, 5.000],  loss: 0.013857, mae: 2.577233, mean_q: 3.105903, mean_eps: 0.010000
 542209/1000000: episode: 735, duration: 63.833s, episode steps: 694, steps per second:  11, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.014072, mae: 2.565853, mean_q: 3.090326, mean_eps: 0.010000
 542700/1000000: episode: 736, duration: 44.634s, episode steps: 491, steps per second:  11, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.635 [0.000, 5.000],  loss: 0.014194, mae: 2.575792, mean_q: 3.102122, mean_eps: 0.010000
 543843/1000000: episode: 737, duration: 104.317s, episode steps: 1143, steps per second:  11, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.014164, mae: 2.577391, mean_q: 3.103745, mean_eps: 0.010000
 545045/1000000: episode: 738, duration: 111.405s, episode steps: 1202, steps per second:  11, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.719 [0.000, 5.000],  loss: 0.013520, mae: 2.599105, mean_q: 3.132282, mean_eps: 0.010000
 545726/1000000: episode: 739, duration: 63.787s, episode steps: 681, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.014160, mae: 2.582653, mean_q: 3.109798, mean_eps: 0.010000
 546565/1000000: episode: 740, duration: 78.330s, episode steps: 839, steps per second:  11, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.014071, mae: 2.581318, mean_q: 3.108960, mean_eps: 0.010000
 547703/1000000: episode: 741, duration: 106.642s, episode steps: 1138, steps per second:  11, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.012973, mae: 2.575628, mean_q: 3.103059, mean_eps: 0.010000
 548348/1000000: episode: 742, duration: 58.246s, episode steps: 645, steps per second:  11, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.015606, mae: 2.606439, mean_q: 3.139853, mean_eps: 0.010000
 549398/1000000: episode: 743, duration: 97.997s, episode steps: 1050, steps per second:  11, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.014313, mae: 2.566524, mean_q: 3.091800, mean_eps: 0.010000
 550315/1000000: episode: 744, duration: 87.338s, episode steps: 917, steps per second:  10, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.015003, mae: 2.596630, mean_q: 3.127172, mean_eps: 0.010000
 551463/1000000: episode: 745, duration: 105.700s, episode steps: 1148, steps per second:  11, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.015265, mae: 2.578391, mean_q: 3.106721, mean_eps: 0.010000
 552125/1000000: episode: 746, duration: 62.099s, episode steps: 662, steps per second:  11, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.014417, mae: 2.589979, mean_q: 3.121502, mean_eps: 0.010000
 552585/1000000: episode: 747, duration: 43.625s, episode steps: 460, steps per second:  11, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.559 [0.000, 5.000],  loss: 0.016513, mae: 2.589146, mean_q: 3.117643, mean_eps: 0.010000
 553558/1000000: episode: 748, duration: 91.160s, episode steps: 973, steps per second:  11, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.664 [0.000, 5.000],  loss: 0.014462, mae: 2.591841, mean_q: 3.122148, mean_eps: 0.010000
 554271/1000000: episode: 749, duration: 66.800s, episode steps: 713, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.014942, mae: 2.598701, mean_q: 3.129825, mean_eps: 0.010000
 555029/1000000: episode: 750, duration: 71.347s, episode steps: 758, steps per second:  11, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.012924, mae: 2.578240, mean_q: 3.106352, mean_eps: 0.010000
 555658/1000000: episode: 751, duration: 58.907s, episode steps: 629, steps per second:  11, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.126 [0.000, 5.000],  loss: 0.015797, mae: 2.588005, mean_q: 3.115547, mean_eps: 0.010000
 556411/1000000: episode: 752, duration: 69.361s, episode steps: 753, steps per second:  11, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.904 [0.000, 5.000],  loss: 0.014058, mae: 2.595070, mean_q: 3.126703, mean_eps: 0.010000
 557252/1000000: episode: 753, duration: 80.906s, episode steps: 841, steps per second:  10, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.014623, mae: 2.602862, mean_q: 3.136028, mean_eps: 0.010000
 558476/1000000: episode: 754, duration: 116.776s, episode steps: 1224, steps per second:  10, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.014746, mae: 2.609349, mean_q: 3.142599, mean_eps: 0.010000
 559406/1000000: episode: 755, duration: 86.679s, episode steps: 930, steps per second:  11, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.015252, mae: 2.601192, mean_q: 3.132600, mean_eps: 0.010000
 560288/1000000: episode: 756, duration: 84.234s, episode steps: 882, steps per second:  10, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.012904, mae: 2.579568, mean_q: 3.107150, mean_eps: 0.010000
 561116/1000000: episode: 757, duration: 79.297s, episode steps: 828, steps per second:  10, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.013954, mae: 2.591995, mean_q: 3.120829, mean_eps: 0.010000
 561938/1000000: episode: 758, duration: 77.642s, episode steps: 822, steps per second:  11, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.013398, mae: 2.593108, mean_q: 3.123816, mean_eps: 0.010000
 562861/1000000: episode: 759, duration: 84.893s, episode steps: 923, steps per second:  11, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.015590, mae: 2.592072, mean_q: 3.121478, mean_eps: 0.010000
 563903/1000000: episode: 760, duration: 98.755s, episode steps: 1042, steps per second:  11, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.885 [0.000, 5.000],  loss: 0.014842, mae: 2.585231, mean_q: 3.113126, mean_eps: 0.010000
 564567/1000000: episode: 761, duration: 63.114s, episode steps: 664, steps per second:  11, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.834 [0.000, 5.000],  loss: 0.013801, mae: 2.592950, mean_q: 3.123738, mean_eps: 0.010000
 565199/1000000: episode: 762, duration: 58.441s, episode steps: 632, steps per second:  11, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.013197, mae: 2.596748, mean_q: 3.128118, mean_eps: 0.010000
 565601/1000000: episode: 763, duration: 39.023s, episode steps: 402, steps per second:  10, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.014161, mae: 2.601585, mean_q: 3.133637, mean_eps: 0.010000
 566254/1000000: episode: 764, duration: 60.326s, episode steps: 653, steps per second:  11, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.013026, mae: 2.600470, mean_q: 3.134959, mean_eps: 0.010000
 567266/1000000: episode: 765, duration: 95.018s, episode steps: 1012, steps per second:  11, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.014207, mae: 2.599076, mean_q: 3.132001, mean_eps: 0.010000
 568323/1000000: episode: 766, duration: 100.140s, episode steps: 1057, steps per second:  11, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.015841, mae: 2.590541, mean_q: 3.121453, mean_eps: 0.010000
 569173/1000000: episode: 767, duration: 79.888s, episode steps: 850, steps per second:  11, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.709 [0.000, 5.000],  loss: 0.014420, mae: 2.592476, mean_q: 3.123658, mean_eps: 0.010000
 569911/1000000: episode: 768, duration: 70.159s, episode steps: 738, steps per second:  11, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.013242, mae: 2.613364, mean_q: 3.147737, mean_eps: 0.010000
 570831/1000000: episode: 769, duration: 86.235s, episode steps: 920, steps per second:  11, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.792 [0.000, 5.000],  loss: 0.012670, mae: 2.591704, mean_q: 3.122829, mean_eps: 0.010000
 571522/1000000: episode: 770, duration: 65.273s, episode steps: 691, steps per second:  11, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.802 [0.000, 5.000],  loss: 0.015310, mae: 2.576493, mean_q: 3.103894, mean_eps: 0.010000
 572208/1000000: episode: 771, duration: 67.424s, episode steps: 686, steps per second:  10, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.016308, mae: 2.585995, mean_q: 3.117655, mean_eps: 0.010000
 572751/1000000: episode: 772, duration: 51.687s, episode steps: 543, steps per second:  11, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.014126, mae: 2.605444, mean_q: 3.138594, mean_eps: 0.010000
 573222/1000000: episode: 773, duration: 45.352s, episode steps: 471, steps per second:  10, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.013446, mae: 2.600986, mean_q: 3.134671, mean_eps: 0.010000
 574273/1000000: episode: 774, duration: 100.417s, episode steps: 1051, steps per second:  10, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.015290, mae: 2.597301, mean_q: 3.127481, mean_eps: 0.010000
 574969/1000000: episode: 775, duration: 59.307s, episode steps: 696, steps per second:  12, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.014225, mae: 2.596773, mean_q: 3.127914, mean_eps: 0.010000
 575654/1000000: episode: 776, duration: 61.664s, episode steps: 685, steps per second:  11, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.017054, mae: 2.594453, mean_q: 3.125492, mean_eps: 0.010000
 576324/1000000: episode: 777, duration: 66.013s, episode steps: 670, steps per second:  10, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.013872, mae: 2.595253, mean_q: 3.126584, mean_eps: 0.010000
 576831/1000000: episode: 778, duration: 48.700s, episode steps: 507, steps per second:  10, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.015887, mae: 2.587951, mean_q: 3.118619, mean_eps: 0.010000
 577684/1000000: episode: 779, duration: 82.286s, episode steps: 853, steps per second:  10, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.015494, mae: 2.597236, mean_q: 3.128561, mean_eps: 0.010000
 578554/1000000: episode: 780, duration: 81.790s, episode steps: 870, steps per second:  11, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.014525, mae: 2.594933, mean_q: 3.124191, mean_eps: 0.010000
 579423/1000000: episode: 781, duration: 74.967s, episode steps: 869, steps per second:  12, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.014187, mae: 2.591684, mean_q: 3.120485, mean_eps: 0.010000
 580139/1000000: episode: 782, duration: 68.312s, episode steps: 716, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.015511, mae: 2.603066, mean_q: 3.135689, mean_eps: 0.010000
 581001/1000000: episode: 783, duration: 81.726s, episode steps: 862, steps per second:  11, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.012616, mae: 2.598410, mean_q: 3.129499, mean_eps: 0.010000
 581978/1000000: episode: 784, duration: 94.293s, episode steps: 977, steps per second:  10, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.014115, mae: 2.571401, mean_q: 3.096660, mean_eps: 0.010000
 582701/1000000: episode: 785, duration: 68.049s, episode steps: 723, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.014755, mae: 2.602325, mean_q: 3.134689, mean_eps: 0.010000
 583282/1000000: episode: 786, duration: 53.508s, episode steps: 581, steps per second:  11, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.013495, mae: 2.585821, mean_q: 3.114750, mean_eps: 0.010000
 584133/1000000: episode: 787, duration: 83.189s, episode steps: 851, steps per second:  10, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.549 [0.000, 5.000],  loss: 0.013109, mae: 2.586232, mean_q: 3.116215, mean_eps: 0.010000
 585372/1000000: episode: 788, duration: 118.701s, episode steps: 1239, steps per second:  10, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.015371, mae: 2.597094, mean_q: 3.126969, mean_eps: 0.010000
 585879/1000000: episode: 789, duration: 48.605s, episode steps: 507, steps per second:  10, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.013958, mae: 2.611202, mean_q: 3.148016, mean_eps: 0.010000
 586317/1000000: episode: 790, duration: 43.208s, episode steps: 438, steps per second:  10, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.013736, mae: 2.585608, mean_q: 3.115036, mean_eps: 0.010000
 587438/1000000: episode: 791, duration: 109.297s, episode steps: 1121, steps per second:  10, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.014387, mae: 2.568088, mean_q: 3.094213, mean_eps: 0.010000
 588547/1000000: episode: 792, duration: 95.603s, episode steps: 1109, steps per second:  12, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.015307, mae: 2.597251, mean_q: 3.128352, mean_eps: 0.010000
 589411/1000000: episode: 793, duration: 73.341s, episode steps: 864, steps per second:  12, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.013351, mae: 2.581218, mean_q: 3.109272, mean_eps: 0.010000
 590121/1000000: episode: 794, duration: 61.203s, episode steps: 710, steps per second:  12, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.013633, mae: 2.595845, mean_q: 3.128891, mean_eps: 0.010000
 590951/1000000: episode: 795, duration: 71.691s, episode steps: 830, steps per second:  12, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.014980, mae: 2.594571, mean_q: 3.125065, mean_eps: 0.010000
 591731/1000000: episode: 796, duration: 66.705s, episode steps: 780, steps per second:  12, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.013106, mae: 2.604440, mean_q: 3.138118, mean_eps: 0.010000
 592476/1000000: episode: 797, duration: 65.106s, episode steps: 745, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.980 [0.000, 5.000],  loss: 0.014077, mae: 2.574033, mean_q: 3.100449, mean_eps: 0.010000
 593104/1000000: episode: 798, duration: 53.395s, episode steps: 628, steps per second:  12, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.014138, mae: 2.584927, mean_q: 3.114076, mean_eps: 0.010000
 593822/1000000: episode: 799, duration: 62.234s, episode steps: 718, steps per second:  12, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.013770, mae: 2.597674, mean_q: 3.129390, mean_eps: 0.010000
 594853/1000000: episode: 800, duration: 91.985s, episode steps: 1031, steps per second:  11, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.014180, mae: 2.585449, mean_q: 3.113453, mean_eps: 0.010000
 595660/1000000: episode: 801, duration: 71.021s, episode steps: 807, steps per second:  11, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: 0.015526, mae: 2.577417, mean_q: 3.103410, mean_eps: 0.010000
 596344/1000000: episode: 802, duration: 67.557s, episode steps: 684, steps per second:  10, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.014124, mae: 2.587738, mean_q: 3.115877, mean_eps: 0.010000
 597176/1000000: episode: 803, duration: 80.136s, episode steps: 832, steps per second:  10, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.013495, mae: 2.610211, mean_q: 3.142346, mean_eps: 0.010000
 598075/1000000: episode: 804, duration: 89.708s, episode steps: 899, steps per second:  10, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014888, mae: 2.586838, mean_q: 3.115054, mean_eps: 0.010000
 598883/1000000: episode: 805, duration: 79.018s, episode steps: 808, steps per second:  10, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.013704, mae: 2.587240, mean_q: 3.115236, mean_eps: 0.010000
 599664/1000000: episode: 806, duration: 75.127s, episode steps: 781, steps per second:  10, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.013845, mae: 2.595008, mean_q: 3.125311, mean_eps: 0.010000
 600708/1000000: episode: 807, duration: 98.410s, episode steps: 1044, steps per second:  11, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.014136, mae: 2.588312, mean_q: 3.117396, mean_eps: 0.010000
 601738/1000000: episode: 808, duration: 100.685s, episode steps: 1030, steps per second:  10, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.012825, mae: 2.556804, mean_q: 3.081589, mean_eps: 0.010000
 602518/1000000: episode: 809, duration: 75.928s, episode steps: 780, steps per second:  10, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.013040, mae: 2.577779, mean_q: 3.107201, mean_eps: 0.010000
 603268/1000000: episode: 810, duration: 87.508s, episode steps: 750, steps per second:   9, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.831 [0.000, 5.000],  loss: 0.014820, mae: 2.577014, mean_q: 3.103273, mean_eps: 0.010000
 604074/1000000: episode: 811, duration: 98.814s, episode steps: 806, steps per second:   8, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.011643, mae: 2.558224, mean_q: 3.081201, mean_eps: 0.010000
 604678/1000000: episode: 812, duration: 67.507s, episode steps: 604, steps per second:   9, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.014904, mae: 2.569251, mean_q: 3.093112, mean_eps: 0.010000
 605404/1000000: episode: 813, duration: 67.930s, episode steps: 726, steps per second:  11, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.698 [0.000, 5.000],  loss: 0.012290, mae: 2.565393, mean_q: 3.089691, mean_eps: 0.010000
 606476/1000000: episode: 814, duration: 94.888s, episode steps: 1072, steps per second:  11, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.015494, mae: 2.553990, mean_q: 3.076111, mean_eps: 0.010000
 607428/1000000: episode: 815, duration: 83.058s, episode steps: 952, steps per second:  11, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.013831, mae: 2.571183, mean_q: 3.097883, mean_eps: 0.010000
 607988/1000000: episode: 816, duration: 49.626s, episode steps: 560, steps per second:  11, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.841 [0.000, 5.000],  loss: 0.013872, mae: 2.570359, mean_q: 3.095089, mean_eps: 0.010000
 608723/1000000: episode: 817, duration: 64.800s, episode steps: 735, steps per second:  11, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.012833, mae: 2.560277, mean_q: 3.085511, mean_eps: 0.010000
 610106/1000000: episode: 818, duration: 122.229s, episode steps: 1383, steps per second:  11, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.014301, mae: 2.562779, mean_q: 3.087877, mean_eps: 0.010000
 611213/1000000: episode: 819, duration: 96.622s, episode steps: 1107, steps per second:  11, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.013206, mae: 2.572069, mean_q: 3.099083, mean_eps: 0.010000
 612290/1000000: episode: 820, duration: 95.208s, episode steps: 1077, steps per second:  11, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.386 [0.000, 5.000],  loss: 0.014906, mae: 2.572409, mean_q: 3.098880, mean_eps: 0.010000
 613254/1000000: episode: 821, duration: 88.757s, episode steps: 964, steps per second:  11, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.014237, mae: 2.576737, mean_q: 3.104808, mean_eps: 0.010000
 614326/1000000: episode: 822, duration: 105.398s, episode steps: 1072, steps per second:  10, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.013758, mae: 2.580236, mean_q: 3.107638, mean_eps: 0.010000
 614864/1000000: episode: 823, duration: 53.520s, episode steps: 538, steps per second:  10, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.014431, mae: 2.571355, mean_q: 3.097227, mean_eps: 0.010000
 615984/1000000: episode: 824, duration: 112.054s, episode steps: 1120, steps per second:  10, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.014380, mae: 2.575204, mean_q: 3.102095, mean_eps: 0.010000
 616875/1000000: episode: 825, duration: 90.424s, episode steps: 891, steps per second:  10, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.493 [0.000, 5.000],  loss: 0.014705, mae: 2.561316, mean_q: 3.085422, mean_eps: 0.010000
 617585/1000000: episode: 826, duration: 69.438s, episode steps: 710, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.014725, mae: 2.556320, mean_q: 3.078818, mean_eps: 0.010000
 618249/1000000: episode: 827, duration: 65.670s, episode steps: 664, steps per second:  10, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.014289, mae: 2.578483, mean_q: 3.105668, mean_eps: 0.010000
 619066/1000000: episode: 828, duration: 71.541s, episode steps: 817, steps per second:  11, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.015189, mae: 2.562838, mean_q: 3.085562, mean_eps: 0.010000
 620425/1000000: episode: 829, duration: 120.788s, episode steps: 1359, steps per second:  11, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.723 [0.000, 5.000],  loss: 0.013665, mae: 2.561310, mean_q: 3.084750, mean_eps: 0.010000
 621086/1000000: episode: 830, duration: 58.950s, episode steps: 661, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.935 [0.000, 5.000],  loss: 0.016217, mae: 2.582236, mean_q: 3.108616, mean_eps: 0.010000
 622004/1000000: episode: 831, duration: 81.374s, episode steps: 918, steps per second:  11, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.672 [0.000, 5.000],  loss: 0.013770, mae: 2.575871, mean_q: 3.104111, mean_eps: 0.010000
 622676/1000000: episode: 832, duration: 61.085s, episode steps: 672, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.015 [0.000, 5.000],  loss: 0.013147, mae: 2.557278, mean_q: 3.080693, mean_eps: 0.010000
 623145/1000000: episode: 833, duration: 41.687s, episode steps: 469, steps per second:  11, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.175 [0.000, 5.000],  loss: 0.013177, mae: 2.573887, mean_q: 3.101531, mean_eps: 0.010000
 623792/1000000: episode: 834, duration: 57.397s, episode steps: 647, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.978 [0.000, 5.000],  loss: 0.014048, mae: 2.559543, mean_q: 3.080934, mean_eps: 0.010000
 624550/1000000: episode: 835, duration: 83.380s, episode steps: 758, steps per second:   9, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.014072, mae: 2.563430, mean_q: 3.088857, mean_eps: 0.010000
 625647/1000000: episode: 836, duration: 121.683s, episode steps: 1097, steps per second:   9, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.013059, mae: 2.569312, mean_q: 3.093913, mean_eps: 0.010000
 626169/1000000: episode: 837, duration: 55.588s, episode steps: 522, steps per second:   9, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.789 [0.000, 5.000],  loss: 0.014529, mae: 2.552671, mean_q: 3.074727, mean_eps: 0.010000
 626909/1000000: episode: 838, duration: 81.121s, episode steps: 740, steps per second:   9, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.013666, mae: 2.565201, mean_q: 3.088553, mean_eps: 0.010000
 627776/1000000: episode: 839, duration: 96.287s, episode steps: 867, steps per second:   9, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.957 [0.000, 5.000],  loss: 0.014529, mae: 2.567005, mean_q: 3.090952, mean_eps: 0.010000
 628409/1000000: episode: 840, duration: 74.319s, episode steps: 633, steps per second:   9, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.016261, mae: 2.585639, mean_q: 3.113860, mean_eps: 0.010000
 629227/1000000: episode: 841, duration: 78.641s, episode steps: 818, steps per second:  10, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.014459, mae: 2.582966, mean_q: 3.110633, mean_eps: 0.010000
 630014/1000000: episode: 842, duration: 72.115s, episode steps: 787, steps per second:  11, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.016533, mae: 2.574749, mean_q: 3.101231, mean_eps: 0.010000
 631105/1000000: episode: 843, duration: 99.653s, episode steps: 1091, steps per second:  11, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.014922, mae: 2.572069, mean_q: 3.098048, mean_eps: 0.010000
 631814/1000000: episode: 844, duration: 63.385s, episode steps: 709, steps per second:  11, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.013060, mae: 2.544223, mean_q: 3.065265, mean_eps: 0.010000
 632826/1000000: episode: 845, duration: 91.199s, episode steps: 1012, steps per second:  11, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.014047, mae: 2.588972, mean_q: 3.118680, mean_eps: 0.010000
 633701/1000000: episode: 846, duration: 79.957s, episode steps: 875, steps per second:  11, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.016936, mae: 2.563411, mean_q: 3.087404, mean_eps: 0.010000
 634423/1000000: episode: 847, duration: 64.567s, episode steps: 722, steps per second:  11, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.015570, mae: 2.562314, mean_q: 3.089222, mean_eps: 0.010000
 635496/1000000: episode: 848, duration: 99.478s, episode steps: 1073, steps per second:  11, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.014362, mae: 2.578394, mean_q: 3.108528, mean_eps: 0.010000
 636248/1000000: episode: 849, duration: 68.812s, episode steps: 752, steps per second:  11, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.013599, mae: 2.573199, mean_q: 3.100838, mean_eps: 0.010000
 636784/1000000: episode: 850, duration: 50.541s, episode steps: 536, steps per second:  11, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012615, mae: 2.567625, mean_q: 3.094079, mean_eps: 0.010000
 637744/1000000: episode: 851, duration: 89.940s, episode steps: 960, steps per second:  11, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.586 [0.000, 5.000],  loss: 0.013906, mae: 2.557531, mean_q: 3.082544, mean_eps: 0.010000
 638424/1000000: episode: 852, duration: 68.666s, episode steps: 680, steps per second:  10, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.015615, mae: 2.568510, mean_q: 3.094387, mean_eps: 0.010000
 639232/1000000: episode: 853, duration: 80.319s, episode steps: 808, steps per second:  10, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.015349, mae: 2.574034, mean_q: 3.101920, mean_eps: 0.010000
 640194/1000000: episode: 854, duration: 95.410s, episode steps: 962, steps per second:  10, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.014253, mae: 2.545949, mean_q: 3.066731, mean_eps: 0.010000
 641066/1000000: episode: 855, duration: 86.728s, episode steps: 872, steps per second:  10, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.013855, mae: 2.556852, mean_q: 3.079458, mean_eps: 0.010000
 641839/1000000: episode: 856, duration: 79.197s, episode steps: 773, steps per second:  10, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.013788, mae: 2.598270, mean_q: 3.129287, mean_eps: 0.010000
 642551/1000000: episode: 857, duration: 72.491s, episode steps: 712, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.016733, mae: 2.580562, mean_q: 3.108226, mean_eps: 0.010000
 643258/1000000: episode: 858, duration: 73.371s, episode steps: 707, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.255 [0.000, 5.000],  loss: 0.013503, mae: 2.561125, mean_q: 3.087082, mean_eps: 0.010000
 644105/1000000: episode: 859, duration: 98.183s, episode steps: 847, steps per second:   9, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.015778, mae: 2.574985, mean_q: 3.101459, mean_eps: 0.010000
 644739/1000000: episode: 860, duration: 69.310s, episode steps: 634, steps per second:   9, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.016350, mae: 2.554717, mean_q: 3.076967, mean_eps: 0.010000
 645254/1000000: episode: 861, duration: 55.662s, episode steps: 515, steps per second:   9, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.433 [0.000, 5.000],  loss: 0.015768, mae: 2.577237, mean_q: 3.106267, mean_eps: 0.010000
 645800/1000000: episode: 862, duration: 61.614s, episode steps: 546, steps per second:   9, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.930 [0.000, 5.000],  loss: 0.014266, mae: 2.550636, mean_q: 3.075136, mean_eps: 0.010000
 646317/1000000: episode: 863, duration: 58.295s, episode steps: 517, steps per second:   9, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.013487, mae: 2.558044, mean_q: 3.082168, mean_eps: 0.010000
 646842/1000000: episode: 864, duration: 57.051s, episode steps: 525, steps per second:   9, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.014376, mae: 2.560991, mean_q: 3.087551, mean_eps: 0.010000
 647664/1000000: episode: 865, duration: 75.552s, episode steps: 822, steps per second:  11, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.015234, mae: 2.566006, mean_q: 3.090528, mean_eps: 0.010000
 648528/1000000: episode: 866, duration: 79.616s, episode steps: 864, steps per second:  11, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.206 [0.000, 5.000],  loss: 0.012749, mae: 2.555924, mean_q: 3.078852, mean_eps: 0.010000
 649673/1000000: episode: 867, duration: 104.168s, episode steps: 1145, steps per second:  11, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.014506, mae: 2.574211, mean_q: 3.101553, mean_eps: 0.010000
 650339/1000000: episode: 868, duration: 60.360s, episode steps: 666, steps per second:  11, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.011611, mae: 2.565339, mean_q: 3.091422, mean_eps: 0.010000
 651002/1000000: episode: 869, duration: 59.473s, episode steps: 663, steps per second:  11, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.013350, mae: 2.575022, mean_q: 3.102626, mean_eps: 0.010000
 652028/1000000: episode: 870, duration: 93.227s, episode steps: 1026, steps per second:  11, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.013924, mae: 2.565738, mean_q: 3.091276, mean_eps: 0.010000
 653010/1000000: episode: 871, duration: 88.716s, episode steps: 982, steps per second:  11, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.014713, mae: 2.558436, mean_q: 3.080707, mean_eps: 0.010000
 653414/1000000: episode: 872, duration: 36.136s, episode steps: 404, steps per second:  11, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.018096, mae: 2.564822, mean_q: 3.090759, mean_eps: 0.010000
 654240/1000000: episode: 873, duration: 76.733s, episode steps: 826, steps per second:  11, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.958 [0.000, 5.000],  loss: 0.014088, mae: 2.578479, mean_q: 3.105635, mean_eps: 0.010000
 655068/1000000: episode: 874, duration: 74.047s, episode steps: 828, steps per second:  11, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.001 [0.000, 5.000],  loss: 0.014831, mae: 2.570536, mean_q: 3.097534, mean_eps: 0.010000
 655753/1000000: episode: 875, duration: 67.019s, episode steps: 685, steps per second:  10, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.728 [0.000, 5.000],  loss: 0.015142, mae: 2.556567, mean_q: 3.079132, mean_eps: 0.010000
 656488/1000000: episode: 876, duration: 73.278s, episode steps: 735, steps per second:  10, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.012677, mae: 2.566419, mean_q: 3.092021, mean_eps: 0.010000
 656941/1000000: episode: 877, duration: 47.554s, episode steps: 453, steps per second:  10, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.014583, mae: 2.569225, mean_q: 3.093585, mean_eps: 0.010000
 657573/1000000: episode: 878, duration: 62.688s, episode steps: 632, steps per second:  10, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.016535, mae: 2.555952, mean_q: 3.078897, mean_eps: 0.010000
 658290/1000000: episode: 879, duration: 72.081s, episode steps: 717, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013955, mae: 2.575615, mean_q: 3.100829, mean_eps: 0.010000
 658956/1000000: episode: 880, duration: 68.933s, episode steps: 666, steps per second:  10, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.934 [0.000, 5.000],  loss: 0.015331, mae: 2.570232, mean_q: 3.095629, mean_eps: 0.010000
 659724/1000000: episode: 881, duration: 72.935s, episode steps: 768, steps per second:  11, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.015259, mae: 2.578091, mean_q: 3.104535, mean_eps: 0.010000
 660229/1000000: episode: 882, duration: 46.496s, episode steps: 505, steps per second:  11, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.016078, mae: 2.574816, mean_q: 3.099474, mean_eps: 0.010000
 661576/1000000: episode: 883, duration: 138.598s, episode steps: 1347, steps per second:  10, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.014454, mae: 2.562704, mean_q: 3.086745, mean_eps: 0.010000
 662221/1000000: episode: 884, duration: 64.217s, episode steps: 645, steps per second:  10, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.267 [0.000, 5.000],  loss: 0.014447, mae: 2.561591, mean_q: 3.086840, mean_eps: 0.010000
 662922/1000000: episode: 885, duration: 70.067s, episode steps: 701, steps per second:  10, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.014175, mae: 2.555499, mean_q: 3.077723, mean_eps: 0.010000
 663602/1000000: episode: 886, duration: 70.207s, episode steps: 680, steps per second:  10, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.013420, mae: 2.553711, mean_q: 3.076630, mean_eps: 0.010000
 664444/1000000: episode: 887, duration: 88.248s, episode steps: 842, steps per second:  10, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.014151, mae: 2.551313, mean_q: 3.072814, mean_eps: 0.010000
 665280/1000000: episode: 888, duration: 84.918s, episode steps: 836, steps per second:  10, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.989 [0.000, 5.000],  loss: 0.013747, mae: 2.566876, mean_q: 3.091894, mean_eps: 0.010000
 665885/1000000: episode: 889, duration: 63.281s, episode steps: 605, steps per second:  10, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.821 [0.000, 5.000],  loss: 0.016138, mae: 2.563793, mean_q: 3.087420, mean_eps: 0.010000
 666779/1000000: episode: 890, duration: 91.159s, episode steps: 894, steps per second:  10, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.012922, mae: 2.562129, mean_q: 3.087223, mean_eps: 0.010000
 667586/1000000: episode: 891, duration: 81.089s, episode steps: 807, steps per second:  10, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.985 [0.000, 5.000],  loss: 0.014030, mae: 2.560007, mean_q: 3.084252, mean_eps: 0.010000
 668384/1000000: episode: 892, duration: 83.997s, episode steps: 798, steps per second:  10, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.013693, mae: 2.572981, mean_q: 3.099188, mean_eps: 0.010000
 669084/1000000: episode: 893, duration: 72.165s, episode steps: 700, steps per second:  10, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.014797, mae: 2.550564, mean_q: 3.071760, mean_eps: 0.010000
 669678/1000000: episode: 894, duration: 62.134s, episode steps: 594, steps per second:  10, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.017598, mae: 2.566552, mean_q: 3.090792, mean_eps: 0.010000
 670237/1000000: episode: 895, duration: 58.166s, episode steps: 559, steps per second:  10, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.012830, mae: 2.561556, mean_q: 3.085498, mean_eps: 0.010000
 670982/1000000: episode: 896, duration: 76.487s, episode steps: 745, steps per second:  10, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.015069, mae: 2.555286, mean_q: 3.077025, mean_eps: 0.010000
 672066/1000000: episode: 897, duration: 109.422s, episode steps: 1084, steps per second:  10, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.708 [0.000, 5.000],  loss: 0.013171, mae: 2.566194, mean_q: 3.091379, mean_eps: 0.010000
 673165/1000000: episode: 898, duration: 101.761s, episode steps: 1099, steps per second:  11, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.015773, mae: 2.586096, mean_q: 3.115289, mean_eps: 0.010000
 673675/1000000: episode: 899, duration: 46.907s, episode steps: 510, steps per second:  11, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.016111, mae: 2.588128, mean_q: 3.119878, mean_eps: 0.010000
 674347/1000000: episode: 900, duration: 60.509s, episode steps: 672, steps per second:  11, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.015775, mae: 2.568778, mean_q: 3.093466, mean_eps: 0.010000
 675615/1000000: episode: 901, duration: 124.494s, episode steps: 1268, steps per second:  10, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.125 [0.000, 5.000],  loss: 0.013530, mae: 2.550641, mean_q: 3.071276, mean_eps: 0.010000
 676583/1000000: episode: 902, duration: 99.758s, episode steps: 968, steps per second:  10, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.014048, mae: 2.577314, mean_q: 3.106129, mean_eps: 0.010000
 677849/1000000: episode: 903, duration: 130.618s, episode steps: 1266, steps per second:  10, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.015687, mae: 2.561754, mean_q: 3.084695, mean_eps: 0.010000
 678734/1000000: episode: 904, duration: 91.323s, episode steps: 885, steps per second:  10, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.013746, mae: 2.574011, mean_q: 3.100974, mean_eps: 0.010000
 679572/1000000: episode: 905, duration: 85.413s, episode steps: 838, steps per second:  10, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.015889, mae: 2.564846, mean_q: 3.087875, mean_eps: 0.010000
 680371/1000000: episode: 906, duration: 72.182s, episode steps: 799, steps per second:  11, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.014174, mae: 2.560128, mean_q: 3.083872, mean_eps: 0.010000
 681163/1000000: episode: 907, duration: 72.697s, episode steps: 792, steps per second:  11, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.491 [0.000, 5.000],  loss: 0.013052, mae: 2.547781, mean_q: 3.067867, mean_eps: 0.010000
 682077/1000000: episode: 908, duration: 84.654s, episode steps: 914, steps per second:  11, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012709, mae: 2.550333, mean_q: 3.070692, mean_eps: 0.010000
 682740/1000000: episode: 909, duration: 59.822s, episode steps: 663, steps per second:  11, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.016148, mae: 2.559594, mean_q: 3.079416, mean_eps: 0.010000
 683770/1000000: episode: 910, duration: 94.728s, episode steps: 1030, steps per second:  11, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.598 [0.000, 5.000],  loss: 0.016585, mae: 2.557235, mean_q: 3.078574, mean_eps: 0.010000
 684493/1000000: episode: 911, duration: 66.167s, episode steps: 723, steps per second:  11, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012780, mae: 2.561517, mean_q: 3.083647, mean_eps: 0.010000
 684987/1000000: episode: 912, duration: 45.728s, episode steps: 494, steps per second:  11, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.011882, mae: 2.556367, mean_q: 3.077438, mean_eps: 0.010000
 685963/1000000: episode: 913, duration: 88.212s, episode steps: 976, steps per second:  11, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.014145, mae: 2.560061, mean_q: 3.082538, mean_eps: 0.010000
 686642/1000000: episode: 914, duration: 62.237s, episode steps: 679, steps per second:  11, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.015164, mae: 2.558505, mean_q: 3.080216, mean_eps: 0.010000
 687233/1000000: episode: 915, duration: 52.981s, episode steps: 591, steps per second:  11, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.013247, mae: 2.560944, mean_q: 3.083207, mean_eps: 0.010000
 687748/1000000: episode: 916, duration: 49.407s, episode steps: 515, steps per second:  10, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.014487, mae: 2.552119, mean_q: 3.073789, mean_eps: 0.010000
 688728/1000000: episode: 917, duration: 91.988s, episode steps: 980, steps per second:  11, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.468 [0.000, 5.000],  loss: 0.015606, mae: 2.558741, mean_q: 3.082357, mean_eps: 0.010000
 689238/1000000: episode: 918, duration: 53.198s, episode steps: 510, steps per second:  10, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.992 [0.000, 5.000],  loss: 0.014334, mae: 2.574759, mean_q: 3.101591, mean_eps: 0.010000
 690108/1000000: episode: 919, duration: 90.802s, episode steps: 870, steps per second:  10, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.015081, mae: 2.564442, mean_q: 3.089506, mean_eps: 0.010000
 690491/1000000: episode: 920, duration: 39.456s, episode steps: 383, steps per second:  10, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.010 [0.000, 5.000],  loss: 0.017942, mae: 2.556768, mean_q: 3.079192, mean_eps: 0.010000
 691211/1000000: episode: 921, duration: 75.112s, episode steps: 720, steps per second:  10, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.015825, mae: 2.560614, mean_q: 3.082940, mean_eps: 0.010000
 691878/1000000: episode: 922, duration: 66.016s, episode steps: 667, steps per second:  10, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.015499, mae: 2.566600, mean_q: 3.091339, mean_eps: 0.010000
 692435/1000000: episode: 923, duration: 57.821s, episode steps: 557, steps per second:  10, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.013716, mae: 2.559294, mean_q: 3.082094, mean_eps: 0.010000
 693099/1000000: episode: 924, duration: 69.675s, episode steps: 664, steps per second:  10, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013513, mae: 2.572248, mean_q: 3.097730, mean_eps: 0.010000
 693845/1000000: episode: 925, duration: 77.780s, episode steps: 746, steps per second:  10, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.016125, mae: 2.568062, mean_q: 3.093281, mean_eps: 0.010000
 694510/1000000: episode: 926, duration: 67.423s, episode steps: 665, steps per second:  10, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.159 [0.000, 5.000],  loss: 0.014117, mae: 2.562240, mean_q: 3.085018, mean_eps: 0.010000
 695218/1000000: episode: 927, duration: 74.291s, episode steps: 708, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.015830, mae: 2.582627, mean_q: 3.111026, mean_eps: 0.010000
 696177/1000000: episode: 928, duration: 98.350s, episode steps: 959, steps per second:  10, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.634 [0.000, 5.000],  loss: 0.012664, mae: 2.556419, mean_q: 3.080671, mean_eps: 0.010000
 697019/1000000: episode: 929, duration: 87.563s, episode steps: 842, steps per second:  10, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.451 [0.000, 5.000],  loss: 0.012782, mae: 2.570757, mean_q: 3.096180, mean_eps: 0.010000
 698099/1000000: episode: 930, duration: 105.248s, episode steps: 1080, steps per second:  10, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.014986, mae: 2.558215, mean_q: 3.081708, mean_eps: 0.010000
 698620/1000000: episode: 931, duration: 48.835s, episode steps: 521, steps per second:  11, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.883 [0.000, 5.000],  loss: 0.015356, mae: 2.541488, mean_q: 3.061975, mean_eps: 0.010000
 699275/1000000: episode: 932, duration: 61.409s, episode steps: 655, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.016739, mae: 2.571156, mean_q: 3.095649, mean_eps: 0.010000
 699682/1000000: episode: 933, duration: 37.389s, episode steps: 407, steps per second:  11, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.821 [0.000, 5.000],  loss: 0.019192, mae: 2.579241, mean_q: 3.106645, mean_eps: 0.010000
 700392/1000000: episode: 934, duration: 66.405s, episode steps: 710, steps per second:  11, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.449 [0.000, 5.000],  loss: 0.014949, mae: 2.577958, mean_q: 3.105040, mean_eps: 0.010000
 701460/1000000: episode: 935, duration: 102.294s, episode steps: 1068, steps per second:  10, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.013282, mae: 2.550680, mean_q: 3.072524, mean_eps: 0.010000
 702198/1000000: episode: 936, duration: 67.848s, episode steps: 738, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.939 [0.000, 5.000],  loss: 0.012963, mae: 2.549140, mean_q: 3.071644, mean_eps: 0.010000
 702909/1000000: episode: 937, duration: 66.663s, episode steps: 711, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.014250, mae: 2.536218, mean_q: 3.054447, mean_eps: 0.010000
 703326/1000000: episode: 938, duration: 39.449s, episode steps: 417, steps per second:  11, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.015694, mae: 2.544207, mean_q: 3.063787, mean_eps: 0.010000
 704076/1000000: episode: 939, duration: 89.579s, episode steps: 750, steps per second:   8, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.013361, mae: 2.523154, mean_q: 3.038972, mean_eps: 0.010000
 705133/1000000: episode: 940, duration: 119.693s, episode steps: 1057, steps per second:   9, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.014343, mae: 2.547354, mean_q: 3.068602, mean_eps: 0.010000
 706070/1000000: episode: 941, duration: 105.463s, episode steps: 937, steps per second:   9, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.016546, mae: 2.550916, mean_q: 3.071797, mean_eps: 0.010000
 706775/1000000: episode: 942, duration: 77.770s, episode steps: 705, steps per second:   9, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.013546, mae: 2.538198, mean_q: 3.056345, mean_eps: 0.010000
 707292/1000000: episode: 943, duration: 55.499s, episode steps: 517, steps per second:   9, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.814 [0.000, 5.000],  loss: 0.012744, mae: 2.555583, mean_q: 3.078029, mean_eps: 0.010000
 708518/1000000: episode: 944, duration: 119.474s, episode steps: 1226, steps per second:  10, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.015007, mae: 2.545207, mean_q: 3.064352, mean_eps: 0.010000
 709229/1000000: episode: 945, duration: 66.348s, episode steps: 711, steps per second:  11, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.014863, mae: 2.544268, mean_q: 3.064875, mean_eps: 0.010000
 709904/1000000: episode: 946, duration: 62.755s, episode steps: 675, steps per second:  11, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.014159, mae: 2.555179, mean_q: 3.077291, mean_eps: 0.010000
 710561/1000000: episode: 947, duration: 61.744s, episode steps: 657, steps per second:  11, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.014992, mae: 2.542825, mean_q: 3.061285, mean_eps: 0.010000
 711426/1000000: episode: 948, duration: 81.493s, episode steps: 865, steps per second:  11, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.014264, mae: 2.562485, mean_q: 3.087103, mean_eps: 0.010000
 712365/1000000: episode: 949, duration: 88.469s, episode steps: 939, steps per second:  11, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.015543, mae: 2.533647, mean_q: 3.049925, mean_eps: 0.010000
 713206/1000000: episode: 950, duration: 78.259s, episode steps: 841, steps per second:  11, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.013731, mae: 2.550562, mean_q: 3.072060, mean_eps: 0.010000
 713861/1000000: episode: 951, duration: 61.136s, episode steps: 655, steps per second:  11, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.013387, mae: 2.547571, mean_q: 3.068971, mean_eps: 0.010000
 714748/1000000: episode: 952, duration: 83.360s, episode steps: 887, steps per second:  11, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.909 [0.000, 5.000],  loss: 0.017027, mae: 2.543298, mean_q: 3.061756, mean_eps: 0.010000
 715726/1000000: episode: 953, duration: 96.534s, episode steps: 978, steps per second:  10, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.647 [0.000, 5.000],  loss: 0.014384, mae: 2.539805, mean_q: 3.057948, mean_eps: 0.010000
 716618/1000000: episode: 954, duration: 88.037s, episode steps: 892, steps per second:  10, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.013850, mae: 2.545072, mean_q: 3.064656, mean_eps: 0.010000
 717536/1000000: episode: 955, duration: 85.750s, episode steps: 918, steps per second:  11, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.013731, mae: 2.551954, mean_q: 3.073854, mean_eps: 0.010000
 718563/1000000: episode: 956, duration: 106.777s, episode steps: 1027, steps per second:  10, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.015689, mae: 2.544800, mean_q: 3.063363, mean_eps: 0.010000
 719432/1000000: episode: 957, duration: 87.882s, episode steps: 869, steps per second:  10, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.819 [0.000, 5.000],  loss: 0.015407, mae: 2.532645, mean_q: 3.050872, mean_eps: 0.010000
 720580/1000000: episode: 958, duration: 120.978s, episode steps: 1148, steps per second:   9, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.013315, mae: 2.528044, mean_q: 3.045831, mean_eps: 0.010000
 721640/1000000: episode: 959, duration: 125.706s, episode steps: 1060, steps per second:   8, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.016038, mae: 2.553750, mean_q: 3.075059, mean_eps: 0.010000
 722563/1000000: episode: 960, duration: 109.221s, episode steps: 923, steps per second:   8, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.014444, mae: 2.545783, mean_q: 3.065844, mean_eps: 0.010000
 722942/1000000: episode: 961, duration: 38.496s, episode steps: 379, steps per second:  10, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.012946, mae: 2.571004, mean_q: 3.095201, mean_eps: 0.010000
 723859/1000000: episode: 962, duration: 90.243s, episode steps: 917, steps per second:  10, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 0.015059, mae: 2.547698, mean_q: 3.067445, mean_eps: 0.010000
 724757/1000000: episode: 963, duration: 84.406s, episode steps: 898, steps per second:  11, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.013696, mae: 2.546582, mean_q: 3.067602, mean_eps: 0.010000
 725291/1000000: episode: 964, duration: 49.656s, episode steps: 534, steps per second:  11, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.014066, mae: 2.543596, mean_q: 3.064587, mean_eps: 0.010000
 726125/1000000: episode: 965, duration: 79.954s, episode steps: 834, steps per second:  10, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.014720, mae: 2.542603, mean_q: 3.061579, mean_eps: 0.010000
 727133/1000000: episode: 966, duration: 93.587s, episode steps: 1008, steps per second:  11, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.014776, mae: 2.556173, mean_q: 3.077054, mean_eps: 0.010000
 727955/1000000: episode: 967, duration: 77.223s, episode steps: 822, steps per second:  11, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.013679, mae: 2.543146, mean_q: 3.063335, mean_eps: 0.010000
 729458/1000000: episode: 968, duration: 140.358s, episode steps: 1503, steps per second:  11, episode reward: 32.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.014687, mae: 2.551184, mean_q: 3.073841, mean_eps: 0.010000
 730242/1000000: episode: 969, duration: 73.313s, episode steps: 784, steps per second:  11, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.013470, mae: 2.536232, mean_q: 3.055780, mean_eps: 0.010000
 731143/1000000: episode: 970, duration: 82.792s, episode steps: 901, steps per second:  11, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.015072, mae: 2.552430, mean_q: 3.074962, mean_eps: 0.010000
 732038/1000000: episode: 971, duration: 83.975s, episode steps: 895, steps per second:  11, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.014670, mae: 2.554111, mean_q: 3.076924, mean_eps: 0.010000
 732753/1000000: episode: 972, duration: 67.440s, episode steps: 715, steps per second:  11, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.012889, mae: 2.541657, mean_q: 3.061170, mean_eps: 0.010000
 733414/1000000: episode: 973, duration: 61.984s, episode steps: 661, steps per second:  11, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.947 [0.000, 5.000],  loss: 0.015561, mae: 2.533885, mean_q: 3.050629, mean_eps: 0.010000
 734132/1000000: episode: 974, duration: 68.913s, episode steps: 718, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.843 [0.000, 5.000],  loss: 0.016910, mae: 2.543032, mean_q: 3.062310, mean_eps: 0.010000
 735290/1000000: episode: 975, duration: 114.996s, episode steps: 1158, steps per second:  10, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.014587, mae: 2.553200, mean_q: 3.075372, mean_eps: 0.010000
 735818/1000000: episode: 976, duration: 51.699s, episode steps: 528, steps per second:  10, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.909 [0.000, 5.000],  loss: 0.013840, mae: 2.524572, mean_q: 3.040438, mean_eps: 0.010000
 736730/1000000: episode: 977, duration: 103.830s, episode steps: 912, steps per second:   9, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.013154, mae: 2.564154, mean_q: 3.088335, mean_eps: 0.010000
 737416/1000000: episode: 978, duration: 79.353s, episode steps: 686, steps per second:   9, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.014161, mae: 2.541602, mean_q: 3.060417, mean_eps: 0.010000
 738294/1000000: episode: 979, duration: 99.579s, episode steps: 878, steps per second:   9, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.713 [0.000, 5.000],  loss: 0.013931, mae: 2.531470, mean_q: 3.047696, mean_eps: 0.010000
 738957/1000000: episode: 980, duration: 80.965s, episode steps: 663, steps per second:   8, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.014491, mae: 2.551833, mean_q: 3.071961, mean_eps: 0.010000
 739769/1000000: episode: 981, duration: 91.438s, episode steps: 812, steps per second:   9, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.933 [0.000, 5.000],  loss: 0.013358, mae: 2.556276, mean_q: 3.079924, mean_eps: 0.010000
 740440/1000000: episode: 982, duration: 69.897s, episode steps: 671, steps per second:  10, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.013066, mae: 2.539394, mean_q: 3.058867, mean_eps: 0.010000
 741385/1000000: episode: 983, duration: 93.395s, episode steps: 945, steps per second:  10, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.014656, mae: 2.545642, mean_q: 3.067027, mean_eps: 0.010000
 742258/1000000: episode: 984, duration: 82.636s, episode steps: 873, steps per second:  11, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.014150, mae: 2.533230, mean_q: 3.050991, mean_eps: 0.010000
 743004/1000000: episode: 985, duration: 71.430s, episode steps: 746, steps per second:  10, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.015415, mae: 2.539116, mean_q: 3.057300, mean_eps: 0.010000
 743769/1000000: episode: 986, duration: 71.675s, episode steps: 765, steps per second:  11, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.014643, mae: 2.543693, mean_q: 3.065256, mean_eps: 0.010000
 744980/1000000: episode: 987, duration: 115.583s, episode steps: 1211, steps per second:  10, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.014016, mae: 2.547649, mean_q: 3.069086, mean_eps: 0.010000
 745876/1000000: episode: 988, duration: 84.490s, episode steps: 896, steps per second:  11, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.892 [0.000, 5.000],  loss: 0.014739, mae: 2.543166, mean_q: 3.062938, mean_eps: 0.010000
 746361/1000000: episode: 989, duration: 46.345s, episode steps: 485, steps per second:  10, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.419 [0.000, 5.000],  loss: 0.014883, mae: 2.541281, mean_q: 3.061228, mean_eps: 0.010000
 747145/1000000: episode: 990, duration: 72.070s, episode steps: 784, steps per second:  11, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.014665, mae: 2.569825, mean_q: 3.096527, mean_eps: 0.010000
 748329/1000000: episode: 991, duration: 113.410s, episode steps: 1184, steps per second:  10, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.615 [0.000, 5.000],  loss: 0.013208, mae: 2.542659, mean_q: 3.064252, mean_eps: 0.010000
 749413/1000000: episode: 992, duration: 105.463s, episode steps: 1084, steps per second:  10, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.016035, mae: 2.549035, mean_q: 3.069714, mean_eps: 0.010000
 749954/1000000: episode: 993, duration: 51.412s, episode steps: 541, steps per second:  11, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.014924, mae: 2.550833, mean_q: 3.071907, mean_eps: 0.010000
 750799/1000000: episode: 994, duration: 87.910s, episode steps: 845, steps per second:  10, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.013690, mae: 2.536601, mean_q: 3.057090, mean_eps: 0.010000
 751489/1000000: episode: 995, duration: 71.647s, episode steps: 690, steps per second:  10, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.013794, mae: 2.555522, mean_q: 3.078631, mean_eps: 0.010000
 752238/1000000: episode: 996, duration: 75.713s, episode steps: 749, steps per second:  10, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.013399, mae: 2.540425, mean_q: 3.061305, mean_eps: 0.010000
 753065/1000000: episode: 997, duration: 84.394s, episode steps: 827, steps per second:  10, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.015232, mae: 2.552778, mean_q: 3.074596, mean_eps: 0.010000
 753726/1000000: episode: 998, duration: 68.988s, episode steps: 661, steps per second:  10, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.013079, mae: 2.539408, mean_q: 3.059133, mean_eps: 0.010000
 754457/1000000: episode: 999, duration: 76.175s, episode steps: 731, steps per second:  10, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.013887, mae: 2.530835, mean_q: 3.047545, mean_eps: 0.010000
 754846/1000000: episode: 1000, duration: 41.010s, episode steps: 389, steps per second:   9, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.016947, mae: 2.579809, mean_q: 3.105834, mean_eps: 0.010000
 755506/1000000: episode: 1001, duration: 70.042s, episode steps: 660, steps per second:   9, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.016522, mae: 2.552314, mean_q: 3.073566, mean_eps: 0.010000
 756033/1000000: episode: 1002, duration: 56.897s, episode steps: 527, steps per second:   9, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.016030, mae: 2.541703, mean_q: 3.059975, mean_eps: 0.010000
 756741/1000000: episode: 1003, duration: 73.946s, episode steps: 708, steps per second:  10, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.015953, mae: 2.539859, mean_q: 3.059179, mean_eps: 0.010000
 757443/1000000: episode: 1004, duration: 75.297s, episode steps: 702, steps per second:   9, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.015958, mae: 2.552319, mean_q: 3.076300, mean_eps: 0.010000
 758163/1000000: episode: 1005, duration: 75.971s, episode steps: 720, steps per second:   9, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.014810, mae: 2.578887, mean_q: 3.106992, mean_eps: 0.010000
 759051/1000000: episode: 1006, duration: 87.844s, episode steps: 888, steps per second:  10, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.012836, mae: 2.566360, mean_q: 3.093805, mean_eps: 0.010000
 759694/1000000: episode: 1007, duration: 60.852s, episode steps: 643, steps per second:  11, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.015364, mae: 2.543424, mean_q: 3.064500, mean_eps: 0.010000
 760478/1000000: episode: 1008, duration: 74.868s, episode steps: 784, steps per second:  10, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.934 [0.000, 5.000],  loss: 0.015898, mae: 2.549602, mean_q: 3.073480, mean_eps: 0.010000
 760969/1000000: episode: 1009, duration: 46.059s, episode steps: 491, steps per second:  11, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.015520, mae: 2.520252, mean_q: 3.036741, mean_eps: 0.010000
 761455/1000000: episode: 1010, duration: 44.808s, episode steps: 486, steps per second:  11, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.014330, mae: 2.546632, mean_q: 3.068583, mean_eps: 0.010000
 762158/1000000: episode: 1011, duration: 66.319s, episode steps: 703, steps per second:  11, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.015006, mae: 2.529267, mean_q: 3.046633, mean_eps: 0.010000
 762863/1000000: episode: 1012, duration: 67.745s, episode steps: 705, steps per second:  10, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.018589, mae: 2.547064, mean_q: 3.067711, mean_eps: 0.010000
 763712/1000000: episode: 1013, duration: 80.491s, episode steps: 849, steps per second:  11, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.014805, mae: 2.544599, mean_q: 3.064795, mean_eps: 0.010000
 764406/1000000: episode: 1014, duration: 64.715s, episode steps: 694, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.801 [0.000, 5.000],  loss: 0.015456, mae: 2.540871, mean_q: 3.060805, mean_eps: 0.010000
 765157/1000000: episode: 1015, duration: 70.677s, episode steps: 751, steps per second:  11, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.014629, mae: 2.551203, mean_q: 3.072214, mean_eps: 0.010000
 765807/1000000: episode: 1016, duration: 61.358s, episode steps: 650, steps per second:  11, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014227, mae: 2.560772, mean_q: 3.082885, mean_eps: 0.010000
 766763/1000000: episode: 1017, duration: 95.600s, episode steps: 956, steps per second:  10, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.015446, mae: 2.536496, mean_q: 3.054982, mean_eps: 0.010000
 767135/1000000: episode: 1018, duration: 37.997s, episode steps: 372, steps per second:  10, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.015531, mae: 2.579191, mean_q: 3.106650, mean_eps: 0.010000
 767885/1000000: episode: 1019, duration: 80.074s, episode steps: 750, steps per second:   9, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.013780, mae: 2.538883, mean_q: 3.056411, mean_eps: 0.010000
 768366/1000000: episode: 1020, duration: 50.779s, episode steps: 481, steps per second:   9, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.016848, mae: 2.580259, mean_q: 3.107875, mean_eps: 0.010000
 769061/1000000: episode: 1021, duration: 71.944s, episode steps: 695, steps per second:  10, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.055 [0.000, 5.000],  loss: 0.014937, mae: 2.558743, mean_q: 3.080669, mean_eps: 0.010000
 769907/1000000: episode: 1022, duration: 83.975s, episode steps: 846, steps per second:  10, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.014033, mae: 2.549921, mean_q: 3.071213, mean_eps: 0.010000
 770435/1000000: episode: 1023, duration: 57.523s, episode steps: 528, steps per second:   9, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.018316, mae: 2.551067, mean_q: 3.070243, mean_eps: 0.010000
 771278/1000000: episode: 1024, duration: 88.202s, episode steps: 843, steps per second:  10, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.015165, mae: 2.559476, mean_q: 3.081727, mean_eps: 0.010000
 772414/1000000: episode: 1025, duration: 121.682s, episode steps: 1136, steps per second:   9, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.016273, mae: 2.549144, mean_q: 3.069898, mean_eps: 0.010000
 772911/1000000: episode: 1026, duration: 52.297s, episode steps: 497, steps per second:  10, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.012819, mae: 2.543745, mean_q: 3.064600, mean_eps: 0.010000
 773641/1000000: episode: 1027, duration: 78.086s, episode steps: 730, steps per second:   9, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013008, mae: 2.547125, mean_q: 3.067588, mean_eps: 0.010000
 774357/1000000: episode: 1028, duration: 78.314s, episode steps: 716, steps per second:   9, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.012911, mae: 2.540364, mean_q: 3.060325, mean_eps: 0.010000
 775160/1000000: episode: 1029, duration: 82.046s, episode steps: 803, steps per second:  10, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.014100, mae: 2.553447, mean_q: 3.076127, mean_eps: 0.010000
 776194/1000000: episode: 1030, duration: 99.593s, episode steps: 1034, steps per second:  10, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.014981, mae: 2.558387, mean_q: 3.080953, mean_eps: 0.010000
 776751/1000000: episode: 1031, duration: 52.276s, episode steps: 557, steps per second:  11, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.014137, mae: 2.540613, mean_q: 3.063061, mean_eps: 0.010000
 777305/1000000: episode: 1032, duration: 52.427s, episode steps: 554, steps per second:  11, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.872 [0.000, 5.000],  loss: 0.013283, mae: 2.516544, mean_q: 3.031206, mean_eps: 0.010000
 777805/1000000: episode: 1033, duration: 48.682s, episode steps: 500, steps per second:  10, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.596 [0.000, 5.000],  loss: 0.012582, mae: 2.537636, mean_q: 3.056708, mean_eps: 0.010000
 778463/1000000: episode: 1034, duration: 63.069s, episode steps: 658, steps per second:  10, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.014700, mae: 2.564303, mean_q: 3.087228, mean_eps: 0.010000
 779253/1000000: episode: 1035, duration: 84.453s, episode steps: 790, steps per second:   9, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.013455, mae: 2.536926, mean_q: 3.054972, mean_eps: 0.010000
 780060/1000000: episode: 1036, duration: 89.477s, episode steps: 807, steps per second:   9, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.013829, mae: 2.545214, mean_q: 3.065168, mean_eps: 0.010000
 780992/1000000: episode: 1037, duration: 100.757s, episode steps: 932, steps per second:   9, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.014147, mae: 2.533166, mean_q: 3.050121, mean_eps: 0.010000
 781895/1000000: episode: 1038, duration: 85.511s, episode steps: 903, steps per second:  11, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.014245, mae: 2.553778, mean_q: 3.074681, mean_eps: 0.010000
 782813/1000000: episode: 1039, duration: 87.262s, episode steps: 918, steps per second:  11, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.015067, mae: 2.535627, mean_q: 3.053379, mean_eps: 0.010000
 783622/1000000: episode: 1040, duration: 77.109s, episode steps: 809, steps per second:  10, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.014869, mae: 2.538806, mean_q: 3.059764, mean_eps: 0.010000
 784328/1000000: episode: 1041, duration: 67.267s, episode steps: 706, steps per second:  10, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.751 [0.000, 5.000],  loss: 0.016488, mae: 2.552463, mean_q: 3.074041, mean_eps: 0.010000
 785142/1000000: episode: 1042, duration: 77.886s, episode steps: 814, steps per second:  10, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.013260, mae: 2.545494, mean_q: 3.066228, mean_eps: 0.010000
 785831/1000000: episode: 1043, duration: 65.187s, episode steps: 689, steps per second:  11, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.014237, mae: 2.535116, mean_q: 3.052701, mean_eps: 0.010000
 786486/1000000: episode: 1044, duration: 65.520s, episode steps: 655, steps per second:  10, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.998 [0.000, 5.000],  loss: 0.013485, mae: 2.551484, mean_q: 3.078302, mean_eps: 0.010000
 787553/1000000: episode: 1045, duration: 104.328s, episode steps: 1067, steps per second:  10, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.013943, mae: 2.528720, mean_q: 3.046064, mean_eps: 0.010000
 788103/1000000: episode: 1046, duration: 58.952s, episode steps: 550, steps per second:   9, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.012999, mae: 2.521289, mean_q: 3.037073, mean_eps: 0.010000
 789096/1000000: episode: 1047, duration: 107.397s, episode steps: 993, steps per second:   9, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.237 [0.000, 5.000],  loss: 0.014842, mae: 2.533130, mean_q: 3.050403, mean_eps: 0.010000
 789731/1000000: episode: 1048, duration: 67.503s, episode steps: 635, steps per second:   9, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.012805, mae: 2.549762, mean_q: 3.072895, mean_eps: 0.010000
 790456/1000000: episode: 1049, duration: 79.193s, episode steps: 725, steps per second:   9, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.014304, mae: 2.528829, mean_q: 3.048218, mean_eps: 0.010000
 791168/1000000: episode: 1050, duration: 77.503s, episode steps: 712, steps per second:   9, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.015391, mae: 2.539713, mean_q: 3.058133, mean_eps: 0.010000
 791844/1000000: episode: 1051, duration: 70.921s, episode steps: 676, steps per second:  10, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.899 [0.000, 5.000],  loss: 0.016265, mae: 2.508720, mean_q: 3.020874, mean_eps: 0.010000
 792357/1000000: episode: 1052, duration: 57.235s, episode steps: 513, steps per second:   9, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.805 [0.000, 5.000],  loss: 0.016612, mae: 2.527173, mean_q: 3.043385, mean_eps: 0.010000
 793145/1000000: episode: 1053, duration: 83.911s, episode steps: 788, steps per second:   9, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.014293, mae: 2.557506, mean_q: 3.079992, mean_eps: 0.010000
 793768/1000000: episode: 1054, duration: 68.664s, episode steps: 623, steps per second:   9, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.868 [0.000, 5.000],  loss: 0.013373, mae: 2.548995, mean_q: 3.070779, mean_eps: 0.010000
 794376/1000000: episode: 1055, duration: 68.152s, episode steps: 608, steps per second:   9, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.014418, mae: 2.543771, mean_q: 3.064389, mean_eps: 0.010000
 795016/1000000: episode: 1056, duration: 61.064s, episode steps: 640, steps per second:  10, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.016326, mae: 2.533052, mean_q: 3.049973, mean_eps: 0.010000
 795630/1000000: episode: 1057, duration: 60.266s, episode steps: 614, steps per second:  10, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.012157, mae: 2.543771, mean_q: 3.063841, mean_eps: 0.010000
 796496/1000000: episode: 1058, duration: 86.208s, episode steps: 866, steps per second:  10, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: 0.014468, mae: 2.553235, mean_q: 3.076008, mean_eps: 0.010000
 797346/1000000: episode: 1059, duration: 80.947s, episode steps: 850, steps per second:  11, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.013272, mae: 2.534696, mean_q: 3.053205, mean_eps: 0.010000
 798146/1000000: episode: 1060, duration: 78.415s, episode steps: 800, steps per second:  10, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.630 [0.000, 5.000],  loss: 0.015496, mae: 2.547940, mean_q: 3.068597, mean_eps: 0.010000
 799232/1000000: episode: 1061, duration: 114.616s, episode steps: 1086, steps per second:   9, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.015098, mae: 2.539170, mean_q: 3.058867, mean_eps: 0.010000
 799940/1000000: episode: 1062, duration: 78.422s, episode steps: 708, steps per second:   9, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.015273, mae: 2.532688, mean_q: 3.049704, mean_eps: 0.010000
 800582/1000000: episode: 1063, duration: 71.272s, episode steps: 642, steps per second:   9, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.573 [0.000, 5.000],  loss: 0.015808, mae: 2.546735, mean_q: 3.066988, mean_eps: 0.010000
 801029/1000000: episode: 1064, duration: 49.023s, episode steps: 447, steps per second:   9, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.275 [0.000, 5.000],  loss: 0.013460, mae: 2.547646, mean_q: 3.069884, mean_eps: 0.010000
 801589/1000000: episode: 1065, duration: 62.863s, episode steps: 560, steps per second:   9, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.013436, mae: 2.549466, mean_q: 3.069957, mean_eps: 0.010000
 802897/1000000: episode: 1066, duration: 128.718s, episode steps: 1308, steps per second:  10, episode reward: 20.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.905 [0.000, 5.000],  loss: 0.014585, mae: 2.552792, mean_q: 3.076310, mean_eps: 0.010000
 803832/1000000: episode: 1067, duration: 96.735s, episode steps: 935, steps per second:  10, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.012984, mae: 2.533711, mean_q: 3.052801, mean_eps: 0.010000
 804519/1000000: episode: 1068, duration: 79.804s, episode steps: 687, steps per second:   9, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.014134, mae: 2.557698, mean_q: 3.079738, mean_eps: 0.010000
 805312/1000000: episode: 1069, duration: 82.472s, episode steps: 793, steps per second:  10, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.015312, mae: 2.557436, mean_q: 3.080890, mean_eps: 0.010000
 806189/1000000: episode: 1070, duration: 87.085s, episode steps: 877, steps per second:  10, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.014773, mae: 2.549419, mean_q: 3.068327, mean_eps: 0.010000
 806800/1000000: episode: 1071, duration: 61.728s, episode steps: 611, steps per second:  10, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.008 [0.000, 5.000],  loss: 0.014729, mae: 2.583270, mean_q: 3.110819, mean_eps: 0.010000
 807326/1000000: episode: 1072, duration: 52.395s, episode steps: 526, steps per second:  10, episode reward: 14.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.755 [0.000, 5.000],  loss: 0.013701, mae: 2.556070, mean_q: 3.078339, mean_eps: 0.010000
 808245/1000000: episode: 1073, duration: 91.217s, episode steps: 919, steps per second:  10, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.014616, mae: 2.551441, mean_q: 3.073396, mean_eps: 0.010000
 808829/1000000: episode: 1074, duration: 55.952s, episode steps: 584, steps per second:  10, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.014974, mae: 2.547003, mean_q: 3.067794, mean_eps: 0.010000
 809525/1000000: episode: 1075, duration: 69.781s, episode steps: 696, steps per second:  10, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.015209, mae: 2.531800, mean_q: 3.048865, mean_eps: 0.010000
 810177/1000000: episode: 1076, duration: 64.929s, episode steps: 652, steps per second:  10, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.015976, mae: 2.579062, mean_q: 3.104690, mean_eps: 0.010000
 811145/1000000: episode: 1077, duration: 97.600s, episode steps: 968, steps per second:  10, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.868 [0.000, 5.000],  loss: 0.015075, mae: 2.555753, mean_q: 3.077141, mean_eps: 0.010000
 811945/1000000: episode: 1078, duration: 83.819s, episode steps: 800, steps per second:  10, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.012375, mae: 2.552784, mean_q: 3.074176, mean_eps: 0.010000
 812610/1000000: episode: 1079, duration: 69.986s, episode steps: 665, steps per second:  10, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.015920, mae: 2.561252, mean_q: 3.083622, mean_eps: 0.010000
 813379/1000000: episode: 1080, duration: 84.760s, episode steps: 769, steps per second:   9, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.057 [0.000, 5.000],  loss: 0.014933, mae: 2.550152, mean_q: 3.070926, mean_eps: 0.010000
 813770/1000000: episode: 1081, duration: 43.417s, episode steps: 391, steps per second:   9, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.014591, mae: 2.568907, mean_q: 3.094194, mean_eps: 0.010000
 814250/1000000: episode: 1082, duration: 55.291s, episode steps: 480, steps per second:   9, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.011540, mae: 2.552962, mean_q: 3.074500, mean_eps: 0.010000
 814731/1000000: episode: 1083, duration: 64.910s, episode steps: 481, steps per second:   7, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.017042, mae: 2.548341, mean_q: 3.065936, mean_eps: 0.010000
 815768/1000000: episode: 1084, duration: 110.799s, episode steps: 1037, steps per second:   9, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.014922, mae: 2.549125, mean_q: 3.069109, mean_eps: 0.010000
 816436/1000000: episode: 1085, duration: 70.299s, episode steps: 668, steps per second:  10, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.014471, mae: 2.566380, mean_q: 3.090056, mean_eps: 0.010000
 817425/1000000: episode: 1086, duration: 107.184s, episode steps: 989, steps per second:   9, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.011933, mae: 2.542783, mean_q: 3.066114, mean_eps: 0.010000
 818344/1000000: episode: 1087, duration: 92.922s, episode steps: 919, steps per second:  10, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.015440, mae: 2.539640, mean_q: 3.057306, mean_eps: 0.010000
 818857/1000000: episode: 1088, duration: 51.311s, episode steps: 513, steps per second:  10, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.011529, mae: 2.537061, mean_q: 3.054992, mean_eps: 0.010000
 819791/1000000: episode: 1089, duration: 90.491s, episode steps: 934, steps per second:  10, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.015033, mae: 2.544920, mean_q: 3.064169, mean_eps: 0.010000
 820256/1000000: episode: 1090, duration: 46.200s, episode steps: 465, steps per second:  10, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.012960, mae: 2.549339, mean_q: 3.070070, mean_eps: 0.010000
 820961/1000000: episode: 1091, duration: 70.897s, episode steps: 705, steps per second:  10, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015424, mae: 2.550724, mean_q: 3.071675, mean_eps: 0.010000
 821867/1000000: episode: 1092, duration: 87.804s, episode steps: 906, steps per second:  10, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.013166, mae: 2.560146, mean_q: 3.083273, mean_eps: 0.010000
 822385/1000000: episode: 1093, duration: 51.728s, episode steps: 518, steps per second:  10, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.015299, mae: 2.562299, mean_q: 3.085581, mean_eps: 0.010000
 823060/1000000: episode: 1094, duration: 66.521s, episode steps: 675, steps per second:  10, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.351 [0.000, 5.000],  loss: 0.013290, mae: 2.545579, mean_q: 3.067944, mean_eps: 0.010000
 823948/1000000: episode: 1095, duration: 88.016s, episode steps: 888, steps per second:  10, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.012461, mae: 2.529745, mean_q: 3.048203, mean_eps: 0.010000
 824554/1000000: episode: 1096, duration: 59.740s, episode steps: 606, steps per second:  10, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.860 [0.000, 5.000],  loss: 0.015294, mae: 2.570405, mean_q: 3.098235, mean_eps: 0.010000
 825209/1000000: episode: 1097, duration: 65.218s, episode steps: 655, steps per second:  10, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.722 [0.000, 5.000],  loss: 0.014057, mae: 2.558932, mean_q: 3.082206, mean_eps: 0.010000
 826127/1000000: episode: 1098, duration: 93.864s, episode steps: 918, steps per second:  10, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.013809, mae: 2.550312, mean_q: 3.071795, mean_eps: 0.010000
 826756/1000000: episode: 1099, duration: 62.814s, episode steps: 629, steps per second:  10, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.014605, mae: 2.539558, mean_q: 3.057328, mean_eps: 0.010000
 827692/1000000: episode: 1100, duration: 98.236s, episode steps: 936, steps per second:  10, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.015654, mae: 2.553891, mean_q: 3.074533, mean_eps: 0.010000
 828098/1000000: episode: 1101, duration: 39.215s, episode steps: 406, steps per second:  10, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.059 [0.000, 5.000],  loss: 0.013864, mae: 2.563339, mean_q: 3.086785, mean_eps: 0.010000
 828668/1000000: episode: 1102, duration: 64.617s, episode steps: 570, steps per second:   9, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.015240, mae: 2.554264, mean_q: 3.074444, mean_eps: 0.010000
 829704/1000000: episode: 1103, duration: 117.917s, episode steps: 1036, steps per second:   9, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.893 [0.000, 5.000],  loss: 0.014097, mae: 2.551004, mean_q: 3.072633, mean_eps: 0.010000
 830248/1000000: episode: 1104, duration: 53.482s, episode steps: 544, steps per second:  10, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.824 [0.000, 5.000],  loss: 0.015432, mae: 2.554671, mean_q: 3.079864, mean_eps: 0.010000
 830809/1000000: episode: 1105, duration: 58.093s, episode steps: 561, steps per second:  10, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.015480, mae: 2.544642, mean_q: 3.063659, mean_eps: 0.010000
 831878/1000000: episode: 1106, duration: 105.517s, episode steps: 1069, steps per second:  10, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.016013, mae: 2.543585, mean_q: 3.065158, mean_eps: 0.010000
 832531/1000000: episode: 1107, duration: 69.824s, episode steps: 653, steps per second:   9, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.013391, mae: 2.548379, mean_q: 3.068950, mean_eps: 0.010000
 833442/1000000: episode: 1108, duration: 104.028s, episode steps: 911, steps per second:   9, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.014298, mae: 2.560426, mean_q: 3.083801, mean_eps: 0.010000
 834439/1000000: episode: 1109, duration: 110.147s, episode steps: 997, steps per second:   9, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.013789, mae: 2.548851, mean_q: 3.070783, mean_eps: 0.010000
 835037/1000000: episode: 1110, duration: 68.168s, episode steps: 598, steps per second:   9, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.016377, mae: 2.585417, mean_q: 3.113449, mean_eps: 0.010000
 835976/1000000: episode: 1111, duration: 109.052s, episode steps: 939, steps per second:   9, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.014947, mae: 2.549088, mean_q: 3.072041, mean_eps: 0.010000
 836606/1000000: episode: 1112, duration: 79.637s, episode steps: 630, steps per second:   8, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.233 [0.000, 5.000],  loss: 0.013719, mae: 2.556718, mean_q: 3.080079, mean_eps: 0.010000
 837393/1000000: episode: 1113, duration: 85.013s, episode steps: 787, steps per second:   9, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.013240, mae: 2.543569, mean_q: 3.064521, mean_eps: 0.010000
 837940/1000000: episode: 1114, duration: 65.780s, episode steps: 547, steps per second:   8, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.669 [0.000, 5.000],  loss: 0.014600, mae: 2.532563, mean_q: 3.050966, mean_eps: 0.010000
 838295/1000000: episode: 1115, duration: 43.646s, episode steps: 355, steps per second:   8, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.013360, mae: 2.551076, mean_q: 3.073218, mean_eps: 0.010000
 839236/1000000: episode: 1116, duration: 111.024s, episode steps: 941, steps per second:   8, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.015615, mae: 2.555779, mean_q: 3.076705, mean_eps: 0.010000
 839738/1000000: episode: 1117, duration: 53.003s, episode steps: 502, steps per second:   9, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.440 [0.000, 5.000],  loss: 0.015959, mae: 2.557900, mean_q: 3.081415, mean_eps: 0.010000
 840846/1000000: episode: 1118, duration: 114.865s, episode steps: 1108, steps per second:  10, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.014640, mae: 2.567281, mean_q: 3.090928, mean_eps: 0.010000
 841584/1000000: episode: 1119, duration: 72.956s, episode steps: 738, steps per second:  10, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012763, mae: 2.556698, mean_q: 3.077810, mean_eps: 0.010000
 842686/1000000: episode: 1120, duration: 109.687s, episode steps: 1102, steps per second:  10, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.013590, mae: 2.547706, mean_q: 3.067163, mean_eps: 0.010000
 843213/1000000: episode: 1121, duration: 51.907s, episode steps: 527, steps per second:  10, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.012370, mae: 2.544865, mean_q: 3.064319, mean_eps: 0.010000
 843910/1000000: episode: 1122, duration: 71.400s, episode steps: 697, steps per second:  10, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.015355, mae: 2.548754, mean_q: 3.069018, mean_eps: 0.010000
 844750/1000000: episode: 1123, duration: 85.457s, episode steps: 840, steps per second:  10, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.014336, mae: 2.541092, mean_q: 3.057173, mean_eps: 0.010000
 845244/1000000: episode: 1124, duration: 48.148s, episode steps: 494, steps per second:  10, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.011324, mae: 2.555827, mean_q: 3.079063, mean_eps: 0.010000
 845950/1000000: episode: 1125, duration: 72.493s, episode steps: 706, steps per second:  10, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.013379, mae: 2.547903, mean_q: 3.068303, mean_eps: 0.010000
 846440/1000000: episode: 1126, duration: 50.332s, episode steps: 490, steps per second:  10, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.015128, mae: 2.563272, mean_q: 3.087377, mean_eps: 0.010000
 847205/1000000: episode: 1127, duration: 76.773s, episode steps: 765, steps per second:  10, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.015267, mae: 2.568763, mean_q: 3.093197, mean_eps: 0.010000
 847569/1000000: episode: 1128, duration: 37.897s, episode steps: 364, steps per second:  10, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013734, mae: 2.563306, mean_q: 3.087155, mean_eps: 0.010000
 848332/1000000: episode: 1129, duration: 78.889s, episode steps: 763, steps per second:  10, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.015436, mae: 2.541681, mean_q: 3.061791, mean_eps: 0.010000
 848949/1000000: episode: 1130, duration: 63.790s, episode steps: 617, steps per second:  10, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.016488, mae: 2.543977, mean_q: 3.062085, mean_eps: 0.010000
 850070/1000000: episode: 1131, duration: 131.216s, episode steps: 1121, steps per second:   9, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.014641, mae: 2.544909, mean_q: 3.064563, mean_eps: 0.010000
 850751/1000000: episode: 1132, duration: 80.926s, episode steps: 681, steps per second:   8, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.013647, mae: 2.522031, mean_q: 3.037302, mean_eps: 0.010000
 851447/1000000: episode: 1133, duration: 81.450s, episode steps: 696, steps per second:   9, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.014223, mae: 2.552748, mean_q: 3.075274, mean_eps: 0.010000
 852196/1000000: episode: 1134, duration: 86.887s, episode steps: 749, steps per second:   9, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.013711, mae: 2.530457, mean_q: 3.048884, mean_eps: 0.010000
 853012/1000000: episode: 1135, duration: 94.094s, episode steps: 816, steps per second:   9, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.014569, mae: 2.526719, mean_q: 3.041840, mean_eps: 0.010000
 853384/1000000: episode: 1136, duration: 43.529s, episode steps: 372, steps per second:   9, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.460 [0.000, 5.000],  loss: 0.013555, mae: 2.529757, mean_q: 3.047553, mean_eps: 0.010000
 854488/1000000: episode: 1137, duration: 127.995s, episode steps: 1104, steps per second:   9, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.755 [0.000, 5.000],  loss: 0.014041, mae: 2.541266, mean_q: 3.060915, mean_eps: 0.010000
 855647/1000000: episode: 1138, duration: 134.684s, episode steps: 1159, steps per second:   9, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.014003, mae: 2.539488, mean_q: 3.058003, mean_eps: 0.010000
 856728/1000000: episode: 1139, duration: 124.869s, episode steps: 1081, steps per second:   9, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.015089, mae: 2.530696, mean_q: 3.046706, mean_eps: 0.010000
 857273/1000000: episode: 1140, duration: 59.323s, episode steps: 545, steps per second:   9, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.014648, mae: 2.534697, mean_q: 3.052359, mean_eps: 0.010000
 858222/1000000: episode: 1141, duration: 96.333s, episode steps: 949, steps per second:  10, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.684 [0.000, 5.000],  loss: 0.015782, mae: 2.545945, mean_q: 3.066105, mean_eps: 0.010000
 859014/1000000: episode: 1142, duration: 79.690s, episode steps: 792, steps per second:  10, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.015730, mae: 2.537425, mean_q: 3.054486, mean_eps: 0.010000
 859812/1000000: episode: 1143, duration: 98.695s, episode steps: 798, steps per second:   8, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.014210, mae: 2.537575, mean_q: 3.055222, mean_eps: 0.010000
 860585/1000000: episode: 1144, duration: 87.047s, episode steps: 773, steps per second:   9, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.013307, mae: 2.522929, mean_q: 3.038508, mean_eps: 0.010000
 861436/1000000: episode: 1145, duration: 108.150s, episode steps: 851, steps per second:   8, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.014901, mae: 2.536324, mean_q: 3.052423, mean_eps: 0.010000
 862302/1000000: episode: 1146, duration: 98.378s, episode steps: 866, steps per second:   9, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.014712, mae: 2.536346, mean_q: 3.052897, mean_eps: 0.010000
 863009/1000000: episode: 1147, duration: 72.402s, episode steps: 707, steps per second:  10, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.866 [0.000, 5.000],  loss: 0.016783, mae: 2.573842, mean_q: 3.099721, mean_eps: 0.010000
 863550/1000000: episode: 1148, duration: 55.286s, episode steps: 541, steps per second:  10, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.012174, mae: 2.538767, mean_q: 3.057304, mean_eps: 0.010000
 864047/1000000: episode: 1149, duration: 51.815s, episode steps: 497, steps per second:  10, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.014582, mae: 2.525998, mean_q: 3.041701, mean_eps: 0.010000
 864873/1000000: episode: 1150, duration: 83.998s, episode steps: 826, steps per second:  10, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.015693, mae: 2.542348, mean_q: 3.061571, mean_eps: 0.010000
 865547/1000000: episode: 1151, duration: 67.085s, episode steps: 674, steps per second:  10, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.015316, mae: 2.533857, mean_q: 3.049569, mean_eps: 0.010000
 866119/1000000: episode: 1152, duration: 60.194s, episode steps: 572, steps per second:  10, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.657 [0.000, 5.000],  loss: 0.014112, mae: 2.536436, mean_q: 3.053138, mean_eps: 0.010000
 867223/1000000: episode: 1153, duration: 112.124s, episode steps: 1104, steps per second:  10, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.013809, mae: 2.540992, mean_q: 3.058529, mean_eps: 0.010000
 868131/1000000: episode: 1154, duration: 92.983s, episode steps: 908, steps per second:  10, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.015571, mae: 2.551236, mean_q: 3.069723, mean_eps: 0.010000
 868922/1000000: episode: 1155, duration: 82.714s, episode steps: 791, steps per second:  10, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.014335, mae: 2.550858, mean_q: 3.070891, mean_eps: 0.010000
 869608/1000000: episode: 1156, duration: 69.411s, episode steps: 686, steps per second:  10, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.015137, mae: 2.526742, mean_q: 3.040131, mean_eps: 0.010000
 870360/1000000: episode: 1157, duration: 78.308s, episode steps: 752, steps per second:  10, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.015794, mae: 2.560328, mean_q: 3.082104, mean_eps: 0.010000
 870960/1000000: episode: 1158, duration: 66.141s, episode steps: 600, steps per second:   9, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.016684, mae: 2.525639, mean_q: 3.039009, mean_eps: 0.010000
 872022/1000000: episode: 1159, duration: 122.910s, episode steps: 1062, steps per second:   9, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.015008, mae: 2.536147, mean_q: 3.053464, mean_eps: 0.010000
 872533/1000000: episode: 1160, duration: 54.021s, episode steps: 511, steps per second:   9, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.013150, mae: 2.535349, mean_q: 3.052221, mean_eps: 0.010000
 872919/1000000: episode: 1161, duration: 43.364s, episode steps: 386, steps per second:   9, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.018336, mae: 2.546457, mean_q: 3.065667, mean_eps: 0.010000
 873562/1000000: episode: 1162, duration: 75.586s, episode steps: 643, steps per second:   9, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.012843, mae: 2.536427, mean_q: 3.054117, mean_eps: 0.010000
 873907/1000000: episode: 1163, duration: 39.834s, episode steps: 345, steps per second:   9, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.018516, mae: 2.531423, mean_q: 3.044475, mean_eps: 0.010000
 874574/1000000: episode: 1164, duration: 79.597s, episode steps: 667, steps per second:   8, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.013336, mae: 2.512471, mean_q: 3.026720, mean_eps: 0.010000
 875232/1000000: episode: 1165, duration: 78.513s, episode steps: 658, steps per second:   8, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.854 [0.000, 5.000],  loss: 0.015259, mae: 2.537149, mean_q: 3.053900, mean_eps: 0.010000
 875921/1000000: episode: 1166, duration: 79.991s, episode steps: 689, steps per second:   9, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.014656, mae: 2.542902, mean_q: 3.063068, mean_eps: 0.010000
 876617/1000000: episode: 1167, duration: 85.171s, episode steps: 696, steps per second:   8, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.016132, mae: 2.528487, mean_q: 3.043510, mean_eps: 0.010000
 877675/1000000: episode: 1168, duration: 122.412s, episode steps: 1058, steps per second:   9, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.013894, mae: 2.536325, mean_q: 3.051738, mean_eps: 0.010000
 878303/1000000: episode: 1169, duration: 65.083s, episode steps: 628, steps per second:  10, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.013740, mae: 2.534829, mean_q: 3.051086, mean_eps: 0.010000
 879034/1000000: episode: 1170, duration: 74.434s, episode steps: 731, steps per second:  10, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.014586, mae: 2.537960, mean_q: 3.055664, mean_eps: 0.010000
 879603/1000000: episode: 1171, duration: 57.957s, episode steps: 569, steps per second:  10, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.682 [0.000, 5.000],  loss: 0.015376, mae: 2.550820, mean_q: 3.070677, mean_eps: 0.010000
 880289/1000000: episode: 1172, duration: 72.221s, episode steps: 686, steps per second:   9, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.012905, mae: 2.540401, mean_q: 3.059265, mean_eps: 0.010000
 880814/1000000: episode: 1173, duration: 52.745s, episode steps: 525, steps per second:  10, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.170 [0.000, 5.000],  loss: 0.017608, mae: 2.541039, mean_q: 3.057778, mean_eps: 0.010000
 881921/1000000: episode: 1174, duration: 117.958s, episode steps: 1107, steps per second:   9, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.013815, mae: 2.524398, mean_q: 3.038420, mean_eps: 0.010000
 882582/1000000: episode: 1175, duration: 72.581s, episode steps: 661, steps per second:   9, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.014347, mae: 2.534289, mean_q: 3.049826, mean_eps: 0.010000
 883228/1000000: episode: 1176, duration: 72.368s, episode steps: 646, steps per second:   9, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.013107, mae: 2.531506, mean_q: 3.047511, mean_eps: 0.010000
 884235/1000000: episode: 1177, duration: 114.287s, episode steps: 1007, steps per second:   9, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.015599, mae: 2.541617, mean_q: 3.058945, mean_eps: 0.010000
 884999/1000000: episode: 1178, duration: 90.037s, episode steps: 764, steps per second:   8, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.014140, mae: 2.554901, mean_q: 3.077456, mean_eps: 0.010000
 885812/1000000: episode: 1179, duration: 96.542s, episode steps: 813, steps per second:   8, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.018108, mae: 2.555013, mean_q: 3.075885, mean_eps: 0.010000
 886573/1000000: episode: 1180, duration: 93.771s, episode steps: 761, steps per second:   8, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.015357, mae: 2.542428, mean_q: 3.059553, mean_eps: 0.010000
 887234/1000000: episode: 1181, duration: 78.381s, episode steps: 661, steps per second:   8, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.136 [0.000, 5.000],  loss: 0.013863, mae: 2.525467, mean_q: 3.040642, mean_eps: 0.010000
 887620/1000000: episode: 1182, duration: 47.910s, episode steps: 386, steps per second:   8, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014119, mae: 2.578866, mean_q: 3.105582, mean_eps: 0.010000
 888627/1000000: episode: 1183, duration: 105.675s, episode steps: 1007, steps per second:  10, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.013310, mae: 2.526777, mean_q: 3.042338, mean_eps: 0.010000
 889536/1000000: episode: 1184, duration: 95.689s, episode steps: 909, steps per second:   9, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.013892, mae: 2.547841, mean_q: 3.067315, mean_eps: 0.010000
 890633/1000000: episode: 1185, duration: 115.958s, episode steps: 1097, steps per second:   9, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.015653, mae: 2.530909, mean_q: 3.046960, mean_eps: 0.010000
 891595/1000000: episode: 1186, duration: 108.774s, episode steps: 962, steps per second:   9, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.015030, mae: 2.523599, mean_q: 3.038166, mean_eps: 0.010000
 892141/1000000: episode: 1187, duration: 61.563s, episode steps: 546, steps per second:   9, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014936, mae: 2.554362, mean_q: 3.077222, mean_eps: 0.010000
 892726/1000000: episode: 1188, duration: 67.851s, episode steps: 585, steps per second:   9, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.535 [0.000, 5.000],  loss: 0.014342, mae: 2.535443, mean_q: 3.054296, mean_eps: 0.010000
 893723/1000000: episode: 1189, duration: 114.992s, episode steps: 997, steps per second:   9, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.016275, mae: 2.540644, mean_q: 3.057929, mean_eps: 0.010000
 894434/1000000: episode: 1190, duration: 84.442s, episode steps: 711, steps per second:   8, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 0.013428, mae: 2.528237, mean_q: 3.043999, mean_eps: 0.010000
 895441/1000000: episode: 1191, duration: 118.619s, episode steps: 1007, steps per second:   8, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.013930, mae: 2.528728, mean_q: 3.044674, mean_eps: 0.010000
 895892/1000000: episode: 1192, duration: 53.097s, episode steps: 451, steps per second:   8, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.014890, mae: 2.516022, mean_q: 3.029303, mean_eps: 0.010000
 896386/1000000: episode: 1193, duration: 61.834s, episode steps: 494, steps per second:   8, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.702 [0.000, 5.000],  loss: 0.014133, mae: 2.547361, mean_q: 3.068871, mean_eps: 0.010000
 896959/1000000: episode: 1194, duration: 67.946s, episode steps: 573, steps per second:   8, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.014780, mae: 2.528911, mean_q: 3.042962, mean_eps: 0.010000
 897656/1000000: episode: 1195, duration: 85.401s, episode steps: 697, steps per second:   8, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.013655, mae: 2.543683, mean_q: 3.062582, mean_eps: 0.010000
 898864/1000000: episode: 1196, duration: 142.806s, episode steps: 1208, steps per second:   8, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.013842, mae: 2.529719, mean_q: 3.045674, mean_eps: 0.010000
 900163/1000000: episode: 1197, duration: 154.102s, episode steps: 1299, steps per second:   8, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.014178, mae: 2.540894, mean_q: 3.058736, mean_eps: 0.010000
 900965/1000000: episode: 1198, duration: 95.150s, episode steps: 802, steps per second:   8, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.013906, mae: 2.526590, mean_q: 3.043157, mean_eps: 0.010000
 902029/1000000: episode: 1199, duration: 122.627s, episode steps: 1064, steps per second:   9, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.015039, mae: 2.548206, mean_q: 3.068910, mean_eps: 0.010000
 902682/1000000: episode: 1200, duration: 66.784s, episode steps: 653, steps per second:  10, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.013630, mae: 2.539315, mean_q: 3.057502, mean_eps: 0.010000
 903586/1000000: episode: 1201, duration: 106.390s, episode steps: 904, steps per second:   8, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.014071, mae: 2.534894, mean_q: 3.051719, mean_eps: 0.010000
 904401/1000000: episode: 1202, duration: 96.226s, episode steps: 815, steps per second:   8, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.013637, mae: 2.529226, mean_q: 3.045682, mean_eps: 0.010000
 905107/1000000: episode: 1203, duration: 83.408s, episode steps: 706, steps per second:   8, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.765 [0.000, 5.000],  loss: 0.014215, mae: 2.509185, mean_q: 3.022307, mean_eps: 0.010000
 906242/1000000: episode: 1204, duration: 133.910s, episode steps: 1135, steps per second:   8, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.870 [0.000, 5.000],  loss: 0.013903, mae: 2.548764, mean_q: 3.068682, mean_eps: 0.010000
 907234/1000000: episode: 1205, duration: 119.611s, episode steps: 992, steps per second:   8, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.014895, mae: 2.533603, mean_q: 3.050731, mean_eps: 0.010000
 907914/1000000: episode: 1206, duration: 72.830s, episode steps: 680, steps per second:   9, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.819 [0.000, 5.000],  loss: 0.014374, mae: 2.553018, mean_q: 3.073523, mean_eps: 0.010000
 908659/1000000: episode: 1207, duration: 79.375s, episode steps: 745, steps per second:   9, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.013279, mae: 2.525301, mean_q: 3.041270, mean_eps: 0.010000
 909311/1000000: episode: 1208, duration: 70.313s, episode steps: 652, steps per second:   9, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.014440, mae: 2.525832, mean_q: 3.039536, mean_eps: 0.010000
 910272/1000000: episode: 1209, duration: 116.782s, episode steps: 961, steps per second:   8, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.014943, mae: 2.539290, mean_q: 3.057369, mean_eps: 0.010000
 911077/1000000: episode: 1210, duration: 93.385s, episode steps: 805, steps per second:   9, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.015508, mae: 2.543667, mean_q: 3.065699, mean_eps: 0.010000
 912101/1000000: episode: 1211, duration: 122.931s, episode steps: 1024, steps per second:   8, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.014599, mae: 2.536280, mean_q: 3.052127, mean_eps: 0.010000
 912994/1000000: episode: 1212, duration: 107.009s, episode steps: 893, steps per second:   8, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.878 [0.000, 5.000],  loss: 0.015322, mae: 2.562325, mean_q: 3.083846, mean_eps: 0.010000
 913801/1000000: episode: 1213, duration: 98.436s, episode steps: 807, steps per second:   8, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.778 [0.000, 5.000],  loss: 0.014104, mae: 2.542669, mean_q: 3.061227, mean_eps: 0.010000
 914732/1000000: episode: 1214, duration: 96.264s, episode steps: 931, steps per second:  10, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.014877, mae: 2.557077, mean_q: 3.079945, mean_eps: 0.010000
 915506/1000000: episode: 1215, duration: 80.658s, episode steps: 774, steps per second:  10, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.013832, mae: 2.538365, mean_q: 3.055644, mean_eps: 0.010000
 916442/1000000: episode: 1216, duration: 99.040s, episode steps: 936, steps per second:   9, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.015009, mae: 2.530471, mean_q: 3.044060, mean_eps: 0.010000
 917399/1000000: episode: 1217, duration: 132.821s, episode steps: 957, steps per second:   7, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.016087, mae: 2.542029, mean_q: 3.058335, mean_eps: 0.010000
 918327/1000000: episode: 1218, duration: 124.564s, episode steps: 928, steps per second:   7, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.015071, mae: 2.547316, mean_q: 3.066702, mean_eps: 0.010000
 919283/1000000: episode: 1219, duration: 124.022s, episode steps: 956, steps per second:   8, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.349 [0.000, 5.000],  loss: 0.014831, mae: 2.514966, mean_q: 3.027502, mean_eps: 0.010000
 919822/1000000: episode: 1220, duration: 65.838s, episode steps: 539, steps per second:   8, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.224 [0.000, 5.000],  loss: 0.015844, mae: 2.554237, mean_q: 3.073543, mean_eps: 0.010000
 920188/1000000: episode: 1221, duration: 43.994s, episode steps: 366, steps per second:   8, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.008 [0.000, 5.000],  loss: 0.012802, mae: 2.514854, mean_q: 3.027281, mean_eps: 0.010000
 921010/1000000: episode: 1222, duration: 91.090s, episode steps: 822, steps per second:   9, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.015289, mae: 2.558920, mean_q: 3.079211, mean_eps: 0.010000
 921821/1000000: episode: 1223, duration: 84.142s, episode steps: 811, steps per second:  10, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.737 [0.000, 5.000],  loss: 0.013220, mae: 2.541991, mean_q: 3.061523, mean_eps: 0.010000
 922335/1000000: episode: 1224, duration: 54.774s, episode steps: 514, steps per second:   9, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.014411, mae: 2.542600, mean_q: 3.060479, mean_eps: 0.010000
 923255/1000000: episode: 1225, duration: 95.684s, episode steps: 920, steps per second:  10, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.804 [0.000, 5.000],  loss: 0.013562, mae: 2.531719, mean_q: 3.048953, mean_eps: 0.010000
 924032/1000000: episode: 1226, duration: 81.508s, episode steps: 777, steps per second:  10, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.013433, mae: 2.538511, mean_q: 3.055671, mean_eps: 0.010000
 924767/1000000: episode: 1227, duration: 78.729s, episode steps: 735, steps per second:   9, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.667 [0.000, 5.000],  loss: 0.014858, mae: 2.528493, mean_q: 3.045151, mean_eps: 0.010000
 925429/1000000: episode: 1228, duration: 69.753s, episode steps: 662, steps per second:   9, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014975, mae: 2.545073, mean_q: 3.063305, mean_eps: 0.010000
 926197/1000000: episode: 1229, duration: 82.143s, episode steps: 768, steps per second:   9, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.014790, mae: 2.537792, mean_q: 3.055526, mean_eps: 0.010000
 926807/1000000: episode: 1230, duration: 66.189s, episode steps: 610, steps per second:   9, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.192 [0.000, 5.000],  loss: 0.013846, mae: 2.550951, mean_q: 3.071188, mean_eps: 0.010000
 927548/1000000: episode: 1231, duration: 100.289s, episode steps: 741, steps per second:   7, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.876 [0.000, 5.000],  loss: 0.016940, mae: 2.562924, mean_q: 3.085200, mean_eps: 0.010000
 928064/1000000: episode: 1232, duration: 69.158s, episode steps: 516, steps per second:   7, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.012629, mae: 2.565022, mean_q: 3.089112, mean_eps: 0.010000
 928761/1000000: episode: 1233, duration: 94.931s, episode steps: 697, steps per second:   7, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.948 [0.000, 5.000],  loss: 0.014228, mae: 2.539032, mean_q: 3.056074, mean_eps: 0.010000
 929413/1000000: episode: 1234, duration: 77.678s, episode steps: 652, steps per second:   8, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.419 [0.000, 5.000],  loss: 0.014998, mae: 2.527011, mean_q: 3.042971, mean_eps: 0.010000
 930039/1000000: episode: 1235, duration: 66.723s, episode steps: 626, steps per second:   9, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.043 [0.000, 5.000],  loss: 0.014060, mae: 2.549332, mean_q: 3.068610, mean_eps: 0.010000
 930621/1000000: episode: 1236, duration: 61.772s, episode steps: 582, steps per second:   9, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.220 [0.000, 5.000],  loss: 0.014578, mae: 2.545733, mean_q: 3.065329, mean_eps: 0.010000
 931225/1000000: episode: 1237, duration: 67.227s, episode steps: 604, steps per second:   9, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.099 [0.000, 5.000],  loss: 0.014535, mae: 2.549053, mean_q: 3.069051, mean_eps: 0.010000
 931962/1000000: episode: 1238, duration: 77.036s, episode steps: 737, steps per second:  10, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.080 [0.000, 5.000],  loss: 0.013623, mae: 2.551237, mean_q: 3.074197, mean_eps: 0.010000
 932982/1000000: episode: 1239, duration: 106.658s, episode steps: 1020, steps per second:  10, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.014240, mae: 2.562208, mean_q: 3.084916, mean_eps: 0.010000
 933952/1000000: episode: 1240, duration: 102.092s, episode steps: 970, steps per second:  10, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.016233, mae: 2.546171, mean_q: 3.066930, mean_eps: 0.010000
 934681/1000000: episode: 1241, duration: 77.251s, episode steps: 729, steps per second:   9, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.016639, mae: 2.543771, mean_q: 3.061607, mean_eps: 0.010000
 935414/1000000: episode: 1242, duration: 76.237s, episode steps: 733, steps per second:  10, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.014865, mae: 2.537667, mean_q: 3.053684, mean_eps: 0.010000
 936562/1000000: episode: 1243, duration: 121.213s, episode steps: 1148, steps per second:   9, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.014529, mae: 2.557337, mean_q: 3.077948, mean_eps: 0.010000
 937296/1000000: episode: 1244, duration: 78.898s, episode steps: 734, steps per second:   9, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.014685, mae: 2.549142, mean_q: 3.068335, mean_eps: 0.010000
 938171/1000000: episode: 1245, duration: 92.417s, episode steps: 875, steps per second:   9, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.013633, mae: 2.544275, mean_q: 3.063203, mean_eps: 0.010000
 939189/1000000: episode: 1246, duration: 106.990s, episode steps: 1018, steps per second:  10, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.016720, mae: 2.545715, mean_q: 3.064987, mean_eps: 0.010000
 939910/1000000: episode: 1247, duration: 76.702s, episode steps: 721, steps per second:   9, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.703 [0.000, 5.000],  loss: 0.012240, mae: 2.519397, mean_q: 3.033052, mean_eps: 0.010000
 940460/1000000: episode: 1248, duration: 61.342s, episode steps: 550, steps per second:   9, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.016456, mae: 2.534734, mean_q: 3.051726, mean_eps: 0.010000
 940988/1000000: episode: 1249, duration: 55.781s, episode steps: 528, steps per second:   9, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.951 [0.000, 5.000],  loss: 0.012094, mae: 2.549736, mean_q: 3.074431, mean_eps: 0.010000
 941968/1000000: episode: 1250, duration: 111.420s, episode steps: 980, steps per second:   9, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.015242, mae: 2.517761, mean_q: 3.031282, mean_eps: 0.010000
 942630/1000000: episode: 1251, duration: 80.471s, episode steps: 662, steps per second:   8, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.724 [0.000, 5.000],  loss: 0.014670, mae: 2.525155, mean_q: 3.039371, mean_eps: 0.010000
 943264/1000000: episode: 1252, duration: 80.152s, episode steps: 634, steps per second:   8, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.015590, mae: 2.531658, mean_q: 3.047093, mean_eps: 0.010000
 943817/1000000: episode: 1253, duration: 68.405s, episode steps: 553, steps per second:   8, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.015955, mae: 2.541744, mean_q: 3.059151, mean_eps: 0.010000
 944210/1000000: episode: 1254, duration: 47.647s, episode steps: 393, steps per second:   8, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.014301, mae: 2.546028, mean_q: 3.063251, mean_eps: 0.010000
 944950/1000000: episode: 1255, duration: 90.261s, episode steps: 740, steps per second:   8, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.016351, mae: 2.531812, mean_q: 3.046933, mean_eps: 0.010000
 945845/1000000: episode: 1256, duration: 109.824s, episode steps: 895, steps per second:   8, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.014422, mae: 2.551106, mean_q: 3.072301, mean_eps: 0.010000
 946547/1000000: episode: 1257, duration: 87.869s, episode steps: 702, steps per second:   8, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.014033, mae: 2.550384, mean_q: 3.070736, mean_eps: 0.010000
 947366/1000000: episode: 1258, duration: 108.875s, episode steps: 819, steps per second:   8, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.015262, mae: 2.561800, mean_q: 3.086203, mean_eps: 0.010000
 947995/1000000: episode: 1259, duration: 89.380s, episode steps: 629, steps per second:   7, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.017246, mae: 2.527763, mean_q: 3.042256, mean_eps: 0.010000
 948699/1000000: episode: 1260, duration: 93.246s, episode steps: 704, steps per second:   8, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.015581, mae: 2.543895, mean_q: 3.062611, mean_eps: 0.010000
 949409/1000000: episode: 1261, duration: 93.086s, episode steps: 710, steps per second:   8, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.462 [0.000, 5.000],  loss: 0.014916, mae: 2.544601, mean_q: 3.062525, mean_eps: 0.010000
 949896/1000000: episode: 1262, duration: 56.044s, episode steps: 487, steps per second:   9, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.014807, mae: 2.552954, mean_q: 3.072121, mean_eps: 0.010000
 950771/1000000: episode: 1263, duration: 99.553s, episode steps: 875, steps per second:   9, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.655 [0.000, 5.000],  loss: 0.013689, mae: 2.549491, mean_q: 3.070364, mean_eps: 0.010000
 951543/1000000: episode: 1264, duration: 89.284s, episode steps: 772, steps per second:   9, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.014756, mae: 2.552681, mean_q: 3.073069, mean_eps: 0.010000
 952230/1000000: episode: 1265, duration: 75.961s, episode steps: 687, steps per second:   9, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.389 [0.000, 5.000],  loss: 0.013012, mae: 2.548013, mean_q: 3.068299, mean_eps: 0.010000
 952964/1000000: episode: 1266, duration: 78.952s, episode steps: 734, steps per second:   9, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.013171, mae: 2.522579, mean_q: 3.036448, mean_eps: 0.010000
 953640/1000000: episode: 1267, duration: 70.915s, episode steps: 676, steps per second:  10, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.013325, mae: 2.526401, mean_q: 3.044221, mean_eps: 0.010000
 954501/1000000: episode: 1268, duration: 92.441s, episode steps: 861, steps per second:   9, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.626 [0.000, 5.000],  loss: 0.015077, mae: 2.524480, mean_q: 3.040468, mean_eps: 0.010000
 955525/1000000: episode: 1269, duration: 108.848s, episode steps: 1024, steps per second:   9, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.701 [0.000, 5.000],  loss: 0.013081, mae: 2.530929, mean_q: 3.047745, mean_eps: 0.010000
 956383/1000000: episode: 1270, duration: 91.583s, episode steps: 858, steps per second:   9, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.013110, mae: 2.534351, mean_q: 3.051761, mean_eps: 0.010000
 956767/1000000: episode: 1271, duration: 39.889s, episode steps: 384, steps per second:  10, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.339 [0.000, 5.000],  loss: 0.013739, mae: 2.511372, mean_q: 3.022714, mean_eps: 0.010000
 957683/1000000: episode: 1272, duration: 97.164s, episode steps: 916, steps per second:   9, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.014660, mae: 2.546972, mean_q: 3.067385, mean_eps: 0.010000
 958483/1000000: episode: 1273, duration: 85.206s, episode steps: 800, steps per second:   9, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.013910, mae: 2.529241, mean_q: 3.044395, mean_eps: 0.010000
 959160/1000000: episode: 1274, duration: 72.107s, episode steps: 677, steps per second:   9, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.052 [0.000, 5.000],  loss: 0.013597, mae: 2.529917, mean_q: 3.043247, mean_eps: 0.010000
 959938/1000000: episode: 1275, duration: 83.192s, episode steps: 778, steps per second:   9, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.044 [0.000, 5.000],  loss: 0.014447, mae: 2.516722, mean_q: 3.028831, mean_eps: 0.010000
 960648/1000000: episode: 1276, duration: 75.093s, episode steps: 710, steps per second:   9, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014036, mae: 2.516896, mean_q: 3.028759, mean_eps: 0.010000
 961181/1000000: episode: 1277, duration: 58.257s, episode steps: 533, steps per second:   9, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.012672, mae: 2.510155, mean_q: 3.023419, mean_eps: 0.010000
 962017/1000000: episode: 1278, duration: 102.376s, episode steps: 836, steps per second:   8, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.015330, mae: 2.517383, mean_q: 3.030261, mean_eps: 0.010000
 962399/1000000: episode: 1279, duration: 49.526s, episode steps: 382, steps per second:   8, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.194 [0.000, 5.000],  loss: 0.016621, mae: 2.512180, mean_q: 3.024353, mean_eps: 0.010000
 963045/1000000: episode: 1280, duration: 84.774s, episode steps: 646, steps per second:   8, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.014406, mae: 2.528454, mean_q: 3.044080, mean_eps: 0.010000
 963780/1000000: episode: 1281, duration: 98.597s, episode steps: 735, steps per second:   7, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.588 [0.000, 5.000],  loss: 0.012080, mae: 2.534941, mean_q: 3.051930, mean_eps: 0.010000
 964467/1000000: episode: 1282, duration: 82.145s, episode steps: 687, steps per second:   8, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.726 [0.000, 5.000],  loss: 0.013252, mae: 2.535510, mean_q: 3.052442, mean_eps: 0.010000
 965161/1000000: episode: 1283, duration: 85.357s, episode steps: 694, steps per second:   8, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.499 [0.000, 5.000],  loss: 0.015302, mae: 2.520013, mean_q: 3.032015, mean_eps: 0.010000
 965881/1000000: episode: 1284, duration: 82.865s, episode steps: 720, steps per second:   9, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.015932, mae: 2.526438, mean_q: 3.038730, mean_eps: 0.010000
 967019/1000000: episode: 1285, duration: 120.726s, episode steps: 1138, steps per second:   9, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.706 [0.000, 5.000],  loss: 0.014302, mae: 2.507466, mean_q: 3.018292, mean_eps: 0.010000
 967952/1000000: episode: 1286, duration: 100.364s, episode steps: 933, steps per second:   9, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.017002, mae: 2.515219, mean_q: 3.025556, mean_eps: 0.010000
 968719/1000000: episode: 1287, duration: 82.739s, episode steps: 767, steps per second:   9, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.014344, mae: 2.520332, mean_q: 3.033025, mean_eps: 0.010000
 969284/1000000: episode: 1288, duration: 60.789s, episode steps: 565, steps per second:   9, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.015889, mae: 2.547889, mean_q: 3.066880, mean_eps: 0.010000
 970010/1000000: episode: 1289, duration: 78.082s, episode steps: 726, steps per second:   9, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.734 [0.000, 5.000],  loss: 0.013685, mae: 2.533626, mean_q: 3.050882, mean_eps: 0.010000
 970577/1000000: episode: 1290, duration: 61.611s, episode steps: 567, steps per second:   9, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.019 [0.000, 5.000],  loss: 0.012706, mae: 2.521307, mean_q: 3.033775, mean_eps: 0.010000
 971559/1000000: episode: 1291, duration: 108.924s, episode steps: 982, steps per second:   9, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014223, mae: 2.552096, mean_q: 3.072468, mean_eps: 0.010000
 972354/1000000: episode: 1292, duration: 85.648s, episode steps: 795, steps per second:   9, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.015094, mae: 2.513782, mean_q: 3.026822, mean_eps: 0.010000
 972855/1000000: episode: 1293, duration: 60.253s, episode steps: 501, steps per second:   8, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.569 [0.000, 5.000],  loss: 0.015453, mae: 2.512953, mean_q: 3.025833, mean_eps: 0.010000
 973506/1000000: episode: 1294, duration: 78.781s, episode steps: 651, steps per second:   8, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.014027, mae: 2.519855, mean_q: 3.034492, mean_eps: 0.010000
 974587/1000000: episode: 1295, duration: 136.772s, episode steps: 1081, steps per second:   8, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.013465, mae: 2.535343, mean_q: 3.052770, mean_eps: 0.010000
 975132/1000000: episode: 1296, duration: 68.788s, episode steps: 545, steps per second:   8, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.015595, mae: 2.530369, mean_q: 3.043898, mean_eps: 0.010000
 976068/1000000: episode: 1297, duration: 116.717s, episode steps: 936, steps per second:   8, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.014738, mae: 2.515425, mean_q: 3.029208, mean_eps: 0.010000
 977236/1000000: episode: 1298, duration: 146.855s, episode steps: 1168, steps per second:   8, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.015061, mae: 2.516722, mean_q: 3.030373, mean_eps: 0.010000
 978372/1000000: episode: 1299, duration: 130.442s, episode steps: 1136, steps per second:   9, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.015246, mae: 2.533307, mean_q: 3.049566, mean_eps: 0.010000
 979397/1000000: episode: 1300, duration: 110.610s, episode steps: 1025, steps per second:   9, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.239 [0.000, 5.000],  loss: 0.014797, mae: 2.530803, mean_q: 3.047499, mean_eps: 0.010000
 980044/1000000: episode: 1301, duration: 71.078s, episode steps: 647, steps per second:   9, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.013565, mae: 2.524950, mean_q: 3.039452, mean_eps: 0.010000
 980404/1000000: episode: 1302, duration: 38.763s, episode steps: 360, steps per second:   9, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.222 [1.000, 5.000],  loss: 0.018221, mae: 2.525021, mean_q: 3.040261, mean_eps: 0.010000
 980884/1000000: episode: 1303, duration: 51.846s, episode steps: 480, steps per second:   9, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.013354, mae: 2.525030, mean_q: 3.037841, mean_eps: 0.010000
 981538/1000000: episode: 1304, duration: 73.284s, episode steps: 654, steps per second:   9, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.012608, mae: 2.517279, mean_q: 3.029868, mean_eps: 0.010000
 982147/1000000: episode: 1305, duration: 68.090s, episode steps: 609, steps per second:   9, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.014888, mae: 2.539216, mean_q: 3.056687, mean_eps: 0.010000
 983248/1000000: episode: 1306, duration: 138.233s, episode steps: 1101, steps per second:   8, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.015436, mae: 2.521037, mean_q: 3.033568, mean_eps: 0.010000
 983893/1000000: episode: 1307, duration: 81.278s, episode steps: 645, steps per second:   8, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.695 [0.000, 5.000],  loss: 0.013572, mae: 2.530086, mean_q: 3.045472, mean_eps: 0.010000
 984405/1000000: episode: 1308, duration: 62.824s, episode steps: 512, steps per second:   8, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.031 [0.000, 5.000],  loss: 0.013495, mae: 2.563959, mean_q: 3.087693, mean_eps: 0.010000
 985108/1000000: episode: 1309, duration: 84.679s, episode steps: 703, steps per second:   8, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.014836, mae: 2.535543, mean_q: 3.050650, mean_eps: 0.010000
 985612/1000000: episode: 1310, duration: 62.729s, episode steps: 504, steps per second:   8, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.052 [0.000, 5.000],  loss: 0.013666, mae: 2.517185, mean_q: 3.030571, mean_eps: 0.010000
 986361/1000000: episode: 1311, duration: 94.746s, episode steps: 749, steps per second:   8, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.015831, mae: 2.540979, mean_q: 3.058112, mean_eps: 0.010000
 987090/1000000: episode: 1312, duration: 96.713s, episode steps: 729, steps per second:   8, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.872 [0.000, 5.000],  loss: 0.014174, mae: 2.549729, mean_q: 3.069925, mean_eps: 0.010000
 987755/1000000: episode: 1313, duration: 79.051s, episode steps: 665, steps per second:   8, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.013923, mae: 2.536727, mean_q: 3.053069, mean_eps: 0.010000
 989172/1000000: episode: 1314, duration: 169.293s, episode steps: 1417, steps per second:   8, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.015889, mae: 2.521051, mean_q: 3.032298, mean_eps: 0.010000
 990014/1000000: episode: 1315, duration: 95.758s, episode steps: 842, steps per second:   9, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.540 [0.000, 5.000],  loss: 0.014907, mae: 2.530200, mean_q: 3.045312, mean_eps: 0.010000
 990551/1000000: episode: 1316, duration: 59.643s, episode steps: 537, steps per second:   9, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.016011, mae: 2.528439, mean_q: 3.043654, mean_eps: 0.010000
 991491/1000000: episode: 1317, duration: 107.091s, episode steps: 940, steps per second:   9, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.013715, mae: 2.524624, mean_q: 3.038670, mean_eps: 0.010000
 992136/1000000: episode: 1318, duration: 69.919s, episode steps: 645, steps per second:   9, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.013478, mae: 2.539401, mean_q: 3.056879, mean_eps: 0.010000
 992848/1000000: episode: 1319, duration: 76.679s, episode steps: 712, steps per second:   9, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.012717, mae: 2.527414, mean_q: 3.042254, mean_eps: 0.010000
 993678/1000000: episode: 1320, duration: 89.320s, episode steps: 830, steps per second:   9, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.015018, mae: 2.532766, mean_q: 3.048446, mean_eps: 0.010000
 994589/1000000: episode: 1321, duration: 99.240s, episode steps: 911, steps per second:   9, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.014394, mae: 2.536053, mean_q: 3.052667, mean_eps: 0.010000
 995542/1000000: episode: 1322, duration: 103.439s, episode steps: 953, steps per second:   9, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.013453, mae: 2.526101, mean_q: 3.041072, mean_eps: 0.010000
 995933/1000000: episode: 1323, duration: 42.826s, episode steps: 391, steps per second:   9, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.184 [0.000, 5.000],  loss: 0.012894, mae: 2.532634, mean_q: 3.049077, mean_eps: 0.010000
 996564/1000000: episode: 1324, duration: 70.645s, episode steps: 631, steps per second:   9, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.013581, mae: 2.536838, mean_q: 3.053009, mean_eps: 0.010000
 997322/1000000: episode: 1325, duration: 82.839s, episode steps: 758, steps per second:   9, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.015193, mae: 2.531709, mean_q: 3.047728, mean_eps: 0.010000
 997817/1000000: episode: 1326, duration: 54.021s, episode steps: 495, steps per second:   9, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.871 [0.000, 5.000],  loss: 0.012409, mae: 2.518575, mean_q: 3.035158, mean_eps: 0.010000
 998963/1000000: episode: 1327, duration: 148.539s, episode steps: 1146, steps per second:   8, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.014144, mae: 2.517324, mean_q: 3.029438, mean_eps: 0.010000
 999431/1000000: episode: 1328, duration: 62.437s, episode steps: 468, steps per second:   7, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.318 [0.000, 5.000],  loss: 0.014319, mae: 2.530456, mean_q: 3.047816, mean_eps: 0.010000
done, took 83880.892 seconds
########################################################
PROCESO TERMINADO
########################################################