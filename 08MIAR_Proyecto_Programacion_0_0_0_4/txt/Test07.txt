['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_250000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
    420/250000: episode: 1, duration: 3.667s, episode steps: 420, steps per second: 115, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 5.484s, episode steps: 711, steps per second: 130, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 5.661s, episode steps: 810, steps per second: 143, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2827/250000: episode: 4, duration: 7.091s, episode steps: 886, steps per second: 125, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3345/250000: episode: 5, duration: 3.726s, episode steps: 518, steps per second: 139, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3990/250000: episode: 6, duration: 3.930s, episode steps: 645, steps per second: 164, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4451/250000: episode: 7, duration: 3.617s, episode steps: 461, steps per second: 127, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5247/250000: episode: 8, duration: 5.371s, episode steps: 796, steps per second: 148, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5629/250000: episode: 9, duration: 2.672s, episode steps: 382, steps per second: 143, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7090/250000: episode: 10, duration: 9.186s, episode steps: 1461, steps per second: 159, episode reward: 13.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7471/250000: episode: 11, duration: 2.681s, episode steps: 381, steps per second: 142, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8016/250000: episode: 12, duration: 3.859s, episode steps: 545, steps per second: 141, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8654/250000: episode: 13, duration: 3.988s, episode steps: 638, steps per second: 160, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9492/250000: episode: 14, duration: 5.219s, episode steps: 838, steps per second: 161, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10126/250000: episode: 15, duration: 13.216s, episode steps: 634, steps per second:  48, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.012153, mae: 1.353028, mean_q: 1.675443, mean_eps: 0.996377
  10457/250000: episode: 16, duration: 21.083s, episode steps: 331, steps per second:  16, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.011432, mae: 1.386444, mean_q: 1.715488, mean_eps: 0.996295
  10857/250000: episode: 17, duration: 22.421s, episode steps: 400, steps per second:  18, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.010742, mae: 1.396411, mean_q: 1.726952, mean_eps: 0.996163
  11307/250000: episode: 18, duration: 25.375s, episode steps: 450, steps per second:  18, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.012732, mae: 1.390221, mean_q: 1.716782, mean_eps: 0.996010
  11972/250000: episode: 19, duration: 39.265s, episode steps: 665, steps per second:  17, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.011635, mae: 1.399194, mean_q: 1.730451, mean_eps: 0.995810
  12545/250000: episode: 20, duration: 33.807s, episode steps: 573, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.011127, mae: 1.414691, mean_q: 1.749527, mean_eps: 0.995587
  13479/250000: episode: 21, duration: 52.178s, episode steps: 934, steps per second:  18, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.011135, mae: 1.395023, mean_q: 1.725806, mean_eps: 0.995316
  14277/250000: episode: 22, duration: 60.569s, episode steps: 798, steps per second:  13, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.010426, mae: 1.399668, mean_q: 1.732577, mean_eps: 0.995004
  14866/250000: episode: 23, duration: 45.562s, episode steps: 589, steps per second:  13, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.009405, mae: 1.394295, mean_q: 1.725989, mean_eps: 0.994754
  15422/250000: episode: 24, duration: 34.791s, episode steps: 556, steps per second:  16, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010272, mae: 1.388524, mean_q: 1.717283, mean_eps: 0.994548
  16062/250000: episode: 25, duration: 35.232s, episode steps: 640, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008720, mae: 1.389406, mean_q: 1.719721, mean_eps: 0.994333
  16579/250000: episode: 26, duration: 31.365s, episode steps: 517, steps per second:  16, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.009910, mae: 1.377919, mean_q: 1.704660, mean_eps: 0.994125
  17221/250000: episode: 27, duration: 35.581s, episode steps: 642, steps per second:  18, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.010431, mae: 1.387616, mean_q: 1.715085, mean_eps: 0.993916
  17989/250000: episode: 28, duration: 46.963s, episode steps: 768, steps per second:  16, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.009053, mae: 1.373703, mean_q: 1.699018, mean_eps: 0.993662
  18841/250000: episode: 29, duration: 49.116s, episode steps: 852, steps per second:  17, episode reward:  7.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.009752, mae: 1.375340, mean_q: 1.701889, mean_eps: 0.993370
  19463/250000: episode: 30, duration: 36.219s, episode steps: 622, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.010255, mae: 1.373103, mean_q: 1.698370, mean_eps: 0.993105
  20100/250000: episode: 31, duration: 36.733s, episode steps: 637, steps per second:  17, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011003, mae: 1.382361, mean_q: 1.710274, mean_eps: 0.992879
  20845/250000: episode: 32, duration: 43.776s, episode steps: 745, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.008307, mae: 1.370092, mean_q: 1.695321, mean_eps: 0.992630
  21512/250000: episode: 33, duration: 40.066s, episode steps: 667, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.008026, mae: 1.377014, mean_q: 1.704155, mean_eps: 0.992376
  22157/250000: episode: 34, duration: 36.754s, episode steps: 645, steps per second:  18, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.008829, mae: 1.373843, mean_q: 1.699314, mean_eps: 0.992140
  22536/250000: episode: 35, duration: 23.115s, episode steps: 379, steps per second:  16, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009566, mae: 1.368100, mean_q: 1.693589, mean_eps: 0.991955
  23209/250000: episode: 36, duration: 37.648s, episode steps: 673, steps per second:  18, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007835, mae: 1.393532, mean_q: 1.725956, mean_eps: 0.991766
  23893/250000: episode: 37, duration: 40.075s, episode steps: 684, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.009548, mae: 1.377073, mean_q: 1.705130, mean_eps: 0.991521
  24845/250000: episode: 38, duration: 54.333s, episode steps: 952, steps per second:  18, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.010292, mae: 1.371238, mean_q: 1.697908, mean_eps: 0.991227
  25365/250000: episode: 39, duration: 29.446s, episode steps: 520, steps per second:  18, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.009986, mae: 1.381693, mean_q: 1.711481, mean_eps: 0.990962
  26004/250000: episode: 40, duration: 37.177s, episode steps: 639, steps per second:  17, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.008520, mae: 1.366576, mean_q: 1.692706, mean_eps: 0.990754
  26634/250000: episode: 41, duration: 35.508s, episode steps: 630, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.008283, mae: 1.368292, mean_q: 1.694546, mean_eps: 0.990526
  27489/250000: episode: 42, duration: 49.970s, episode steps: 855, steps per second:  17, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.009701, mae: 1.394963, mean_q: 1.727838, mean_eps: 0.990258
  28137/250000: episode: 43, duration: 37.757s, episode steps: 648, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.007843, mae: 1.384309, mean_q: 1.717157, mean_eps: 0.989987
  28819/250000: episode: 44, duration: 40.851s, episode steps: 682, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.007944, mae: 1.391765, mean_q: 1.725818, mean_eps: 0.989748
  29741/250000: episode: 45, duration: 53.868s, episode steps: 922, steps per second:  17, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.009294, mae: 1.375196, mean_q: 1.705665, mean_eps: 0.989459
  30422/250000: episode: 46, duration: 39.166s, episode steps: 681, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.010147, mae: 1.396220, mean_q: 1.732113, mean_eps: 0.989170
  31657/250000: episode: 47, duration: 71.515s, episode steps: 1235, steps per second:  17, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.009258, mae: 1.383595, mean_q: 1.717761, mean_eps: 0.988826
  32342/250000: episode: 48, duration: 37.886s, episode steps: 685, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.009913, mae: 1.376927, mean_q: 1.708897, mean_eps: 0.988480
  33553/250000: episode: 49, duration: 73.464s, episode steps: 1211, steps per second:  16, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.008311, mae: 1.376602, mean_q: 1.707418, mean_eps: 0.988139
  34280/250000: episode: 50, duration: 42.406s, episode steps: 727, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.008396, mae: 1.384646, mean_q: 1.716962, mean_eps: 0.987790
  35546/250000: episode: 51, duration: 71.546s, episode steps: 1266, steps per second:  18, episode reward: 14.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.009054, mae: 1.380630, mean_q: 1.712302, mean_eps: 0.987432
  36198/250000: episode: 52, duration: 37.791s, episode steps: 652, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.008676, mae: 1.388411, mean_q: 1.722834, mean_eps: 0.987086
  36637/250000: episode: 53, duration: 24.600s, episode steps: 439, steps per second:  18, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.008876, mae: 1.392815, mean_q: 1.727088, mean_eps: 0.986890
  37284/250000: episode: 54, duration: 36.813s, episode steps: 647, steps per second:  18, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.008032, mae: 1.388628, mean_q: 1.721975, mean_eps: 0.986694
  37857/250000: episode: 55, duration: 31.880s, episode steps: 573, steps per second:  18, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.007723, mae: 1.379780, mean_q: 1.711484, mean_eps: 0.986475
  38536/250000: episode: 56, duration: 42.332s, episode steps: 679, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.009096, mae: 1.378352, mean_q: 1.710917, mean_eps: 0.986249
  39366/250000: episode: 57, duration: 47.201s, episode steps: 830, steps per second:  18, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.009664, mae: 1.394111, mean_q: 1.730265, mean_eps: 0.985978
  39899/250000: episode: 58, duration: 29.875s, episode steps: 533, steps per second:  18, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.008491, mae: 1.373887, mean_q: 1.707220, mean_eps: 0.985732
  40763/250000: episode: 59, duration: 50.405s, episode steps: 864, steps per second:  17, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.008631, mae: 1.376879, mean_q: 1.709846, mean_eps: 0.985481
  41484/250000: episode: 60, duration: 40.075s, episode steps: 721, steps per second:  18, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.008952, mae: 1.375424, mean_q: 1.707088, mean_eps: 0.985196
  42201/250000: episode: 61, duration: 42.210s, episode steps: 717, steps per second:  17, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.007612, mae: 1.382819, mean_q: 1.716561, mean_eps: 0.984937
  42856/250000: episode: 62, duration: 40.231s, episode steps: 655, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.009870, mae: 1.374958, mean_q: 1.706528, mean_eps: 0.984690
  43914/250000: episode: 63, duration: 64.457s, episode steps: 1058, steps per second:  16, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.008662, mae: 1.373182, mean_q: 1.704859, mean_eps: 0.984382
  44512/250000: episode: 64, duration: 34.863s, episode steps: 598, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.007678, mae: 1.366490, mean_q: 1.697408, mean_eps: 0.984084
  45138/250000: episode: 65, duration: 36.807s, episode steps: 626, steps per second:  17, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.008430, mae: 1.371436, mean_q: 1.703583, mean_eps: 0.983863
  45642/250000: episode: 66, duration: 28.354s, episode steps: 504, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.008146, mae: 1.372614, mean_q: 1.705129, mean_eps: 0.983660
  45982/250000: episode: 67, duration: 18.810s, episode steps: 340, steps per second:  18, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.007042, mae: 1.371592, mean_q: 1.704874, mean_eps: 0.983508
  46788/250000: episode: 68, duration: 46.566s, episode steps: 806, steps per second:  17, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.008709, mae: 1.379998, mean_q: 1.714360, mean_eps: 0.983302
  47311/250000: episode: 69, duration: 30.188s, episode steps: 523, steps per second:  17, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.009576, mae: 1.363075, mean_q: 1.692241, mean_eps: 0.983063
  48090/250000: episode: 70, duration: 44.486s, episode steps: 779, steps per second:  18, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.008686, mae: 1.369257, mean_q: 1.699284, mean_eps: 0.982828
  48881/250000: episode: 71, duration: 46.380s, episode steps: 791, steps per second:  17, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.008243, mae: 1.382642, mean_q: 1.717356, mean_eps: 0.982545
  49361/250000: episode: 72, duration: 27.779s, episode steps: 480, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.009814, mae: 1.379455, mean_q: 1.711560, mean_eps: 0.982316
  49875/250000: episode: 73, duration: 29.773s, episode steps: 514, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.009208, mae: 1.368844, mean_q: 1.698701, mean_eps: 0.982138
  50554/250000: episode: 74, duration: 38.475s, episode steps: 679, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.010968, mae: 1.422188, mean_q: 1.763451, mean_eps: 0.981923
  51102/250000: episode: 75, duration: 32.114s, episode steps: 548, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.010548, mae: 1.441205, mean_q: 1.786547, mean_eps: 0.981702
  51593/250000: episode: 76, duration: 28.988s, episode steps: 491, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.010829, mae: 1.443648, mean_q: 1.787746, mean_eps: 0.981515
  52098/250000: episode: 77, duration: 29.466s, episode steps: 505, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.012521, mae: 1.466860, mean_q: 1.818160, mean_eps: 0.981335
  52661/250000: episode: 78, duration: 32.785s, episode steps: 563, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010550, mae: 1.440866, mean_q: 1.785169, mean_eps: 0.981143
  53334/250000: episode: 79, duration: 42.447s, episode steps: 673, steps per second:  16, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.010618, mae: 1.458486, mean_q: 1.808137, mean_eps: 0.980921
  54365/250000: episode: 80, duration: 59.148s, episode steps: 1031, steps per second:  17, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.010821, mae: 1.439268, mean_q: 1.784621, mean_eps: 0.980614
  55206/250000: episode: 81, duration: 49.662s, episode steps: 841, steps per second:  17, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.010188, mae: 1.444331, mean_q: 1.789186, mean_eps: 0.980277
  55651/250000: episode: 82, duration: 26.240s, episode steps: 445, steps per second:  17, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.010509, mae: 1.457442, mean_q: 1.805045, mean_eps: 0.980046
  56048/250000: episode: 83, duration: 22.340s, episode steps: 397, steps per second:  18, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.009205, mae: 1.466205, mean_q: 1.818845, mean_eps: 0.979895
  56767/250000: episode: 84, duration: 41.991s, episode steps: 719, steps per second:  17, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.010726, mae: 1.457500, mean_q: 1.808630, mean_eps: 0.979694
  57117/250000: episode: 85, duration: 20.161s, episode steps: 350, steps per second:  17, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.009420, mae: 1.482268, mean_q: 1.839948, mean_eps: 0.979501
  57845/250000: episode: 86, duration: 45.258s, episode steps: 728, steps per second:  16, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010197, mae: 1.448058, mean_q: 1.794632, mean_eps: 0.979306
  58468/250000: episode: 87, duration: 34.839s, episode steps: 623, steps per second:  18, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.008686, mae: 1.440071, mean_q: 1.785298, mean_eps: 0.979064
  58869/250000: episode: 88, duration: 23.738s, episode steps: 401, steps per second:  17, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.009457, mae: 1.443072, mean_q: 1.789681, mean_eps: 0.978880
  59572/250000: episode: 89, duration: 43.725s, episode steps: 703, steps per second:  16, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.010699, mae: 1.456159, mean_q: 1.803628, mean_eps: 0.978681
  60124/250000: episode: 90, duration: 32.334s, episode steps: 552, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.009940, mae: 1.460392, mean_q: 1.810291, mean_eps: 0.978455
  60562/250000: episode: 91, duration: 24.457s, episode steps: 438, steps per second:  18, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.008989, mae: 1.460510, mean_q: 1.808420, mean_eps: 0.978277
  61602/250000: episode: 92, duration: 60.239s, episode steps: 1040, steps per second:  17, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.009911, mae: 1.451274, mean_q: 1.798290, mean_eps: 0.978010
  62674/250000: episode: 93, duration: 61.119s, episode steps: 1072, steps per second:  18, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.010381, mae: 1.442325, mean_q: 1.786902, mean_eps: 0.977630
  63141/250000: episode: 94, duration: 29.364s, episode steps: 467, steps per second:  16, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.010586, mae: 1.442903, mean_q: 1.788350, mean_eps: 0.977353
  63778/250000: episode: 95, duration: 39.481s, episode steps: 637, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.010005, mae: 1.441875, mean_q: 1.787124, mean_eps: 0.977154
  64444/250000: episode: 96, duration: 39.902s, episode steps: 666, steps per second:  17, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.009784, mae: 1.458890, mean_q: 1.808164, mean_eps: 0.976920
  65116/250000: episode: 97, duration: 38.082s, episode steps: 672, steps per second:  18, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.010635, mae: 1.462354, mean_q: 1.810087, mean_eps: 0.976680
  65832/250000: episode: 98, duration: 165.023s, episode steps: 716, steps per second:   4, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.009974, mae: 1.447312, mean_q: 1.793869, mean_eps: 0.976430
  66473/250000: episode: 99, duration: 91.058s, episode steps: 641, steps per second:   7, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.008898, mae: 1.439970, mean_q: 1.784886, mean_eps: 0.976185
  66881/250000: episode: 100, duration: 32.882s, episode steps: 408, steps per second:  12, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.010559, mae: 1.421873, mean_q: 1.763288, mean_eps: 0.975996
  68378/250000: episode: 101, duration: 133.262s, episode steps: 1497, steps per second:  11, episode reward: 14.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.009752, mae: 1.451229, mean_q: 1.797654, mean_eps: 0.975653
  68811/250000: episode: 102, duration: 32.156s, episode steps: 433, steps per second:  13, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.009178, mae: 1.433070, mean_q: 1.774524, mean_eps: 0.975306
  69604/250000: episode: 103, duration: 50.273s, episode steps: 793, steps per second:  16, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.010095, mae: 1.437343, mean_q: 1.779259, mean_eps: 0.975086
  70021/250000: episode: 104, duration: 26.322s, episode steps: 417, steps per second:  16, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.009891, mae: 1.437573, mean_q: 1.780647, mean_eps: 0.974868
  70507/250000: episode: 105, duration: 33.333s, episode steps: 486, steps per second:  15, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.011033, mae: 1.438105, mean_q: 1.781690, mean_eps: 0.974705
  71312/250000: episode: 106, duration: 52.043s, episode steps: 805, steps per second:  15, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.010243, mae: 1.461443, mean_q: 1.811227, mean_eps: 0.974473
  71717/250000: episode: 107, duration: 27.019s, episode steps: 405, steps per second:  15, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.009943, mae: 1.488074, mean_q: 1.845062, mean_eps: 0.974255
  72485/250000: episode: 108, duration: 46.992s, episode steps: 768, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.009485, mae: 1.462117, mean_q: 1.812108, mean_eps: 0.974043
  73003/250000: episode: 109, duration: 29.880s, episode steps: 518, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.008741, mae: 1.470319, mean_q: 1.821366, mean_eps: 0.973812
  73793/250000: episode: 110, duration: 46.605s, episode steps: 790, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.008839, mae: 1.458368, mean_q: 1.808213, mean_eps: 0.973577
  74622/250000: episode: 111, duration: 51.733s, episode steps: 829, steps per second:  16, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.008402, mae: 1.450530, mean_q: 1.798537, mean_eps: 0.973285
  75147/250000: episode: 112, duration: 31.497s, episode steps: 525, steps per second:  17, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.010131, mae: 1.453797, mean_q: 1.798574, mean_eps: 0.973042
  75817/250000: episode: 113, duration: 40.597s, episode steps: 670, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.010251, mae: 1.451832, mean_q: 1.797767, mean_eps: 0.972826
  77187/250000: episode: 114, duration: 81.297s, episode steps: 1370, steps per second:  17, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.009457, mae: 1.452135, mean_q: 1.798290, mean_eps: 0.972459
  77724/250000: episode: 115, duration: 34.849s, episode steps: 537, steps per second:  15, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.009872, mae: 1.441885, mean_q: 1.786847, mean_eps: 0.972117
  78752/250000: episode: 116, duration: 65.090s, episode steps: 1028, steps per second:  16, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.009489, mae: 1.461197, mean_q: 1.809210, mean_eps: 0.971835
  79440/250000: episode: 117, duration: 40.032s, episode steps: 688, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010075, mae: 1.461439, mean_q: 1.812248, mean_eps: 0.971526
  80500/250000: episode: 118, duration: 61.470s, episode steps: 1060, steps per second:  17, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.009070, mae: 1.454092, mean_q: 1.800546, mean_eps: 0.971212
  81237/250000: episode: 119, duration: 44.116s, episode steps: 737, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.010867, mae: 1.435626, mean_q: 1.778605, mean_eps: 0.970888
  81745/250000: episode: 120, duration: 30.585s, episode steps: 508, steps per second:  17, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.010542, mae: 1.469865, mean_q: 1.822064, mean_eps: 0.970663
  82428/250000: episode: 121, duration: 43.339s, episode steps: 683, steps per second:  16, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.008876, mae: 1.461487, mean_q: 1.811360, mean_eps: 0.970449
  82907/250000: episode: 122, duration: 28.608s, episode steps: 479, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.010125, mae: 1.453076, mean_q: 1.799806, mean_eps: 0.970240
  83653/250000: episode: 123, duration: 43.214s, episode steps: 746, steps per second:  17, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.010019, mae: 1.463500, mean_q: 1.813606, mean_eps: 0.970019
  84287/250000: episode: 124, duration: 37.708s, episode steps: 634, steps per second:  17, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.010183, mae: 1.445498, mean_q: 1.791276, mean_eps: 0.969771
  84671/250000: episode: 125, duration: 22.579s, episode steps: 384, steps per second:  17, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.007410, mae: 1.436023, mean_q: 1.781800, mean_eps: 0.969588
  85775/250000: episode: 126, duration: 65.792s, episode steps: 1104, steps per second:  17, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.009769, mae: 1.449700, mean_q: 1.795628, mean_eps: 0.969320
  86162/250000: episode: 127, duration: 22.325s, episode steps: 387, steps per second:  17, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.011979, mae: 1.466541, mean_q: 1.815468, mean_eps: 0.969052
  86820/250000: episode: 128, duration: 39.722s, episode steps: 658, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010684, mae: 1.444003, mean_q: 1.791310, mean_eps: 0.968864
  87656/250000: episode: 129, duration: 51.673s, episode steps: 836, steps per second:  16, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.009164, mae: 1.449453, mean_q: 1.794975, mean_eps: 0.968595
  88377/250000: episode: 130, duration: 40.367s, episode steps: 721, steps per second:  18, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.008753, mae: 1.459816, mean_q: 1.808586, mean_eps: 0.968314
  89373/250000: episode: 131, duration: 57.076s, episode steps: 996, steps per second:  17, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.009836, mae: 1.462696, mean_q: 1.810377, mean_eps: 0.968005
  89937/250000: episode: 132, duration: 33.039s, episode steps: 564, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.009964, mae: 1.459070, mean_q: 1.808565, mean_eps: 0.967724
  90661/250000: episode: 133, duration: 41.221s, episode steps: 724, steps per second:  18, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.009927, mae: 1.459094, mean_q: 1.809443, mean_eps: 0.967492
  91481/250000: episode: 134, duration: 47.120s, episode steps: 820, steps per second:  17, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.011095, mae: 1.473118, mean_q: 1.825138, mean_eps: 0.967214
  92250/250000: episode: 135, duration: 46.835s, episode steps: 769, steps per second:  16, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.009471, mae: 1.454379, mean_q: 1.802858, mean_eps: 0.966928
  93070/250000: episode: 136, duration: 47.695s, episode steps: 820, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010827, mae: 1.456470, mean_q: 1.804097, mean_eps: 0.966642
  93687/250000: episode: 137, duration: 35.831s, episode steps: 617, steps per second:  17, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.009768, mae: 1.482539, mean_q: 1.836656, mean_eps: 0.966384
  94444/250000: episode: 138, duration: 45.094s, episode steps: 757, steps per second:  17, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.010065, mae: 1.451941, mean_q: 1.799052, mean_eps: 0.966137
  95238/250000: episode: 139, duration: 45.314s, episode steps: 794, steps per second:  18, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.010215, mae: 1.454981, mean_q: 1.803872, mean_eps: 0.965858
  95858/250000: episode: 140, duration: 37.473s, episode steps: 620, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.009766, mae: 1.445939, mean_q: 1.791750, mean_eps: 0.965603
  96724/250000: episode: 141, duration: 50.687s, episode steps: 866, steps per second:  17, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.012139, mae: 1.458465, mean_q: 1.807409, mean_eps: 0.965336
  97226/250000: episode: 142, duration: 28.688s, episode steps: 502, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.008993, mae: 1.461393, mean_q: 1.811739, mean_eps: 0.965089
  98506/250000: episode: 143, duration: 74.030s, episode steps: 1280, steps per second:  17, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.009115, mae: 1.456706, mean_q: 1.803652, mean_eps: 0.964768
  99224/250000: episode: 144, duration: 43.928s, episode steps: 718, steps per second:  16, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.009578, mae: 1.452031, mean_q: 1.798493, mean_eps: 0.964409
 100033/250000: episode: 145, duration: 46.956s, episode steps: 809, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.009020, mae: 1.457895, mean_q: 1.806795, mean_eps: 0.964134
 100675/250000: episode: 146, duration: 36.458s, episode steps: 642, steps per second:  18, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.011507, mae: 1.503946, mean_q: 1.861336, mean_eps: 0.963873
 101315/250000: episode: 147, duration: 38.892s, episode steps: 640, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012279, mae: 1.513677, mean_q: 1.872688, mean_eps: 0.963642
 102360/250000: episode: 148, duration: 60.492s, episode steps: 1045, steps per second:  17, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.010925, mae: 1.491544, mean_q: 1.847651, mean_eps: 0.963339
 103138/250000: episode: 149, duration: 44.249s, episode steps: 778, steps per second:  18, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.011163, mae: 1.502276, mean_q: 1.858278, mean_eps: 0.963011
 103594/250000: episode: 150, duration: 27.358s, episode steps: 456, steps per second:  17, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.012126, mae: 1.506201, mean_q: 1.864980, mean_eps: 0.962788
 104706/250000: episode: 151, duration: 64.414s, episode steps: 1112, steps per second:  17, episode reward:  9.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010423, mae: 1.498474, mean_q: 1.856492, mean_eps: 0.962506
 105340/250000: episode: 152, duration: 35.955s, episode steps: 634, steps per second:  18, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.011094, mae: 1.492230, mean_q: 1.850487, mean_eps: 0.962192
 106554/250000: episode: 153, duration: 71.044s, episode steps: 1214, steps per second:  17, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.010167, mae: 1.505151, mean_q: 1.864231, mean_eps: 0.961859
 107217/250000: episode: 154, duration: 39.493s, episode steps: 663, steps per second:  17, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.010529, mae: 1.498335, mean_q: 1.856441, mean_eps: 0.961521
 107897/250000: episode: 155, duration: 41.840s, episode steps: 680, steps per second:  16, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.010657, mae: 1.509241, mean_q: 1.869993, mean_eps: 0.961279
 108339/250000: episode: 156, duration: 25.977s, episode steps: 442, steps per second:  17, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.010839, mae: 1.521462, mean_q: 1.883839, mean_eps: 0.961078
 109072/250000: episode: 157, duration: 44.052s, episode steps: 733, steps per second:  17, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.009540, mae: 1.506125, mean_q: 1.866693, mean_eps: 0.960867
 109830/250000: episode: 158, duration: 43.772s, episode steps: 758, steps per second:  17, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.010131, mae: 1.498590, mean_q: 1.854698, mean_eps: 0.960598
 110910/250000: episode: 159, duration: 63.742s, episode steps: 1080, steps per second:  17, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.010363, mae: 1.497078, mean_q: 1.853125, mean_eps: 0.960267
 111301/250000: episode: 160, duration: 24.608s, episode steps: 391, steps per second:  16, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011469, mae: 1.515213, mean_q: 1.873847, mean_eps: 0.960002
 111815/250000: episode: 161, duration: 28.897s, episode steps: 514, steps per second:  18, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.010045, mae: 1.503706, mean_q: 1.859087, mean_eps: 0.959839
 112345/250000: episode: 162, duration: 32.029s, episode steps: 530, steps per second:  17, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.010925, mae: 1.498363, mean_q: 1.853368, mean_eps: 0.959651
 112843/250000: episode: 163, duration: 28.391s, episode steps: 498, steps per second:  18, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.009919, mae: 1.530208, mean_q: 1.895507, mean_eps: 0.959466
 113453/250000: episode: 164, duration: 36.352s, episode steps: 610, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.009847, mae: 1.508628, mean_q: 1.866176, mean_eps: 0.959267
 114231/250000: episode: 165, duration: 44.557s, episode steps: 778, steps per second:  17, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.011128, mae: 1.511583, mean_q: 1.871524, mean_eps: 0.959017
 114893/250000: episode: 166, duration: 40.231s, episode steps: 662, steps per second:  16, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.009812, mae: 1.518977, mean_q: 1.879585, mean_eps: 0.958758
 115376/250000: episode: 167, duration: 27.091s, episode steps: 483, steps per second:  18, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.009134, mae: 1.520752, mean_q: 1.882748, mean_eps: 0.958552
 115898/250000: episode: 168, duration: 33.222s, episode steps: 522, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.009260, mae: 1.493416, mean_q: 1.850361, mean_eps: 0.958371
 116261/250000: episode: 169, duration: 21.734s, episode steps: 363, steps per second:  17, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011317, mae: 1.510336, mean_q: 1.869987, mean_eps: 0.958211
 117003/250000: episode: 170, duration: 44.130s, episode steps: 742, steps per second:  17, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.011375, mae: 1.502606, mean_q: 1.860639, mean_eps: 0.958012
 117524/250000: episode: 171, duration: 29.567s, episode steps: 521, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.009668, mae: 1.498921, mean_q: 1.857129, mean_eps: 0.957786
 118306/250000: episode: 172, duration: 45.810s, episode steps: 782, steps per second:  17, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.010063, mae: 1.513144, mean_q: 1.872158, mean_eps: 0.957551
 118982/250000: episode: 173, duration: 40.042s, episode steps: 676, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.011281, mae: 1.513728, mean_q: 1.872375, mean_eps: 0.957288
 120074/250000: episode: 174, duration: 64.476s, episode steps: 1092, steps per second:  17, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011144, mae: 1.507884, mean_q: 1.866753, mean_eps: 0.956970
 121159/250000: episode: 175, duration: 62.576s, episode steps: 1085, steps per second:  17, episode reward:  9.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.009824, mae: 1.510475, mean_q: 1.868425, mean_eps: 0.956578
 121657/250000: episode: 176, duration: 29.762s, episode steps: 498, steps per second:  17, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.009421, mae: 1.519932, mean_q: 1.881266, mean_eps: 0.956293
 122030/250000: episode: 177, duration: 21.175s, episode steps: 373, steps per second:  18, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010591, mae: 1.526602, mean_q: 1.889021, mean_eps: 0.956136
 122419/250000: episode: 178, duration: 23.194s, episode steps: 389, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.011436, mae: 1.494236, mean_q: 1.849295, mean_eps: 0.955999
 123803/250000: episode: 179, duration: 81.459s, episode steps: 1384, steps per second:  17, episode reward: 20.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.010695, mae: 1.510949, mean_q: 1.870671, mean_eps: 0.955680
 124668/250000: episode: 180, duration: 51.139s, episode steps: 865, steps per second:  17, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.010910, mae: 1.508905, mean_q: 1.867413, mean_eps: 0.955276
 125848/250000: episode: 181, duration: 69.635s, episode steps: 1180, steps per second:  17, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.010955, mae: 1.511305, mean_q: 1.872345, mean_eps: 0.954908
 126354/250000: episode: 182, duration: 30.102s, episode steps: 506, steps per second:  17, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.010403, mae: 1.521485, mean_q: 1.883249, mean_eps: 0.954604
 126756/250000: episode: 183, duration: 23.110s, episode steps: 402, steps per second:  17, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.009951, mae: 1.514574, mean_q: 1.877388, mean_eps: 0.954441
 127399/250000: episode: 184, duration: 38.388s, episode steps: 643, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.010320, mae: 1.503260, mean_q: 1.861772, mean_eps: 0.954253
 128122/250000: episode: 185, duration: 42.655s, episode steps: 723, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.009762, mae: 1.509059, mean_q: 1.870992, mean_eps: 0.954006
 129168/250000: episode: 186, duration: 61.356s, episode steps: 1046, steps per second:  17, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.008834, mae: 1.504056, mean_q: 1.863167, mean_eps: 0.953688
 129799/250000: episode: 187, duration: 36.945s, episode steps: 631, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.009864, mae: 1.506988, mean_q: 1.865410, mean_eps: 0.953386
 130264/250000: episode: 188, duration: 28.096s, episode steps: 465, steps per second:  17, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.010448, mae: 1.531892, mean_q: 1.893928, mean_eps: 0.953189
 130790/250000: episode: 189, duration: 30.580s, episode steps: 526, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.010483, mae: 1.530110, mean_q: 1.895124, mean_eps: 0.953011
 131424/250000: episode: 190, duration: 38.280s, episode steps: 634, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.009884, mae: 1.486687, mean_q: 1.840371, mean_eps: 0.952802
 131943/250000: episode: 191, duration: 29.321s, episode steps: 519, steps per second:  18, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.011054, mae: 1.514449, mean_q: 1.873837, mean_eps: 0.952594
 132604/250000: episode: 192, duration: 39.564s, episode steps: 661, steps per second:  17, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.010320, mae: 1.517686, mean_q: 1.878628, mean_eps: 0.952382
 133271/250000: episode: 193, duration: 37.831s, episode steps: 667, steps per second:  18, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.011546, mae: 1.495498, mean_q: 1.849599, mean_eps: 0.952143
 134080/250000: episode: 194, duration: 48.179s, episode steps: 809, steps per second:  17, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.010061, mae: 1.517127, mean_q: 1.876172, mean_eps: 0.951877
 134754/250000: episode: 195, duration: 40.464s, episode steps: 674, steps per second:  17, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.008996, mae: 1.525872, mean_q: 1.888131, mean_eps: 0.951610
 135415/250000: episode: 196, duration: 38.142s, episode steps: 661, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.009696, mae: 1.502500, mean_q: 1.860675, mean_eps: 0.951370
 135817/250000: episode: 197, duration: 24.931s, episode steps: 402, steps per second:  16, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.011184, mae: 1.522721, mean_q: 1.887295, mean_eps: 0.951178
 136407/250000: episode: 198, duration: 35.498s, episode steps: 590, steps per second:  17, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.011073, mae: 1.527763, mean_q: 1.892618, mean_eps: 0.951000
 136929/250000: episode: 199, duration: 30.528s, episode steps: 522, steps per second:  17, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.010231, mae: 1.518243, mean_q: 1.877744, mean_eps: 0.950800
 137526/250000: episode: 200, duration: 34.919s, episode steps: 597, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010682, mae: 1.506665, mean_q: 1.863501, mean_eps: 0.950598
 138875/250000: episode: 201, duration: 78.738s, episode steps: 1349, steps per second:  17, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010654, mae: 1.522877, mean_q: 1.884035, mean_eps: 0.950248
 139671/250000: episode: 202, duration: 46.965s, episode steps: 796, steps per second:  17, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.009736, mae: 1.502630, mean_q: 1.859481, mean_eps: 0.949862
 140272/250000: episode: 203, duration: 35.548s, episode steps: 601, steps per second:  17, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.010594, mae: 1.523989, mean_q: 1.884809, mean_eps: 0.949611
 140959/250000: episode: 204, duration: 40.740s, episode steps: 687, steps per second:  17, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.010361, mae: 1.498868, mean_q: 1.854751, mean_eps: 0.949379
 141337/250000: episode: 205, duration: 22.020s, episode steps: 378, steps per second:  17, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.009983, mae: 1.522271, mean_q: 1.883236, mean_eps: 0.949187
 141841/250000: episode: 206, duration: 30.174s, episode steps: 504, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.009159, mae: 1.522868, mean_q: 1.885723, mean_eps: 0.949028
 142540/250000: episode: 207, duration: 40.815s, episode steps: 699, steps per second:  17, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.009935, mae: 1.524698, mean_q: 1.886195, mean_eps: 0.948812
 143020/250000: episode: 208, duration: 28.415s, episode steps: 480, steps per second:  17, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.010085, mae: 1.511830, mean_q: 1.872248, mean_eps: 0.948600
 143647/250000: episode: 209, duration: 37.427s, episode steps: 627, steps per second:  17, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.011469, mae: 1.509908, mean_q: 1.868136, mean_eps: 0.948400
 144137/250000: episode: 210, duration: 28.800s, episode steps: 490, steps per second:  17, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.010572, mae: 1.519887, mean_q: 1.879848, mean_eps: 0.948199
 144915/250000: episode: 211, duration: 46.698s, episode steps: 778, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.009269, mae: 1.520392, mean_q: 1.881618, mean_eps: 0.947971
 145696/250000: episode: 212, duration: 45.249s, episode steps: 781, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.009918, mae: 1.508720, mean_q: 1.866647, mean_eps: 0.947691
 146470/250000: episode: 213, duration: 47.575s, episode steps: 774, steps per second:  16, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.009780, mae: 1.511824, mean_q: 1.871747, mean_eps: 0.947410
 146954/250000: episode: 214, duration: 28.525s, episode steps: 484, steps per second:  17, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.009528, mae: 1.506985, mean_q: 1.865011, mean_eps: 0.947184
 147672/250000: episode: 215, duration: 42.631s, episode steps: 718, steps per second:  17, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.009816, mae: 1.511254, mean_q: 1.868539, mean_eps: 0.946968
 148207/250000: episode: 216, duration: 31.971s, episode steps: 535, steps per second:  17, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.009839, mae: 1.494366, mean_q: 1.850943, mean_eps: 0.946742
 149005/250000: episode: 217, duration: 46.757s, episode steps: 798, steps per second:  17, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.010086, mae: 1.519119, mean_q: 1.880587, mean_eps: 0.946502
 149615/250000: episode: 218, duration: 36.861s, episode steps: 610, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.009059, mae: 1.531346, mean_q: 1.895502, mean_eps: 0.946248
 149972/250000: episode: 219, duration: 20.539s, episode steps: 357, steps per second:  17, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.011648, mae: 1.517406, mean_q: 1.876160, mean_eps: 0.946075
 150943/250000: episode: 220, duration: 58.847s, episode steps: 971, steps per second:  17, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.011133, mae: 1.537936, mean_q: 1.900009, mean_eps: 0.945836
 151446/250000: episode: 221, duration: 30.105s, episode steps: 503, steps per second:  17, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.011758, mae: 1.530907, mean_q: 1.892920, mean_eps: 0.945570
 152033/250000: episode: 222, duration: 34.651s, episode steps: 587, steps per second:  17, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.012917, mae: 1.557623, mean_q: 1.926869, mean_eps: 0.945374
 152431/250000: episode: 223, duration: 22.967s, episode steps: 398, steps per second:  17, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.011468, mae: 1.548709, mean_q: 1.917172, mean_eps: 0.945196
 152947/250000: episode: 224, duration: 33.026s, episode steps: 516, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.011624, mae: 1.544979, mean_q: 1.911528, mean_eps: 0.945032
 153585/250000: episode: 225, duration: 38.614s, episode steps: 638, steps per second:  17, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.011450, mae: 1.549352, mean_q: 1.916307, mean_eps: 0.944824
 154657/250000: episode: 226, duration: 63.550s, episode steps: 1072, steps per second:  17, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010687, mae: 1.555317, mean_q: 1.923601, mean_eps: 0.944516
 155216/250000: episode: 227, duration: 33.229s, episode steps: 559, steps per second:  17, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.011346, mae: 1.576157, mean_q: 1.949683, mean_eps: 0.944223
 155774/250000: episode: 228, duration: 32.616s, episode steps: 558, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.011396, mae: 1.571665, mean_q: 1.945133, mean_eps: 0.944022
 156614/250000: episode: 229, duration: 49.696s, episode steps: 840, steps per second:  17, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.011610, mae: 1.562008, mean_q: 1.932708, mean_eps: 0.943770
 157023/250000: episode: 230, duration: 24.030s, episode steps: 409, steps per second:  17, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.011519, mae: 1.549534, mean_q: 1.915340, mean_eps: 0.943546
 157560/250000: episode: 231, duration: 31.726s, episode steps: 537, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.011100, mae: 1.556283, mean_q: 1.924636, mean_eps: 0.943376
 158211/250000: episode: 232, duration: 39.199s, episode steps: 651, steps per second:  17, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.008750, mae: 1.535432, mean_q: 1.900185, mean_eps: 0.943162
 159434/250000: episode: 233, duration: 71.850s, episode steps: 1223, steps per second:  17, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.011804, mae: 1.553192, mean_q: 1.921757, mean_eps: 0.942824
 159981/250000: episode: 234, duration: 31.797s, episode steps: 547, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012594, mae: 1.549653, mean_q: 1.917612, mean_eps: 0.942505
 160563/250000: episode: 235, duration: 35.600s, episode steps: 582, steps per second:  16, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.009887, mae: 1.564405, mean_q: 1.936212, mean_eps: 0.942302
 161256/250000: episode: 236, duration: 40.152s, episode steps: 693, steps per second:  17, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012186, mae: 1.559260, mean_q: 1.928024, mean_eps: 0.942073
 161651/250000: episode: 237, duration: 25.115s, episode steps: 395, steps per second:  16, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010293, mae: 1.550794, mean_q: 1.922185, mean_eps: 0.941877
 162656/250000: episode: 238, duration: 58.718s, episode steps: 1005, steps per second:  17, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012569, mae: 1.552968, mean_q: 1.922051, mean_eps: 0.941625
 163178/250000: episode: 239, duration: 30.771s, episode steps: 522, steps per second:  17, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.011965, mae: 1.556115, mean_q: 1.926937, mean_eps: 0.941350
 164433/250000: episode: 240, duration: 74.164s, episode steps: 1255, steps per second:  17, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.011192, mae: 1.548083, mean_q: 1.917302, mean_eps: 0.941030
 164767/250000: episode: 241, duration: 19.327s, episode steps: 334, steps per second:  17, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.009565, mae: 1.523095, mean_q: 1.885091, mean_eps: 0.940744
 165748/250000: episode: 242, duration: 58.719s, episode steps: 981, steps per second:  17, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010848, mae: 1.543818, mean_q: 1.911581, mean_eps: 0.940508
 166639/250000: episode: 243, duration: 53.437s, episode steps: 891, steps per second:  17, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.010421, mae: 1.543241, mean_q: 1.909364, mean_eps: 0.940171
 167035/250000: episode: 244, duration: 24.310s, episode steps: 396, steps per second:  16, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.010976, mae: 1.582921, mean_q: 1.959668, mean_eps: 0.939939
 168178/250000: episode: 245, duration: 68.594s, episode steps: 1143, steps per second:  17, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.011248, mae: 1.548695, mean_q: 1.913676, mean_eps: 0.939662
 168790/250000: episode: 246, duration: 36.509s, episode steps: 612, steps per second:  17, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.012838, mae: 1.565071, mean_q: 1.934178, mean_eps: 0.939346
 169557/250000: episode: 247, duration: 46.278s, episode steps: 767, steps per second:  17, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011766, mae: 1.553154, mean_q: 1.919550, mean_eps: 0.939097
 170252/250000: episode: 248, duration: 41.732s, episode steps: 695, steps per second:  17, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.010430, mae: 1.550787, mean_q: 1.917138, mean_eps: 0.938835
 170626/250000: episode: 249, duration: 23.417s, episode steps: 374, steps per second:  16, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.012179, mae: 1.552572, mean_q: 1.919761, mean_eps: 0.938642
 171422/250000: episode: 250, duration: 47.369s, episode steps: 796, steps per second:  17, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011096, mae: 1.538094, mean_q: 1.902853, mean_eps: 0.938431
 172145/250000: episode: 251, duration: 42.732s, episode steps: 723, steps per second:  17, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.012201, mae: 1.560238, mean_q: 1.930257, mean_eps: 0.938158
 172695/250000: episode: 252, duration: 32.827s, episode steps: 550, steps per second:  17, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.010296, mae: 1.546827, mean_q: 1.913306, mean_eps: 0.937929
 173390/250000: episode: 253, duration: 40.357s, episode steps: 695, steps per second:  17, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.012227, mae: 1.566978, mean_q: 1.937573, mean_eps: 0.937705
 174276/250000: episode: 254, duration: 52.992s, episode steps: 886, steps per second:  17, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.011282, mae: 1.562501, mean_q: 1.933837, mean_eps: 0.937420
 175240/250000: episode: 255, duration: 57.156s, episode steps: 964, steps per second:  17, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011681, mae: 1.544208, mean_q: 1.912571, mean_eps: 0.937088
 176086/250000: episode: 256, duration: 50.947s, episode steps: 846, steps per second:  17, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010295, mae: 1.552615, mean_q: 1.920713, mean_eps: 0.936762
 176643/250000: episode: 257, duration: 35.542s, episode steps: 557, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.011001, mae: 1.566164, mean_q: 1.939001, mean_eps: 0.936509
 177638/250000: episode: 258, duration: 62.387s, episode steps: 995, steps per second:  16, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.010368, mae: 1.552274, mean_q: 1.921485, mean_eps: 0.936230
 178125/250000: episode: 259, duration: 31.229s, episode steps: 487, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.012050, mae: 1.570741, mean_q: 1.940534, mean_eps: 0.935962
 178771/250000: episode: 260, duration: 40.071s, episode steps: 646, steps per second:  16, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.011261, mae: 1.580462, mean_q: 1.954915, mean_eps: 0.935759
 179629/250000: episode: 261, duration: 53.549s, episode steps: 858, steps per second:  16, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.010881, mae: 1.555211, mean_q: 1.920807, mean_eps: 0.935488
 180271/250000: episode: 262, duration: 42.118s, episode steps: 642, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.010875, mae: 1.543390, mean_q: 1.908286, mean_eps: 0.935218
 180614/250000: episode: 263, duration: 28.613s, episode steps: 343, steps per second:  12, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011301, mae: 1.553306, mean_q: 1.919482, mean_eps: 0.935041
 181147/250000: episode: 264, duration: 37.484s, episode steps: 533, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.010798, mae: 1.528958, mean_q: 1.890016, mean_eps: 0.934883
 181826/250000: episode: 265, duration: 42.199s, episode steps: 679, steps per second:  16, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.009543, mae: 1.541364, mean_q: 1.906896, mean_eps: 0.934665
 182291/250000: episode: 266, duration: 28.050s, episode steps: 465, steps per second:  17, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.009888, mae: 1.539989, mean_q: 1.905416, mean_eps: 0.934459
 182732/250000: episode: 267, duration: 25.356s, episode steps: 441, steps per second:  17, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.010044, mae: 1.554494, mean_q: 1.924029, mean_eps: 0.934296
 183127/250000: episode: 268, duration: 23.950s, episode steps: 395, steps per second:  16, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.010314, mae: 1.590607, mean_q: 1.969841, mean_eps: 0.934146
 183809/250000: episode: 269, duration: 40.590s, episode steps: 682, steps per second:  17, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.010989, mae: 1.553196, mean_q: 1.920228, mean_eps: 0.933952
 184596/250000: episode: 270, duration: 47.550s, episode steps: 787, steps per second:  17, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.009922, mae: 1.561971, mean_q: 1.930567, mean_eps: 0.933687
 185365/250000: episode: 271, duration: 49.424s, episode steps: 769, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010631, mae: 1.548839, mean_q: 1.915384, mean_eps: 0.933407
 186109/250000: episode: 272, duration: 47.069s, episode steps: 744, steps per second:  16, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.010326, mae: 1.567658, mean_q: 1.936734, mean_eps: 0.933134
 186798/250000: episode: 273, duration: 42.110s, episode steps: 689, steps per second:  16, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.011776, mae: 1.559108, mean_q: 1.924505, mean_eps: 0.932877
 187355/250000: episode: 274, duration: 32.981s, episode steps: 557, steps per second:  17, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.011304, mae: 1.557783, mean_q: 1.926954, mean_eps: 0.932653
 188005/250000: episode: 275, duration: 38.887s, episode steps: 650, steps per second:  17, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.011870, mae: 1.542171, mean_q: 1.906857, mean_eps: 0.932435
 188643/250000: episode: 276, duration: 39.160s, episode steps: 638, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011092, mae: 1.574498, mean_q: 1.947783, mean_eps: 0.932203
 189230/250000: episode: 277, duration: 46.926s, episode steps: 587, steps per second:  13, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.012139, mae: 1.571184, mean_q: 1.943952, mean_eps: 0.931983
 190106/250000: episode: 278, duration: 182.259s, episode steps: 876, steps per second:   5, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.011054, mae: 1.569908, mean_q: 1.941292, mean_eps: 0.931720
 190810/250000: episode: 279, duration: 106.963s, episode steps: 704, steps per second:   7, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.010787, mae: 1.545548, mean_q: 1.911916, mean_eps: 0.931435
 192334/250000: episode: 280, duration: 116.564s, episode steps: 1524, steps per second:  13, episode reward: 14.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.011098, mae: 1.547596, mean_q: 1.912714, mean_eps: 0.931034
 193450/250000: episode: 281, duration: 79.745s, episode steps: 1116, steps per second:  14, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.010829, mae: 1.562881, mean_q: 1.933185, mean_eps: 0.930559
 194016/250000: episode: 282, duration: 40.243s, episode steps: 566, steps per second:  14, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010595, mae: 1.551694, mean_q: 1.917798, mean_eps: 0.930256
 194567/250000: episode: 283, duration: 38.653s, episode steps: 551, steps per second:  14, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.011305, mae: 1.562425, mean_q: 1.930241, mean_eps: 0.930056
 195346/250000: episode: 284, duration: 55.612s, episode steps: 779, steps per second:  14, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.010934, mae: 1.549692, mean_q: 1.915772, mean_eps: 0.929816
 196252/250000: episode: 285, duration: 64.584s, episode steps: 906, steps per second:  14, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.010698, mae: 1.561091, mean_q: 1.930775, mean_eps: 0.929513
 196760/250000: episode: 286, duration: 39.580s, episode steps: 508, steps per second:  13, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.010471, mae: 1.563762, mean_q: 1.934221, mean_eps: 0.929259
 197479/250000: episode: 287, duration: 48.697s, episode steps: 719, steps per second:  15, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.010420, mae: 1.557931, mean_q: 1.925904, mean_eps: 0.929038
 197902/250000: episode: 288, duration: 27.899s, episode steps: 423, steps per second:  15, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.011336, mae: 1.550610, mean_q: 1.915354, mean_eps: 0.928832
 198309/250000: episode: 289, duration: 28.107s, episode steps: 407, steps per second:  14, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.009675, mae: 1.553519, mean_q: 1.919453, mean_eps: 0.928682
 199205/250000: episode: 290, duration: 80.897s, episode steps: 896, steps per second:  11, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.010875, mae: 1.573541, mean_q: 1.944168, mean_eps: 0.928447
 199715/250000: episode: 291, duration: 53.631s, episode steps: 510, steps per second:  10, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.012447, mae: 1.566311, mean_q: 1.934328, mean_eps: 0.928194
 200098/250000: episode: 292, duration: 38.198s, episode steps: 383, steps per second:  10, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.010385, mae: 1.578384, mean_q: 1.948797, mean_eps: 0.928034
 200488/250000: episode: 293, duration: 30.297s, episode steps: 390, steps per second:  13, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010961, mae: 1.596595, mean_q: 1.969585, mean_eps: 0.927895
 201034/250000: episode: 294, duration: 46.913s, episode steps: 546, steps per second:  12, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012926, mae: 1.614944, mean_q: 1.994962, mean_eps: 0.927726
 201576/250000: episode: 295, duration: 39.056s, episode steps: 542, steps per second:  14, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012408, mae: 1.616598, mean_q: 1.997078, mean_eps: 0.927531
 202212/250000: episode: 296, duration: 52.340s, episode steps: 636, steps per second:  12, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.011775, mae: 1.618760, mean_q: 1.994664, mean_eps: 0.927319
 202800/250000: episode: 297, duration: 43.658s, episode steps: 588, steps per second:  13, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012224, mae: 1.601685, mean_q: 1.979285, mean_eps: 0.927099
 203798/250000: episode: 298, duration: 89.465s, episode steps: 998, steps per second:  11, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.011901, mae: 1.618760, mean_q: 1.999474, mean_eps: 0.926813
 204191/250000: episode: 299, duration: 83.721s, episode steps: 393, steps per second:   5, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.009021, mae: 1.631887, mean_q: 2.015667, mean_eps: 0.926562
 204747/250000: episode: 300, duration: 54.122s, episode steps: 556, steps per second:  10, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012089, mae: 1.616477, mean_q: 1.993527, mean_eps: 0.926392
 205473/250000: episode: 301, duration: 76.859s, episode steps: 726, steps per second:   9, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.011313, mae: 1.614174, mean_q: 1.993014, mean_eps: 0.926160
 206215/250000: episode: 302, duration: 99.268s, episode steps: 742, steps per second:   7, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012576, mae: 1.602138, mean_q: 1.977523, mean_eps: 0.925896
 207076/250000: episode: 303, duration: 63.203s, episode steps: 861, steps per second:  14, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.011313, mae: 1.593529, mean_q: 1.966216, mean_eps: 0.925608
 207789/250000: episode: 304, duration: 49.666s, episode steps: 713, steps per second:  14, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010388, mae: 1.613983, mean_q: 1.994043, mean_eps: 0.925324
 208444/250000: episode: 305, duration: 44.325s, episode steps: 655, steps per second:  15, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.012194, mae: 1.631622, mean_q: 2.014578, mean_eps: 0.925078
 209041/250000: episode: 306, duration: 40.346s, episode steps: 597, steps per second:  15, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.010348, mae: 1.599838, mean_q: 1.975634, mean_eps: 0.924853
 209423/250000: episode: 307, duration: 24.758s, episode steps: 382, steps per second:  15, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.011644, mae: 1.601995, mean_q: 1.978133, mean_eps: 0.924676
 210266/250000: episode: 308, duration: 56.477s, episode steps: 843, steps per second:  15, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011913, mae: 1.608590, mean_q: 1.985902, mean_eps: 0.924456
 211070/250000: episode: 309, duration: 52.817s, episode steps: 804, steps per second:  15, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.011664, mae: 1.600441, mean_q: 1.974214, mean_eps: 0.924160
 211592/250000: episode: 310, duration: 39.636s, episode steps: 522, steps per second:  13, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011721, mae: 1.604659, mean_q: 1.978811, mean_eps: 0.923921
 213013/250000: episode: 311, duration: 98.478s, episode steps: 1421, steps per second:  14, episode reward: 19.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.011404, mae: 1.599971, mean_q: 1.973032, mean_eps: 0.923571
 213533/250000: episode: 312, duration: 33.460s, episode steps: 520, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.011352, mae: 1.618384, mean_q: 1.997671, mean_eps: 0.923221
 213986/250000: episode: 313, duration: 29.899s, episode steps: 453, steps per second:  15, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012409, mae: 1.584734, mean_q: 1.956418, mean_eps: 0.923046
 215030/250000: episode: 314, duration: 66.873s, episode steps: 1044, steps per second:  16, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011602, mae: 1.606500, mean_q: 1.984114, mean_eps: 0.922777
 215938/250000: episode: 315, duration: 66.901s, episode steps: 908, steps per second:  14, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.009902, mae: 1.606196, mean_q: 1.982907, mean_eps: 0.922426
 216891/250000: episode: 316, duration: 71.335s, episode steps: 953, steps per second:  13, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.011063, mae: 1.610409, mean_q: 1.989552, mean_eps: 0.922091
 217411/250000: episode: 317, duration: 38.233s, episode steps: 520, steps per second:  14, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.011324, mae: 1.611435, mean_q: 1.990737, mean_eps: 0.921826
 217918/250000: episode: 318, duration: 40.386s, episode steps: 507, steps per second:  13, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013752, mae: 1.615659, mean_q: 1.991462, mean_eps: 0.921641
 218446/250000: episode: 319, duration: 40.180s, episode steps: 528, steps per second:  13, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.011442, mae: 1.590125, mean_q: 1.962060, mean_eps: 0.921454
 219166/250000: episode: 320, duration: 55.094s, episode steps: 720, steps per second:  13, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.011692, mae: 1.597397, mean_q: 1.970442, mean_eps: 0.921230
 219895/250000: episode: 321, duration: 59.246s, episode steps: 729, steps per second:  12, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.011330, mae: 1.605863, mean_q: 1.982248, mean_eps: 0.920969
 220416/250000: episode: 322, duration: 40.136s, episode steps: 521, steps per second:  13, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.011223, mae: 1.597738, mean_q: 1.972345, mean_eps: 0.920745
 221073/250000: episode: 323, duration: 49.654s, episode steps: 657, steps per second:  13, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.011671, mae: 1.616685, mean_q: 1.995632, mean_eps: 0.920532
 221463/250000: episode: 324, duration: 28.600s, episode steps: 390, steps per second:  14, episode reward: 10.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.010826, mae: 1.616147, mean_q: 1.997387, mean_eps: 0.920344
 222245/250000: episode: 325, duration: 56.942s, episode steps: 782, steps per second:  14, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.010903, mae: 1.604424, mean_q: 1.980869, mean_eps: 0.920133
 222916/250000: episode: 326, duration: 48.973s, episode steps: 671, steps per second:  14, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.011388, mae: 1.589892, mean_q: 1.961545, mean_eps: 0.919871
 223552/250000: episode: 327, duration: 43.965s, episode steps: 636, steps per second:  14, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.011269, mae: 1.603182, mean_q: 1.978731, mean_eps: 0.919636
 224016/250000: episode: 328, duration: 32.933s, episode steps: 464, steps per second:  14, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010480, mae: 1.614059, mean_q: 1.991332, mean_eps: 0.919438
 224444/250000: episode: 329, duration: 30.884s, episode steps: 428, steps per second:  14, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.009555, mae: 1.617695, mean_q: 1.998970, mean_eps: 0.919278
 225308/250000: episode: 330, duration: 56.886s, episode steps: 864, steps per second:  15, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.010690, mae: 1.615186, mean_q: 1.994183, mean_eps: 0.919045
 225800/250000: episode: 331, duration: 34.431s, episode steps: 492, steps per second:  14, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.009906, mae: 1.601125, mean_q: 1.976304, mean_eps: 0.918801
 226219/250000: episode: 332, duration: 27.276s, episode steps: 419, steps per second:  15, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011561, mae: 1.585589, mean_q: 1.954718, mean_eps: 0.918637
 226661/250000: episode: 333, duration: 31.062s, episode steps: 442, steps per second:  14, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010036, mae: 1.587503, mean_q: 1.960029, mean_eps: 0.918482
 227603/250000: episode: 334, duration: 72.000s, episode steps: 942, steps per second:  13, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.010993, mae: 1.618858, mean_q: 1.996253, mean_eps: 0.918232
 228480/250000: episode: 335, duration: 72.795s, episode steps: 877, steps per second:  12, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.010258, mae: 1.611015, mean_q: 1.987899, mean_eps: 0.917906
 228929/250000: episode: 336, duration: 35.459s, episode steps: 449, steps per second:  13, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.011642, mae: 1.600863, mean_q: 1.975919, mean_eps: 0.917667
 229461/250000: episode: 337, duration: 35.781s, episode steps: 532, steps per second:  15, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.011197, mae: 1.610376, mean_q: 1.985325, mean_eps: 0.917489
 230172/250000: episode: 338, duration: 50.296s, episode steps: 711, steps per second:  14, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010712, mae: 1.607487, mean_q: 1.983852, mean_eps: 0.917266
 230570/250000: episode: 339, duration: 27.815s, episode steps: 398, steps per second:  14, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.010716, mae: 1.599656, mean_q: 1.974114, mean_eps: 0.917067
 231201/250000: episode: 340, duration: 42.807s, episode steps: 631, steps per second:  15, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011207, mae: 1.614314, mean_q: 1.991583, mean_eps: 0.916881
 231743/250000: episode: 341, duration: 33.280s, episode steps: 542, steps per second:  16, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011791, mae: 1.606665, mean_q: 1.982356, mean_eps: 0.916670
 232491/250000: episode: 342, duration: 47.756s, episode steps: 748, steps per second:  16, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011450, mae: 1.593711, mean_q: 1.964936, mean_eps: 0.916438
 233227/250000: episode: 343, duration: 49.198s, episode steps: 736, steps per second:  15, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.009720, mae: 1.622180, mean_q: 2.001061, mean_eps: 0.916171
 233845/250000: episode: 344, duration: 37.703s, episode steps: 618, steps per second:  16, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.012751, mae: 1.620042, mean_q: 1.997248, mean_eps: 0.915927
 234386/250000: episode: 345, duration: 34.346s, episode steps: 541, steps per second:  16, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.011725, mae: 1.622698, mean_q: 2.001261, mean_eps: 0.915718
 234884/250000: episode: 346, duration: 30.828s, episode steps: 498, steps per second:  16, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.009680, mae: 1.618331, mean_q: 1.996898, mean_eps: 0.915532
 235407/250000: episode: 347, duration: 33.217s, episode steps: 523, steps per second:  16, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.012811, mae: 1.611803, mean_q: 1.987387, mean_eps: 0.915348
 236051/250000: episode: 348, duration: 42.005s, episode steps: 644, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.011442, mae: 1.595014, mean_q: 1.969125, mean_eps: 0.915138
 237057/250000: episode: 349, duration: 69.309s, episode steps: 1006, steps per second:  15, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.012403, mae: 1.601834, mean_q: 1.975629, mean_eps: 0.914841
 237687/250000: episode: 350, duration: 40.298s, episode steps: 630, steps per second:  16, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011407, mae: 1.591646, mean_q: 1.964411, mean_eps: 0.914546
 238076/250000: episode: 351, duration: 24.763s, episode steps: 389, steps per second:  16, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.010430, mae: 1.576071, mean_q: 1.946120, mean_eps: 0.914363
 238555/250000: episode: 352, duration: 30.184s, episode steps: 479, steps per second:  16, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.011110, mae: 1.591946, mean_q: 1.964137, mean_eps: 0.914207
 239717/250000: episode: 353, duration: 93.095s, episode steps: 1162, steps per second:  12, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.010991, mae: 1.603961, mean_q: 1.979280, mean_eps: 0.913911
 240452/250000: episode: 354, duration: 51.527s, episode steps: 735, steps per second:  14, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.010972, mae: 1.610140, mean_q: 1.986969, mean_eps: 0.913570
 241220/250000: episode: 355, duration: 58.878s, episode steps: 768, steps per second:  13, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011049, mae: 1.612036, mean_q: 1.989658, mean_eps: 0.913300
 241963/250000: episode: 356, duration: 57.821s, episode steps: 743, steps per second:  13, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.010951, mae: 1.617012, mean_q: 1.994449, mean_eps: 0.913028
 242483/250000: episode: 357, duration: 40.420s, episode steps: 520, steps per second:  13, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.010774, mae: 1.611639, mean_q: 1.987332, mean_eps: 0.912800
 243141/250000: episode: 358, duration: 49.733s, episode steps: 658, steps per second:  13, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.011774, mae: 1.598603, mean_q: 1.974136, mean_eps: 0.912588
 243829/250000: episode: 359, duration: 49.516s, episode steps: 688, steps per second:  14, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010109, mae: 1.610623, mean_q: 1.986543, mean_eps: 0.912345
 244857/250000: episode: 360, duration: 65.133s, episode steps: 1028, steps per second:  16, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011401, mae: 1.610888, mean_q: 1.986416, mean_eps: 0.912036
 245360/250000: episode: 361, duration: 36.831s, episode steps: 503, steps per second:  14, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.010060, mae: 1.598242, mean_q: 1.972311, mean_eps: 0.911761
 245999/250000: episode: 362, duration: 39.133s, episode steps: 639, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.010451, mae: 1.601521, mean_q: 1.975170, mean_eps: 0.911556
 246710/250000: episode: 363, duration: 45.104s, episode steps: 711, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.010149, mae: 1.608368, mean_q: 1.983997, mean_eps: 0.911313
 247633/250000: episode: 364, duration: 59.020s, episode steps: 923, steps per second:  16, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.010897, mae: 1.612951, mean_q: 1.988059, mean_eps: 0.911018
 248429/250000: episode: 365, duration: 51.075s, episode steps: 796, steps per second:  16, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.010723, mae: 1.606262, mean_q: 1.982510, mean_eps: 0.910708
 248925/250000: episode: 366, duration: 36.018s, episode steps: 496, steps per second:  14, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.012004, mae: 1.620286, mean_q: 2.000737, mean_eps: 0.910476
 249348/250000: episode: 367, duration: 33.748s, episode steps: 423, steps per second:  13, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.012075, mae: 1.607343, mean_q: 1.983497, mean_eps: 0.910311
done, took 15669.561 seconds
########################################################
PROCESO TERMINADO
########################################################