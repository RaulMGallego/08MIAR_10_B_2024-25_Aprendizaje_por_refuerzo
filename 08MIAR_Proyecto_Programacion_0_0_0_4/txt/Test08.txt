['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_250000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
    420/250000: episode: 1, duration: 2.068s, episode steps: 420, steps per second: 203, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 2.944s, episode steps: 711, steps per second: 242, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 3.617s, episode steps: 810, steps per second: 224, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2827/250000: episode: 4, duration: 3.954s, episode steps: 886, steps per second: 224, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3345/250000: episode: 5, duration: 2.988s, episode steps: 518, steps per second: 173, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3990/250000: episode: 6, duration: 3.849s, episode steps: 645, steps per second: 168, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4451/250000: episode: 7, duration: 2.646s, episode steps: 461, steps per second: 174, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5247/250000: episode: 8, duration: 4.123s, episode steps: 796, steps per second: 193, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5629/250000: episode: 9, duration: 2.117s, episode steps: 382, steps per second: 180, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6850/250000: episode: 10, duration: 6.315s, episode steps: 1221, steps per second: 193, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7580/250000: episode: 11, duration: 3.317s, episode steps: 730, steps per second: 220, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8538/250000: episode: 12, duration: 4.138s, episode steps: 958, steps per second: 232, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9208/250000: episode: 13, duration: 3.038s, episode steps: 670, steps per second: 221, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9708/250000: episode: 14, duration: 2.517s, episode steps: 500, steps per second: 199, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10467/250000: episode: 15, duration: 22.159s, episode steps: 759, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.014764, mae: 1.681051, mean_q: 2.076683, mean_eps: 0.996316
  11018/250000: episode: 16, duration: 26.883s, episode steps: 551, steps per second:  20, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.012593, mae: 1.710315, mean_q: 2.110778, mean_eps: 0.996133
  11879/250000: episode: 17, duration: 42.351s, episode steps: 861, steps per second:  20, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.011648, mae: 1.700203, mean_q: 2.098985, mean_eps: 0.995879
  12548/250000: episode: 18, duration: 34.328s, episode steps: 669, steps per second:  19, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.010474, mae: 1.695429, mean_q: 2.093637, mean_eps: 0.995604
  13555/250000: episode: 19, duration: 43.659s, episode steps: 1007, steps per second:  23, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.009764, mae: 1.703876, mean_q: 2.106787, mean_eps: 0.995302
  14190/250000: episode: 20, duration: 24.486s, episode steps: 635, steps per second:  26, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.009657, mae: 1.696330, mean_q: 2.097894, mean_eps: 0.995006
  14945/250000: episode: 21, duration: 28.776s, episode steps: 755, steps per second:  26, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.009707, mae: 1.688062, mean_q: 2.087543, mean_eps: 0.994756
  15399/250000: episode: 22, duration: 18.376s, episode steps: 454, steps per second:  25, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.009443, mae: 1.695294, mean_q: 2.097875, mean_eps: 0.994538
  16048/250000: episode: 23, duration: 23.958s, episode steps: 649, steps per second:  27, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.010499, mae: 1.691915, mean_q: 2.095810, mean_eps: 0.994340
  16447/250000: episode: 24, duration: 16.340s, episode steps: 399, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.008212, mae: 1.674459, mean_q: 2.073002, mean_eps: 0.994151
  17049/250000: episode: 25, duration: 24.267s, episode steps: 602, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.009439, mae: 1.680874, mean_q: 2.081245, mean_eps: 0.993971
  18030/250000: episode: 26, duration: 36.923s, episode steps: 981, steps per second:  27, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.009428, mae: 1.689216, mean_q: 2.091698, mean_eps: 0.993686
  18914/250000: episode: 27, duration: 34.560s, episode steps: 884, steps per second:  26, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.008949, mae: 1.680480, mean_q: 2.081399, mean_eps: 0.993350
  19510/250000: episode: 28, duration: 24.537s, episode steps: 596, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.009699, mae: 1.684773, mean_q: 2.086131, mean_eps: 0.993084
  20313/250000: episode: 29, duration: 34.383s, episode steps: 803, steps per second:  23, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.009802, mae: 1.658273, mean_q: 2.052723, mean_eps: 0.992832
  21041/250000: episode: 30, duration: 29.162s, episode steps: 728, steps per second:  25, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.010072, mae: 1.673679, mean_q: 2.069828, mean_eps: 0.992556
  21579/250000: episode: 31, duration: 21.575s, episode steps: 538, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.009036, mae: 1.653323, mean_q: 2.048435, mean_eps: 0.992328
  22560/250000: episode: 32, duration: 40.463s, episode steps: 981, steps per second:  24, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.009276, mae: 1.660422, mean_q: 2.053862, mean_eps: 0.992056
  23077/250000: episode: 33, duration: 20.887s, episode steps: 517, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.008673, mae: 1.639132, mean_q: 2.025231, mean_eps: 0.991786
  23470/250000: episode: 34, duration: 15.436s, episode steps: 393, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009164, mae: 1.659551, mean_q: 2.053848, mean_eps: 0.991621
  24281/250000: episode: 35, duration: 31.126s, episode steps: 811, steps per second:  26, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008864, mae: 1.655000, mean_q: 2.046598, mean_eps: 0.991405
  25475/250000: episode: 36, duration: 46.588s, episode steps: 1194, steps per second:  26, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.008884, mae: 1.658337, mean_q: 2.051241, mean_eps: 0.991044
  26547/250000: episode: 37, duration: 40.852s, episode steps: 1072, steps per second:  26, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.009124, mae: 1.658451, mean_q: 2.052114, mean_eps: 0.990636
  27340/250000: episode: 38, duration: 31.361s, episode steps: 793, steps per second:  25, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.009147, mae: 1.661423, mean_q: 2.058312, mean_eps: 0.990301
  28044/250000: episode: 39, duration: 27.821s, episode steps: 704, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.009388, mae: 1.675662, mean_q: 2.074920, mean_eps: 0.990032
  28816/250000: episode: 40, duration: 30.890s, episode steps: 772, steps per second:  25, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.007990, mae: 1.647414, mean_q: 2.040555, mean_eps: 0.989766
  29473/250000: episode: 41, duration: 24.236s, episode steps: 657, steps per second:  27, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.008310, mae: 1.648837, mean_q: 2.042535, mean_eps: 0.989508
  30237/250000: episode: 42, duration: 30.620s, episode steps: 764, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.009906, mae: 1.650298, mean_q: 2.040205, mean_eps: 0.989252
  30856/250000: episode: 43, duration: 24.427s, episode steps: 619, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.008409, mae: 1.629885, mean_q: 2.015252, mean_eps: 0.989003
  31549/250000: episode: 44, duration: 26.922s, episode steps: 693, steps per second:  26, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.010468, mae: 1.648395, mean_q: 2.040256, mean_eps: 0.988767
  32241/250000: episode: 45, duration: 27.176s, episode steps: 692, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.009091, mae: 1.646651, mean_q: 2.037518, mean_eps: 0.988517
  33039/250000: episode: 46, duration: 30.791s, episode steps: 798, steps per second:  26, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.008971, mae: 1.638126, mean_q: 2.026252, mean_eps: 0.988250
  33712/250000: episode: 47, duration: 27.424s, episode steps: 673, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.009568, mae: 1.635474, mean_q: 2.022784, mean_eps: 0.987985
  34262/250000: episode: 48, duration: 20.622s, episode steps: 550, steps per second:  27, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.009626, mae: 1.643414, mean_q: 2.033988, mean_eps: 0.987765
  34716/250000: episode: 49, duration: 18.024s, episode steps: 454, steps per second:  25, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.009610, mae: 1.615985, mean_q: 2.001782, mean_eps: 0.987584
  35344/250000: episode: 50, duration: 25.696s, episode steps: 628, steps per second:  24, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.009667, mae: 1.642764, mean_q: 2.030108, mean_eps: 0.987390
  35981/250000: episode: 51, duration: 23.475s, episode steps: 637, steps per second:  27, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.010300, mae: 1.659623, mean_q: 2.052973, mean_eps: 0.987162
  36534/250000: episode: 52, duration: 22.680s, episode steps: 553, steps per second:  24, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.010339, mae: 1.643311, mean_q: 2.033627, mean_eps: 0.986947
  37859/250000: episode: 53, duration: 50.951s, episode steps: 1325, steps per second:  26, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.009709, mae: 1.651984, mean_q: 2.042332, mean_eps: 0.986609
  38485/250000: episode: 54, duration: 24.558s, episode steps: 626, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.009078, mae: 1.643775, mean_q: 2.033553, mean_eps: 0.986258
  39127/250000: episode: 55, duration: 25.603s, episode steps: 642, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011313, mae: 1.646824, mean_q: 2.036631, mean_eps: 0.986030
  39784/250000: episode: 56, duration: 25.867s, episode steps: 657, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.009199, mae: 1.662561, mean_q: 2.056122, mean_eps: 0.985797
  40543/250000: episode: 57, duration: 29.541s, episode steps: 759, steps per second:  26, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.009535, mae: 1.657072, mean_q: 2.051173, mean_eps: 0.985542
  41268/250000: episode: 58, duration: 27.282s, episode steps: 725, steps per second:  27, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.009568, mae: 1.632970, mean_q: 2.020017, mean_eps: 0.985275
  42053/250000: episode: 59, duration: 31.858s, episode steps: 785, steps per second:  25, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.010327, mae: 1.635921, mean_q: 2.023292, mean_eps: 0.985002
  42455/250000: episode: 60, duration: 15.431s, episode steps: 402, steps per second:  26, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.008770, mae: 1.650130, mean_q: 2.039963, mean_eps: 0.984789
  43038/250000: episode: 61, duration: 22.468s, episode steps: 583, steps per second:  26, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.008694, mae: 1.626720, mean_q: 2.009848, mean_eps: 0.984611
  43534/250000: episode: 62, duration: 20.036s, episode steps: 496, steps per second:  25, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.010622, mae: 1.638247, mean_q: 2.026243, mean_eps: 0.984417
  44177/250000: episode: 63, duration: 24.883s, episode steps: 643, steps per second:  26, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.010016, mae: 1.644065, mean_q: 2.034124, mean_eps: 0.984212
  44998/250000: episode: 64, duration: 31.359s, episode steps: 821, steps per second:  26, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.009984, mae: 1.637341, mean_q: 2.025291, mean_eps: 0.983948
  45759/250000: episode: 65, duration: 30.435s, episode steps: 761, steps per second:  25, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.009701, mae: 1.635983, mean_q: 2.024171, mean_eps: 0.983664
  46640/250000: episode: 66, duration: 34.634s, episode steps: 881, steps per second:  25, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.009189, mae: 1.619503, mean_q: 2.004544, mean_eps: 0.983369
  47280/250000: episode: 67, duration: 25.901s, episode steps: 640, steps per second:  25, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.009659, mae: 1.636427, mean_q: 2.024416, mean_eps: 0.983095
  47989/250000: episode: 68, duration: 27.662s, episode steps: 709, steps per second:  26, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.009728, mae: 1.628459, mean_q: 2.013864, mean_eps: 0.982852
  48343/250000: episode: 69, duration: 14.180s, episode steps: 354, steps per second:  25, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.010271, mae: 1.636192, mean_q: 2.023816, mean_eps: 0.982660
  49524/250000: episode: 70, duration: 45.664s, episode steps: 1181, steps per second:  26, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008709, mae: 1.634465, mean_q: 2.021995, mean_eps: 0.982384
  50443/250000: episode: 71, duration: 39.893s, episode steps: 919, steps per second:  23, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.010429, mae: 1.664244, mean_q: 2.057670, mean_eps: 0.982006
  51022/250000: episode: 72, duration: 23.231s, episode steps: 579, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.011560, mae: 1.677377, mean_q: 2.071753, mean_eps: 0.981736
  51895/250000: episode: 73, duration: 36.123s, episode steps: 873, steps per second:  24, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.010259, mae: 1.662664, mean_q: 2.051854, mean_eps: 0.981475
  52904/250000: episode: 74, duration: 37.692s, episode steps: 1009, steps per second:  27, episode reward: 10.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.010156, mae: 1.668739, mean_q: 2.059991, mean_eps: 0.981137
  53518/250000: episode: 75, duration: 24.435s, episode steps: 614, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.013910, mae: 1.683702, mean_q: 2.077864, mean_eps: 0.980844
  54168/250000: episode: 76, duration: 25.038s, episode steps: 650, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012453, mae: 1.668916, mean_q: 2.061824, mean_eps: 0.980617
  54858/250000: episode: 77, duration: 27.002s, episode steps: 690, steps per second:  26, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.010359, mae: 1.695834, mean_q: 2.094241, mean_eps: 0.980376
  55530/250000: episode: 78, duration: 26.392s, episode steps: 672, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.011002, mae: 1.685910, mean_q: 2.081854, mean_eps: 0.980130
  55926/250000: episode: 79, duration: 14.698s, episode steps: 396, steps per second:  27, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.011350, mae: 1.694004, mean_q: 2.092296, mean_eps: 0.979938
  56331/250000: episode: 80, duration: 15.657s, episode steps: 405, steps per second:  26, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.011432, mae: 1.670649, mean_q: 2.064454, mean_eps: 0.979794
  57012/250000: episode: 81, duration: 27.428s, episode steps: 681, steps per second:  25, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.009761, mae: 1.685333, mean_q: 2.080563, mean_eps: 0.979599
  57651/250000: episode: 82, duration: 24.431s, episode steps: 639, steps per second:  26, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010714, mae: 1.701058, mean_q: 2.103371, mean_eps: 0.979361
  58487/250000: episode: 83, duration: 35.863s, episode steps: 836, steps per second:  23, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.010330, mae: 1.695626, mean_q: 2.096446, mean_eps: 0.979096
  59232/250000: episode: 84, duration: 31.042s, episode steps: 745, steps per second:  24, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.009092, mae: 1.663596, mean_q: 2.054172, mean_eps: 0.978811
  60139/250000: episode: 85, duration: 36.125s, episode steps: 907, steps per second:  25, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.010370, mae: 1.679359, mean_q: 2.074394, mean_eps: 0.978514
  60753/250000: episode: 86, duration: 23.601s, episode steps: 614, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.010912, mae: 1.686074, mean_q: 2.081462, mean_eps: 0.978239
  61689/250000: episode: 87, duration: 36.541s, episode steps: 936, steps per second:  26, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.009412, mae: 1.682366, mean_q: 2.076393, mean_eps: 0.977960
  62195/250000: episode: 88, duration: 20.211s, episode steps: 506, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.011901, mae: 1.683750, mean_q: 2.081777, mean_eps: 0.977701
  62869/250000: episode: 89, duration: 25.895s, episode steps: 674, steps per second:  26, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.009962, mae: 1.689501, mean_q: 2.086086, mean_eps: 0.977488
  63674/250000: episode: 90, duration: 33.903s, episode steps: 805, steps per second:  24, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.009387, mae: 1.690456, mean_q: 2.089868, mean_eps: 0.977222
  64080/250000: episode: 91, duration: 15.747s, episode steps: 406, steps per second:  26, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.009677, mae: 1.676126, mean_q: 2.068906, mean_eps: 0.977005
  65127/250000: episode: 92, duration: 40.505s, episode steps: 1047, steps per second:  26, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.011501, mae: 1.692305, mean_q: 2.088088, mean_eps: 0.976743
  66417/250000: episode: 93, duration: 51.561s, episode steps: 1290, steps per second:  25, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.009627, mae: 1.696653, mean_q: 2.094230, mean_eps: 0.976322
  67031/250000: episode: 94, duration: 25.312s, episode steps: 614, steps per second:  24, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.009965, mae: 1.695898, mean_q: 2.094756, mean_eps: 0.975979
  67691/250000: episode: 95, duration: 25.880s, episode steps: 660, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.010509, mae: 1.658294, mean_q: 2.043484, mean_eps: 0.975750
  68660/250000: episode: 96, duration: 38.146s, episode steps: 969, steps per second:  25, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.010427, mae: 1.690740, mean_q: 2.085961, mean_eps: 0.975457
  69408/250000: episode: 97, duration: 29.012s, episode steps: 748, steps per second:  26, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.008467, mae: 1.703047, mean_q: 2.100900, mean_eps: 0.975148
  69990/250000: episode: 98, duration: 22.193s, episode steps: 582, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.010137, mae: 1.686280, mean_q: 2.081011, mean_eps: 0.974909
  70389/250000: episode: 99, duration: 16.782s, episode steps: 399, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.010579, mae: 1.707906, mean_q: 2.109379, mean_eps: 0.974732
  71134/250000: episode: 100, duration: 28.245s, episode steps: 745, steps per second:  26, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010846, mae: 1.698075, mean_q: 2.095893, mean_eps: 0.974526
  71830/250000: episode: 101, duration: 27.856s, episode steps: 696, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.011112, mae: 1.698387, mean_q: 2.096459, mean_eps: 0.974266
  72355/250000: episode: 102, duration: 20.301s, episode steps: 525, steps per second:  26, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009909, mae: 1.691090, mean_q: 2.087084, mean_eps: 0.974047
  73037/250000: episode: 103, duration: 27.553s, episode steps: 682, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.010039, mae: 1.685030, mean_q: 2.078246, mean_eps: 0.973829
  73661/250000: episode: 104, duration: 24.989s, episode steps: 624, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.012469, mae: 1.688784, mean_q: 2.084010, mean_eps: 0.973594
  74270/250000: episode: 105, duration: 23.219s, episode steps: 609, steps per second:  26, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.010267, mae: 1.671758, mean_q: 2.066086, mean_eps: 0.973372
  74968/250000: episode: 106, duration: 27.814s, episode steps: 698, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.009928, mae: 1.677045, mean_q: 2.070701, mean_eps: 0.973138
  75562/250000: episode: 107, duration: 23.618s, episode steps: 594, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.009744, mae: 1.675451, mean_q: 2.070065, mean_eps: 0.972905
  76281/250000: episode: 108, duration: 28.695s, episode steps: 719, steps per second:  25, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.009468, mae: 1.681310, mean_q: 2.078999, mean_eps: 0.972668
  76843/250000: episode: 109, duration: 21.807s, episode steps: 562, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010293, mae: 1.658115, mean_q: 2.047958, mean_eps: 0.972438
  77209/250000: episode: 110, duration: 14.603s, episode steps: 366, steps per second:  25, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.008876, mae: 1.651958, mean_q: 2.039288, mean_eps: 0.972271
  78081/250000: episode: 111, duration: 33.944s, episode steps: 872, steps per second:  26, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.009486, mae: 1.666190, mean_q: 2.058201, mean_eps: 0.972047
  78611/250000: episode: 112, duration: 22.056s, episode steps: 530, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.009041, mae: 1.666835, mean_q: 2.060115, mean_eps: 0.971795
  79251/250000: episode: 113, duration: 24.286s, episode steps: 640, steps per second:  26, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.009770, mae: 1.665647, mean_q: 2.057728, mean_eps: 0.971585
  79978/250000: episode: 114, duration: 28.933s, episode steps: 727, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.010938, mae: 1.662152, mean_q: 2.052542, mean_eps: 0.971339
  80481/250000: episode: 115, duration: 20.215s, episode steps: 503, steps per second:  25, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.012284, mae: 1.679100, mean_q: 2.072963, mean_eps: 0.971117
  80987/250000: episode: 116, duration: 19.739s, episode steps: 506, steps per second:  26, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.009541, mae: 1.663384, mean_q: 2.052371, mean_eps: 0.970936
  81366/250000: episode: 117, duration: 14.865s, episode steps: 379, steps per second:  25, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008690, mae: 1.671332, mean_q: 2.061227, mean_eps: 0.970777
  82466/250000: episode: 118, duration: 43.647s, episode steps: 1100, steps per second:  25, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.010157, mae: 1.685228, mean_q: 2.079054, mean_eps: 0.970510
  83582/250000: episode: 119, duration: 44.322s, episode steps: 1116, steps per second:  25, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.010356, mae: 1.685917, mean_q: 2.080490, mean_eps: 0.970111
  84132/250000: episode: 120, duration: 20.940s, episode steps: 550, steps per second:  26, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.008790, mae: 1.693319, mean_q: 2.089355, mean_eps: 0.969812
  84891/250000: episode: 121, duration: 29.813s, episode steps: 759, steps per second:  25, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.009307, mae: 1.678973, mean_q: 2.073313, mean_eps: 0.969576
  85738/250000: episode: 122, duration: 34.052s, episode steps: 847, steps per second:  25, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.011042, mae: 1.689734, mean_q: 2.086876, mean_eps: 0.969287
  86442/250000: episode: 123, duration: 27.726s, episode steps: 704, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.009419, mae: 1.686007, mean_q: 2.081354, mean_eps: 0.969008
  87232/250000: episode: 124, duration: 31.920s, episode steps: 790, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010253, mae: 1.677118, mean_q: 2.070788, mean_eps: 0.968739
  87743/250000: episode: 125, duration: 20.829s, episode steps: 511, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.011166, mae: 1.687985, mean_q: 2.083443, mean_eps: 0.968505
  88351/250000: episode: 126, duration: 27.573s, episode steps: 608, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.010135, mae: 1.674817, mean_q: 2.067143, mean_eps: 0.968303
  89400/250000: episode: 127, duration: 40.300s, episode steps: 1049, steps per second:  26, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.010694, mae: 1.668569, mean_q: 2.059322, mean_eps: 0.968005
  89905/250000: episode: 128, duration: 19.954s, episode steps: 505, steps per second:  25, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011342, mae: 1.686853, mean_q: 2.083621, mean_eps: 0.967725
  90569/250000: episode: 129, duration: 26.183s, episode steps: 664, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.010054, mae: 1.678327, mean_q: 2.072255, mean_eps: 0.967514
  91543/250000: episode: 130, duration: 39.883s, episode steps: 974, steps per second:  24, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.010369, mae: 1.679328, mean_q: 2.070351, mean_eps: 0.967220
  92023/250000: episode: 131, duration: 19.199s, episode steps: 480, steps per second:  25, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.008954, mae: 1.670746, mean_q: 2.063910, mean_eps: 0.966958
  92661/250000: episode: 132, duration: 24.402s, episode steps: 638, steps per second:  26, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.010472, mae: 1.690364, mean_q: 2.086238, mean_eps: 0.966757
  93067/250000: episode: 133, duration: 15.876s, episode steps: 406, steps per second:  26, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.013281, mae: 1.689945, mean_q: 2.085799, mean_eps: 0.966569
  93970/250000: episode: 134, duration: 35.221s, episode steps: 903, steps per second:  26, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.009471, mae: 1.677630, mean_q: 2.071139, mean_eps: 0.966334
  94544/250000: episode: 135, duration: 22.624s, episode steps: 574, steps per second:  25, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.009812, mae: 1.686874, mean_q: 2.081496, mean_eps: 0.966068
  95129/250000: episode: 136, duration: 23.249s, episode steps: 585, steps per second:  25, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.009518, mae: 1.691203, mean_q: 2.087883, mean_eps: 0.965859
  95986/250000: episode: 137, duration: 32.789s, episode steps: 857, steps per second:  26, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.009507, mae: 1.658946, mean_q: 2.047179, mean_eps: 0.965599
  96393/250000: episode: 138, duration: 16.138s, episode steps: 407, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.009710, mae: 1.664106, mean_q: 2.052192, mean_eps: 0.965372
  97176/250000: episode: 139, duration: 31.706s, episode steps: 783, steps per second:  25, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.009797, mae: 1.673083, mean_q: 2.062974, mean_eps: 0.965158
  97736/250000: episode: 140, duration: 21.863s, episode steps: 560, steps per second:  26, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.011100, mae: 1.675138, mean_q: 2.067954, mean_eps: 0.964917
  98304/250000: episode: 141, duration: 24.155s, episode steps: 568, steps per second:  24, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.010685, mae: 1.667951, mean_q: 2.060278, mean_eps: 0.964714
  98972/250000: episode: 142, duration: 26.941s, episode steps: 668, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.009821, mae: 1.663210, mean_q: 2.053057, mean_eps: 0.964491
  99808/250000: episode: 143, duration: 33.265s, episode steps: 836, steps per second:  25, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.009721, mae: 1.685898, mean_q: 2.081616, mean_eps: 0.964220
 100361/250000: episode: 144, duration: 24.096s, episode steps: 553, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.009099, mae: 1.707738, mean_q: 2.106747, mean_eps: 0.963970
 101182/250000: episode: 145, duration: 31.895s, episode steps: 821, steps per second:  26, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.010185, mae: 1.722248, mean_q: 2.125488, mean_eps: 0.963722
 101613/250000: episode: 146, duration: 17.517s, episode steps: 431, steps per second:  25, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.011006, mae: 1.746719, mean_q: 2.154895, mean_eps: 0.963497
 102305/250000: episode: 147, duration: 28.194s, episode steps: 692, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.011609, mae: 1.741471, mean_q: 2.147084, mean_eps: 0.963294
 102706/250000: episode: 148, duration: 15.912s, episode steps: 401, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.011151, mae: 1.707318, mean_q: 2.103555, mean_eps: 0.963098
 103902/250000: episode: 149, duration: 47.801s, episode steps: 1196, steps per second:  25, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.012061, mae: 1.723737, mean_q: 2.124002, mean_eps: 0.962811
 104777/250000: episode: 150, duration: 34.422s, episode steps: 875, steps per second:  25, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.012018, mae: 1.735713, mean_q: 2.138952, mean_eps: 0.962438
 105348/250000: episode: 151, duration: 22.970s, episode steps: 571, steps per second:  25, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.011048, mae: 1.742097, mean_q: 2.146776, mean_eps: 0.962178
 105874/250000: episode: 152, duration: 21.068s, episode steps: 526, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.012247, mae: 1.717091, mean_q: 2.115765, mean_eps: 0.961980
 106428/250000: episode: 153, duration: 21.863s, episode steps: 554, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.010510, mae: 1.717189, mean_q: 2.117960, mean_eps: 0.961786
 107376/250000: episode: 154, duration: 37.483s, episode steps: 948, steps per second:  25, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.010680, mae: 1.728974, mean_q: 2.130931, mean_eps: 0.961516
 107898/250000: episode: 155, duration: 21.926s, episode steps: 522, steps per second:  24, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.010267, mae: 1.719351, mean_q: 2.117542, mean_eps: 0.961251
 108403/250000: episode: 156, duration: 19.966s, episode steps: 505, steps per second:  25, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.011838, mae: 1.729472, mean_q: 2.132423, mean_eps: 0.961066
 109062/250000: episode: 157, duration: 25.751s, episode steps: 659, steps per second:  26, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.010970, mae: 1.736232, mean_q: 2.137700, mean_eps: 0.960856
 109858/250000: episode: 158, duration: 33.665s, episode steps: 796, steps per second:  24, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.010870, mae: 1.724050, mean_q: 2.125665, mean_eps: 0.960594
 110518/250000: episode: 159, duration: 25.880s, episode steps: 660, steps per second:  26, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010842, mae: 1.744189, mean_q: 2.152773, mean_eps: 0.960332
 111339/250000: episode: 160, duration: 31.847s, episode steps: 821, steps per second:  26, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.010767, mae: 1.743849, mean_q: 2.150198, mean_eps: 0.960066
 112024/250000: episode: 161, duration: 27.743s, episode steps: 685, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011609, mae: 1.714685, mean_q: 2.113222, mean_eps: 0.959795
 112418/250000: episode: 162, duration: 15.102s, episode steps: 394, steps per second:  26, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.012992, mae: 1.723839, mean_q: 2.123221, mean_eps: 0.959601
 113060/250000: episode: 163, duration: 25.760s, episode steps: 642, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.011361, mae: 1.733796, mean_q: 2.136463, mean_eps: 0.959414
 113441/250000: episode: 164, duration: 15.267s, episode steps: 381, steps per second:  25, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.009778, mae: 1.737692, mean_q: 2.139418, mean_eps: 0.959230
 114090/250000: episode: 165, duration: 24.804s, episode steps: 649, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.010935, mae: 1.736147, mean_q: 2.137738, mean_eps: 0.959044
 114556/250000: episode: 166, duration: 18.196s, episode steps: 466, steps per second:  26, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.011335, mae: 1.732721, mean_q: 2.130360, mean_eps: 0.958844
 115075/250000: episode: 167, duration: 21.175s, episode steps: 519, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.009576, mae: 1.728167, mean_q: 2.129636, mean_eps: 0.958667
 115588/250000: episode: 168, duration: 19.946s, episode steps: 513, steps per second:  26, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.011428, mae: 1.721106, mean_q: 2.117846, mean_eps: 0.958481
 116841/250000: episode: 169, duration: 50.057s, episode steps: 1253, steps per second:  25, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010788, mae: 1.728416, mean_q: 2.129537, mean_eps: 0.958163
 117466/250000: episode: 170, duration: 23.769s, episode steps: 625, steps per second:  26, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.011483, mae: 1.732319, mean_q: 2.133347, mean_eps: 0.957825
 117917/250000: episode: 171, duration: 17.919s, episode steps: 451, steps per second:  25, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.011303, mae: 1.724670, mean_q: 2.126247, mean_eps: 0.957631
 118318/250000: episode: 172, duration: 16.592s, episode steps: 401, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011461, mae: 1.712496, mean_q: 2.113380, mean_eps: 0.957478
 119530/250000: episode: 173, duration: 47.138s, episode steps: 1212, steps per second:  26, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010931, mae: 1.732583, mean_q: 2.136252, mean_eps: 0.957187
 120162/250000: episode: 174, duration: 25.422s, episode steps: 632, steps per second:  25, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012436, mae: 1.734539, mean_q: 2.137906, mean_eps: 0.956855
 120921/250000: episode: 175, duration: 29.759s, episode steps: 759, steps per second:  26, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.010423, mae: 1.744366, mean_q: 2.151095, mean_eps: 0.956605
 121461/250000: episode: 176, duration: 20.877s, episode steps: 540, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013282, mae: 1.738978, mean_q: 2.141638, mean_eps: 0.956371
 122537/250000: episode: 177, duration: 42.862s, episode steps: 1076, steps per second:  25, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.010247, mae: 1.739730, mean_q: 2.144478, mean_eps: 0.956080
 123160/250000: episode: 178, duration: 25.926s, episode steps: 623, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.010790, mae: 1.732845, mean_q: 2.134664, mean_eps: 0.955775
 123822/250000: episode: 179, duration: 27.061s, episode steps: 662, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.010859, mae: 1.749772, mean_q: 2.155846, mean_eps: 0.955544
 124357/250000: episode: 180, duration: 23.256s, episode steps: 535, steps per second:  23, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.011139, mae: 1.710448, mean_q: 2.105952, mean_eps: 0.955328
 125033/250000: episode: 181, duration: 27.558s, episode steps: 676, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.011657, mae: 1.736375, mean_q: 2.139591, mean_eps: 0.955109
 125798/250000: episode: 182, duration: 30.876s, episode steps: 765, steps per second:  25, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.011439, mae: 1.716645, mean_q: 2.115890, mean_eps: 0.954850
 126542/250000: episode: 183, duration: 29.969s, episode steps: 744, steps per second:  25, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011125, mae: 1.718812, mean_q: 2.118234, mean_eps: 0.954579
 127209/250000: episode: 184, duration: 26.498s, episode steps: 667, steps per second:  25, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.010943, mae: 1.722631, mean_q: 2.124045, mean_eps: 0.954325
 127616/250000: episode: 185, duration: 16.407s, episode steps: 407, steps per second:  25, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.009417, mae: 1.717542, mean_q: 2.119093, mean_eps: 0.954132
 128102/250000: episode: 186, duration: 19.321s, episode steps: 486, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010958, mae: 1.734049, mean_q: 2.140123, mean_eps: 0.953971
 129135/250000: episode: 187, duration: 42.051s, episode steps: 1033, steps per second:  25, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.011733, mae: 1.734792, mean_q: 2.137666, mean_eps: 0.953698
 129912/250000: episode: 188, duration: 31.334s, episode steps: 777, steps per second:  25, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.012768, mae: 1.740425, mean_q: 2.144384, mean_eps: 0.953372
 130817/250000: episode: 189, duration: 35.537s, episode steps: 905, steps per second:  25, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.012168, mae: 1.723950, mean_q: 2.123962, mean_eps: 0.953069
 131342/250000: episode: 190, duration: 20.934s, episode steps: 525, steps per second:  25, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.009303, mae: 1.716480, mean_q: 2.115939, mean_eps: 0.952811
 131755/250000: episode: 191, duration: 16.681s, episode steps: 413, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.010940, mae: 1.763480, mean_q: 2.172305, mean_eps: 0.952643
 132378/250000: episode: 192, duration: 27.180s, episode steps: 623, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.010109, mae: 1.721835, mean_q: 2.121996, mean_eps: 0.952456
 133276/250000: episode: 193, duration: 36.947s, episode steps: 898, steps per second:  24, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.009750, mae: 1.698842, mean_q: 2.093538, mean_eps: 0.952183
 134068/250000: episode: 194, duration: 31.682s, episode steps: 792, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.011756, mae: 1.736895, mean_q: 2.139533, mean_eps: 0.951879
 134830/250000: episode: 195, duration: 30.248s, episode steps: 762, steps per second:  25, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010815, mae: 1.715275, mean_q: 2.111816, mean_eps: 0.951599
 135810/250000: episode: 196, duration: 38.203s, episode steps: 980, steps per second:  26, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010782, mae: 1.710641, mean_q: 2.107929, mean_eps: 0.951285
 136750/250000: episode: 197, duration: 37.120s, episode steps: 940, steps per second:  25, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012162, mae: 1.710804, mean_q: 2.107193, mean_eps: 0.950939
 137408/250000: episode: 198, duration: 26.629s, episode steps: 658, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010926, mae: 1.740205, mean_q: 2.143954, mean_eps: 0.950652
 137904/250000: episode: 199, duration: 19.798s, episode steps: 496, steps per second:  25, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.010771, mae: 1.728002, mean_q: 2.128124, mean_eps: 0.950445
 138291/250000: episode: 200, duration: 15.135s, episode steps: 387, steps per second:  26, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.010649, mae: 1.717478, mean_q: 2.113062, mean_eps: 0.950285
 138877/250000: episode: 201, duration: 22.963s, episode steps: 586, steps per second:  26, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.011481, mae: 1.713921, mean_q: 2.109210, mean_eps: 0.950110
 139671/250000: episode: 202, duration: 32.854s, episode steps: 794, steps per second:  24, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.010896, mae: 1.733420, mean_q: 2.134257, mean_eps: 0.949861
 140340/250000: episode: 203, duration: 28.795s, episode steps: 669, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011693, mae: 1.727592, mean_q: 2.127465, mean_eps: 0.949599
 141440/250000: episode: 204, duration: 45.819s, episode steps: 1100, steps per second:  24, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.010146, mae: 1.715569, mean_q: 2.110567, mean_eps: 0.949280
 141824/250000: episode: 205, duration: 15.482s, episode steps: 384, steps per second:  25, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.010619, mae: 1.715056, mean_q: 2.111924, mean_eps: 0.949013
 142551/250000: episode: 206, duration: 29.111s, episode steps: 727, steps per second:  25, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.011519, mae: 1.728243, mean_q: 2.125656, mean_eps: 0.948813
 142939/250000: episode: 207, duration: 16.068s, episode steps: 388, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.011728, mae: 1.713623, mean_q: 2.110397, mean_eps: 0.948612
 143407/250000: episode: 208, duration: 18.051s, episode steps: 468, steps per second:  26, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.012050, mae: 1.725612, mean_q: 2.125376, mean_eps: 0.948458
 143887/250000: episode: 209, duration: 19.328s, episode steps: 480, steps per second:  25, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.010306, mae: 1.723347, mean_q: 2.122670, mean_eps: 0.948287
 144251/250000: episode: 210, duration: 15.430s, episode steps: 364, steps per second:  24, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.009978, mae: 1.729173, mean_q: 2.129788, mean_eps: 0.948136
 144896/250000: episode: 211, duration: 28.033s, episode steps: 645, steps per second:  23, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.011192, mae: 1.722834, mean_q: 2.121163, mean_eps: 0.947954
 145541/250000: episode: 212, duration: 30.802s, episode steps: 645, steps per second:  21, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010728, mae: 1.728006, mean_q: 2.126361, mean_eps: 0.947722
 146099/250000: episode: 213, duration: 22.536s, episode steps: 558, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.009712, mae: 1.733405, mean_q: 2.134708, mean_eps: 0.947505
 146774/250000: episode: 214, duration: 27.445s, episode steps: 675, steps per second:  25, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.011224, mae: 1.746108, mean_q: 2.150427, mean_eps: 0.947283
 147384/250000: episode: 215, duration: 25.176s, episode steps: 610, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.010899, mae: 1.735224, mean_q: 2.139404, mean_eps: 0.947052
 148080/250000: episode: 216, duration: 28.631s, episode steps: 696, steps per second:  24, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.010528, mae: 1.708361, mean_q: 2.101069, mean_eps: 0.946817
 148450/250000: episode: 217, duration: 15.140s, episode steps: 370, steps per second:  24, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.010810, mae: 1.743664, mean_q: 2.148552, mean_eps: 0.946625
 149134/250000: episode: 218, duration: 27.312s, episode steps: 684, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.010475, mae: 1.725361, mean_q: 2.122403, mean_eps: 0.946435
 150224/250000: episode: 219, duration: 42.658s, episode steps: 1090, steps per second:  26, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.011942, mae: 1.747480, mean_q: 2.152148, mean_eps: 0.946116
 150706/250000: episode: 220, duration: 20.269s, episode steps: 482, steps per second:  24, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011237, mae: 1.811608, mean_q: 2.228319, mean_eps: 0.945833
 151778/250000: episode: 221, duration: 41.635s, episode steps: 1072, steps per second:  26, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.013720, mae: 1.817128, mean_q: 2.234726, mean_eps: 0.945553
 152714/250000: episode: 222, duration: 37.503s, episode steps: 936, steps per second:  25, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.012178, mae: 1.797605, mean_q: 2.210349, mean_eps: 0.945191
 153360/250000: episode: 223, duration: 25.890s, episode steps: 646, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.012005, mae: 1.814827, mean_q: 2.233031, mean_eps: 0.944907
 153859/250000: episode: 224, duration: 23.767s, episode steps: 499, steps per second:  21, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.012563, mae: 1.815501, mean_q: 2.230271, mean_eps: 0.944701
 155215/250000: episode: 225, duration: 55.985s, episode steps: 1356, steps per second:  24, episode reward: 15.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.011910, mae: 1.792474, mean_q: 2.203621, mean_eps: 0.944367
 155866/250000: episode: 226, duration: 26.357s, episode steps: 651, steps per second:  25, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.012028, mae: 1.791290, mean_q: 2.203176, mean_eps: 0.944006
 156668/250000: episode: 227, duration: 32.335s, episode steps: 802, steps per second:  25, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.012139, mae: 1.780195, mean_q: 2.192052, mean_eps: 0.943744
 157316/250000: episode: 228, duration: 26.499s, episode steps: 648, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012649, mae: 1.796127, mean_q: 2.210295, mean_eps: 0.943484
 158116/250000: episode: 229, duration: 31.355s, episode steps: 800, steps per second:  26, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.012475, mae: 1.807828, mean_q: 2.223375, mean_eps: 0.943223
 158619/250000: episode: 230, duration: 19.516s, episode steps: 503, steps per second:  26, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014947, mae: 1.807498, mean_q: 2.221414, mean_eps: 0.942988
 159289/250000: episode: 231, duration: 26.765s, episode steps: 670, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011945, mae: 1.795336, mean_q: 2.210311, mean_eps: 0.942777
 159925/250000: episode: 232, duration: 26.181s, episode steps: 636, steps per second:  24, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.012962, mae: 1.778413, mean_q: 2.187646, mean_eps: 0.942541
 161277/250000: episode: 233, duration: 53.718s, episode steps: 1352, steps per second:  25, episode reward: 13.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011459, mae: 1.802411, mean_q: 2.215994, mean_eps: 0.942183
 161907/250000: episode: 234, duration: 26.876s, episode steps: 630, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.010124, mae: 1.804088, mean_q: 2.219590, mean_eps: 0.941827
 162610/250000: episode: 235, duration: 30.708s, episode steps: 703, steps per second:  23, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.012325, mae: 1.774041, mean_q: 2.180576, mean_eps: 0.941587
 163538/250000: episode: 236, duration: 42.596s, episode steps: 928, steps per second:  22, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.011197, mae: 1.803930, mean_q: 2.217605, mean_eps: 0.941293
 164001/250000: episode: 237, duration: 18.251s, episode steps: 463, steps per second:  25, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.012936, mae: 1.806547, mean_q: 2.222440, mean_eps: 0.941043
 164364/250000: episode: 238, duration: 14.650s, episode steps: 363, steps per second:  25, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.011189, mae: 1.796843, mean_q: 2.212121, mean_eps: 0.940894
 164988/250000: episode: 239, duration: 24.404s, episode steps: 624, steps per second:  26, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.013165, mae: 1.790425, mean_q: 2.202211, mean_eps: 0.940717
 165443/250000: episode: 240, duration: 19.261s, episode steps: 455, steps per second:  24, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.011671, mae: 1.804805, mean_q: 2.220356, mean_eps: 0.940523
 166072/250000: episode: 241, duration: 25.201s, episode steps: 629, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.011904, mae: 1.797697, mean_q: 2.212723, mean_eps: 0.940328
 166474/250000: episode: 242, duration: 15.506s, episode steps: 402, steps per second:  26, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012293, mae: 1.800992, mean_q: 2.216023, mean_eps: 0.940142
 167332/250000: episode: 243, duration: 34.200s, episode steps: 858, steps per second:  25, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.011792, mae: 1.811133, mean_q: 2.228479, mean_eps: 0.939915
 168089/250000: episode: 244, duration: 30.055s, episode steps: 757, steps per second:  25, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.012044, mae: 1.806438, mean_q: 2.222175, mean_eps: 0.939624
 168597/250000: episode: 245, duration: 21.521s, episode steps: 508, steps per second:  24, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.010833, mae: 1.772487, mean_q: 2.179018, mean_eps: 0.939396
 169163/250000: episode: 246, duration: 22.017s, episode steps: 566, steps per second:  26, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.012472, mae: 1.782633, mean_q: 2.193751, mean_eps: 0.939203
 169591/250000: episode: 247, duration: 18.562s, episode steps: 428, steps per second:  23, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.011887, mae: 1.801225, mean_q: 2.217483, mean_eps: 0.939025
 170245/250000: episode: 248, duration: 29.054s, episode steps: 654, steps per second:  23, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.013384, mae: 1.801871, mean_q: 2.217362, mean_eps: 0.938830
 171193/250000: episode: 249, duration: 38.943s, episode steps: 948, steps per second:  24, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.013159, mae: 1.788695, mean_q: 2.201239, mean_eps: 0.938541
 171747/250000: episode: 250, duration: 25.561s, episode steps: 554, steps per second:  22, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.012878, mae: 1.807454, mean_q: 2.223709, mean_eps: 0.938271
 172455/250000: episode: 251, duration: 27.712s, episode steps: 708, steps per second:  26, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012640, mae: 1.797063, mean_q: 2.207801, mean_eps: 0.938044
 172829/250000: episode: 252, duration: 14.415s, episode steps: 374, steps per second:  26, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.010426, mae: 1.775172, mean_q: 2.182715, mean_eps: 0.937849
 173423/250000: episode: 253, duration: 24.826s, episode steps: 594, steps per second:  24, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.014456, mae: 1.798716, mean_q: 2.210010, mean_eps: 0.937675
 173838/250000: episode: 254, duration: 16.288s, episode steps: 415, steps per second:  25, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.011963, mae: 1.790546, mean_q: 2.201193, mean_eps: 0.937493
 174791/250000: episode: 255, duration: 38.562s, episode steps: 953, steps per second:  25, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.011694, mae: 1.784451, mean_q: 2.193504, mean_eps: 0.937247
 176004/250000: episode: 256, duration: 48.475s, episode steps: 1213, steps per second:  25, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.012337, mae: 1.811538, mean_q: 2.227404, mean_eps: 0.936857
 176389/250000: episode: 257, duration: 15.591s, episode steps: 385, steps per second:  25, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.011458, mae: 1.796835, mean_q: 2.212220, mean_eps: 0.936569
 177232/250000: episode: 258, duration: 33.991s, episode steps: 843, steps per second:  25, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.010871, mae: 1.788763, mean_q: 2.200220, mean_eps: 0.936348
 177824/250000: episode: 259, duration: 24.453s, episode steps: 592, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.011913, mae: 1.787590, mean_q: 2.195562, mean_eps: 0.936091
 178399/250000: episode: 260, duration: 23.527s, episode steps: 575, steps per second:  24, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.011437, mae: 1.788068, mean_q: 2.198629, mean_eps: 0.935880
 179062/250000: episode: 261, duration: 26.743s, episode steps: 663, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011586, mae: 1.803385, mean_q: 2.217223, mean_eps: 0.935657
 179778/250000: episode: 262, duration: 29.304s, episode steps: 716, steps per second:  24, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.013792, mae: 1.789631, mean_q: 2.197505, mean_eps: 0.935409
 180496/250000: episode: 263, duration: 31.159s, episode steps: 718, steps per second:  23, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.012554, mae: 1.795897, mean_q: 2.208950, mean_eps: 0.935151
 181127/250000: episode: 264, duration: 25.043s, episode steps: 631, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.011258, mae: 1.790209, mean_q: 2.201488, mean_eps: 0.934908
 181935/250000: episode: 265, duration: 32.887s, episode steps: 808, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012563, mae: 1.803257, mean_q: 2.217978, mean_eps: 0.934649
 182480/250000: episode: 266, duration: 22.195s, episode steps: 545, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.011698, mae: 1.774198, mean_q: 2.182527, mean_eps: 0.934406
 182947/250000: episode: 267, duration: 18.791s, episode steps: 467, steps per second:  25, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.012163, mae: 1.807445, mean_q: 2.223793, mean_eps: 0.934224
 183331/250000: episode: 268, duration: 15.626s, episode steps: 384, steps per second:  25, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.010744, mae: 1.776487, mean_q: 2.183605, mean_eps: 0.934070
 183879/250000: episode: 269, duration: 22.431s, episode steps: 548, steps per second:  24, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.013052, mae: 1.778182, mean_q: 2.183676, mean_eps: 0.933903
 184831/250000: episode: 270, duration: 38.557s, episode steps: 952, steps per second:  25, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.011464, mae: 1.785728, mean_q: 2.195052, mean_eps: 0.933633
 185385/250000: episode: 271, duration: 22.082s, episode steps: 554, steps per second:  25, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.011963, mae: 1.811346, mean_q: 2.225934, mean_eps: 0.933361
 186249/250000: episode: 272, duration: 36.950s, episode steps: 864, steps per second:  23, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011051, mae: 1.792636, mean_q: 2.203487, mean_eps: 0.933106
 186829/250000: episode: 273, duration: 22.937s, episode steps: 580, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.012715, mae: 1.785213, mean_q: 2.195391, mean_eps: 0.932846
 187625/250000: episode: 274, duration: 32.307s, episode steps: 796, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.012305, mae: 1.802129, mean_q: 2.216025, mean_eps: 0.932598
 188334/250000: episode: 275, duration: 31.587s, episode steps: 709, steps per second:  22, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.011768, mae: 1.784233, mean_q: 2.194091, mean_eps: 0.932327
 189478/250000: episode: 276, duration: 48.066s, episode steps: 1144, steps per second:  24, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012883, mae: 1.769916, mean_q: 2.175659, mean_eps: 0.931994
 190121/250000: episode: 277, duration: 25.607s, episode steps: 643, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.013726, mae: 1.789836, mean_q: 2.199753, mean_eps: 0.931672
 191143/250000: episode: 278, duration: 41.386s, episode steps: 1022, steps per second:  25, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012849, mae: 1.780945, mean_q: 2.190745, mean_eps: 0.931372
 192148/250000: episode: 279, duration: 40.806s, episode steps: 1005, steps per second:  25, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.011322, mae: 1.780170, mean_q: 2.186637, mean_eps: 0.931008
 192772/250000: episode: 280, duration: 25.312s, episode steps: 624, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.011698, mae: 1.781294, mean_q: 2.188229, mean_eps: 0.930715
 193512/250000: episode: 281, duration: 29.733s, episode steps: 740, steps per second:  25, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.011513, mae: 1.790257, mean_q: 2.199796, mean_eps: 0.930470
 194027/250000: episode: 282, duration: 21.055s, episode steps: 515, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.011261, mae: 1.776160, mean_q: 2.182682, mean_eps: 0.930244
 195090/250000: episode: 283, duration: 45.438s, episode steps: 1063, steps per second:  23, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.010748, mae: 1.798083, mean_q: 2.211056, mean_eps: 0.929959
 196027/250000: episode: 284, duration: 40.172s, episode steps: 937, steps per second:  23, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.011529, mae: 1.781030, mean_q: 2.190944, mean_eps: 0.929599
 197003/250000: episode: 285, duration: 38.643s, episode steps: 976, steps per second:  25, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.012495, mae: 1.784270, mean_q: 2.193948, mean_eps: 0.929255
 197499/250000: episode: 286, duration: 21.339s, episode steps: 496, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010623, mae: 1.785695, mean_q: 2.194236, mean_eps: 0.928990
 198255/250000: episode: 287, duration: 30.407s, episode steps: 756, steps per second:  25, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.011301, mae: 1.798117, mean_q: 2.207705, mean_eps: 0.928765
 198679/250000: episode: 288, duration: 17.447s, episode steps: 424, steps per second:  24, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.013027, mae: 1.792366, mean_q: 2.201206, mean_eps: 0.928552
 199272/250000: episode: 289, duration: 24.184s, episode steps: 593, steps per second:  25, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011214, mae: 1.774188, mean_q: 2.180239, mean_eps: 0.928369
 199847/250000: episode: 290, duration: 23.513s, episode steps: 575, steps per second:  24, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010894, mae: 1.787192, mean_q: 2.198272, mean_eps: 0.928159
 200235/250000: episode: 291, duration: 15.933s, episode steps: 388, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.013907, mae: 1.824870, mean_q: 2.239346, mean_eps: 0.927986
 200939/250000: episode: 292, duration: 28.944s, episode steps: 704, steps per second:  24, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.012307, mae: 1.836326, mean_q: 2.256368, mean_eps: 0.927789
 201468/250000: episode: 293, duration: 23.880s, episode steps: 529, steps per second:  22, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011900, mae: 1.835890, mean_q: 2.256950, mean_eps: 0.927567
 201946/250000: episode: 294, duration: 20.251s, episode steps: 478, steps per second:  24, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.012820, mae: 1.807553, mean_q: 2.222333, mean_eps: 0.927386
 202464/250000: episode: 295, duration: 21.994s, episode steps: 518, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013425, mae: 1.836598, mean_q: 2.258384, mean_eps: 0.927207
 202850/250000: episode: 296, duration: 15.307s, episode steps: 386, steps per second:  25, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013473, mae: 1.821193, mean_q: 2.237765, mean_eps: 0.927044
 203965/250000: episode: 297, duration: 44.880s, episode steps: 1115, steps per second:  25, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.012713, mae: 1.825655, mean_q: 2.241405, mean_eps: 0.926773
 204610/250000: episode: 298, duration: 26.597s, episode steps: 645, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.012882, mae: 1.820831, mean_q: 2.236389, mean_eps: 0.926456
 205412/250000: episode: 299, duration: 32.747s, episode steps: 802, steps per second:  24, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.012681, mae: 1.831654, mean_q: 2.250628, mean_eps: 0.926196
 205991/250000: episode: 300, duration: 22.227s, episode steps: 579, steps per second:  26, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.012697, mae: 1.818694, mean_q: 2.234483, mean_eps: 0.925948
 206482/250000: episode: 301, duration: 20.077s, episode steps: 491, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.012401, mae: 1.813773, mean_q: 2.227693, mean_eps: 0.925755
 207079/250000: episode: 302, duration: 24.635s, episode steps: 597, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.012836, mae: 1.830945, mean_q: 2.248847, mean_eps: 0.925559
 207672/250000: episode: 303, duration: 23.532s, episode steps: 593, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013474, mae: 1.833309, mean_q: 2.253117, mean_eps: 0.925345
 208213/250000: episode: 304, duration: 21.788s, episode steps: 541, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012434, mae: 1.844905, mean_q: 2.268879, mean_eps: 0.925141
 208856/250000: episode: 305, duration: 28.096s, episode steps: 643, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011550, mae: 1.816436, mean_q: 2.232783, mean_eps: 0.924928
 209523/250000: episode: 306, duration: 27.776s, episode steps: 667, steps per second:  24, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011970, mae: 1.807934, mean_q: 2.223802, mean_eps: 0.924692
 210772/250000: episode: 307, duration: 53.327s, episode steps: 1249, steps per second:  23, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.013255, mae: 1.813583, mean_q: 2.227901, mean_eps: 0.924347
 211729/250000: episode: 308, duration: 39.010s, episode steps: 957, steps per second:  25, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012776, mae: 1.811335, mean_q: 2.222658, mean_eps: 0.923950
 212109/250000: episode: 309, duration: 15.116s, episode steps: 380, steps per second:  25, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.013055, mae: 1.830854, mean_q: 2.248268, mean_eps: 0.923709
 213073/250000: episode: 310, duration: 40.074s, episode steps: 964, steps per second:  24, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.013044, mae: 1.821344, mean_q: 2.237287, mean_eps: 0.923467
 214042/250000: episode: 311, duration: 38.658s, episode steps: 969, steps per second:  25, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012918, mae: 1.819411, mean_q: 2.236280, mean_eps: 0.923119
 214652/250000: episode: 312, duration: 25.371s, episode steps: 610, steps per second:  24, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.013260, mae: 1.821470, mean_q: 2.236086, mean_eps: 0.922835
 215116/250000: episode: 313, duration: 19.111s, episode steps: 464, steps per second:  24, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.013070, mae: 1.831495, mean_q: 2.249027, mean_eps: 0.922642
 215774/250000: episode: 314, duration: 26.497s, episode steps: 658, steps per second:  25, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.012956, mae: 1.836070, mean_q: 2.253317, mean_eps: 0.922440
 216411/250000: episode: 315, duration: 25.599s, episode steps: 637, steps per second:  25, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011293, mae: 1.828028, mean_q: 2.244103, mean_eps: 0.922207
 216947/250000: episode: 316, duration: 22.255s, episode steps: 536, steps per second:  24, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.011657, mae: 1.833685, mean_q: 2.251337, mean_eps: 0.921996
 217464/250000: episode: 317, duration: 20.882s, episode steps: 517, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.012522, mae: 1.832440, mean_q: 2.249610, mean_eps: 0.921807
 217837/250000: episode: 318, duration: 14.903s, episode steps: 373, steps per second:  25, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.013387, mae: 1.808019, mean_q: 2.221815, mean_eps: 0.921646
 218498/250000: episode: 319, duration: 27.104s, episode steps: 661, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.013759, mae: 1.827297, mean_q: 2.240849, mean_eps: 0.921460
 219079/250000: episode: 320, duration: 23.406s, episode steps: 581, steps per second:  25, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.012638, mae: 1.809933, mean_q: 2.220385, mean_eps: 0.921236
 219562/250000: episode: 321, duration: 19.148s, episode steps: 483, steps per second:  25, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011617, mae: 1.818458, mean_q: 2.232340, mean_eps: 0.921045
 220074/250000: episode: 322, duration: 22.143s, episode steps: 512, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012730, mae: 1.803220, mean_q: 2.212253, mean_eps: 0.920866
 220531/250000: episode: 323, duration: 18.553s, episode steps: 457, steps per second:  25, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.012225, mae: 1.826337, mean_q: 2.244115, mean_eps: 0.920691
 221796/250000: episode: 324, duration: 51.353s, episode steps: 1265, steps per second:  25, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.014267, mae: 1.825502, mean_q: 2.238928, mean_eps: 0.920382
 222713/250000: episode: 325, duration: 36.642s, episode steps: 917, steps per second:  25, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.013355, mae: 1.826594, mean_q: 2.245741, mean_eps: 0.919989
 223327/250000: episode: 326, duration: 25.840s, episode steps: 614, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.012479, mae: 1.807194, mean_q: 2.218525, mean_eps: 0.919713
 223788/250000: episode: 327, duration: 17.949s, episode steps: 461, steps per second:  26, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.015182, mae: 1.836550, mean_q: 2.255548, mean_eps: 0.919520
 224431/250000: episode: 328, duration: 25.856s, episode steps: 643, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.011535, mae: 1.838082, mean_q: 2.257346, mean_eps: 0.919321
 225125/250000: episode: 329, duration: 27.821s, episode steps: 694, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.014717, mae: 1.833259, mean_q: 2.250224, mean_eps: 0.919080
 226118/250000: episode: 330, duration: 41.218s, episode steps: 993, steps per second:  24, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.011750, mae: 1.817736, mean_q: 2.233174, mean_eps: 0.918776
 226890/250000: episode: 331, duration: 31.297s, episode steps: 772, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012340, mae: 1.830697, mean_q: 2.247350, mean_eps: 0.918459
 227701/250000: episode: 332, duration: 33.221s, episode steps: 811, steps per second:  24, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.012288, mae: 1.814768, mean_q: 2.227534, mean_eps: 0.918173
 228519/250000: episode: 333, duration: 33.338s, episode steps: 818, steps per second:  25, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012686, mae: 1.822898, mean_q: 2.237188, mean_eps: 0.917880
 228920/250000: episode: 334, duration: 16.434s, episode steps: 401, steps per second:  24, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.013747, mae: 1.839030, mean_q: 2.256418, mean_eps: 0.917662
 229728/250000: episode: 335, duration: 33.454s, episode steps: 808, steps per second:  24, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.013407, mae: 1.815563, mean_q: 2.230825, mean_eps: 0.917444
 230224/250000: episode: 336, duration: 20.341s, episode steps: 496, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.496 [0.000, 5.000],  loss: 0.011017, mae: 1.810192, mean_q: 2.225213, mean_eps: 0.917209
 230792/250000: episode: 337, duration: 23.895s, episode steps: 568, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.012141, mae: 1.812297, mean_q: 2.224870, mean_eps: 0.917018
 231421/250000: episode: 338, duration: 30.071s, episode steps: 629, steps per second:  21, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012038, mae: 1.810069, mean_q: 2.222389, mean_eps: 0.916802
 232192/250000: episode: 339, duration: 31.921s, episode steps: 771, steps per second:  24, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013240, mae: 1.825669, mean_q: 2.244680, mean_eps: 0.916550
 232725/250000: episode: 340, duration: 22.452s, episode steps: 533, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.012167, mae: 1.818843, mean_q: 2.235592, mean_eps: 0.916315
 233330/250000: episode: 341, duration: 24.431s, episode steps: 605, steps per second:  25, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.729 [0.000, 5.000],  loss: 0.011958, mae: 1.827214, mean_q: 2.245935, mean_eps: 0.916110
 233840/250000: episode: 342, duration: 20.685s, episode steps: 510, steps per second:  25, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.012636, mae: 1.821832, mean_q: 2.238673, mean_eps: 0.915910
 234879/250000: episode: 343, duration: 43.819s, episode steps: 1039, steps per second:  24, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.012210, mae: 1.834802, mean_q: 2.252534, mean_eps: 0.915631
 235429/250000: episode: 344, duration: 22.115s, episode steps: 550, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.012701, mae: 1.817657, mean_q: 2.231505, mean_eps: 0.915345
 236039/250000: episode: 345, duration: 26.226s, episode steps: 610, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011968, mae: 1.814002, mean_q: 2.227657, mean_eps: 0.915136
 236706/250000: episode: 346, duration: 26.215s, episode steps: 667, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.014135, mae: 1.815288, mean_q: 2.229657, mean_eps: 0.914906
 237375/250000: episode: 347, duration: 28.062s, episode steps: 669, steps per second:  24, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.014412, mae: 1.841651, mean_q: 2.259263, mean_eps: 0.914666
 238333/250000: episode: 348, duration: 37.795s, episode steps: 958, steps per second:  25, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010807, mae: 1.812862, mean_q: 2.228847, mean_eps: 0.914373
 239220/250000: episode: 349, duration: 38.004s, episode steps: 887, steps per second:  23, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012651, mae: 1.830047, mean_q: 2.244784, mean_eps: 0.914041
 239846/250000: episode: 350, duration: 24.314s, episode steps: 626, steps per second:  26, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.014060, mae: 1.829779, mean_q: 2.246878, mean_eps: 0.913768
 240675/250000: episode: 351, duration: 34.923s, episode steps: 829, steps per second:  24, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012534, mae: 1.835669, mean_q: 2.254506, mean_eps: 0.913506
 241221/250000: episode: 352, duration: 21.398s, episode steps: 546, steps per second:  26, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.010589, mae: 1.818176, mean_q: 2.232170, mean_eps: 0.913259
 241895/250000: episode: 353, duration: 28.068s, episode steps: 674, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.011037, mae: 1.803974, mean_q: 2.214448, mean_eps: 0.913039
 242482/250000: episode: 354, duration: 24.216s, episode steps: 587, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.012213, mae: 1.823244, mean_q: 2.238210, mean_eps: 0.912812
 243171/250000: episode: 355, duration: 27.797s, episode steps: 689, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.012283, mae: 1.820173, mean_q: 2.234817, mean_eps: 0.912583
 244007/250000: episode: 356, duration: 34.019s, episode steps: 836, steps per second:  25, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.012572, mae: 1.820298, mean_q: 2.235210, mean_eps: 0.912308
 244560/250000: episode: 357, duration: 22.147s, episode steps: 553, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.013272, mae: 1.818710, mean_q: 2.233330, mean_eps: 0.912058
 245148/250000: episode: 358, duration: 24.751s, episode steps: 588, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.011662, mae: 1.800373, mean_q: 2.210308, mean_eps: 0.911853
 245963/250000: episode: 359, duration: 34.062s, episode steps: 815, steps per second:  24, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011489, mae: 1.823259, mean_q: 2.237861, mean_eps: 0.911601
 246479/250000: episode: 360, duration: 20.793s, episode steps: 516, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.012256, mae: 1.841794, mean_q: 2.259196, mean_eps: 0.911361
 246964/250000: episode: 361, duration: 20.509s, episode steps: 485, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.013384, mae: 1.808997, mean_q: 2.219177, mean_eps: 0.911181
 247568/250000: episode: 362, duration: 24.661s, episode steps: 604, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.012327, mae: 1.831100, mean_q: 2.245345, mean_eps: 0.910985
 248117/250000: episode: 363, duration: 23.004s, episode steps: 549, steps per second:  24, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013084, mae: 1.819709, mean_q: 2.231114, mean_eps: 0.910777
 249168/250000: episode: 364, duration: 42.529s, episode steps: 1051, steps per second:  25, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.011356, mae: 1.812914, mean_q: 2.224325, mean_eps: 0.910489
 249718/250000: episode: 365, duration: 25.003s, episode steps: 550, steps per second:  22, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.013034, mae: 1.805827, mean_q: 2.216480, mean_eps: 0.910201
done, took 9762.571 seconds
########################################################
PROCESO TERMINADO
########################################################