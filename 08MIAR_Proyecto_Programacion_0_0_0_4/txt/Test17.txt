['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_1000000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_260000.h5f.index', './models\\dqn_weights_270000.h5f.index', './models\\dqn_weights_280000.h5f.index', './models\\dqn_weights_290000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_300000.h5f.index', './models\\dqn_weights_310000.h5f.index', './models\\dqn_weights_320000.h5f.index', './models\\dqn_weights_330000.h5f.index', './models\\dqn_weights_340000.h5f.index', './models\\dqn_weights_350000.h5f.index', './models\\dqn_weights_360000.h5f.index', './models\\dqn_weights_370000.h5f.index', './models\\dqn_weights_380000.h5f.index', './models\\dqn_weights_390000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_400000.h5f.index', './models\\dqn_weights_410000.h5f.index', './models\\dqn_weights_420000.h5f.index', './models\\dqn_weights_430000.h5f.index', './models\\dqn_weights_440000.h5f.index', './models\\dqn_weights_450000.h5f.index', './models\\dqn_weights_460000.h5f.index', './models\\dqn_weights_470000.h5f.index', './models\\dqn_weights_480000.h5f.index', './models\\dqn_weights_490000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_500000.h5f.index', './models\\dqn_weights_510000.h5f.index', './models\\dqn_weights_520000.h5f.index', './models\\dqn_weights_530000.h5f.index', './models\\dqn_weights_540000.h5f.index', './models\\dqn_weights_550000.h5f.index', './models\\dqn_weights_560000.h5f.index', './models\\dqn_weights_570000.h5f.index', './models\\dqn_weights_580000.h5f.index', './models\\dqn_weights_590000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_600000.h5f.index', './models\\dqn_weights_610000.h5f.index', './models\\dqn_weights_620000.h5f.index', './models\\dqn_weights_630000.h5f.index', './models\\dqn_weights_640000.h5f.index', './models\\dqn_weights_650000.h5f.index', './models\\dqn_weights_660000.h5f.index', './models\\dqn_weights_670000.h5f.index', './models\\dqn_weights_680000.h5f.index', './models\\dqn_weights_690000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_700000.h5f.index', './models\\dqn_weights_710000.h5f.index', './models\\dqn_weights_720000.h5f.index', './models\\dqn_weights_730000.h5f.index', './models\\dqn_weights_740000.h5f.index', './models\\dqn_weights_750000.h5f.index', './models\\dqn_weights_760000.h5f.index', './models\\dqn_weights_770000.h5f.index', './models\\dqn_weights_780000.h5f.index', './models\\dqn_weights_790000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_800000.h5f.index', './models\\dqn_weights_810000.h5f.index', './models\\dqn_weights_820000.h5f.index', './models\\dqn_weights_830000.h5f.index', './models\\dqn_weights_840000.h5f.index', './models\\dqn_weights_850000.h5f.index', './models\\dqn_weights_860000.h5f.index', './models\\dqn_weights_870000.h5f.index', './models\\dqn_weights_880000.h5f.index', './models\\dqn_weights_890000.h5f.index', './models\\dqn_weights_90000.h5f.index', './models\\dqn_weights_900000.h5f.index', './models\\dqn_weights_910000.h5f.index', './models\\dqn_weights_920000.h5f.index', './models\\dqn_weights_930000.h5f.index', './models\\dqn_weights_940000.h5f.index', './models\\dqn_weights_950000.h5f.index', './models\\dqn_weights_960000.h5f.index', './models\\dqn_weights_970000.h5f.index', './models\\dqn_weights_980000.h5f.index', './models\\dqn_weights_990000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_1000000.h5f
Hay pesos anteriores y se van a cargar
Training for 100000 steps ...
C:\Users\Rociniel\Anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
   420/100000: episode: 1, duration: 4.383s, episode steps: 420, steps per second:  96, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  1242/100000: episode: 2, duration: 7.695s, episode steps: 822, steps per second: 107, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  1870/100000: episode: 3, duration: 5.536s, episode steps: 628, steps per second: 113, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  2558/100000: episode: 4, duration: 5.043s, episode steps: 688, steps per second: 136, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  2954/100000: episode: 5, duration: 2.281s, episode steps: 396, steps per second: 174, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  3560/100000: episode: 6, duration: 5.257s, episode steps: 606, steps per second: 115, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  4191/100000: episode: 7, duration: 5.856s, episode steps: 631, steps per second: 108, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  4557/100000: episode: 8, duration: 2.452s, episode steps: 366, steps per second: 149, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
C:\Users\Rociniel\Anaconda3\envs\miar_rl\lib\site-packages\tensorflow\python\keras\engine\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
  5580/100000: episode: 9, duration: 45.479s, episode steps: 1023, steps per second:  22, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.021064, mae: 2.892812, mean_q: 3.482731, mean_eps: 0.952390
  6622/100000: episode: 10, duration: 65.970s, episode steps: 1042, steps per second:  16, episode reward:  7.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.016070, mae: 2.846563, mean_q: 3.430616, mean_eps: 0.945100
  7280/100000: episode: 11, duration: 42.836s, episode steps: 658, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.014234, mae: 2.866731, mean_q: 3.457438, mean_eps: 0.937450
  7776/100000: episode: 12, duration: 32.378s, episode steps: 496, steps per second:  15, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.016337, mae: 2.861814, mean_q: 3.451287, mean_eps: 0.932266
  8572/100000: episode: 13, duration: 52.026s, episode steps: 796, steps per second:  15, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.013610, mae: 2.834000, mean_q: 3.418113, mean_eps: 0.926452
  9332/100000: episode: 14, duration: 49.148s, episode steps: 760, steps per second:  15, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.013783, mae: 2.834004, mean_q: 3.419742, mean_eps: 0.919450
  9807/100000: episode: 15, duration: 33.325s, episode steps: 475, steps per second:  14, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.012725, mae: 2.814185, mean_q: 3.393834, mean_eps: 0.913888
 10421/100000: episode: 16, duration: 47.126s, episode steps: 614, steps per second:  13, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012812, mae: 2.793216, mean_q: 3.371191, mean_eps: 0.908974
 11008/100000: episode: 17, duration: 37.862s, episode steps: 587, steps per second:  16, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.013679, mae: 2.784442, mean_q: 3.358614, mean_eps: 0.903574
 11929/100000: episode: 18, duration: 60.349s, episode steps: 921, steps per second:  15, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012343, mae: 2.792070, mean_q: 3.367130, mean_eps: 0.896788
 12329/100000: episode: 19, duration: 28.710s, episode steps: 400, steps per second:  14, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.012982, mae: 2.775917, mean_q: 3.346784, mean_eps: 0.890830
 12959/100000: episode: 20, duration: 38.388s, episode steps: 630, steps per second:  16, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.013310, mae: 2.764067, mean_q: 3.335233, mean_eps: 0.886204
 13624/100000: episode: 21, duration: 44.415s, episode steps: 665, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.011286, mae: 2.786705, mean_q: 3.362762, mean_eps: 0.880390
 14466/100000: episode: 22, duration: 60.979s, episode steps: 842, steps per second:  14, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.013073, mae: 2.737588, mean_q: 3.302267, mean_eps: 0.873604
 15091/100000: episode: 23, duration: 40.051s, episode steps: 625, steps per second:  16, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.011939, mae: 2.725645, mean_q: 3.288013, mean_eps: 0.866998
 16085/100000: episode: 24, duration: 64.988s, episode steps: 994, steps per second:  15, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.014633, mae: 2.716699, mean_q: 3.277165, mean_eps: 0.859708
 16471/100000: episode: 25, duration: 30.369s, episode steps: 386, steps per second:  13, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.014802, mae: 2.724195, mean_q: 3.285817, mean_eps: 0.853498
 16935/100000: episode: 26, duration: 31.637s, episode steps: 464, steps per second:  15, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.011463, mae: 2.702361, mean_q: 3.261097, mean_eps: 0.849682
 17993/100000: episode: 27, duration: 79.331s, episode steps: 1058, steps per second:  13, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.013951, mae: 2.706970, mean_q: 3.265862, mean_eps: 0.842824
 18649/100000: episode: 28, duration: 42.451s, episode steps: 656, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.010843, mae: 2.705850, mean_q: 3.266735, mean_eps: 0.835102
 19305/100000: episode: 29, duration: 42.791s, episode steps: 656, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010639, mae: 2.691842, mean_q: 3.248655, mean_eps: 0.829198
 20044/100000: episode: 30, duration: 47.619s, episode steps: 739, steps per second:  16, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.012864, mae: 2.701199, mean_q: 3.259368, mean_eps: 0.822934
 20878/100000: episode: 31, duration: 51.631s, episode steps: 834, steps per second:  16, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012403, mae: 2.715481, mean_q: 3.276940, mean_eps: 0.815860
 21629/100000: episode: 32, duration: 50.177s, episode steps: 751, steps per second:  15, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.013993, mae: 2.693481, mean_q: 3.249784, mean_eps: 0.808714
 22843/100000: episode: 33, duration: 85.067s, episode steps: 1214, steps per second:  14, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.011671, mae: 2.703664, mean_q: 3.264417, mean_eps: 0.799876
 23755/100000: episode: 34, duration: 60.587s, episode steps: 912, steps per second:  15, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.013998, mae: 2.711556, mean_q: 3.271920, mean_eps: 0.790318
 24269/100000: episode: 35, duration: 33.717s, episode steps: 514, steps per second:  15, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.011716, mae: 2.680704, mean_q: 3.233610, mean_eps: 0.783892
 25021/100000: episode: 36, duration: 50.016s, episode steps: 752, steps per second:  15, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.013019, mae: 2.672366, mean_q: 3.224816, mean_eps: 0.778186
 25609/100000: episode: 37, duration: 37.868s, episode steps: 588, steps per second:  16, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.014541, mae: 2.691505, mean_q: 3.248800, mean_eps: 0.772156
 26328/100000: episode: 38, duration: 48.313s, episode steps: 719, steps per second:  15, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.010607, mae: 2.678826, mean_q: 3.233685, mean_eps: 0.766288
 27156/100000: episode: 39, duration: 54.349s, episode steps: 828, steps per second:  15, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010611, mae: 2.675089, mean_q: 3.229552, mean_eps: 0.759340
 27645/100000: episode: 40, duration: 30.859s, episode steps: 489, steps per second:  16, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011378, mae: 2.684937, mean_q: 3.239748, mean_eps: 0.753400
 28078/100000: episode: 41, duration: 29.395s, episode steps: 433, steps per second:  15, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.829 [0.000, 5.000],  loss: 0.013163, mae: 2.695318, mean_q: 3.253176, mean_eps: 0.749242
 28475/100000: episode: 42, duration: 25.337s, episode steps: 397, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013794, mae: 2.709294, mean_q: 3.269631, mean_eps: 0.745516
 28936/100000: episode: 43, duration: 33.957s, episode steps: 461, steps per second:  14, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.014781, mae: 2.691909, mean_q: 3.248302, mean_eps: 0.741664
 29464/100000: episode: 44, duration: 34.836s, episode steps: 528, steps per second:  15, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011406, mae: 2.700904, mean_q: 3.262057, mean_eps: 0.737218
 30615/100000: episode: 45, duration: 74.150s, episode steps: 1151, steps per second:  16, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.011747, mae: 2.675974, mean_q: 3.230913, mean_eps: 0.729658
 31160/100000: episode: 46, duration: 39.496s, episode steps: 545, steps per second:  14, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.011818, mae: 2.681261, mean_q: 3.237440, mean_eps: 0.722026
 31857/100000: episode: 47, duration: 46.474s, episode steps: 697, steps per second:  15, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.011537, mae: 2.714816, mean_q: 3.279431, mean_eps: 0.716428
 32503/100000: episode: 48, duration: 40.455s, episode steps: 646, steps per second:  16, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.011184, mae: 2.693703, mean_q: 3.252031, mean_eps: 0.710380
 32899/100000: episode: 49, duration: 28.077s, episode steps: 396, steps per second:  14, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.012705, mae: 2.665158, mean_q: 3.215777, mean_eps: 0.705700
 33599/100000: episode: 50, duration: 44.804s, episode steps: 700, steps per second:  16, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.011986, mae: 2.673771, mean_q: 3.227754, mean_eps: 0.700768
 34582/100000: episode: 51, duration: 65.916s, episode steps: 983, steps per second:  15, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.012618, mae: 2.670545, mean_q: 3.225041, mean_eps: 0.693190
 34969/100000: episode: 52, duration: 30.296s, episode steps: 387, steps per second:  13, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.011712, mae: 2.643805, mean_q: 3.190430, mean_eps: 0.687016
 35826/100000: episode: 53, duration: 58.414s, episode steps: 857, steps per second:  15, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.010836, mae: 2.688035, mean_q: 3.245240, mean_eps: 0.681418
 36168/100000: episode: 54, duration: 23.317s, episode steps: 342, steps per second:  15, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.011357, mae: 2.679508, mean_q: 3.234768, mean_eps: 0.676036
 37502/100000: episode: 55, duration: 89.279s, episode steps: 1334, steps per second:  15, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.012122, mae: 2.680122, mean_q: 3.235393, mean_eps: 0.668494
 37870/100000: episode: 56, duration: 25.821s, episode steps: 368, steps per second:  14, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.011242, mae: 2.680863, mean_q: 3.236834, mean_eps: 0.660826
 38766/100000: episode: 57, duration: 60.785s, episode steps: 896, steps per second:  15, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.010721, mae: 2.667749, mean_q: 3.219881, mean_eps: 0.655138
 40397/100000: episode: 58, duration: 109.706s, episode steps: 1631, steps per second:  15, episode reward: 34.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.012249, mae: 2.684116, mean_q: 3.240781, mean_eps: 0.643762
 41053/100000: episode: 59, duration: 43.947s, episode steps: 656, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011739, mae: 2.663657, mean_q: 3.215535, mean_eps: 0.633466
 41678/100000: episode: 60, duration: 42.383s, episode steps: 625, steps per second:  15, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.011821, mae: 2.677923, mean_q: 3.231534, mean_eps: 0.627706
 42254/100000: episode: 61, duration: 39.007s, episode steps: 576, steps per second:  15, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.011896, mae: 2.691792, mean_q: 3.249864, mean_eps: 0.622306
 43459/100000: episode: 62, duration: 79.139s, episode steps: 1205, steps per second:  15, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013541, mae: 2.672147, mean_q: 3.226715, mean_eps: 0.614296
 44460/100000: episode: 63, duration: 68.410s, episode steps: 1001, steps per second:  15, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.012069, mae: 2.668959, mean_q: 3.222405, mean_eps: 0.604378
 45132/100000: episode: 64, duration: 47.965s, episode steps: 672, steps per second:  14, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.010840, mae: 2.664830, mean_q: 3.216944, mean_eps: 0.596854
 46041/100000: episode: 65, duration: 62.637s, episode steps: 909, steps per second:  15, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.010774, mae: 2.663793, mean_q: 3.215966, mean_eps: 0.589726
 46411/100000: episode: 66, duration: 24.564s, episode steps: 370, steps per second:  15, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.011828, mae: 2.651796, mean_q: 3.201267, mean_eps: 0.583966
 46796/100000: episode: 67, duration: 25.240s, episode steps: 385, steps per second:  15, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.011840, mae: 2.687953, mean_q: 3.244545, mean_eps: 0.580582
 47350/100000: episode: 68, duration: 37.495s, episode steps: 554, steps per second:  15, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.012509, mae: 2.645508, mean_q: 3.193134, mean_eps: 0.576352
 48290/100000: episode: 69, duration: 62.362s, episode steps: 940, steps per second:  15, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.011975, mae: 2.662900, mean_q: 3.215958, mean_eps: 0.569620
 48954/100000: episode: 70, duration: 49.488s, episode steps: 664, steps per second:  13, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.014271, mae: 2.675512, mean_q: 3.227663, mean_eps: 0.562402
 49441/100000: episode: 71, duration: 32.654s, episode steps: 487, steps per second:  15, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013364, mae: 2.644578, mean_q: 3.191070, mean_eps: 0.557218
 50165/100000: episode: 72, duration: 51.977s, episode steps: 724, steps per second:  14, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.013856, mae: 2.660538, mean_q: 3.212108, mean_eps: 0.551764
 50736/100000: episode: 73, duration: 38.715s, episode steps: 571, steps per second:  15, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.013418, mae: 2.645491, mean_q: 3.192621, mean_eps: 0.545950
 51309/100000: episode: 74, duration: 38.940s, episode steps: 573, steps per second:  15, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.016124, mae: 2.658050, mean_q: 3.206207, mean_eps: 0.540802
 52535/100000: episode: 75, duration: 81.234s, episode steps: 1226, steps per second:  15, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.013056, mae: 2.682375, mean_q: 3.237137, mean_eps: 0.532702
 53072/100000: episode: 76, duration: 36.753s, episode steps: 537, steps per second:  15, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.014661, mae: 2.672007, mean_q: 3.223956, mean_eps: 0.524782
 53876/100000: episode: 77, duration: 53.457s, episode steps: 804, steps per second:  15, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.014786, mae: 2.656901, mean_q: 3.207581, mean_eps: 0.518752
 54538/100000: episode: 78, duration: 45.500s, episode steps: 662, steps per second:  15, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.009987, mae: 2.663981, mean_q: 3.215676, mean_eps: 0.512146
 55188/100000: episode: 79, duration: 45.634s, episode steps: 650, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.013746, mae: 2.651944, mean_q: 3.199336, mean_eps: 0.506242
 55826/100000: episode: 80, duration: 42.811s, episode steps: 638, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.013085, mae: 2.642078, mean_q: 3.187732, mean_eps: 0.500446
 56528/100000: episode: 81, duration: 47.241s, episode steps: 702, steps per second:  15, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011445, mae: 2.647036, mean_q: 3.193298, mean_eps: 0.494416
 57252/100000: episode: 82, duration: 50.184s, episode steps: 724, steps per second:  14, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.013152, mae: 2.633666, mean_q: 3.178153, mean_eps: 0.488008
 57723/100000: episode: 83, duration: 30.411s, episode steps: 471, steps per second:  15, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.013653, mae: 2.649090, mean_q: 3.196117, mean_eps: 0.482626
 58292/100000: episode: 84, duration: 39.198s, episode steps: 569, steps per second:  15, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014527, mae: 2.640804, mean_q: 3.187287, mean_eps: 0.477946
 59291/100000: episode: 85, duration: 67.166s, episode steps: 999, steps per second:  15, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011308, mae: 2.653626, mean_q: 3.202803, mean_eps: 0.470890
 60234/100000: episode: 86, duration: 63.475s, episode steps: 943, steps per second:  15, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.011910, mae: 2.635687, mean_q: 3.181263, mean_eps: 0.462142
 60940/100000: episode: 87, duration: 53.340s, episode steps: 706, steps per second:  13, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.013072, mae: 2.639880, mean_q: 3.187271, mean_eps: 0.454726
 61716/100000: episode: 88, duration: 56.372s, episode steps: 776, steps per second:  14, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.011742, mae: 2.650947, mean_q: 3.199375, mean_eps: 0.448066
 62310/100000: episode: 89, duration: 39.460s, episode steps: 594, steps per second:  15, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.012239, mae: 2.662199, mean_q: 3.212746, mean_eps: 0.441892
 62824/100000: episode: 90, duration: 39.248s, episode steps: 514, steps per second:  13, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.014548, mae: 2.627644, mean_q: 3.169847, mean_eps: 0.436906
 63289/100000: episode: 91, duration: 30.890s, episode steps: 465, steps per second:  15, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.012092, mae: 2.637978, mean_q: 3.183659, mean_eps: 0.432496
 64073/100000: episode: 92, duration: 51.888s, episode steps: 784, steps per second:  15, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.013007, mae: 2.640668, mean_q: 3.186888, mean_eps: 0.426862
 64773/100000: episode: 93, duration: 48.208s, episode steps: 700, steps per second:  15, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011840, mae: 2.649610, mean_q: 3.197743, mean_eps: 0.420184
 65257/100000: episode: 94, duration: 32.316s, episode steps: 484, steps per second:  15, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.010435, mae: 2.635949, mean_q: 3.181825, mean_eps: 0.414856
 66069/100000: episode: 95, duration: 58.765s, episode steps: 812, steps per second:  14, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013637, mae: 2.649132, mean_q: 3.196005, mean_eps: 0.409024
 67217/100000: episode: 96, duration: 77.359s, episode steps: 1148, steps per second:  15, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.011874, mae: 2.638493, mean_q: 3.183713, mean_eps: 0.400204
 67865/100000: episode: 97, duration: 43.448s, episode steps: 648, steps per second:  15, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.011001, mae: 2.627408, mean_q: 3.169826, mean_eps: 0.392122
 68373/100000: episode: 98, duration: 33.327s, episode steps: 508, steps per second:  15, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.015198, mae: 2.649879, mean_q: 3.198182, mean_eps: 0.386920
 68930/100000: episode: 99, duration: 39.079s, episode steps: 557, steps per second:  14, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.013576, mae: 2.642979, mean_q: 3.186668, mean_eps: 0.382132
 69605/100000: episode: 100, duration: 46.226s, episode steps: 675, steps per second:  15, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.012651, mae: 2.639997, mean_q: 3.184518, mean_eps: 0.376588
 70334/100000: episode: 101, duration: 48.433s, episode steps: 729, steps per second:  15, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012767, mae: 2.660932, mean_q: 3.210530, mean_eps: 0.370270
 70699/100000: episode: 102, duration: 24.928s, episode steps: 365, steps per second:  15, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.013180, mae: 2.620437, mean_q: 3.161007, mean_eps: 0.365356
 71272/100000: episode: 103, duration: 41.970s, episode steps: 573, steps per second:  14, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.011842, mae: 2.632109, mean_q: 3.175207, mean_eps: 0.361144
 71913/100000: episode: 104, duration: 43.266s, episode steps: 641, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.011252, mae: 2.626101, mean_q: 3.167553, mean_eps: 0.355672
 72573/100000: episode: 105, duration: 45.731s, episode steps: 660, steps per second:  14, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.014496, mae: 2.630665, mean_q: 3.173400, mean_eps: 0.349804
 73354/100000: episode: 106, duration: 53.588s, episode steps: 781, steps per second:  15, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014551, mae: 2.636245, mean_q: 3.179500, mean_eps: 0.343324
 73976/100000: episode: 107, duration: 41.586s, episode steps: 622, steps per second:  15, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.009649, mae: 2.615516, mean_q: 3.155361, mean_eps: 0.337024
 74830/100000: episode: 108, duration: 67.635s, episode steps: 854, steps per second:  13, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.011744, mae: 2.628043, mean_q: 3.170276, mean_eps: 0.330382
 75463/100000: episode: 109, duration: 42.442s, episode steps: 633, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.013036, mae: 2.626977, mean_q: 3.169126, mean_eps: 0.323686
 76111/100000: episode: 110, duration: 44.606s, episode steps: 648, steps per second:  15, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.011843, mae: 2.649357, mean_q: 3.197687, mean_eps: 0.317926
 76893/100000: episode: 111, duration: 54.827s, episode steps: 782, steps per second:  14, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.013290, mae: 2.631249, mean_q: 3.176778, mean_eps: 0.311482
 77541/100000: episode: 112, duration: 42.134s, episode steps: 648, steps per second:  15, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.013055, mae: 2.615572, mean_q: 3.155128, mean_eps: 0.305038
 78212/100000: episode: 113, duration: 45.602s, episode steps: 671, steps per second:  15, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.011758, mae: 2.604810, mean_q: 3.142256, mean_eps: 0.299116
 79323/100000: episode: 114, duration: 73.757s, episode steps: 1111, steps per second:  15, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.012277, mae: 2.616370, mean_q: 3.156612, mean_eps: 0.291106
 79980/100000: episode: 115, duration: 45.266s, episode steps: 657, steps per second:  15, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.988 [0.000, 5.000],  loss: 0.013428, mae: 2.621151, mean_q: 3.161252, mean_eps: 0.283150
 81285/100000: episode: 116, duration: 91.694s, episode steps: 1305, steps per second:  14, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.013416, mae: 2.616633, mean_q: 3.156662, mean_eps: 0.274312
 81765/100000: episode: 117, duration: 39.357s, episode steps: 480, steps per second:  12, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.011513, mae: 2.618012, mean_q: 3.160342, mean_eps: 0.266266
 82431/100000: episode: 118, duration: 44.474s, episode steps: 666, steps per second:  15, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.011343, mae: 2.611914, mean_q: 3.154175, mean_eps: 0.261118
 82929/100000: episode: 119, duration: 35.501s, episode steps: 498, steps per second:  14, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.013040, mae: 2.635240, mean_q: 3.180074, mean_eps: 0.255880
 83756/100000: episode: 120, duration: 55.426s, episode steps: 827, steps per second:  15, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.033 [0.000, 5.000],  loss: 0.011722, mae: 2.605630, mean_q: 3.144572, mean_eps: 0.249922
 84309/100000: episode: 121, duration: 37.238s, episode steps: 553, steps per second:  15, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.011921, mae: 2.640911, mean_q: 3.187417, mean_eps: 0.243712
 85325/100000: episode: 122, duration: 69.934s, episode steps: 1016, steps per second:  15, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.013013, mae: 2.642291, mean_q: 3.187913, mean_eps: 0.236638
 86360/100000: episode: 123, duration: 68.822s, episode steps: 1035, steps per second:  15, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.014419, mae: 2.643267, mean_q: 3.188976, mean_eps: 0.227422
 87409/100000: episode: 124, duration: 75.903s, episode steps: 1049, steps per second:  14, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.012849, mae: 2.634407, mean_q: 3.177943, mean_eps: 0.218044
 88752/100000: episode: 125, duration: 90.741s, episode steps: 1343, steps per second:  15, episode reward: 14.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.012779, mae: 2.631349, mean_q: 3.175567, mean_eps: 0.207280
 89151/100000: episode: 126, duration: 25.352s, episode steps: 399, steps per second:  16, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.010445, mae: 2.627973, mean_q: 3.170834, mean_eps: 0.199450
 90344/100000: episode: 127, duration: 84.845s, episode steps: 1193, steps per second:  14, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.886 [0.000, 5.000],  loss: 0.012319, mae: 2.627489, mean_q: 3.169968, mean_eps: 0.192286
 90986/100000: episode: 128, duration: 42.442s, episode steps: 642, steps per second:  15, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013563, mae: 2.630431, mean_q: 3.171894, mean_eps: 0.184024
 91678/100000: episode: 129, duration: 46.409s, episode steps: 692, steps per second:  15, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.007 [0.000, 5.000],  loss: 0.013580, mae: 2.640745, mean_q: 3.184748, mean_eps: 0.178012
 92428/100000: episode: 130, duration: 54.531s, episode steps: 750, steps per second:  14, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.013828, mae: 2.635032, mean_q: 3.180494, mean_eps: 0.171532
 92968/100000: episode: 131, duration: 36.339s, episode steps: 540, steps per second:  15, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.011713, mae: 2.613916, mean_q: 3.153857, mean_eps: 0.165736
 93604/100000: episode: 132, duration: 42.234s, episode steps: 636, steps per second:  15, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011101, mae: 2.613614, mean_q: 3.155146, mean_eps: 0.160444
 94229/100000: episode: 133, duration: 43.947s, episode steps: 625, steps per second:  14, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.012797, mae: 2.610514, mean_q: 3.150493, mean_eps: 0.154756
 95296/100000: episode: 134, duration: 72.521s, episode steps: 1067, steps per second:  15, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.012220, mae: 2.612091, mean_q: 3.151122, mean_eps: 0.147142
 96631/100000: episode: 135, duration: 96.549s, episode steps: 1335, steps per second:  14, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.013944, mae: 2.625758, mean_q: 3.167435, mean_eps: 0.136342
 97204/100000: episode: 136, duration: 38.204s, episode steps: 573, steps per second:  15, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.012342, mae: 2.631640, mean_q: 3.174559, mean_eps: 0.127756
 97764/100000: episode: 137, duration: 37.072s, episode steps: 560, steps per second:  15, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.011455, mae: 2.636921, mean_q: 3.180691, mean_eps: 0.122662
 98403/100000: episode: 138, duration: 45.202s, episode steps: 639, steps per second:  14, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.013248, mae: 2.621141, mean_q: 3.162000, mean_eps: 0.117262
 98917/100000: episode: 139, duration: 36.640s, episode steps: 514, steps per second:  14, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.014100, mae: 2.625363, mean_q: 3.166820, mean_eps: 0.112060
 99297/100000: episode: 140, duration: 25.350s, episode steps: 380, steps per second:  15, episode reward: 10.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.014395, mae: 2.615398, mean_q: 3.156851, mean_eps: 0.108028
 99944/100000: episode: 141, duration: 50.641s, episode steps: 647, steps per second:  13, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.011477, mae: 2.623233, mean_q: 3.165428, mean_eps: 0.103420
done, took 6525.751 seconds
########################################################
PROCESO TERMINADO
########################################################