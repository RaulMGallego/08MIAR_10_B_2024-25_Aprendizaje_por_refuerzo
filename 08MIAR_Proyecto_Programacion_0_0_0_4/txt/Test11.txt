['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_250000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
    420/250000: episode: 1, duration: 2.610s, episode steps: 420, steps per second: 161, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 3.550s, episode steps: 711, steps per second: 200, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 4.577s, episode steps: 810, steps per second: 177, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2827/250000: episode: 4, duration: 4.562s, episode steps: 886, steps per second: 194, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3345/250000: episode: 5, duration: 3.105s, episode steps: 518, steps per second: 167, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3990/250000: episode: 6, duration: 3.459s, episode steps: 645, steps per second: 186, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4451/250000: episode: 7, duration: 2.350s, episode steps: 461, steps per second: 196, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5247/250000: episode: 8, duration: 4.461s, episode steps: 796, steps per second: 178, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5629/250000: episode: 9, duration: 2.094s, episode steps: 382, steps per second: 182, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6746/250000: episode: 10, duration: 6.373s, episode steps: 1117, steps per second: 175, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7257/250000: episode: 11, duration: 3.284s, episode steps: 511, steps per second: 156, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8039/250000: episode: 12, duration: 4.092s, episode steps: 782, steps per second: 191, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8883/250000: episode: 13, duration: 4.660s, episode steps: 844, steps per second: 181, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9939/250000: episode: 14, duration: 5.848s, episode steps: 1056, steps per second: 181, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10752/250000: episode: 15, duration: 33.774s, episode steps: 813, steps per second:  24, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.011627, mae: 2.241352, mean_q: 2.754291, mean_eps: 0.996265
  11284/250000: episode: 16, duration: 32.145s, episode steps: 532, steps per second:  17, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.011059, mae: 2.265719, mean_q: 2.784235, mean_eps: 0.996034
  12735/250000: episode: 17, duration: 68.088s, episode steps: 1451, steps per second:  21, episode reward: 14.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.008864, mae: 2.237455, mean_q: 2.751004, mean_eps: 0.995677
  13369/250000: episode: 18, duration: 29.493s, episode steps: 634, steps per second:  21, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.011262, mae: 2.220683, mean_q: 2.729831, mean_eps: 0.995301
  14214/250000: episode: 19, duration: 40.449s, episode steps: 845, steps per second:  21, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.008840, mae: 2.198912, mean_q: 2.703635, mean_eps: 0.995035
  14782/250000: episode: 20, duration: 23.244s, episode steps: 568, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.008307, mae: 2.202583, mean_q: 2.706915, mean_eps: 0.994781
  15315/250000: episode: 21, duration: 22.237s, episode steps: 533, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.008983, mae: 2.172912, mean_q: 2.674226, mean_eps: 0.994583
  15974/250000: episode: 22, duration: 26.655s, episode steps: 659, steps per second:  25, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.008889, mae: 2.184986, mean_q: 2.685362, mean_eps: 0.994368
  16685/250000: episode: 23, duration: 29.770s, episode steps: 711, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.010618, mae: 2.196502, mean_q: 2.698917, mean_eps: 0.994121
  17614/250000: episode: 24, duration: 41.539s, episode steps: 929, steps per second:  22, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.010916, mae: 2.206935, mean_q: 2.708612, mean_eps: 0.993826
  18273/250000: episode: 25, duration: 26.286s, episode steps: 659, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.010777, mae: 2.171000, mean_q: 2.664364, mean_eps: 0.993540
  18959/250000: episode: 26, duration: 31.346s, episode steps: 686, steps per second:  22, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.007897, mae: 2.187274, mean_q: 2.684418, mean_eps: 0.993298
  19556/250000: episode: 27, duration: 28.112s, episode steps: 597, steps per second:  21, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.008579, mae: 2.204013, mean_q: 2.705210, mean_eps: 0.993068
  20180/250000: episode: 28, duration: 27.718s, episode steps: 624, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.009925, mae: 2.207337, mean_q: 2.708940, mean_eps: 0.992848
  20855/250000: episode: 29, duration: 32.084s, episode steps: 675, steps per second:  21, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008505, mae: 2.217525, mean_q: 2.722838, mean_eps: 0.992614
  21541/250000: episode: 30, duration: 33.708s, episode steps: 686, steps per second:  20, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.009048, mae: 2.192372, mean_q: 2.693866, mean_eps: 0.992369
  22069/250000: episode: 31, duration: 25.672s, episode steps: 528, steps per second:  21, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.010943, mae: 2.173926, mean_q: 2.666512, mean_eps: 0.992150
  22723/250000: episode: 32, duration: 29.087s, episode steps: 654, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.010617, mae: 2.175082, mean_q: 2.669629, mean_eps: 0.991937
  23435/250000: episode: 33, duration: 32.029s, episode steps: 712, steps per second:  22, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.008931, mae: 2.178565, mean_q: 2.674698, mean_eps: 0.991692
  23955/250000: episode: 34, duration: 27.253s, episode steps: 520, steps per second:  19, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.009862, mae: 2.177052, mean_q: 2.671755, mean_eps: 0.991470
  24490/250000: episode: 35, duration: 21.096s, episode steps: 535, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.010575, mae: 2.156377, mean_q: 2.644637, mean_eps: 0.991280
  24834/250000: episode: 36, duration: 14.796s, episode steps: 344, steps per second:  23, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.007581, mae: 2.168678, mean_q: 2.659761, mean_eps: 0.991122
  25545/250000: episode: 37, duration: 30.097s, episode steps: 711, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.009769, mae: 2.166215, mean_q: 2.656106, mean_eps: 0.990932
  26385/250000: episode: 38, duration: 34.541s, episode steps: 840, steps per second:  24, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.008112, mae: 2.179001, mean_q: 2.673715, mean_eps: 0.990652
  27002/250000: episode: 39, duration: 25.504s, episode steps: 617, steps per second:  24, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.009082, mae: 2.161881, mean_q: 2.650645, mean_eps: 0.990390
  27523/250000: episode: 40, duration: 20.262s, episode steps: 521, steps per second:  26, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010922, mae: 2.156323, mean_q: 2.643435, mean_eps: 0.990186
  28312/250000: episode: 41, duration: 32.991s, episode steps: 789, steps per second:  24, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.010274, mae: 2.190908, mean_q: 2.685225, mean_eps: 0.989950
  29039/250000: episode: 42, duration: 29.767s, episode steps: 727, steps per second:  24, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.009276, mae: 2.181303, mean_q: 2.674028, mean_eps: 0.989677
  29431/250000: episode: 43, duration: 18.325s, episode steps: 392, steps per second:  21, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.011911, mae: 2.168250, mean_q: 2.657546, mean_eps: 0.989476
  29834/250000: episode: 44, duration: 20.297s, episode steps: 403, steps per second:  20, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.009725, mae: 2.188673, mean_q: 2.684276, mean_eps: 0.989332
  30701/250000: episode: 45, duration: 39.240s, episode steps: 867, steps per second:  22, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.008771, mae: 2.173928, mean_q: 2.666250, mean_eps: 0.989104
  31663/250000: episode: 46, duration: 41.524s, episode steps: 962, steps per second:  23, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010266, mae: 2.154084, mean_q: 2.640138, mean_eps: 0.988774
  32359/250000: episode: 47, duration: 30.454s, episode steps: 696, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.009898, mae: 2.161397, mean_q: 2.648539, mean_eps: 0.988476
  32871/250000: episode: 48, duration: 21.646s, episode steps: 512, steps per second:  24, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.009996, mae: 2.171220, mean_q: 2.661394, mean_eps: 0.988259
  33892/250000: episode: 49, duration: 41.306s, episode steps: 1021, steps per second:  25, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.009575, mae: 2.174010, mean_q: 2.663137, mean_eps: 0.987983
  34555/250000: episode: 50, duration: 27.060s, episode steps: 663, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.009374, mae: 2.168576, mean_q: 2.654771, mean_eps: 0.987680
  34953/250000: episode: 51, duration: 16.333s, episode steps: 398, steps per second:  24, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.010710, mae: 2.187064, mean_q: 2.679140, mean_eps: 0.987489
  35564/250000: episode: 52, duration: 25.297s, episode steps: 611, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.009142, mae: 2.178340, mean_q: 2.667682, mean_eps: 0.987307
  36468/250000: episode: 53, duration: 36.933s, episode steps: 904, steps per second:  24, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.009643, mae: 2.161325, mean_q: 2.645250, mean_eps: 0.987035
  37024/250000: episode: 54, duration: 22.304s, episode steps: 556, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.009625, mae: 2.149918, mean_q: 2.631137, mean_eps: 0.986772
  37697/250000: episode: 55, duration: 28.229s, episode steps: 673, steps per second:  24, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010427, mae: 2.152095, mean_q: 2.634442, mean_eps: 0.986550
  38435/250000: episode: 56, duration: 29.959s, episode steps: 738, steps per second:  25, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.010598, mae: 2.180621, mean_q: 2.669555, mean_eps: 0.986296
  39094/250000: episode: 57, duration: 27.641s, episode steps: 659, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.010888, mae: 2.161166, mean_q: 2.644107, mean_eps: 0.986045
  39971/250000: episode: 58, duration: 34.791s, episode steps: 877, steps per second:  25, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010149, mae: 2.139520, mean_q: 2.619641, mean_eps: 0.985768
  40879/250000: episode: 59, duration: 36.763s, episode steps: 908, steps per second:  25, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.010415, mae: 2.161470, mean_q: 2.645631, mean_eps: 0.985447
  41783/250000: episode: 60, duration: 35.961s, episode steps: 904, steps per second:  25, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.010486, mae: 2.158489, mean_q: 2.642326, mean_eps: 0.985121
  42618/250000: episode: 61, duration: 34.473s, episode steps: 835, steps per second:  24, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.008948, mae: 2.152810, mean_q: 2.635205, mean_eps: 0.984808
  43639/250000: episode: 62, duration: 40.511s, episode steps: 1021, steps per second:  25, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.009754, mae: 2.153170, mean_q: 2.634427, mean_eps: 0.984474
  44263/250000: episode: 63, duration: 28.160s, episode steps: 624, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.010427, mae: 2.143490, mean_q: 2.623080, mean_eps: 0.984178
  44936/250000: episode: 64, duration: 30.430s, episode steps: 673, steps per second:  22, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.010608, mae: 2.161601, mean_q: 2.644524, mean_eps: 0.983945
  45313/250000: episode: 65, duration: 20.231s, episode steps: 377, steps per second:  19, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.008907, mae: 2.132830, mean_q: 2.610661, mean_eps: 0.983755
  46284/250000: episode: 66, duration: 41.386s, episode steps: 971, steps per second:  23, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011346, mae: 2.152637, mean_q: 2.633509, mean_eps: 0.983513
  46817/250000: episode: 67, duration: 21.897s, episode steps: 533, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.011662, mae: 2.167062, mean_q: 2.650926, mean_eps: 0.983242
  47615/250000: episode: 68, duration: 32.709s, episode steps: 798, steps per second:  24, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011241, mae: 2.168576, mean_q: 2.653436, mean_eps: 0.983002
  48114/250000: episode: 69, duration: 19.530s, episode steps: 499, steps per second:  26, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.011640, mae: 2.158101, mean_q: 2.639194, mean_eps: 0.982769
  48921/250000: episode: 70, duration: 33.075s, episode steps: 807, steps per second:  24, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010174, mae: 2.155742, mean_q: 2.639664, mean_eps: 0.982534
  49623/250000: episode: 71, duration: 28.806s, episode steps: 702, steps per second:  24, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.011049, mae: 2.134678, mean_q: 2.612202, mean_eps: 0.982262
  50207/250000: episode: 72, duration: 25.947s, episode steps: 584, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.011436, mae: 2.163642, mean_q: 2.645876, mean_eps: 0.982031
  50871/250000: episode: 73, duration: 26.619s, episode steps: 664, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011147, mae: 2.201760, mean_q: 2.690949, mean_eps: 0.981806
  51494/250000: episode: 74, duration: 26.208s, episode steps: 623, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.012208, mae: 2.176429, mean_q: 2.660299, mean_eps: 0.981574
  52196/250000: episode: 75, duration: 29.074s, episode steps: 702, steps per second:  24, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011409, mae: 2.151959, mean_q: 2.631855, mean_eps: 0.981336
  52974/250000: episode: 76, duration: 32.182s, episode steps: 778, steps per second:  24, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.012775, mae: 2.178434, mean_q: 2.662653, mean_eps: 0.981070
  53597/250000: episode: 77, duration: 27.782s, episode steps: 623, steps per second:  22, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.010942, mae: 2.173882, mean_q: 2.656372, mean_eps: 0.980817
  54237/250000: episode: 78, duration: 25.151s, episode steps: 640, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.009966, mae: 2.140200, mean_q: 2.614669, mean_eps: 0.980590
  54978/250000: episode: 79, duration: 31.226s, episode steps: 741, steps per second:  24, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.011494, mae: 2.162688, mean_q: 2.642763, mean_eps: 0.980341
  55573/250000: episode: 80, duration: 25.585s, episode steps: 595, steps per second:  23, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.010210, mae: 2.187608, mean_q: 2.673839, mean_eps: 0.980101
  56157/250000: episode: 81, duration: 23.563s, episode steps: 584, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.009924, mae: 2.146517, mean_q: 2.623608, mean_eps: 0.979888
  56930/250000: episode: 82, duration: 34.946s, episode steps: 773, steps per second:  22, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.010461, mae: 2.185670, mean_q: 2.669557, mean_eps: 0.979644
  57633/250000: episode: 83, duration: 33.680s, episode steps: 703, steps per second:  21, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.010481, mae: 2.188331, mean_q: 2.672742, mean_eps: 0.979378
  58113/250000: episode: 84, duration: 23.163s, episode steps: 480, steps per second:  21, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.011581, mae: 2.204960, mean_q: 2.692575, mean_eps: 0.979165
  58726/250000: episode: 85, duration: 29.976s, episode steps: 613, steps per second:  20, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.010477, mae: 2.173936, mean_q: 2.656898, mean_eps: 0.978969
  59129/250000: episode: 86, duration: 20.847s, episode steps: 403, steps per second:  19, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.010252, mae: 2.162082, mean_q: 2.640099, mean_eps: 0.978786
  59820/250000: episode: 87, duration: 29.070s, episode steps: 691, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.012484, mae: 2.161684, mean_q: 2.640465, mean_eps: 0.978589
  60419/250000: episode: 88, duration: 24.824s, episode steps: 599, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.010779, mae: 2.188034, mean_q: 2.672979, mean_eps: 0.978358
  60951/250000: episode: 89, duration: 24.068s, episode steps: 532, steps per second:  22, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.010105, mae: 2.175520, mean_q: 2.657423, mean_eps: 0.978154
  61556/250000: episode: 90, duration: 24.452s, episode steps: 605, steps per second:  25, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010873, mae: 2.160940, mean_q: 2.642279, mean_eps: 0.977949
  62238/250000: episode: 91, duration: 26.804s, episode steps: 682, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.012808, mae: 2.159756, mean_q: 2.638906, mean_eps: 0.977717
  63165/250000: episode: 92, duration: 38.468s, episode steps: 927, steps per second:  24, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.010909, mae: 2.148829, mean_q: 2.624477, mean_eps: 0.977427
  64472/250000: episode: 93, duration: 52.839s, episode steps: 1307, steps per second:  25, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.010171, mae: 2.170463, mean_q: 2.652227, mean_eps: 0.977026
  65000/250000: episode: 94, duration: 21.092s, episode steps: 528, steps per second:  25, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.009676, mae: 2.161272, mean_q: 2.640532, mean_eps: 0.976696
  65534/250000: episode: 95, duration: 21.937s, episode steps: 534, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.009930, mae: 2.175025, mean_q: 2.656205, mean_eps: 0.976504
  66051/250000: episode: 96, duration: 21.390s, episode steps: 517, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.009499, mae: 2.186476, mean_q: 2.672987, mean_eps: 0.976315
  66905/250000: episode: 97, duration: 33.850s, episode steps: 854, steps per second:  25, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.011339, mae: 2.157604, mean_q: 2.635065, mean_eps: 0.976068
  67364/250000: episode: 98, duration: 18.742s, episode steps: 459, steps per second:  24, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.011476, mae: 2.139526, mean_q: 2.611966, mean_eps: 0.975832
  67874/250000: episode: 99, duration: 21.013s, episode steps: 510, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010436, mae: 2.179698, mean_q: 2.662775, mean_eps: 0.975658
  68540/250000: episode: 100, duration: 26.382s, episode steps: 666, steps per second:  25, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011380, mae: 2.173506, mean_q: 2.654676, mean_eps: 0.975446
  69441/250000: episode: 101, duration: 37.277s, episode steps: 901, steps per second:  24, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.010872, mae: 2.172758, mean_q: 2.655404, mean_eps: 0.975164
  70130/250000: episode: 102, duration: 27.338s, episode steps: 689, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.011361, mae: 2.162559, mean_q: 2.643001, mean_eps: 0.974877
  70847/250000: episode: 103, duration: 29.996s, episode steps: 717, steps per second:  24, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011567, mae: 2.177741, mean_q: 2.660349, mean_eps: 0.974624
  71351/250000: episode: 104, duration: 20.020s, episode steps: 504, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.010429, mae: 2.157743, mean_q: 2.637785, mean_eps: 0.974405
  72418/250000: episode: 105, duration: 43.850s, episode steps: 1067, steps per second:  24, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.010400, mae: 2.163848, mean_q: 2.643516, mean_eps: 0.974122
  72876/250000: episode: 106, duration: 17.662s, episode steps: 458, steps per second:  26, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008323, mae: 2.133436, mean_q: 2.605582, mean_eps: 0.973847
  73561/250000: episode: 107, duration: 31.883s, episode steps: 685, steps per second:  21, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010692, mae: 2.160544, mean_q: 2.638343, mean_eps: 0.973642
  74042/250000: episode: 108, duration: 22.162s, episode steps: 481, steps per second:  22, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.011569, mae: 2.176274, mean_q: 2.658774, mean_eps: 0.973431
  74628/250000: episode: 109, duration: 23.311s, episode steps: 586, steps per second:  25, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.011511, mae: 2.168715, mean_q: 2.648798, mean_eps: 0.973240
  75252/250000: episode: 110, duration: 25.791s, episode steps: 624, steps per second:  24, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.009558, mae: 2.130080, mean_q: 2.603319, mean_eps: 0.973022
  75645/250000: episode: 111, duration: 16.463s, episode steps: 393, steps per second:  24, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010285, mae: 2.169882, mean_q: 2.650276, mean_eps: 0.972839
  76051/250000: episode: 112, duration: 15.864s, episode steps: 406, steps per second:  26, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.012862, mae: 2.177042, mean_q: 2.659519, mean_eps: 0.972695
  76800/250000: episode: 113, duration: 29.598s, episode steps: 749, steps per second:  25, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.012809, mae: 2.166450, mean_q: 2.645672, mean_eps: 0.972487
  77342/250000: episode: 114, duration: 22.719s, episode steps: 542, steps per second:  24, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.009909, mae: 2.154288, mean_q: 2.630263, mean_eps: 0.972255
  77731/250000: episode: 115, duration: 15.578s, episode steps: 389, steps per second:  25, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011315, mae: 2.176205, mean_q: 2.658996, mean_eps: 0.972087
  78407/250000: episode: 116, duration: 26.900s, episode steps: 676, steps per second:  25, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.011477, mae: 2.157645, mean_q: 2.633986, mean_eps: 0.971896
  79383/250000: episode: 117, duration: 39.542s, episode steps: 976, steps per second:  25, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.012645, mae: 2.156706, mean_q: 2.634754, mean_eps: 0.971598
  79884/250000: episode: 118, duration: 19.827s, episode steps: 501, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011995, mae: 2.156293, mean_q: 2.633283, mean_eps: 0.971332
  80548/250000: episode: 119, duration: 28.781s, episode steps: 664, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.011273, mae: 2.173414, mean_q: 2.653927, mean_eps: 0.971123
  81005/250000: episode: 120, duration: 18.494s, episode steps: 457, steps per second:  25, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.011786, mae: 2.142627, mean_q: 2.619080, mean_eps: 0.970921
  81552/250000: episode: 121, duration: 24.035s, episode steps: 547, steps per second:  23, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.010631, mae: 2.171394, mean_q: 2.653615, mean_eps: 0.970740
  82784/250000: episode: 122, duration: 53.659s, episode steps: 1232, steps per second:  23, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.009979, mae: 2.158300, mean_q: 2.635933, mean_eps: 0.970420
  83206/250000: episode: 123, duration: 19.406s, episode steps: 422, steps per second:  22, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.011352, mae: 2.160505, mean_q: 2.636833, mean_eps: 0.970122
  83870/250000: episode: 124, duration: 32.063s, episode steps: 664, steps per second:  21, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.011756, mae: 2.157980, mean_q: 2.635480, mean_eps: 0.969926
  84301/250000: episode: 125, duration: 19.078s, episode steps: 431, steps per second:  23, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.011344, mae: 2.152530, mean_q: 2.627730, mean_eps: 0.969729
  84904/250000: episode: 126, duration: 25.421s, episode steps: 603, steps per second:  24, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.010469, mae: 2.145841, mean_q: 2.618604, mean_eps: 0.969543
  85846/250000: episode: 127, duration: 38.125s, episode steps: 942, steps per second:  25, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.011764, mae: 2.160142, mean_q: 2.638654, mean_eps: 0.969265
  86334/250000: episode: 128, duration: 19.996s, episode steps: 488, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.011117, mae: 2.142530, mean_q: 2.614193, mean_eps: 0.969008
  86809/250000: episode: 129, duration: 19.439s, episode steps: 475, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.010365, mae: 2.167082, mean_q: 2.644453, mean_eps: 0.968834
  87323/250000: episode: 130, duration: 20.390s, episode steps: 514, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.010717, mae: 2.155568, mean_q: 2.631578, mean_eps: 0.968656
  88510/250000: episode: 131, duration: 59.739s, episode steps: 1187, steps per second:  20, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.011323, mae: 2.164926, mean_q: 2.641891, mean_eps: 0.968350
  89012/250000: episode: 132, duration: 24.894s, episode steps: 502, steps per second:  20, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.011286, mae: 2.165776, mean_q: 2.642921, mean_eps: 0.968046
  90253/250000: episode: 133, duration: 60.149s, episode steps: 1241, steps per second:  21, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.009577, mae: 2.142016, mean_q: 2.614720, mean_eps: 0.967732
  91125/250000: episode: 134, duration: 47.476s, episode steps: 872, steps per second:  18, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.010841, mae: 2.148246, mean_q: 2.621711, mean_eps: 0.967352
  91740/250000: episode: 135, duration: 29.636s, episode steps: 615, steps per second:  21, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010629, mae: 2.150227, mean_q: 2.624773, mean_eps: 0.967084
  92548/250000: episode: 136, duration: 39.340s, episode steps: 808, steps per second:  21, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.012111, mae: 2.149807, mean_q: 2.623292, mean_eps: 0.966829
  93035/250000: episode: 137, duration: 24.062s, episode steps: 487, steps per second:  20, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.009964, mae: 2.167103, mean_q: 2.646001, mean_eps: 0.966596
  93861/250000: episode: 138, duration: 40.086s, episode steps: 826, steps per second:  21, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.011863, mae: 2.174794, mean_q: 2.653645, mean_eps: 0.966359
  94954/250000: episode: 139, duration: 53.825s, episode steps: 1093, steps per second:  20, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.011101, mae: 2.155576, mean_q: 2.631104, mean_eps: 0.966013
  95635/250000: episode: 140, duration: 34.450s, episode steps: 681, steps per second:  20, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.011622, mae: 2.164385, mean_q: 2.642615, mean_eps: 0.965694
  96120/250000: episode: 141, duration: 24.274s, episode steps: 485, steps per second:  20, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011449, mae: 2.168720, mean_q: 2.649075, mean_eps: 0.965485
  96878/250000: episode: 142, duration: 38.716s, episode steps: 758, steps per second:  20, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.012176, mae: 2.167956, mean_q: 2.646890, mean_eps: 0.965261
  97740/250000: episode: 143, duration: 42.887s, episode steps: 862, steps per second:  20, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.012128, mae: 2.153737, mean_q: 2.628197, mean_eps: 0.964969
  98851/250000: episode: 144, duration: 51.741s, episode steps: 1111, steps per second:  21, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.009316, mae: 2.167531, mean_q: 2.645559, mean_eps: 0.964614
  99536/250000: episode: 145, duration: 28.310s, episode steps: 685, steps per second:  24, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.011142, mae: 2.162145, mean_q: 2.637597, mean_eps: 0.964291
  99908/250000: episode: 146, duration: 15.969s, episode steps: 372, steps per second:  23, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.010078, mae: 2.135787, mean_q: 2.608579, mean_eps: 0.964101
 100587/250000: episode: 147, duration: 34.295s, episode steps: 679, steps per second:  20, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.013059, mae: 2.191582, mean_q: 2.674184, mean_eps: 0.963911
 100992/250000: episode: 148, duration: 16.417s, episode steps: 405, steps per second:  25, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.013317, mae: 2.187022, mean_q: 2.670838, mean_eps: 0.963716
 101382/250000: episode: 149, duration: 17.038s, episode steps: 390, steps per second:  23, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011759, mae: 2.190727, mean_q: 2.671003, mean_eps: 0.963573
 102336/250000: episode: 150, duration: 39.190s, episode steps: 954, steps per second:  24, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.010681, mae: 2.188540, mean_q: 2.669605, mean_eps: 0.963331
 103074/250000: episode: 151, duration: 30.586s, episode steps: 738, steps per second:  24, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.011943, mae: 2.178444, mean_q: 2.658152, mean_eps: 0.963027
 103821/250000: episode: 152, duration: 30.732s, episode steps: 747, steps per second:  24, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.011200, mae: 2.186833, mean_q: 2.667857, mean_eps: 0.962759
 104513/250000: episode: 153, duration: 28.277s, episode steps: 692, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.013643, mae: 2.173125, mean_q: 2.649662, mean_eps: 0.962500
 105208/250000: episode: 154, duration: 29.762s, episode steps: 695, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.010869, mae: 2.182895, mean_q: 2.661655, mean_eps: 0.962250
 106428/250000: episode: 155, duration: 50.101s, episode steps: 1220, steps per second:  24, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.012696, mae: 2.200930, mean_q: 2.683403, mean_eps: 0.961906
 106983/250000: episode: 156, duration: 22.962s, episode steps: 555, steps per second:  24, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.010250, mae: 2.196406, mean_q: 2.679628, mean_eps: 0.961587
 107377/250000: episode: 157, duration: 15.931s, episode steps: 394, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.010647, mae: 2.178400, mean_q: 2.656598, mean_eps: 0.961415
 108501/250000: episode: 158, duration: 47.093s, episode steps: 1124, steps per second:  24, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.011728, mae: 2.196201, mean_q: 2.678889, mean_eps: 0.961142
 109165/250000: episode: 159, duration: 27.796s, episode steps: 664, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.008817, mae: 2.185197, mean_q: 2.666252, mean_eps: 0.960820
 109664/250000: episode: 160, duration: 20.752s, episode steps: 499, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.012658, mae: 2.190595, mean_q: 2.670340, mean_eps: 0.960611
 110113/250000: episode: 161, duration: 19.932s, episode steps: 449, steps per second:  23, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.011263, mae: 2.167111, mean_q: 2.640872, mean_eps: 0.960440
 110676/250000: episode: 162, duration: 24.782s, episode steps: 563, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010449, mae: 2.177931, mean_q: 2.654450, mean_eps: 0.960258
 111287/250000: episode: 163, duration: 30.530s, episode steps: 611, steps per second:  20, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.011501, mae: 2.200915, mean_q: 2.684220, mean_eps: 0.960047
 112243/250000: episode: 164, duration: 47.594s, episode steps: 956, steps per second:  20, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.010123, mae: 2.197379, mean_q: 2.678500, mean_eps: 0.959765
 112832/250000: episode: 165, duration: 29.342s, episode steps: 589, steps per second:  20, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010473, mae: 2.204726, mean_q: 2.688988, mean_eps: 0.959487
 113224/250000: episode: 166, duration: 23.782s, episode steps: 392, steps per second:  16, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011860, mae: 2.173669, mean_q: 2.650735, mean_eps: 0.959311
 113779/250000: episode: 167, duration: 27.071s, episode steps: 555, steps per second:  21, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.011145, mae: 2.178733, mean_q: 2.658399, mean_eps: 0.959140
 114174/250000: episode: 168, duration: 23.639s, episode steps: 395, steps per second:  17, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011041, mae: 2.199457, mean_q: 2.681224, mean_eps: 0.958969
 115203/250000: episode: 169, duration: 51.037s, episode steps: 1029, steps per second:  20, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011506, mae: 2.184886, mean_q: 2.665034, mean_eps: 0.958712
 115960/250000: episode: 170, duration: 37.123s, episode steps: 757, steps per second:  20, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.012479, mae: 2.185116, mean_q: 2.664663, mean_eps: 0.958391
 116629/250000: episode: 171, duration: 32.144s, episode steps: 669, steps per second:  21, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.012630, mae: 2.194500, mean_q: 2.675137, mean_eps: 0.958134
 117279/250000: episode: 172, duration: 30.873s, episode steps: 650, steps per second:  21, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.010291, mae: 2.185400, mean_q: 2.665990, mean_eps: 0.957897
 117762/250000: episode: 173, duration: 24.719s, episode steps: 483, steps per second:  20, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.010800, mae: 2.195269, mean_q: 2.676757, mean_eps: 0.957693
 118442/250000: episode: 174, duration: 34.894s, episode steps: 680, steps per second:  19, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.009658, mae: 2.204873, mean_q: 2.688822, mean_eps: 0.957483
 119075/250000: episode: 175, duration: 32.898s, episode steps: 633, steps per second:  19, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.012378, mae: 2.190800, mean_q: 2.671693, mean_eps: 0.957247
 119913/250000: episode: 176, duration: 40.984s, episode steps: 838, steps per second:  20, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.011147, mae: 2.196700, mean_q: 2.678948, mean_eps: 0.956982
 120541/250000: episode: 177, duration: 31.534s, episode steps: 628, steps per second:  20, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011200, mae: 2.161902, mean_q: 2.637654, mean_eps: 0.956718
 121586/250000: episode: 178, duration: 53.381s, episode steps: 1045, steps per second:  20, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011637, mae: 2.193109, mean_q: 2.675494, mean_eps: 0.956417
 122624/250000: episode: 179, duration: 50.791s, episode steps: 1038, steps per second:  20, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.011500, mae: 2.205922, mean_q: 2.690027, mean_eps: 0.956043
 123218/250000: episode: 180, duration: 24.909s, episode steps: 594, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.010299, mae: 2.175474, mean_q: 2.652806, mean_eps: 0.955749
 123997/250000: episode: 181, duration: 33.185s, episode steps: 779, steps per second:  23, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.010974, mae: 2.194233, mean_q: 2.676517, mean_eps: 0.955501
 124787/250000: episode: 182, duration: 33.529s, episode steps: 790, steps per second:  24, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.011842, mae: 2.194449, mean_q: 2.676980, mean_eps: 0.955219
 125443/250000: episode: 183, duration: 26.685s, episode steps: 656, steps per second:  25, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.010590, mae: 2.171272, mean_q: 2.649076, mean_eps: 0.954959
 126111/250000: episode: 184, duration: 28.057s, episode steps: 668, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.011526, mae: 2.209451, mean_q: 2.694841, mean_eps: 0.954721
 126724/250000: episode: 185, duration: 25.481s, episode steps: 613, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.010352, mae: 2.201574, mean_q: 2.685259, mean_eps: 0.954490
 127475/250000: episode: 186, duration: 35.833s, episode steps: 751, steps per second:  21, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.010479, mae: 2.181926, mean_q: 2.661776, mean_eps: 0.954245
 128125/250000: episode: 187, duration: 26.475s, episode steps: 650, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.011198, mae: 2.183002, mean_q: 2.660829, mean_eps: 0.953992
 128497/250000: episode: 188, duration: 15.654s, episode steps: 372, steps per second:  24, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010231, mae: 2.186540, mean_q: 2.663890, mean_eps: 0.953808
 129186/250000: episode: 189, duration: 28.408s, episode steps: 689, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.010222, mae: 2.188109, mean_q: 2.667234, mean_eps: 0.953617
 129805/250000: episode: 190, duration: 26.048s, episode steps: 619, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.011127, mae: 2.186970, mean_q: 2.667452, mean_eps: 0.953381
 130506/250000: episode: 191, duration: 29.829s, episode steps: 701, steps per second:  24, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.010996, mae: 2.192573, mean_q: 2.673580, mean_eps: 0.953144
 131185/250000: episode: 192, duration: 27.781s, episode steps: 679, steps per second:  24, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.011144, mae: 2.196993, mean_q: 2.679579, mean_eps: 0.952895
 131722/250000: episode: 193, duration: 22.095s, episode steps: 537, steps per second:  24, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.011493, mae: 2.184179, mean_q: 2.662211, mean_eps: 0.952677
 132695/250000: episode: 194, duration: 40.316s, episode steps: 973, steps per second:  24, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012028, mae: 2.169853, mean_q: 2.645371, mean_eps: 0.952405
 133373/250000: episode: 195, duration: 27.778s, episode steps: 678, steps per second:  24, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010452, mae: 2.185591, mean_q: 2.666118, mean_eps: 0.952108
 134389/250000: episode: 196, duration: 42.557s, episode steps: 1016, steps per second:  24, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.010938, mae: 2.207753, mean_q: 2.693064, mean_eps: 0.951802
 134795/250000: episode: 197, duration: 16.581s, episode steps: 406, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.009958, mae: 2.179976, mean_q: 2.658358, mean_eps: 0.951547
 135924/250000: episode: 198, duration: 46.963s, episode steps: 1129, steps per second:  24, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.011714, mae: 2.195120, mean_q: 2.676881, mean_eps: 0.951271
 136645/250000: episode: 199, duration: 30.101s, episode steps: 721, steps per second:  24, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.010689, mae: 2.198134, mean_q: 2.681033, mean_eps: 0.950938
 137402/250000: episode: 200, duration: 30.919s, episode steps: 757, steps per second:  24, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.010370, mae: 2.200595, mean_q: 2.683523, mean_eps: 0.950671
 137904/250000: episode: 201, duration: 20.496s, episode steps: 502, steps per second:  24, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.011002, mae: 2.188861, mean_q: 2.668226, mean_eps: 0.950445
 138438/250000: episode: 202, duration: 22.322s, episode steps: 534, steps per second:  24, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011329, mae: 2.213267, mean_q: 2.700171, mean_eps: 0.950259
 139028/250000: episode: 203, duration: 24.990s, episode steps: 590, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.012904, mae: 2.165610, mean_q: 2.639786, mean_eps: 0.950056
 139622/250000: episode: 204, duration: 24.286s, episode steps: 594, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.009574, mae: 2.185103, mean_q: 2.665116, mean_eps: 0.949843
 140113/250000: episode: 205, duration: 21.973s, episode steps: 491, steps per second:  22, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.014403, mae: 2.204407, mean_q: 2.687460, mean_eps: 0.949648
 140621/250000: episode: 206, duration: 20.304s, episode steps: 508, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.010387, mae: 2.222587, mean_q: 2.708920, mean_eps: 0.949468
 141111/250000: episode: 207, duration: 20.552s, episode steps: 490, steps per second:  24, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.011263, mae: 2.182216, mean_q: 2.657773, mean_eps: 0.949288
 141695/250000: episode: 208, duration: 27.005s, episode steps: 584, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.010914, mae: 2.202265, mean_q: 2.684062, mean_eps: 0.949095
 142076/250000: episode: 209, duration: 16.536s, episode steps: 381, steps per second:  23, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.011031, mae: 2.194044, mean_q: 2.673171, mean_eps: 0.948922
 142493/250000: episode: 210, duration: 16.863s, episode steps: 417, steps per second:  25, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.011795, mae: 2.226233, mean_q: 2.712191, mean_eps: 0.948778
 143149/250000: episode: 211, duration: 28.407s, episode steps: 656, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012232, mae: 2.189454, mean_q: 2.667920, mean_eps: 0.948584
 143720/250000: episode: 212, duration: 22.925s, episode steps: 571, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011326, mae: 2.202190, mean_q: 2.683006, mean_eps: 0.948364
 144621/250000: episode: 213, duration: 38.559s, episode steps: 901, steps per second:  23, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.011805, mae: 2.198139, mean_q: 2.677265, mean_eps: 0.948099
 145636/250000: episode: 214, duration: 41.101s, episode steps: 1015, steps per second:  25, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.010670, mae: 2.186551, mean_q: 2.665112, mean_eps: 0.947754
 146350/250000: episode: 215, duration: 30.215s, episode steps: 714, steps per second:  24, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.010304, mae: 2.209511, mean_q: 2.693167, mean_eps: 0.947443
 147159/250000: episode: 216, duration: 33.427s, episode steps: 809, steps per second:  24, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.010131, mae: 2.194312, mean_q: 2.677013, mean_eps: 0.947169
 147652/250000: episode: 217, duration: 21.337s, episode steps: 493, steps per second:  23, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.011439, mae: 2.181094, mean_q: 2.660504, mean_eps: 0.946935
 148042/250000: episode: 218, duration: 15.876s, episode steps: 390, steps per second:  25, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012042, mae: 2.204701, mean_q: 2.690662, mean_eps: 0.946775
 148711/250000: episode: 219, duration: 27.754s, episode steps: 669, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.012618, mae: 2.189066, mean_q: 2.669312, mean_eps: 0.946585
 149308/250000: episode: 220, duration: 26.054s, episode steps: 597, steps per second:  23, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.013087, mae: 2.194092, mean_q: 2.674603, mean_eps: 0.946357
 149993/250000: episode: 221, duration: 28.723s, episode steps: 685, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.009871, mae: 2.192124, mean_q: 2.674115, mean_eps: 0.946126
 150517/250000: episode: 222, duration: 21.410s, episode steps: 524, steps per second:  24, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.010447, mae: 2.234324, mean_q: 2.726488, mean_eps: 0.945908
 151236/250000: episode: 223, duration: 30.065s, episode steps: 719, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012194, mae: 2.230989, mean_q: 2.720341, mean_eps: 0.945685
 151722/250000: episode: 224, duration: 19.648s, episode steps: 486, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.011178, mae: 2.213246, mean_q: 2.698484, mean_eps: 0.945468
 152410/250000: episode: 225, duration: 29.325s, episode steps: 688, steps per second:  23, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011217, mae: 2.240514, mean_q: 2.730683, mean_eps: 0.945256
 152858/250000: episode: 226, duration: 18.028s, episode steps: 448, steps per second:  25, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.013554, mae: 2.232056, mean_q: 2.719908, mean_eps: 0.945052
 153619/250000: episode: 227, duration: 31.402s, episode steps: 761, steps per second:  24, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.011090, mae: 2.238595, mean_q: 2.725904, mean_eps: 0.944834
 154007/250000: episode: 228, duration: 16.456s, episode steps: 388, steps per second:  24, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010939, mae: 2.244652, mean_q: 2.735418, mean_eps: 0.944628
 154949/250000: episode: 229, duration: 39.220s, episode steps: 942, steps per second:  24, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012215, mae: 2.236084, mean_q: 2.720981, mean_eps: 0.944388
 155489/250000: episode: 230, duration: 23.126s, episode steps: 540, steps per second:  23, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.011466, mae: 2.231276, mean_q: 2.718010, mean_eps: 0.944121
 155870/250000: episode: 231, duration: 20.414s, episode steps: 381, steps per second:  19, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.013300, mae: 2.247656, mean_q: 2.739591, mean_eps: 0.943955
 156486/250000: episode: 232, duration: 26.874s, episode steps: 616, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.012679, mae: 2.237892, mean_q: 2.726363, mean_eps: 0.943776
 156988/250000: episode: 233, duration: 21.163s, episode steps: 502, steps per second:  24, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.010418, mae: 2.215301, mean_q: 2.699998, mean_eps: 0.943575
 157959/250000: episode: 234, duration: 41.580s, episode steps: 971, steps per second:  23, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.009827, mae: 2.244147, mean_q: 2.734485, mean_eps: 0.943310
 158832/250000: episode: 235, duration: 36.502s, episode steps: 873, steps per second:  24, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.011274, mae: 2.226869, mean_q: 2.714592, mean_eps: 0.942978
 159447/250000: episode: 236, duration: 27.221s, episode steps: 615, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012060, mae: 2.228450, mean_q: 2.715063, mean_eps: 0.942710
 160207/250000: episode: 237, duration: 32.832s, episode steps: 760, steps per second:  23, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.011253, mae: 2.243447, mean_q: 2.732170, mean_eps: 0.942463
 160956/250000: episode: 238, duration: 31.681s, episode steps: 749, steps per second:  24, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.011202, mae: 2.230558, mean_q: 2.716307, mean_eps: 0.942191
 161608/250000: episode: 239, duration: 28.432s, episode steps: 652, steps per second:  23, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012683, mae: 2.226926, mean_q: 2.710975, mean_eps: 0.941939
 162288/250000: episode: 240, duration: 29.191s, episode steps: 680, steps per second:  23, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011276, mae: 2.226185, mean_q: 2.713109, mean_eps: 0.941699
 162657/250000: episode: 241, duration: 17.643s, episode steps: 369, steps per second:  21, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.010605, mae: 2.223980, mean_q: 2.710750, mean_eps: 0.941510
 163047/250000: episode: 242, duration: 18.685s, episode steps: 390, steps per second:  21, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.011171, mae: 2.222554, mean_q: 2.707229, mean_eps: 0.941373
 164230/250000: episode: 243, duration: 58.802s, episode steps: 1183, steps per second:  20, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.010720, mae: 2.235545, mean_q: 2.722346, mean_eps: 0.941090
 165206/250000: episode: 244, duration: 46.244s, episode steps: 976, steps per second:  21, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.012089, mae: 2.224682, mean_q: 2.710044, mean_eps: 0.940702
 166186/250000: episode: 245, duration: 44.988s, episode steps: 980, steps per second:  22, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.011120, mae: 2.220002, mean_q: 2.704712, mean_eps: 0.940349
 167003/250000: episode: 246, duration: 37.275s, episode steps: 817, steps per second:  22, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.010384, mae: 2.214265, mean_q: 2.696240, mean_eps: 0.940026
 167756/250000: episode: 247, duration: 37.539s, episode steps: 753, steps per second:  20, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011627, mae: 2.203643, mean_q: 2.685122, mean_eps: 0.939744
 168329/250000: episode: 248, duration: 27.506s, episode steps: 573, steps per second:  21, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.010869, mae: 2.228432, mean_q: 2.714343, mean_eps: 0.939505
 169160/250000: episode: 249, duration: 42.387s, episode steps: 831, steps per second:  20, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.012413, mae: 2.220811, mean_q: 2.705630, mean_eps: 0.939252
 170052/250000: episode: 250, duration: 44.470s, episode steps: 892, steps per second:  20, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.012137, mae: 2.228542, mean_q: 2.715035, mean_eps: 0.938943
 170565/250000: episode: 251, duration: 21.153s, episode steps: 513, steps per second:  24, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010176, mae: 2.221850, mean_q: 2.707359, mean_eps: 0.938689
 171160/250000: episode: 252, duration: 25.225s, episode steps: 595, steps per second:  24, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.011998, mae: 2.236099, mean_q: 2.723626, mean_eps: 0.938490
 171775/250000: episode: 253, duration: 25.700s, episode steps: 615, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.010874, mae: 2.228623, mean_q: 2.715245, mean_eps: 0.938272
 172453/250000: episode: 254, duration: 28.345s, episode steps: 678, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.010136, mae: 2.233673, mean_q: 2.720566, mean_eps: 0.938039
 172954/250000: episode: 255, duration: 21.607s, episode steps: 501, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.011236, mae: 2.206978, mean_q: 2.686605, mean_eps: 0.937827
 173428/250000: episode: 256, duration: 19.747s, episode steps: 474, steps per second:  24, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.010738, mae: 2.197859, mean_q: 2.675394, mean_eps: 0.937652
 174108/250000: episode: 257, duration: 27.890s, episode steps: 680, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.012047, mae: 2.227261, mean_q: 2.713825, mean_eps: 0.937444
 174929/250000: episode: 258, duration: 35.375s, episode steps: 821, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.012010, mae: 2.236142, mean_q: 2.725483, mean_eps: 0.937174
 175857/250000: episode: 259, duration: 38.364s, episode steps: 928, steps per second:  24, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.012540, mae: 2.222902, mean_q: 2.706469, mean_eps: 0.936858
 176420/250000: episode: 260, duration: 25.414s, episode steps: 563, steps per second:  22, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.011946, mae: 2.219925, mean_q: 2.704525, mean_eps: 0.936590
 176938/250000: episode: 261, duration: 21.791s, episode steps: 518, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.012251, mae: 2.201208, mean_q: 2.682128, mean_eps: 0.936396
 177753/250000: episode: 262, duration: 33.906s, episode steps: 815, steps per second:  24, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.010764, mae: 2.194699, mean_q: 2.672222, mean_eps: 0.936155
 178493/250000: episode: 263, duration: 31.707s, episode steps: 740, steps per second:  23, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.010974, mae: 2.226426, mean_q: 2.711087, mean_eps: 0.935875
 178883/250000: episode: 264, duration: 16.422s, episode steps: 390, steps per second:  24, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012155, mae: 2.201251, mean_q: 2.679101, mean_eps: 0.935672
 179265/250000: episode: 265, duration: 16.759s, episode steps: 382, steps per second:  23, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.009965, mae: 2.202721, mean_q: 2.681711, mean_eps: 0.935533
 179708/250000: episode: 266, duration: 39.976s, episode steps: 443, steps per second:  11, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013579, mae: 2.198598, mean_q: 2.676366, mean_eps: 0.935385
 180607/250000: episode: 267, duration: 165.998s, episode steps: 899, steps per second:   5, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.011115, mae: 2.208987, mean_q: 2.689728, mean_eps: 0.935144
 181185/250000: episode: 268, duration: 132.702s, episode steps: 578, steps per second:   4, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.012521, mae: 2.217966, mean_q: 2.701861, mean_eps: 0.934877
 181566/250000: episode: 269, duration: 78.804s, episode steps: 381, steps per second:   5, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.011939, mae: 2.219341, mean_q: 2.702898, mean_eps: 0.934705
 182683/250000: episode: 270, duration: 213.994s, episode steps: 1117, steps per second:   5, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.010336, mae: 2.209741, mean_q: 2.693683, mean_eps: 0.934435
 183192/250000: episode: 271, duration: 80.440s, episode steps: 509, steps per second:   6, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.010037, mae: 2.214103, mean_q: 2.697307, mean_eps: 0.934143
 183575/250000: episode: 272, duration: 54.437s, episode steps: 383, steps per second:   7, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.012367, mae: 2.229874, mean_q: 2.714414, mean_eps: 0.933982
 184253/250000: episode: 273, duration: 180.147s, episode steps: 678, steps per second:   4, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.013128, mae: 2.212523, mean_q: 2.695170, mean_eps: 0.933791
 184864/250000: episode: 274, duration: 175.369s, episode steps: 611, steps per second:   3, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.011771, mae: 2.210672, mean_q: 2.691439, mean_eps: 0.933559
 185285/250000: episode: 275, duration: 104.219s, episode steps: 421, steps per second:   4, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.010835, mae: 2.255619, mean_q: 2.748034, mean_eps: 0.933373
 186080/250000: episode: 276, duration: 192.145s, episode steps: 795, steps per second:   4, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012377, mae: 2.235247, mean_q: 2.723080, mean_eps: 0.933154
 186630/250000: episode: 277, duration: 130.518s, episode steps: 550, steps per second:   4, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.009931, mae: 2.212160, mean_q: 2.694449, mean_eps: 0.932913
 187156/250000: episode: 278, duration: 128.981s, episode steps: 526, steps per second:   4, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.011044, mae: 2.242015, mean_q: 2.731732, mean_eps: 0.932719
 187633/250000: episode: 279, duration: 122.314s, episode steps: 477, steps per second:   4, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.011213, mae: 2.226811, mean_q: 2.713032, mean_eps: 0.932538
 188856/250000: episode: 280, duration: 295.689s, episode steps: 1223, steps per second:   4, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.010276, mae: 2.231420, mean_q: 2.715815, mean_eps: 0.932232
 189616/250000: episode: 281, duration: 96.625s, episode steps: 760, steps per second:   8, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010622, mae: 2.230883, mean_q: 2.716631, mean_eps: 0.931876
 190201/250000: episode: 282, duration: 24.752s, episode steps: 585, steps per second:  24, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012442, mae: 2.218832, mean_q: 2.700578, mean_eps: 0.931633
 191314/250000: episode: 283, duration: 46.917s, episode steps: 1113, steps per second:  24, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.011876, mae: 2.214264, mean_q: 2.694682, mean_eps: 0.931327
 191863/250000: episode: 284, duration: 21.595s, episode steps: 549, steps per second:  25, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.012056, mae: 2.228058, mean_q: 2.714197, mean_eps: 0.931028
 192681/250000: episode: 285, duration: 34.916s, episode steps: 818, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.012118, mae: 2.206705, mean_q: 2.685547, mean_eps: 0.930782
 193262/250000: episode: 286, duration: 23.857s, episode steps: 581, steps per second:  24, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012250, mae: 2.229660, mean_q: 2.713550, mean_eps: 0.930530
 193660/250000: episode: 287, duration: 16.553s, episode steps: 398, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.012545, mae: 2.212176, mean_q: 2.690949, mean_eps: 0.930354
 194264/250000: episode: 288, duration: 25.599s, episode steps: 604, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.013878, mae: 2.212257, mean_q: 2.692170, mean_eps: 0.930174
 195091/250000: episode: 289, duration: 33.695s, episode steps: 827, steps per second:  25, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.011342, mae: 2.213154, mean_q: 2.693171, mean_eps: 0.929917
 195956/250000: episode: 290, duration: 42.322s, episode steps: 865, steps per second:  20, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.012057, mae: 2.222092, mean_q: 2.704947, mean_eps: 0.929612
 196393/250000: episode: 291, duration: 19.053s, episode steps: 437, steps per second:  23, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.011740, mae: 2.218300, mean_q: 2.698719, mean_eps: 0.929377
 196912/250000: episode: 292, duration: 24.041s, episode steps: 519, steps per second:  22, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.011660, mae: 2.207093, mean_q: 2.686133, mean_eps: 0.929205
 197481/250000: episode: 293, duration: 24.101s, episode steps: 569, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.010359, mae: 2.194440, mean_q: 2.669346, mean_eps: 0.929009
 198352/250000: episode: 294, duration: 37.078s, episode steps: 871, steps per second:  23, episode reward:  8.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.011835, mae: 2.221642, mean_q: 2.702586, mean_eps: 0.928750
 198753/250000: episode: 295, duration: 17.566s, episode steps: 401, steps per second:  23, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.010348, mae: 2.196169, mean_q: 2.675300, mean_eps: 0.928521
 199898/250000: episode: 296, duration: 47.706s, episode steps: 1145, steps per second:  24, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.010654, mae: 2.226383, mean_q: 2.711352, mean_eps: 0.928243
 200975/250000: episode: 297, duration: 45.363s, episode steps: 1077, steps per second:  24, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.012251, mae: 2.247050, mean_q: 2.734975, mean_eps: 0.927843
 201617/250000: episode: 298, duration: 28.531s, episode steps: 642, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.012444, mae: 2.247914, mean_q: 2.737251, mean_eps: 0.927533
 202427/250000: episode: 299, duration: 34.466s, episode steps: 810, steps per second:  24, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.012061, mae: 2.259720, mean_q: 2.751524, mean_eps: 0.927272
 203344/250000: episode: 300, duration: 38.315s, episode steps: 917, steps per second:  24, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.011831, mae: 2.239802, mean_q: 2.724739, mean_eps: 0.926962
 203738/250000: episode: 301, duration: 16.406s, episode steps: 394, steps per second:  24, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.015408, mae: 2.233658, mean_q: 2.714945, mean_eps: 0.926726
 204171/250000: episode: 302, duration: 17.183s, episode steps: 433, steps per second:  25, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.013814, mae: 2.262863, mean_q: 2.751904, mean_eps: 0.926577
 205315/250000: episode: 303, duration: 50.302s, episode steps: 1144, steps per second:  23, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012266, mae: 2.219342, mean_q: 2.701716, mean_eps: 0.926293
 205959/250000: episode: 304, duration: 26.859s, episode steps: 644, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011303, mae: 2.240505, mean_q: 2.726638, mean_eps: 0.925971
 206709/250000: episode: 305, duration: 32.053s, episode steps: 750, steps per second:  23, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.011670, mae: 2.254164, mean_q: 2.744703, mean_eps: 0.925720
 207347/250000: episode: 306, duration: 26.290s, episode steps: 638, steps per second:  24, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.011789, mae: 2.253336, mean_q: 2.743215, mean_eps: 0.925470
 207728/250000: episode: 307, duration: 16.243s, episode steps: 381, steps per second:  23, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012955, mae: 2.263171, mean_q: 2.752826, mean_eps: 0.925287
 208405/250000: episode: 308, duration: 29.630s, episode steps: 677, steps per second:  23, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.010548, mae: 2.216328, mean_q: 2.696399, mean_eps: 0.925096
 208978/250000: episode: 309, duration: 25.113s, episode steps: 573, steps per second:  23, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012076, mae: 2.231166, mean_q: 2.716716, mean_eps: 0.924871
 209618/250000: episode: 310, duration: 28.399s, episode steps: 640, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.011506, mae: 2.249261, mean_q: 2.737367, mean_eps: 0.924653
 210314/250000: episode: 311, duration: 29.367s, episode steps: 696, steps per second:  24, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.011953, mae: 2.251104, mean_q: 2.738320, mean_eps: 0.924412
 211264/250000: episode: 312, duration: 41.691s, episode steps: 950, steps per second:  23, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012941, mae: 2.265347, mean_q: 2.757508, mean_eps: 0.924116
 211668/250000: episode: 313, duration: 16.770s, episode steps: 404, steps per second:  24, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.008788, mae: 2.235112, mean_q: 2.720218, mean_eps: 0.923873
 212162/250000: episode: 314, duration: 21.727s, episode steps: 494, steps per second:  23, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.011670, mae: 2.240880, mean_q: 2.725904, mean_eps: 0.923711
 213208/250000: episode: 315, duration: 43.512s, episode steps: 1046, steps per second:  24, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.011927, mae: 2.236119, mean_q: 2.721195, mean_eps: 0.923434
 213748/250000: episode: 316, duration: 23.992s, episode steps: 540, steps per second:  23, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.011771, mae: 2.255589, mean_q: 2.743626, mean_eps: 0.923149
 214259/250000: episode: 317, duration: 21.292s, episode steps: 511, steps per second:  24, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.012940, mae: 2.279072, mean_q: 2.771032, mean_eps: 0.922959
 214800/250000: episode: 318, duration: 22.924s, episode steps: 541, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.010537, mae: 2.250374, mean_q: 2.739125, mean_eps: 0.922770
 215181/250000: episode: 319, duration: 16.313s, episode steps: 381, steps per second:  23, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.010827, mae: 2.229976, mean_q: 2.716195, mean_eps: 0.922604
 215839/250000: episode: 320, duration: 27.183s, episode steps: 658, steps per second:  24, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010986, mae: 2.241873, mean_q: 2.726571, mean_eps: 0.922416
 216372/250000: episode: 321, duration: 23.247s, episode steps: 533, steps per second:  23, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.010870, mae: 2.261115, mean_q: 2.752145, mean_eps: 0.922203
 217144/250000: episode: 322, duration: 34.034s, episode steps: 772, steps per second:  23, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.012818, mae: 2.241685, mean_q: 2.727705, mean_eps: 0.921968
 217877/250000: episode: 323, duration: 30.693s, episode steps: 733, steps per second:  24, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.013058, mae: 2.224278, mean_q: 2.705467, mean_eps: 0.921696
 219037/250000: episode: 324, duration: 49.294s, episode steps: 1160, steps per second:  24, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012526, mae: 2.245962, mean_q: 2.733177, mean_eps: 0.921355
 220009/250000: episode: 325, duration: 40.150s, episode steps: 972, steps per second:  24, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011502, mae: 2.247735, mean_q: 2.735104, mean_eps: 0.920971
 221114/250000: episode: 326, duration: 46.146s, episode steps: 1105, steps per second:  24, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.011214, mae: 2.243257, mean_q: 2.729863, mean_eps: 0.920598
 221754/250000: episode: 327, duration: 27.714s, episode steps: 640, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.012004, mae: 2.246219, mean_q: 2.735546, mean_eps: 0.920284
 222279/250000: episode: 328, duration: 22.334s, episode steps: 525, steps per second:  24, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.010453, mae: 2.218301, mean_q: 2.699880, mean_eps: 0.920074
 223054/250000: episode: 329, duration: 32.434s, episode steps: 775, steps per second:  24, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.012246, mae: 2.234919, mean_q: 2.721134, mean_eps: 0.919840
 223723/250000: episode: 330, duration: 29.005s, episode steps: 669, steps per second:  23, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.010730, mae: 2.239277, mean_q: 2.727196, mean_eps: 0.919580
 224329/250000: episode: 331, duration: 25.882s, episode steps: 606, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.012351, mae: 2.231559, mean_q: 2.715527, mean_eps: 0.919351
 225020/250000: episode: 332, duration: 29.765s, episode steps: 691, steps per second:  23, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.011886, mae: 2.241389, mean_q: 2.727269, mean_eps: 0.919117
 225663/250000: episode: 333, duration: 26.309s, episode steps: 643, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.012335, mae: 2.247645, mean_q: 2.735206, mean_eps: 0.918878
 226289/250000: episode: 334, duration: 27.986s, episode steps: 626, steps per second:  22, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.012763, mae: 2.239570, mean_q: 2.725817, mean_eps: 0.918649
 226985/250000: episode: 335, duration: 29.770s, episode steps: 696, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.011747, mae: 2.234176, mean_q: 2.717188, mean_eps: 0.918410
 227455/250000: episode: 336, duration: 19.472s, episode steps: 470, steps per second:  24, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.013200, mae: 2.215410, mean_q: 2.694181, mean_eps: 0.918201
 228163/250000: episode: 337, duration: 30.238s, episode steps: 708, steps per second:  23, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.013093, mae: 2.252433, mean_q: 2.739477, mean_eps: 0.917989
 228809/250000: episode: 338, duration: 27.436s, episode steps: 646, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.012705, mae: 2.269600, mean_q: 2.760448, mean_eps: 0.917745
 229442/250000: episode: 339, duration: 26.756s, episode steps: 633, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.013041, mae: 2.260697, mean_q: 2.749047, mean_eps: 0.917515
 230158/250000: episode: 340, duration: 30.282s, episode steps: 716, steps per second:  24, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.011786, mae: 2.229714, mean_q: 2.713640, mean_eps: 0.917272
 230905/250000: episode: 341, duration: 31.877s, episode steps: 747, steps per second:  23, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012493, mae: 2.245845, mean_q: 2.733286, mean_eps: 0.917008
 231543/250000: episode: 342, duration: 27.470s, episode steps: 638, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.010082, mae: 2.227981, mean_q: 2.711654, mean_eps: 0.916759
 232188/250000: episode: 343, duration: 27.661s, episode steps: 645, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.012468, mae: 2.253415, mean_q: 2.742147, mean_eps: 0.916529
 232679/250000: episode: 344, duration: 20.916s, episode steps: 491, steps per second:  23, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.010588, mae: 2.232910, mean_q: 2.716047, mean_eps: 0.916324
 233094/250000: episode: 345, duration: 17.614s, episode steps: 415, steps per second:  24, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.795 [0.000, 5.000],  loss: 0.010990, mae: 2.264949, mean_q: 2.757287, mean_eps: 0.916161
 233983/250000: episode: 346, duration: 37.351s, episode steps: 889, steps per second:  24, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.011058, mae: 2.245788, mean_q: 2.732426, mean_eps: 0.915926
 234635/250000: episode: 347, duration: 27.863s, episode steps: 652, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.010957, mae: 2.228108, mean_q: 2.709144, mean_eps: 0.915649
 235121/250000: episode: 348, duration: 20.917s, episode steps: 486, steps per second:  23, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.012241, mae: 2.252164, mean_q: 2.740398, mean_eps: 0.915444
 235762/250000: episode: 349, duration: 26.577s, episode steps: 641, steps per second:  24, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.013839, mae: 2.259248, mean_q: 2.746793, mean_eps: 0.915241
 236159/250000: episode: 350, duration: 16.998s, episode steps: 397, steps per second:  23, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.011140, mae: 2.238449, mean_q: 2.723116, mean_eps: 0.915054
 236753/250000: episode: 351, duration: 25.574s, episode steps: 594, steps per second:  23, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.011999, mae: 2.254232, mean_q: 2.742333, mean_eps: 0.914876
 237262/250000: episode: 352, duration: 21.178s, episode steps: 509, steps per second:  24, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.012326, mae: 2.259620, mean_q: 2.745712, mean_eps: 0.914677
 238028/250000: episode: 353, duration: 32.762s, episode steps: 766, steps per second:  23, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.013247, mae: 2.241506, mean_q: 2.726961, mean_eps: 0.914448
 238539/250000: episode: 354, duration: 22.506s, episode steps: 511, steps per second:  23, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.013585, mae: 2.225783, mean_q: 2.705866, mean_eps: 0.914218
 239755/250000: episode: 355, duration: 51.133s, episode steps: 1216, steps per second:  24, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011160, mae: 2.249338, mean_q: 2.736934, mean_eps: 0.913907
 240495/250000: episode: 356, duration: 32.168s, episode steps: 740, steps per second:  23, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.012922, mae: 2.251735, mean_q: 2.737861, mean_eps: 0.913555
 241228/250000: episode: 357, duration: 30.928s, episode steps: 733, steps per second:  24, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.011948, mae: 2.245702, mean_q: 2.733020, mean_eps: 0.913290
 241709/250000: episode: 358, duration: 20.757s, episode steps: 481, steps per second:  23, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.012157, mae: 2.215136, mean_q: 2.694760, mean_eps: 0.913072
 242795/250000: episode: 359, duration: 45.955s, episode steps: 1086, steps per second:  24, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.013728, mae: 2.239398, mean_q: 2.723602, mean_eps: 0.912789
 243381/250000: episode: 360, duration: 26.204s, episode steps: 586, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.011397, mae: 2.251599, mean_q: 2.741763, mean_eps: 0.912488
 243952/250000: episode: 361, duration: 24.059s, episode steps: 571, steps per second:  24, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013937, mae: 2.247794, mean_q: 2.732527, mean_eps: 0.912280
 244553/250000: episode: 362, duration: 25.993s, episode steps: 601, steps per second:  23, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.011773, mae: 2.246102, mean_q: 2.733452, mean_eps: 0.912069
 245460/250000: episode: 363, duration: 38.524s, episode steps: 907, steps per second:  24, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.011466, mae: 2.243736, mean_q: 2.730380, mean_eps: 0.911798
 246495/250000: episode: 364, duration: 44.722s, episode steps: 1035, steps per second:  23, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.011442, mae: 2.229371, mean_q: 2.712418, mean_eps: 0.911449
 247058/250000: episode: 365, duration: 24.579s, episode steps: 563, steps per second:  23, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.012857, mae: 2.242237, mean_q: 2.728088, mean_eps: 0.911161
 247503/250000: episode: 366, duration: 18.945s, episode steps: 445, steps per second:  23, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.010181, mae: 2.222419, mean_q: 2.704469, mean_eps: 0.910979
 248227/250000: episode: 367, duration: 31.535s, episode steps: 724, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.012590, mae: 2.233968, mean_q: 2.717306, mean_eps: 0.910769
 249315/250000: episode: 368, duration: 49.413s, episode steps: 1088, steps per second:  22, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.011986, mae: 2.237444, mean_q: 2.723035, mean_eps: 0.910443
 249863/250000: episode: 369, duration: 22.545s, episode steps: 548, steps per second:  24, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.011051, mae: 2.237834, mean_q: 2.723306, mean_eps: 0.910148
done, took 12241.657 seconds
########################################################
PROCESO TERMINADO
########################################################