['./models\\dqn_weights_10000.h5f.index', './models\\dqn_weights_100000.h5f.index', './models\\dqn_weights_110000.h5f.index', './models\\dqn_weights_120000.h5f.index', './models\\dqn_weights_130000.h5f.index', './models\\dqn_weights_140000.h5f.index', './models\\dqn_weights_150000.h5f.index', './models\\dqn_weights_160000.h5f.index', './models\\dqn_weights_170000.h5f.index', './models\\dqn_weights_180000.h5f.index', './models\\dqn_weights_190000.h5f.index', './models\\dqn_weights_20000.h5f.index', './models\\dqn_weights_200000.h5f.index', './models\\dqn_weights_210000.h5f.index', './models\\dqn_weights_220000.h5f.index', './models\\dqn_weights_230000.h5f.index', './models\\dqn_weights_240000.h5f.index', './models\\dqn_weights_250000.h5f.index', './models\\dqn_weights_30000.h5f.index', './models\\dqn_weights_40000.h5f.index', './models\\dqn_weights_50000.h5f.index', './models\\dqn_weights_60000.h5f.index', './models\\dqn_weights_70000.h5f.index', './models\\dqn_weights_80000.h5f.index', './models\\dqn_weights_90000.h5f.index']
Cargando pesos desde: ./models\dqn_weights_250000.h5f
Hay pesos anteriores y se van a cargar
Training for 250000 steps ...
    420/250000: episode: 1, duration: 2.656s, episode steps: 420, steps per second: 158, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1131/250000: episode: 2, duration: 4.590s, episode steps: 711, steps per second: 155, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1941/250000: episode: 3, duration: 8.647s, episode steps: 810, steps per second:  94, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2827/250000: episode: 4, duration: 6.427s, episode steps: 886, steps per second: 138, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3345/250000: episode: 5, duration: 3.635s, episode steps: 518, steps per second: 143, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3990/250000: episode: 6, duration: 4.535s, episode steps: 645, steps per second: 142, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4451/250000: episode: 7, duration: 2.921s, episode steps: 461, steps per second: 158, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5247/250000: episode: 8, duration: 4.947s, episode steps: 796, steps per second: 161, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5629/250000: episode: 9, duration: 2.617s, episode steps: 382, steps per second: 146, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6746/250000: episode: 10, duration: 8.588s, episode steps: 1117, steps per second: 130, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7257/250000: episode: 11, duration: 3.353s, episode steps: 511, steps per second: 152, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8039/250000: episode: 12, duration: 5.269s, episode steps: 782, steps per second: 148, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8763/250000: episode: 13, duration: 4.903s, episode steps: 724, steps per second: 148, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9398/250000: episode: 14, duration: 4.138s, episode steps: 635, steps per second: 153, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10244/250000: episode: 15, duration: 18.240s, episode steps: 846, steps per second:  46, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.013163, mae: 2.047519, mean_q: 2.520271, mean_eps: 0.996356
  10908/250000: episode: 16, duration: 31.286s, episode steps: 664, steps per second:  21, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.011351, mae: 2.009137, mean_q: 2.472080, mean_eps: 0.996193
  11502/250000: episode: 17, duration: 25.137s, episode steps: 594, steps per second:  24, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.010412, mae: 2.022121, mean_q: 2.491506, mean_eps: 0.995967
  12188/250000: episode: 18, duration: 25.029s, episode steps: 686, steps per second:  27, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.009819, mae: 2.009649, mean_q: 2.473911, mean_eps: 0.995736
  12667/250000: episode: 19, duration: 19.355s, episode steps: 479, steps per second:  25, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.009283, mae: 2.004260, mean_q: 2.465320, mean_eps: 0.995527
  13598/250000: episode: 20, duration: 36.070s, episode steps: 931, steps per second:  26, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010495, mae: 2.006591, mean_q: 2.467608, mean_eps: 0.995272
  14466/250000: episode: 21, duration: 33.726s, episode steps: 868, steps per second:  26, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.010609, mae: 2.045755, mean_q: 2.517039, mean_eps: 0.994948
  15594/250000: episode: 22, duration: 41.401s, episode steps: 1128, steps per second:  27, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.011059, mae: 2.032349, mean_q: 2.497143, mean_eps: 0.994589
  16295/250000: episode: 23, duration: 28.945s, episode steps: 701, steps per second:  24, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.010747, mae: 2.052119, mean_q: 2.522940, mean_eps: 0.994260
  17267/250000: episode: 24, duration: 39.036s, episode steps: 972, steps per second:  25, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.008940, mae: 2.018633, mean_q: 2.482741, mean_eps: 0.993959
  18173/250000: episode: 25, duration: 37.268s, episode steps: 906, steps per second:  24, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.009801, mae: 2.016716, mean_q: 2.477213, mean_eps: 0.993621
  18705/250000: episode: 26, duration: 19.590s, episode steps: 532, steps per second:  27, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.008244, mae: 2.001432, mean_q: 2.461548, mean_eps: 0.993362
  19345/250000: episode: 27, duration: 28.152s, episode steps: 640, steps per second:  23, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.009086, mae: 2.028455, mean_q: 2.493581, mean_eps: 0.993151
  19892/250000: episode: 28, duration: 23.137s, episode steps: 547, steps per second:  24, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.010308, mae: 1.981308, mean_q: 2.432667, mean_eps: 0.992938
  20537/250000: episode: 29, duration: 24.828s, episode steps: 645, steps per second:  26, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.009915, mae: 2.002440, mean_q: 2.459806, mean_eps: 0.992723
  21079/250000: episode: 30, duration: 21.775s, episode steps: 542, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.010769, mae: 2.007991, mean_q: 2.465218, mean_eps: 0.992509
  21558/250000: episode: 31, duration: 19.164s, episode steps: 479, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.011429, mae: 1.991140, mean_q: 2.445313, mean_eps: 0.992326
  22539/250000: episode: 32, duration: 38.588s, episode steps: 981, steps per second:  25, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.009599, mae: 1.976965, mean_q: 2.427585, mean_eps: 0.992063
  23605/250000: episode: 33, duration: 42.829s, episode steps: 1066, steps per second:  25, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010499, mae: 1.992873, mean_q: 2.449366, mean_eps: 0.991694
  24343/250000: episode: 34, duration: 29.200s, episode steps: 738, steps per second:  25, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010210, mae: 1.996245, mean_q: 2.453707, mean_eps: 0.991369
  24965/250000: episode: 35, duration: 24.102s, episode steps: 622, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.011186, mae: 1.995419, mean_q: 2.452657, mean_eps: 0.991125
  25349/250000: episode: 36, duration: 14.688s, episode steps: 384, steps per second:  26, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.009623, mae: 2.029890, mean_q: 2.495631, mean_eps: 0.990943
  25949/250000: episode: 37, duration: 23.537s, episode steps: 600, steps per second:  25, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.010711, mae: 2.007429, mean_q: 2.465777, mean_eps: 0.990766
  26693/250000: episode: 38, duration: 29.083s, episode steps: 744, steps per second:  26, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.009217, mae: 2.030397, mean_q: 2.496338, mean_eps: 0.990524
  27312/250000: episode: 39, duration: 24.238s, episode steps: 619, steps per second:  26, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.008787, mae: 1.998065, mean_q: 2.454931, mean_eps: 0.990279
  28269/250000: episode: 40, duration: 37.555s, episode steps: 957, steps per second:  25, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.010941, mae: 2.014458, mean_q: 2.475004, mean_eps: 0.989996
  28818/250000: episode: 41, duration: 20.401s, episode steps: 549, steps per second:  27, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.009556, mae: 2.002906, mean_q: 2.459969, mean_eps: 0.989724
  29855/250000: episode: 42, duration: 41.673s, episode steps: 1037, steps per second:  25, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.010381, mae: 2.026882, mean_q: 2.489559, mean_eps: 0.989439
  30399/250000: episode: 43, duration: 20.565s, episode steps: 544, steps per second:  26, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.010081, mae: 2.017317, mean_q: 2.476776, mean_eps: 0.989155
  31131/250000: episode: 44, duration: 29.519s, episode steps: 732, steps per second:  25, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.010352, mae: 2.020263, mean_q: 2.480832, mean_eps: 0.988925
  32310/250000: episode: 45, duration: 45.772s, episode steps: 1179, steps per second:  26, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.009243, mae: 2.009109, mean_q: 2.468614, mean_eps: 0.988581
  32753/250000: episode: 46, duration: 18.059s, episode steps: 443, steps per second:  25, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.007958, mae: 2.005080, mean_q: 2.464418, mean_eps: 0.988288
  33566/250000: episode: 47, duration: 31.294s, episode steps: 813, steps per second:  26, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.009418, mae: 1.988891, mean_q: 2.442486, mean_eps: 0.988062
  34067/250000: episode: 48, duration: 18.551s, episode steps: 501, steps per second:  27, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.009771, mae: 1.987110, mean_q: 2.439072, mean_eps: 0.987826
  34847/250000: episode: 49, duration: 34.145s, episode steps: 780, steps per second:  23, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.008803, mae: 2.004330, mean_q: 2.463621, mean_eps: 0.987596
  35391/250000: episode: 50, duration: 22.430s, episode steps: 544, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.008131, mae: 2.010976, mean_q: 2.469862, mean_eps: 0.987358
  36141/250000: episode: 51, duration: 30.921s, episode steps: 750, steps per second:  24, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.009612, mae: 2.013998, mean_q: 2.474221, mean_eps: 0.987124
  36734/250000: episode: 52, duration: 21.884s, episode steps: 593, steps per second:  27, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.010217, mae: 1.993397, mean_q: 2.448689, mean_eps: 0.986882
  37208/250000: episode: 53, duration: 17.924s, episode steps: 474, steps per second:  26, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.006971, mae: 2.028476, mean_q: 2.493113, mean_eps: 0.986691
  37894/250000: episode: 54, duration: 28.030s, episode steps: 686, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.009303, mae: 1.998078, mean_q: 2.454284, mean_eps: 0.986482
  38386/250000: episode: 55, duration: 18.557s, episode steps: 492, steps per second:  27, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.010593, mae: 2.001785, mean_q: 2.457829, mean_eps: 0.986270
  38918/250000: episode: 56, duration: 20.003s, episode steps: 532, steps per second:  27, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.010025, mae: 1.982286, mean_q: 2.435375, mean_eps: 0.986085
  39319/250000: episode: 57, duration: 16.383s, episode steps: 401, steps per second:  24, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.012678, mae: 2.007295, mean_q: 2.463668, mean_eps: 0.985918
  39725/250000: episode: 58, duration: 15.873s, episode steps: 406, steps per second:  26, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010364, mae: 2.008219, mean_q: 2.466483, mean_eps: 0.985772
  40655/250000: episode: 59, duration: 36.181s, episode steps: 930, steps per second:  26, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.010761, mae: 1.997026, mean_q: 2.452309, mean_eps: 0.985532
  41559/250000: episode: 60, duration: 36.643s, episode steps: 904, steps per second:  25, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.010204, mae: 1.984954, mean_q: 2.438157, mean_eps: 0.985202
  42353/250000: episode: 61, duration: 30.864s, episode steps: 794, steps per second:  26, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.008793, mae: 1.979807, mean_q: 2.430272, mean_eps: 0.984896
  42983/250000: episode: 62, duration: 24.528s, episode steps: 630, steps per second:  26, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.009805, mae: 1.977904, mean_q: 2.429800, mean_eps: 0.984640
  44042/250000: episode: 63, duration: 41.386s, episode steps: 1059, steps per second:  26, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.010113, mae: 1.998094, mean_q: 2.453100, mean_eps: 0.984336
  44426/250000: episode: 64, duration: 16.167s, episode steps: 384, steps per second:  24, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.008639, mae: 1.986525, mean_q: 2.438364, mean_eps: 0.984076
  45111/250000: episode: 65, duration: 28.184s, episode steps: 685, steps per second:  24, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.009181, mae: 1.971814, mean_q: 2.421619, mean_eps: 0.983884
  45950/250000: episode: 66, duration: 33.110s, episode steps: 839, steps per second:  25, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.010477, mae: 1.991988, mean_q: 2.441917, mean_eps: 0.983609
  46613/250000: episode: 67, duration: 24.711s, episode steps: 663, steps per second:  27, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.010343, mae: 2.020882, mean_q: 2.479348, mean_eps: 0.983338
  47585/250000: episode: 68, duration: 38.350s, episode steps: 972, steps per second:  25, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.010797, mae: 2.016728, mean_q: 2.473828, mean_eps: 0.983044
  48320/250000: episode: 69, duration: 28.691s, episode steps: 735, steps per second:  26, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.010887, mae: 2.020408, mean_q: 2.480254, mean_eps: 0.982737
  49012/250000: episode: 70, duration: 26.223s, episode steps: 692, steps per second:  26, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.009458, mae: 2.018713, mean_q: 2.477126, mean_eps: 0.982481
  49518/250000: episode: 71, duration: 20.646s, episode steps: 506, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.009785, mae: 2.048764, mean_q: 2.511680, mean_eps: 0.982265
  49971/250000: episode: 72, duration: 18.685s, episode steps: 453, steps per second:  24, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.010652, mae: 2.020608, mean_q: 2.478403, mean_eps: 0.982092
  50950/250000: episode: 73, duration: 40.515s, episode steps: 979, steps per second:  24, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011508, mae: 2.046310, mean_q: 2.509211, mean_eps: 0.981834
  51651/250000: episode: 74, duration: 28.233s, episode steps: 701, steps per second:  25, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.011369, mae: 2.039677, mean_q: 2.498192, mean_eps: 0.981532
  52534/250000: episode: 75, duration: 35.381s, episode steps: 883, steps per second:  25, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.010615, mae: 2.043381, mean_q: 2.503947, mean_eps: 0.981247
  53470/250000: episode: 76, duration: 36.936s, episode steps: 936, steps per second:  25, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.010873, mae: 2.054063, mean_q: 2.517449, mean_eps: 0.980919
  53903/250000: episode: 77, duration: 17.803s, episode steps: 433, steps per second:  24, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.012631, mae: 2.043847, mean_q: 2.500727, mean_eps: 0.980673
  54293/250000: episode: 78, duration: 17.294s, episode steps: 390, steps per second:  23, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.012813, mae: 2.048642, mean_q: 2.507648, mean_eps: 0.980525
  55562/250000: episode: 79, duration: 49.324s, episode steps: 1269, steps per second:  26, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.010885, mae: 2.053805, mean_q: 2.517274, mean_eps: 0.980226
  56258/250000: episode: 80, duration: 27.477s, episode steps: 696, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.010145, mae: 2.045226, mean_q: 2.504043, mean_eps: 0.979872
  56681/250000: episode: 81, duration: 16.506s, episode steps: 423, steps per second:  26, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.011194, mae: 2.009265, mean_q: 2.462050, mean_eps: 0.979671
  57067/250000: episode: 82, duration: 14.667s, episode steps: 386, steps per second:  26, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.010835, mae: 2.034858, mean_q: 2.492430, mean_eps: 0.979525
  57500/250000: episode: 83, duration: 18.417s, episode steps: 433, steps per second:  24, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.010807, mae: 2.058792, mean_q: 2.521382, mean_eps: 0.979378
  58129/250000: episode: 84, duration: 24.415s, episode steps: 629, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.010955, mae: 2.037789, mean_q: 2.495220, mean_eps: 0.979187
  58669/250000: episode: 85, duration: 20.281s, episode steps: 540, steps per second:  27, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.012341, mae: 2.062352, mean_q: 2.523729, mean_eps: 0.978976
  59045/250000: episode: 86, duration: 15.007s, episode steps: 376, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.010750, mae: 2.052311, mean_q: 2.513222, mean_eps: 0.978811
  59438/250000: episode: 87, duration: 16.061s, episode steps: 393, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011165, mae: 2.064154, mean_q: 2.528365, mean_eps: 0.978673
  60373/250000: episode: 88, duration: 36.889s, episode steps: 935, steps per second:  25, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.009237, mae: 2.043197, mean_q: 2.504154, mean_eps: 0.978434
  60859/250000: episode: 89, duration: 19.854s, episode steps: 486, steps per second:  24, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010092, mae: 2.049451, mean_q: 2.509180, mean_eps: 0.978178
  61368/250000: episode: 90, duration: 19.476s, episode steps: 509, steps per second:  26, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.010996, mae: 2.025865, mean_q: 2.481279, mean_eps: 0.978000
  61852/250000: episode: 91, duration: 18.548s, episode steps: 484, steps per second:  26, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011583, mae: 2.035642, mean_q: 2.493186, mean_eps: 0.977821
  62767/250000: episode: 92, duration: 36.431s, episode steps: 915, steps per second:  25, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.011720, mae: 2.033351, mean_q: 2.488788, mean_eps: 0.977569
  63548/250000: episode: 93, duration: 30.231s, episode steps: 781, steps per second:  26, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.010878, mae: 2.050780, mean_q: 2.513136, mean_eps: 0.977264
  64116/250000: episode: 94, duration: 26.281s, episode steps: 568, steps per second:  22, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.010946, mae: 2.046435, mean_q: 2.503707, mean_eps: 0.977021
  64560/250000: episode: 95, duration: 18.874s, episode steps: 444, steps per second:  24, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.012388, mae: 2.055226, mean_q: 2.515051, mean_eps: 0.976839
  65178/250000: episode: 96, duration: 27.051s, episode steps: 618, steps per second:  23, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011894, mae: 2.038972, mean_q: 2.495131, mean_eps: 0.976648
  65596/250000: episode: 97, duration: 16.470s, episode steps: 418, steps per second:  25, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.012111, mae: 2.064296, mean_q: 2.526946, mean_eps: 0.976461
  66236/250000: episode: 98, duration: 25.200s, episode steps: 640, steps per second:  25, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.012399, mae: 2.037047, mean_q: 2.494334, mean_eps: 0.976271
  66940/250000: episode: 99, duration: 26.727s, episode steps: 704, steps per second:  26, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.009680, mae: 2.051308, mean_q: 2.515150, mean_eps: 0.976029
  67584/250000: episode: 100, duration: 26.653s, episode steps: 644, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011011, mae: 2.055553, mean_q: 2.519456, mean_eps: 0.975786
  68372/250000: episode: 101, duration: 30.045s, episode steps: 788, steps per second:  26, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.011751, mae: 2.048431, mean_q: 2.511056, mean_eps: 0.975529
  69160/250000: episode: 102, duration: 30.344s, episode steps: 788, steps per second:  26, episode reward:  5.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.009961, mae: 2.039760, mean_q: 2.497951, mean_eps: 0.975245
  69815/250000: episode: 103, duration: 25.213s, episode steps: 655, steps per second:  26, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.009060, mae: 2.042318, mean_q: 2.501919, mean_eps: 0.974985
  71013/250000: episode: 104, duration: 50.247s, episode steps: 1198, steps per second:  24, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.009980, mae: 2.033272, mean_q: 2.490852, mean_eps: 0.974651
  71418/250000: episode: 105, duration: 16.146s, episode steps: 405, steps per second:  25, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011201, mae: 2.052146, mean_q: 2.512416, mean_eps: 0.974362
  72276/250000: episode: 106, duration: 36.517s, episode steps: 858, steps per second:  23, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011321, mae: 2.037608, mean_q: 2.493397, mean_eps: 0.974135
  72670/250000: episode: 107, duration: 16.254s, episode steps: 394, steps per second:  24, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.010111, mae: 2.050831, mean_q: 2.512076, mean_eps: 0.973910
  73290/250000: episode: 108, duration: 24.009s, episode steps: 620, steps per second:  26, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.010195, mae: 2.051239, mean_q: 2.511379, mean_eps: 0.973727
  73815/250000: episode: 109, duration: 19.884s, episode steps: 525, steps per second:  26, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.009825, mae: 2.047769, mean_q: 2.508971, mean_eps: 0.973521
  74440/250000: episode: 110, duration: 25.773s, episode steps: 625, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.011838, mae: 2.050345, mean_q: 2.512332, mean_eps: 0.973315
  74956/250000: episode: 111, duration: 19.752s, episode steps: 516, steps per second:  26, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.009833, mae: 2.039115, mean_q: 2.496984, mean_eps: 0.973109
  75521/250000: episode: 112, duration: 22.498s, episode steps: 565, steps per second:  25, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.008376, mae: 2.033983, mean_q: 2.491706, mean_eps: 0.972914
  76191/250000: episode: 113, duration: 26.422s, episode steps: 670, steps per second:  25, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.010200, mae: 2.052878, mean_q: 2.514214, mean_eps: 0.972692
  77023/250000: episode: 114, duration: 32.016s, episode steps: 832, steps per second:  26, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.011387, mae: 2.044240, mean_q: 2.503863, mean_eps: 0.972422
  77655/250000: episode: 115, duration: 26.250s, episode steps: 632, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.011848, mae: 2.043621, mean_q: 2.501650, mean_eps: 0.972158
  78242/250000: episode: 116, duration: 22.171s, episode steps: 587, steps per second:  26, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011315, mae: 2.043213, mean_q: 2.503024, mean_eps: 0.971939
  79014/250000: episode: 117, duration: 31.136s, episode steps: 772, steps per second:  25, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.010398, mae: 2.029190, mean_q: 2.485368, mean_eps: 0.971694
  79593/250000: episode: 118, duration: 23.492s, episode steps: 579, steps per second:  25, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.009343, mae: 2.065555, mean_q: 2.531738, mean_eps: 0.971451
  80309/250000: episode: 119, duration: 31.750s, episode steps: 716, steps per second:  23, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.010735, mae: 2.046942, mean_q: 2.506257, mean_eps: 0.971217
  80885/250000: episode: 120, duration: 23.576s, episode steps: 576, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.009147, mae: 2.048255, mean_q: 2.509485, mean_eps: 0.970985
  81410/250000: episode: 121, duration: 20.349s, episode steps: 525, steps per second:  26, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.010962, mae: 2.072338, mean_q: 2.537532, mean_eps: 0.970787
  81970/250000: episode: 122, duration: 22.653s, episode steps: 560, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.011125, mae: 2.059118, mean_q: 2.520166, mean_eps: 0.970592
  82554/250000: episode: 123, duration: 23.395s, episode steps: 584, steps per second:  25, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.010667, mae: 2.041855, mean_q: 2.500440, mean_eps: 0.970386
  83259/250000: episode: 124, duration: 27.735s, episode steps: 705, steps per second:  25, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.011161, mae: 2.050039, mean_q: 2.508413, mean_eps: 0.970154
  83787/250000: episode: 125, duration: 20.900s, episode steps: 528, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.010977, mae: 2.029798, mean_q: 2.484144, mean_eps: 0.969932
  84599/250000: episode: 126, duration: 31.809s, episode steps: 812, steps per second:  26, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.010014, mae: 2.046241, mean_q: 2.505260, mean_eps: 0.969691
  86173/250000: episode: 127, duration: 62.870s, episode steps: 1574, steps per second:  25, episode reward: 33.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.010502, mae: 2.030544, mean_q: 2.486419, mean_eps: 0.969261
  87566/250000: episode: 128, duration: 55.662s, episode steps: 1393, steps per second:  25, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.010562, mae: 2.042982, mean_q: 2.502627, mean_eps: 0.968727
  88239/250000: episode: 129, duration: 25.332s, episode steps: 673, steps per second:  27, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.010181, mae: 2.040412, mean_q: 2.500332, mean_eps: 0.968355
  88866/250000: episode: 130, duration: 25.734s, episode steps: 627, steps per second:  24, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.011251, mae: 2.045698, mean_q: 2.505723, mean_eps: 0.968121
  89434/250000: episode: 131, duration: 21.623s, episode steps: 568, steps per second:  26, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.009305, mae: 2.026566, mean_q: 2.481110, mean_eps: 0.967906
  89931/250000: episode: 132, duration: 20.672s, episode steps: 497, steps per second:  24, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.012969, mae: 2.051913, mean_q: 2.509267, mean_eps: 0.967714
  90502/250000: episode: 133, duration: 22.767s, episode steps: 571, steps per second:  25, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.011480, mae: 2.022095, mean_q: 2.474816, mean_eps: 0.967522
  91251/250000: episode: 134, duration: 33.478s, episode steps: 749, steps per second:  22, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.010553, mae: 2.043561, mean_q: 2.502301, mean_eps: 0.967285
  91787/250000: episode: 135, duration: 23.885s, episode steps: 536, steps per second:  22, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.011557, mae: 2.012916, mean_q: 2.461477, mean_eps: 0.967054
  92545/250000: episode: 136, duration: 30.008s, episode steps: 758, steps per second:  25, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.011300, mae: 2.027768, mean_q: 2.481509, mean_eps: 0.966820
  93183/250000: episode: 137, duration: 24.962s, episode steps: 638, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.009988, mae: 2.029657, mean_q: 2.485163, mean_eps: 0.966569
  93657/250000: episode: 138, duration: 19.299s, episode steps: 474, steps per second:  25, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.011082, mae: 2.016435, mean_q: 2.469736, mean_eps: 0.966369
  94027/250000: episode: 139, duration: 14.664s, episode steps: 370, steps per second:  25, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.011571, mae: 2.025194, mean_q: 2.479465, mean_eps: 0.966217
  94700/250000: episode: 140, duration: 26.791s, episode steps: 673, steps per second:  25, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.011421, mae: 2.035542, mean_q: 2.494068, mean_eps: 0.966030
  95075/250000: episode: 141, duration: 15.533s, episode steps: 375, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.008597, mae: 2.026128, mean_q: 2.482259, mean_eps: 0.965841
  95864/250000: episode: 142, duration: 34.103s, episode steps: 789, steps per second:  23, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011167, mae: 2.028357, mean_q: 2.482183, mean_eps: 0.965632
  96562/250000: episode: 143, duration: 27.272s, episode steps: 698, steps per second:  26, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.008804, mae: 2.033282, mean_q: 2.489987, mean_eps: 0.965364
  97202/250000: episode: 144, duration: 25.665s, episode steps: 640, steps per second:  25, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.010924, mae: 2.013794, mean_q: 2.464353, mean_eps: 0.965122
  97886/250000: episode: 145, duration: 26.750s, episode steps: 684, steps per second:  26, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.010611, mae: 2.032564, mean_q: 2.487119, mean_eps: 0.964884
  98748/250000: episode: 146, duration: 35.013s, episode steps: 862, steps per second:  25, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.009810, mae: 2.037953, mean_q: 2.494042, mean_eps: 0.964606
  99557/250000: episode: 147, duration: 31.633s, episode steps: 809, steps per second:  26, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.011754, mae: 2.033187, mean_q: 2.490002, mean_eps: 0.964305
 100503/250000: episode: 148, duration: 38.283s, episode steps: 946, steps per second:  25, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.011423, mae: 2.052007, mean_q: 2.510250, mean_eps: 0.963989
 101138/250000: episode: 149, duration: 24.679s, episode steps: 635, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.011003, mae: 2.070623, mean_q: 2.532309, mean_eps: 0.963705
 101694/250000: episode: 150, duration: 21.674s, episode steps: 556, steps per second:  26, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.011641, mae: 2.058136, mean_q: 2.517590, mean_eps: 0.963490
 102303/250000: episode: 151, duration: 24.747s, episode steps: 609, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.010498, mae: 2.049712, mean_q: 2.507321, mean_eps: 0.963281
 103094/250000: episode: 152, duration: 33.042s, episode steps: 791, steps per second:  24, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.010637, mae: 2.065246, mean_q: 2.525771, mean_eps: 0.963029
 103659/250000: episode: 153, duration: 25.367s, episode steps: 565, steps per second:  22, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.010517, mae: 2.045884, mean_q: 2.500894, mean_eps: 0.962785
 104406/250000: episode: 154, duration: 27.827s, episode steps: 747, steps per second:  27, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.011076, mae: 2.042328, mean_q: 2.498111, mean_eps: 0.962548
 104950/250000: episode: 155, duration: 21.844s, episode steps: 544, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.012117, mae: 2.069624, mean_q: 2.531893, mean_eps: 0.962316
 105779/250000: episode: 156, duration: 31.955s, episode steps: 829, steps per second:  26, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.010939, mae: 2.057805, mean_q: 2.518332, mean_eps: 0.962069
 106430/250000: episode: 157, duration: 25.030s, episode steps: 651, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.013092, mae: 2.046550, mean_q: 2.502812, mean_eps: 0.961803
 107151/250000: episode: 158, duration: 28.701s, episode steps: 721, steps per second:  25, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.011333, mae: 2.055294, mean_q: 2.514622, mean_eps: 0.961556
 107525/250000: episode: 159, duration: 14.098s, episode steps: 374, steps per second:  27, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011541, mae: 2.050100, mean_q: 2.507847, mean_eps: 0.961358
 108711/250000: episode: 160, duration: 46.617s, episode steps: 1186, steps per second:  25, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.011278, mae: 2.071862, mean_q: 2.535017, mean_eps: 0.961078
 109652/250000: episode: 161, duration: 37.892s, episode steps: 941, steps per second:  25, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.012353, mae: 2.068371, mean_q: 2.528963, mean_eps: 0.960695
 110509/250000: episode: 162, duration: 40.587s, episode steps: 857, steps per second:  21, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.011494, mae: 2.064156, mean_q: 2.524563, mean_eps: 0.960371
 111008/250000: episode: 163, duration: 20.136s, episode steps: 499, steps per second:  25, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.011844, mae: 2.040687, mean_q: 2.496025, mean_eps: 0.960127
 111953/250000: episode: 164, duration: 37.736s, episode steps: 945, steps per second:  25, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.009758, mae: 2.061149, mean_q: 2.520837, mean_eps: 0.959867
 112515/250000: episode: 165, duration: 23.043s, episode steps: 562, steps per second:  24, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012179, mae: 2.053261, mean_q: 2.511817, mean_eps: 0.959596
 112903/250000: episode: 166, duration: 15.438s, episode steps: 388, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012062, mae: 2.077583, mean_q: 2.541508, mean_eps: 0.959425
 113516/250000: episode: 167, duration: 24.939s, episode steps: 613, steps per second:  25, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.011317, mae: 2.059744, mean_q: 2.520700, mean_eps: 0.959245
 114053/250000: episode: 168, duration: 21.829s, episode steps: 537, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.011265, mae: 2.057117, mean_q: 2.517239, mean_eps: 0.959038
 114732/250000: episode: 169, duration: 26.016s, episode steps: 679, steps per second:  26, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.011740, mae: 2.053729, mean_q: 2.512159, mean_eps: 0.958819
 115190/250000: episode: 170, duration: 19.078s, episode steps: 458, steps per second:  24, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.009226, mae: 2.057090, mean_q: 2.515602, mean_eps: 0.958614
 116414/250000: episode: 171, duration: 48.716s, episode steps: 1224, steps per second:  25, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.010512, mae: 2.055849, mean_q: 2.516150, mean_eps: 0.958311
 116792/250000: episode: 172, duration: 15.557s, episode steps: 378, steps per second:  24, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013420, mae: 2.091206, mean_q: 2.555467, mean_eps: 0.958023
 117305/250000: episode: 173, duration: 20.168s, episode steps: 513, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.011413, mae: 2.063240, mean_q: 2.522287, mean_eps: 0.957863
 117656/250000: episode: 174, duration: 13.333s, episode steps: 351, steps per second:  26, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.012432, mae: 2.060412, mean_q: 2.518178, mean_eps: 0.957707
 118642/250000: episode: 175, duration: 39.637s, episode steps: 986, steps per second:  25, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.011471, mae: 2.054407, mean_q: 2.512215, mean_eps: 0.957467
 119039/250000: episode: 176, duration: 15.246s, episode steps: 397, steps per second:  26, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.012724, mae: 2.086401, mean_q: 2.551316, mean_eps: 0.957218
 119691/250000: episode: 177, duration: 26.642s, episode steps: 652, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.010528, mae: 2.043599, mean_q: 2.500947, mean_eps: 0.957029
 120467/250000: episode: 178, duration: 31.105s, episode steps: 776, steps per second:  25, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.010851, mae: 2.065538, mean_q: 2.526882, mean_eps: 0.956772
 121091/250000: episode: 179, duration: 23.665s, episode steps: 624, steps per second:  26, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.011871, mae: 2.101172, mean_q: 2.569704, mean_eps: 0.956520
 121737/250000: episode: 180, duration: 26.994s, episode steps: 646, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.010934, mae: 2.059108, mean_q: 2.517864, mean_eps: 0.956291
 122531/250000: episode: 181, duration: 30.925s, episode steps: 794, steps per second:  26, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.011434, mae: 2.071097, mean_q: 2.532305, mean_eps: 0.956032
 123118/250000: episode: 182, duration: 23.189s, episode steps: 587, steps per second:  25, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.010868, mae: 2.068787, mean_q: 2.528841, mean_eps: 0.955783
 123629/250000: episode: 183, duration: 20.027s, episode steps: 511, steps per second:  26, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.010239, mae: 2.077695, mean_q: 2.540553, mean_eps: 0.955585
 124278/250000: episode: 184, duration: 25.257s, episode steps: 649, steps per second:  26, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.011921, mae: 2.054420, mean_q: 2.511007, mean_eps: 0.955377
 124930/250000: episode: 185, duration: 26.906s, episode steps: 652, steps per second:  24, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.010307, mae: 2.055923, mean_q: 2.514828, mean_eps: 0.955143
 125623/250000: episode: 186, duration: 35.384s, episode steps: 693, steps per second:  20, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.010295, mae: 2.061399, mean_q: 2.522028, mean_eps: 0.954901
 126247/250000: episode: 187, duration: 25.866s, episode steps: 624, steps per second:  24, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.010289, mae: 2.062078, mean_q: 2.521393, mean_eps: 0.954664
 127179/250000: episode: 188, duration: 36.611s, episode steps: 932, steps per second:  25, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.010011, mae: 2.056332, mean_q: 2.512735, mean_eps: 0.954384
 127866/250000: episode: 189, duration: 26.952s, episode steps: 687, steps per second:  25, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.011825, mae: 2.076838, mean_q: 2.540011, mean_eps: 0.954092
 128655/250000: episode: 190, duration: 30.768s, episode steps: 789, steps per second:  26, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.011412, mae: 2.040722, mean_q: 2.495330, mean_eps: 0.953826
 129314/250000: episode: 191, duration: 25.260s, episode steps: 659, steps per second:  26, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.011485, mae: 2.065216, mean_q: 2.525150, mean_eps: 0.953566
 130432/250000: episode: 192, duration: 44.901s, episode steps: 1118, steps per second:  25, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.011313, mae: 2.055550, mean_q: 2.511074, mean_eps: 0.953246
 131295/250000: episode: 193, duration: 33.963s, episode steps: 863, steps per second:  25, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.011714, mae: 2.083142, mean_q: 2.546568, mean_eps: 0.952890
 132112/250000: episode: 194, duration: 32.953s, episode steps: 817, steps per second:  25, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010564, mae: 2.061329, mean_q: 2.520431, mean_eps: 0.952587
 132702/250000: episode: 195, duration: 23.549s, episode steps: 590, steps per second:  25, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.010573, mae: 2.051421, mean_q: 2.508385, mean_eps: 0.952334
 133918/250000: episode: 196, duration: 52.020s, episode steps: 1216, steps per second:  23, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.011804, mae: 2.060600, mean_q: 2.518113, mean_eps: 0.952008
 134593/250000: episode: 197, duration: 28.273s, episode steps: 675, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.009738, mae: 2.038527, mean_q: 2.494253, mean_eps: 0.951668
 135073/250000: episode: 198, duration: 19.467s, episode steps: 480, steps per second:  25, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.010908, mae: 2.055657, mean_q: 2.513890, mean_eps: 0.951460
 135524/250000: episode: 199, duration: 18.710s, episode steps: 451, steps per second:  24, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.011373, mae: 2.031641, mean_q: 2.484235, mean_eps: 0.951293
 136381/250000: episode: 200, duration: 34.958s, episode steps: 857, steps per second:  25, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011321, mae: 2.036611, mean_q: 2.489954, mean_eps: 0.951057
 137076/250000: episode: 201, duration: 27.468s, episode steps: 695, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012559, mae: 2.056797, mean_q: 2.515260, mean_eps: 0.950778
 137729/250000: episode: 202, duration: 26.781s, episode steps: 653, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.009373, mae: 2.060775, mean_q: 2.520225, mean_eps: 0.950535
 138505/250000: episode: 203, duration: 31.055s, episode steps: 776, steps per second:  25, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.010224, mae: 2.066566, mean_q: 2.525059, mean_eps: 0.950278
 139119/250000: episode: 204, duration: 24.369s, episode steps: 614, steps per second:  25, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010885, mae: 2.039764, mean_q: 2.494020, mean_eps: 0.950028
 139869/250000: episode: 205, duration: 30.226s, episode steps: 750, steps per second:  25, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.010733, mae: 2.042836, mean_q: 2.497148, mean_eps: 0.949782
 140369/250000: episode: 206, duration: 24.209s, episode steps: 500, steps per second:  21, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.011831, mae: 2.042880, mean_q: 2.496227, mean_eps: 0.949557
 140770/250000: episode: 207, duration: 15.916s, episode steps: 401, steps per second:  25, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.012258, mae: 2.056338, mean_q: 2.511594, mean_eps: 0.949395
 142021/250000: episode: 208, duration: 48.952s, episode steps: 1251, steps per second:  26, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.012432, mae: 2.060585, mean_q: 2.517826, mean_eps: 0.949097
 142563/250000: episode: 209, duration: 22.422s, episode steps: 542, steps per second:  24, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.009715, mae: 2.042922, mean_q: 2.498192, mean_eps: 0.948775
 142946/250000: episode: 210, duration: 15.087s, episode steps: 383, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.012511, mae: 2.052213, mean_q: 2.507117, mean_eps: 0.948609
 143538/250000: episode: 211, duration: 22.269s, episode steps: 592, steps per second:  27, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.009111, mae: 2.039025, mean_q: 2.492940, mean_eps: 0.948433
 143965/250000: episode: 212, duration: 16.436s, episode steps: 427, steps per second:  26, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.010316, mae: 2.063621, mean_q: 2.522253, mean_eps: 0.948249
 144792/250000: episode: 213, duration: 42.540s, episode steps: 827, steps per second:  19, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.011720, mae: 2.035675, mean_q: 2.485301, mean_eps: 0.948024
 145595/250000: episode: 214, duration: 48.770s, episode steps: 803, steps per second:  16, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.009572, mae: 2.054658, mean_q: 2.510929, mean_eps: 0.947731
 146010/250000: episode: 215, duration: 23.673s, episode steps: 415, steps per second:  18, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.010382, mae: 2.043177, mean_q: 2.495796, mean_eps: 0.947511
 147146/250000: episode: 216, duration: 50.371s, episode steps: 1136, steps per second:  23, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.010396, mae: 2.055932, mean_q: 2.513923, mean_eps: 0.947232
 147882/250000: episode: 217, duration: 28.635s, episode steps: 736, steps per second:  26, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.011351, mae: 2.043570, mean_q: 2.496506, mean_eps: 0.946895
 148830/250000: episode: 218, duration: 39.914s, episode steps: 948, steps per second:  24, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.011980, mae: 2.046163, mean_q: 2.500669, mean_eps: 0.946592
 149407/250000: episode: 219, duration: 21.944s, episode steps: 577, steps per second:  26, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.010942, mae: 2.046149, mean_q: 2.501705, mean_eps: 0.946318
 149950/250000: episode: 220, duration: 22.870s, episode steps: 543, steps per second:  24, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.010538, mae: 2.068442, mean_q: 2.527528, mean_eps: 0.946116
 151382/250000: episode: 221, duration: 56.779s, episode steps: 1432, steps per second:  25, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.012400, mae: 2.081950, mean_q: 2.544606, mean_eps: 0.945760
 151847/250000: episode: 222, duration: 19.762s, episode steps: 465, steps per second:  24, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.014527, mae: 2.126980, mean_q: 2.596520, mean_eps: 0.945419
 153123/250000: episode: 223, duration: 51.670s, episode steps: 1276, steps per second:  25, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012732, mae: 2.084317, mean_q: 2.545284, mean_eps: 0.945106
 153790/250000: episode: 224, duration: 26.106s, episode steps: 667, steps per second:  26, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.013379, mae: 2.079685, mean_q: 2.541189, mean_eps: 0.944756
 154585/250000: episode: 225, duration: 36.605s, episode steps: 795, steps per second:  22, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.011892, mae: 2.073525, mean_q: 2.533566, mean_eps: 0.944492
 155462/250000: episode: 226, duration: 36.601s, episode steps: 877, steps per second:  24, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.014212, mae: 2.114882, mean_q: 2.584238, mean_eps: 0.944191
 155910/250000: episode: 227, duration: 17.532s, episode steps: 448, steps per second:  26, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013032, mae: 2.091887, mean_q: 2.557254, mean_eps: 0.943953
 156419/250000: episode: 228, duration: 20.552s, episode steps: 509, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.012120, mae: 2.098938, mean_q: 2.563020, mean_eps: 0.943781
 157051/250000: episode: 229, duration: 25.322s, episode steps: 632, steps per second:  25, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.012642, mae: 2.115898, mean_q: 2.585384, mean_eps: 0.943576
 157448/250000: episode: 230, duration: 15.736s, episode steps: 397, steps per second:  25, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.012874, mae: 2.096532, mean_q: 2.559816, mean_eps: 0.943391
 158086/250000: episode: 231, duration: 25.740s, episode steps: 638, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.011364, mae: 2.106556, mean_q: 2.572075, mean_eps: 0.943204
 158935/250000: episode: 232, duration: 33.501s, episode steps: 849, steps per second:  25, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.011646, mae: 2.084444, mean_q: 2.546567, mean_eps: 0.942936
 159491/250000: episode: 233, duration: 23.218s, episode steps: 556, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.010824, mae: 2.107760, mean_q: 2.575268, mean_eps: 0.942684
 159997/250000: episode: 234, duration: 19.938s, episode steps: 506, steps per second:  25, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.013595, mae: 2.078138, mean_q: 2.537974, mean_eps: 0.942492
 160722/250000: episode: 235, duration: 28.933s, episode steps: 725, steps per second:  25, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012480, mae: 2.064730, mean_q: 2.522633, mean_eps: 0.942270
 161396/250000: episode: 236, duration: 28.470s, episode steps: 674, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.013715, mae: 2.091131, mean_q: 2.554220, mean_eps: 0.942019
 161929/250000: episode: 237, duration: 20.842s, episode steps: 533, steps per second:  26, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.011169, mae: 2.109924, mean_q: 2.577869, mean_eps: 0.941802
 162567/250000: episode: 238, duration: 24.848s, episode steps: 638, steps per second:  26, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.009392, mae: 2.075371, mean_q: 2.537642, mean_eps: 0.941591
 162931/250000: episode: 239, duration: 15.472s, episode steps: 364, steps per second:  24, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.013881, mae: 2.123198, mean_q: 2.594703, mean_eps: 0.941411
 163591/250000: episode: 240, duration: 26.011s, episode steps: 660, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012220, mae: 2.108981, mean_q: 2.576867, mean_eps: 0.941226
 163978/250000: episode: 241, duration: 15.407s, episode steps: 387, steps per second:  25, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012493, mae: 2.086443, mean_q: 2.546407, mean_eps: 0.941038
 164544/250000: episode: 242, duration: 23.885s, episode steps: 566, steps per second:  24, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.014121, mae: 2.077217, mean_q: 2.534626, mean_eps: 0.940866
 164944/250000: episode: 243, duration: 16.428s, episode steps: 400, steps per second:  24, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.012797, mae: 2.090895, mean_q: 2.551437, mean_eps: 0.940693
 165872/250000: episode: 244, duration: 35.895s, episode steps: 928, steps per second:  26, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012924, mae: 2.089457, mean_q: 2.552661, mean_eps: 0.940454
 166812/250000: episode: 245, duration: 38.286s, episode steps: 940, steps per second:  25, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.010935, mae: 2.061614, mean_q: 2.517865, mean_eps: 0.940118
 167423/250000: episode: 246, duration: 24.798s, episode steps: 611, steps per second:  25, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.013255, mae: 2.098719, mean_q: 2.562956, mean_eps: 0.939838
 167819/250000: episode: 247, duration: 17.157s, episode steps: 396, steps per second:  23, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.010793, mae: 2.066979, mean_q: 2.522333, mean_eps: 0.939657
 168364/250000: episode: 248, duration: 21.470s, episode steps: 545, steps per second:  25, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.010162, mae: 2.068065, mean_q: 2.524617, mean_eps: 0.939488
 168766/250000: episode: 249, duration: 17.035s, episode steps: 402, steps per second:  24, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.011553, mae: 2.088872, mean_q: 2.551432, mean_eps: 0.939317
 169392/250000: episode: 250, duration: 28.369s, episode steps: 626, steps per second:  22, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012490, mae: 2.095289, mean_q: 2.558258, mean_eps: 0.939132
 170381/250000: episode: 251, duration: 39.445s, episode steps: 989, steps per second:  25, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.012867, mae: 2.086488, mean_q: 2.546308, mean_eps: 0.938841
 171041/250000: episode: 252, duration: 26.580s, episode steps: 660, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.011377, mae: 2.108561, mean_q: 2.576145, mean_eps: 0.938544
 171427/250000: episode: 253, duration: 15.190s, episode steps: 386, steps per second:  25, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.011419, mae: 2.094889, mean_q: 2.557504, mean_eps: 0.938356
 172051/250000: episode: 254, duration: 25.763s, episode steps: 624, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012526, mae: 2.097186, mean_q: 2.559824, mean_eps: 0.938174
 172691/250000: episode: 255, duration: 26.873s, episode steps: 640, steps per second:  24, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011302, mae: 2.096278, mean_q: 2.561047, mean_eps: 0.937947
 173370/250000: episode: 256, duration: 27.554s, episode steps: 679, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.011300, mae: 2.087974, mean_q: 2.549082, mean_eps: 0.937709
 173884/250000: episode: 257, duration: 20.183s, episode steps: 514, steps per second:  25, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.012462, mae: 2.073573, mean_q: 2.533790, mean_eps: 0.937495
 174786/250000: episode: 258, duration: 37.864s, episode steps: 902, steps per second:  24, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.014055, mae: 2.079843, mean_q: 2.538386, mean_eps: 0.937240
 175318/250000: episode: 259, duration: 20.369s, episode steps: 532, steps per second:  26, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.013869, mae: 2.063891, mean_q: 2.520424, mean_eps: 0.936981
 176027/250000: episode: 260, duration: 30.627s, episode steps: 709, steps per second:  23, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.012232, mae: 2.066901, mean_q: 2.523689, mean_eps: 0.936758
 177418/250000: episode: 261, duration: 57.575s, episode steps: 1391, steps per second:  24, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.010763, mae: 2.070746, mean_q: 2.530599, mean_eps: 0.936380
 178066/250000: episode: 262, duration: 26.544s, episode steps: 648, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.012722, mae: 2.081723, mean_q: 2.541006, mean_eps: 0.936013
 178601/250000: episode: 263, duration: 21.701s, episode steps: 535, steps per second:  25, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012550, mae: 2.088219, mean_q: 2.551326, mean_eps: 0.935800
 179290/250000: episode: 264, duration: 29.524s, episode steps: 689, steps per second:  23, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.012210, mae: 2.064151, mean_q: 2.520023, mean_eps: 0.935579
 179972/250000: episode: 265, duration: 27.263s, episode steps: 682, steps per second:  25, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.011563, mae: 2.086219, mean_q: 2.547255, mean_eps: 0.935333
 180505/250000: episode: 266, duration: 22.667s, episode steps: 533, steps per second:  24, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.012266, mae: 2.083846, mean_q: 2.546567, mean_eps: 0.935114
 181033/250000: episode: 267, duration: 22.337s, episode steps: 528, steps per second:  24, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.013152, mae: 2.087184, mean_q: 2.550504, mean_eps: 0.934923
 181532/250000: episode: 268, duration: 19.244s, episode steps: 499, steps per second:  26, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.011343, mae: 2.057766, mean_q: 2.512020, mean_eps: 0.934738
 182162/250000: episode: 269, duration: 26.945s, episode steps: 630, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.010701, mae: 2.082188, mean_q: 2.542774, mean_eps: 0.934535
 182679/250000: episode: 270, duration: 20.402s, episode steps: 517, steps per second:  25, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.011261, mae: 2.081374, mean_q: 2.542810, mean_eps: 0.934329
 183197/250000: episode: 271, duration: 19.733s, episode steps: 518, steps per second:  26, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010401, mae: 2.086615, mean_q: 2.547443, mean_eps: 0.934142
 183884/250000: episode: 272, duration: 33.499s, episode steps: 687, steps per second:  21, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.012150, mae: 2.046748, mean_q: 2.496879, mean_eps: 0.933926
 184403/250000: episode: 273, duration: 21.261s, episode steps: 519, steps per second:  24, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.011612, mae: 2.055328, mean_q: 2.509221, mean_eps: 0.933709
 185236/250000: episode: 274, duration: 34.188s, episode steps: 833, steps per second:  24, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.011234, mae: 2.074749, mean_q: 2.532426, mean_eps: 0.933466
 186053/250000: episode: 275, duration: 32.823s, episode steps: 817, steps per second:  25, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.011570, mae: 2.056527, mean_q: 2.511132, mean_eps: 0.933168
 186595/250000: episode: 276, duration: 21.986s, episode steps: 542, steps per second:  25, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012430, mae: 2.051369, mean_q: 2.503538, mean_eps: 0.932923
 187039/250000: episode: 277, duration: 19.545s, episode steps: 444, steps per second:  23, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.011475, mae: 2.074282, mean_q: 2.532517, mean_eps: 0.932746
 187676/250000: episode: 278, duration: 25.154s, episode steps: 637, steps per second:  25, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.010829, mae: 2.085300, mean_q: 2.548389, mean_eps: 0.932552
 188228/250000: episode: 279, duration: 22.946s, episode steps: 552, steps per second:  24, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.009695, mae: 2.069562, mean_q: 2.524419, mean_eps: 0.932338
 189057/250000: episode: 280, duration: 34.714s, episode steps: 829, steps per second:  24, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.011758, mae: 2.078427, mean_q: 2.535961, mean_eps: 0.932089
 189801/250000: episode: 281, duration: 28.928s, episode steps: 744, steps per second:  26, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.010132, mae: 2.085434, mean_q: 2.546714, mean_eps: 0.931805
 190291/250000: episode: 282, duration: 21.348s, episode steps: 490, steps per second:  23, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012515, mae: 2.073339, mean_q: 2.530476, mean_eps: 0.931583
 190853/250000: episode: 283, duration: 21.911s, episode steps: 562, steps per second:  26, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.011287, mae: 2.085855, mean_q: 2.545442, mean_eps: 0.931394
 191511/250000: episode: 284, duration: 27.444s, episode steps: 658, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.012101, mae: 2.056205, mean_q: 2.509037, mean_eps: 0.931174
 192010/250000: episode: 285, duration: 20.270s, episode steps: 499, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.009582, mae: 2.085733, mean_q: 2.545847, mean_eps: 0.930966
 192645/250000: episode: 286, duration: 25.641s, episode steps: 635, steps per second:  25, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.011013, mae: 2.083508, mean_q: 2.544273, mean_eps: 0.930762
 193166/250000: episode: 287, duration: 20.797s, episode steps: 521, steps per second:  25, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011743, mae: 2.071213, mean_q: 2.529758, mean_eps: 0.930554
 193642/250000: episode: 288, duration: 18.844s, episode steps: 476, steps per second:  25, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.009787, mae: 2.045516, mean_q: 2.494887, mean_eps: 0.930375
 194351/250000: episode: 289, duration: 28.478s, episode steps: 709, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012027, mae: 2.072740, mean_q: 2.528913, mean_eps: 0.930161
 194954/250000: episode: 290, duration: 25.521s, episode steps: 603, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.011165, mae: 2.052744, mean_q: 2.505283, mean_eps: 0.929925
 195472/250000: episode: 291, duration: 20.867s, episode steps: 518, steps per second:  25, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.010510, mae: 2.074563, mean_q: 2.533017, mean_eps: 0.929724
 195896/250000: episode: 292, duration: 16.709s, episode steps: 424, steps per second:  25, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.010790, mae: 2.074894, mean_q: 2.533456, mean_eps: 0.929554
 196733/250000: episode: 293, duration: 34.325s, episode steps: 837, steps per second:  24, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.012303, mae: 2.064995, mean_q: 2.519626, mean_eps: 0.929327
 197534/250000: episode: 294, duration: 31.945s, episode steps: 801, steps per second:  25, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.010976, mae: 2.058188, mean_q: 2.511772, mean_eps: 0.929032
 198042/250000: episode: 295, duration: 20.795s, episode steps: 508, steps per second:  24, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.011084, mae: 2.070805, mean_q: 2.526857, mean_eps: 0.928796
 198671/250000: episode: 296, duration: 29.044s, episode steps: 629, steps per second:  22, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.011725, mae: 2.069384, mean_q: 2.524233, mean_eps: 0.928592
 199207/250000: episode: 297, duration: 22.772s, episode steps: 536, steps per second:  24, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.012801, mae: 2.079664, mean_q: 2.538381, mean_eps: 0.928382
 199868/250000: episode: 298, duration: 27.470s, episode steps: 661, steps per second:  24, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.013151, mae: 2.060194, mean_q: 2.513620, mean_eps: 0.928167
 200565/250000: episode: 299, duration: 28.469s, episode steps: 697, steps per second:  24, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.012505, mae: 2.090886, mean_q: 2.551675, mean_eps: 0.927922
 201249/250000: episode: 300, duration: 28.271s, episode steps: 684, steps per second:  24, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.011184, mae: 2.089307, mean_q: 2.548803, mean_eps: 0.927673
 202369/250000: episode: 301, duration: 45.160s, episode steps: 1120, steps per second:  25, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.011439, mae: 2.110472, mean_q: 2.575699, mean_eps: 0.927348
 203060/250000: episode: 302, duration: 29.128s, episode steps: 691, steps per second:  24, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.013595, mae: 2.086375, mean_q: 2.546060, mean_eps: 0.927023
 203670/250000: episode: 303, duration: 24.138s, episode steps: 610, steps per second:  25, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.011828, mae: 2.100063, mean_q: 2.564041, mean_eps: 0.926789
 204304/250000: episode: 304, duration: 25.935s, episode steps: 634, steps per second:  24, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.011627, mae: 2.082863, mean_q: 2.541724, mean_eps: 0.926565
 204857/250000: episode: 305, duration: 22.856s, episode steps: 553, steps per second:  24, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.011184, mae: 2.084107, mean_q: 2.544561, mean_eps: 0.926351
 205561/250000: episode: 306, duration: 28.280s, episode steps: 704, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.011571, mae: 2.095733, mean_q: 2.556644, mean_eps: 0.926124
 206234/250000: episode: 307, duration: 28.564s, episode steps: 673, steps per second:  24, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.014584, mae: 2.095493, mean_q: 2.557047, mean_eps: 0.925877
 206839/250000: episode: 308, duration: 23.558s, episode steps: 605, steps per second:  26, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.011151, mae: 2.079090, mean_q: 2.538717, mean_eps: 0.925647
 207515/250000: episode: 309, duration: 27.206s, episode steps: 676, steps per second:  25, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.012311, mae: 2.108384, mean_q: 2.572361, mean_eps: 0.925417
 208123/250000: episode: 310, duration: 25.834s, episode steps: 608, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011932, mae: 2.098393, mean_q: 2.561506, mean_eps: 0.925186
 208772/250000: episode: 311, duration: 25.588s, episode steps: 649, steps per second:  25, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.012598, mae: 2.084684, mean_q: 2.542747, mean_eps: 0.924959
 209420/250000: episode: 312, duration: 28.068s, episode steps: 648, steps per second:  23, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.012027, mae: 2.091852, mean_q: 2.552878, mean_eps: 0.924726
 209962/250000: episode: 313, duration: 21.301s, episode steps: 542, steps per second:  25, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011174, mae: 2.116145, mean_q: 2.583512, mean_eps: 0.924512
 210662/250000: episode: 314, duration: 28.222s, episode steps: 700, steps per second:  25, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.011597, mae: 2.105056, mean_q: 2.570216, mean_eps: 0.924288
 211281/250000: episode: 315, duration: 26.635s, episode steps: 619, steps per second:  23, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.011361, mae: 2.095564, mean_q: 2.557693, mean_eps: 0.924050
 211992/250000: episode: 316, duration: 27.476s, episode steps: 711, steps per second:  26, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.013148, mae: 2.086307, mean_q: 2.545601, mean_eps: 0.923811
 212384/250000: episode: 317, duration: 16.226s, episode steps: 392, steps per second:  24, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.012653, mae: 2.095712, mean_q: 2.559068, mean_eps: 0.923613
 213353/250000: episode: 318, duration: 46.355s, episode steps: 969, steps per second:  21, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.010993, mae: 2.098455, mean_q: 2.564918, mean_eps: 0.923368
 213724/250000: episode: 319, duration: 14.950s, episode steps: 371, steps per second:  25, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.011290, mae: 2.090674, mean_q: 2.553605, mean_eps: 0.923126
 214241/250000: episode: 320, duration: 22.833s, episode steps: 517, steps per second:  23, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012397, mae: 2.078853, mean_q: 2.537377, mean_eps: 0.922966
 214840/250000: episode: 321, duration: 23.670s, episode steps: 599, steps per second:  25, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.013004, mae: 2.108360, mean_q: 2.571428, mean_eps: 0.922766
 215489/250000: episode: 322, duration: 29.997s, episode steps: 649, steps per second:  22, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.011669, mae: 2.065182, mean_q: 2.521034, mean_eps: 0.922541
 215991/250000: episode: 323, duration: 20.899s, episode steps: 502, steps per second:  24, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.011492, mae: 2.087696, mean_q: 2.547143, mean_eps: 0.922334
 216438/250000: episode: 324, duration: 17.705s, episode steps: 447, steps per second:  25, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.012711, mae: 2.071524, mean_q: 2.526087, mean_eps: 0.922163
 216995/250000: episode: 325, duration: 22.538s, episode steps: 557, steps per second:  25, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012870, mae: 2.100843, mean_q: 2.560415, mean_eps: 0.921982
 217918/250000: episode: 326, duration: 37.768s, episode steps: 923, steps per second:  24, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.012089, mae: 2.099590, mean_q: 2.560559, mean_eps: 0.921716
 218437/250000: episode: 327, duration: 25.656s, episode steps: 519, steps per second:  20, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.010545, mae: 2.117237, mean_q: 2.581712, mean_eps: 0.921456
 218845/250000: episode: 328, duration: 17.060s, episode steps: 408, steps per second:  24, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.010316, mae: 2.106168, mean_q: 2.565766, mean_eps: 0.921289
 219861/250000: episode: 329, duration: 40.184s, episode steps: 1016, steps per second:  25, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.012498, mae: 2.083157, mean_q: 2.538593, mean_eps: 0.921033
 220454/250000: episode: 330, duration: 25.443s, episode steps: 593, steps per second:  23, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.013130, mae: 2.079613, mean_q: 2.535254, mean_eps: 0.920743
 221247/250000: episode: 331, duration: 36.039s, episode steps: 793, steps per second:  22, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.012046, mae: 2.105519, mean_q: 2.566922, mean_eps: 0.920494
 221935/250000: episode: 332, duration: 29.528s, episode steps: 688, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.010986, mae: 2.096955, mean_q: 2.559215, mean_eps: 0.920228
 222681/250000: episode: 333, duration: 37.763s, episode steps: 746, steps per second:  20, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.012784, mae: 2.091888, mean_q: 2.550935, mean_eps: 0.919969
 223323/250000: episode: 334, duration: 31.846s, episode steps: 642, steps per second:  20, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014427, mae: 2.102682, mean_q: 2.563625, mean_eps: 0.919719
 223952/250000: episode: 335, duration: 31.351s, episode steps: 629, steps per second:  20, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.013934, mae: 2.100120, mean_q: 2.561501, mean_eps: 0.919491
 224726/250000: episode: 336, duration: 32.335s, episode steps: 774, steps per second:  24, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.013538, mae: 2.089184, mean_q: 2.549289, mean_eps: 0.919238
 225635/250000: episode: 337, duration: 37.821s, episode steps: 909, steps per second:  24, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.012216, mae: 2.087025, mean_q: 2.544371, mean_eps: 0.918935
 226253/250000: episode: 338, duration: 24.772s, episode steps: 618, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.013584, mae: 2.114322, mean_q: 2.579614, mean_eps: 0.918660
 227103/250000: episode: 339, duration: 40.035s, episode steps: 850, steps per second:  21, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.010911, mae: 2.110668, mean_q: 2.575616, mean_eps: 0.918396
 227607/250000: episode: 340, duration: 20.172s, episode steps: 504, steps per second:  25, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.014004, mae: 2.076748, mean_q: 2.533059, mean_eps: 0.918153
 228284/250000: episode: 341, duration: 29.397s, episode steps: 677, steps per second:  23, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.011288, mae: 2.080865, mean_q: 2.537835, mean_eps: 0.917940
 228882/250000: episode: 342, duration: 22.978s, episode steps: 598, steps per second:  26, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.011910, mae: 2.094811, mean_q: 2.555896, mean_eps: 0.917710
 229381/250000: episode: 343, duration: 22.081s, episode steps: 499, steps per second:  23, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.011176, mae: 2.086875, mean_q: 2.545873, mean_eps: 0.917512
 230025/250000: episode: 344, duration: 26.130s, episode steps: 644, steps per second:  25, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.012416, mae: 2.110028, mean_q: 2.573655, mean_eps: 0.917307
 230531/250000: episode: 345, duration: 20.783s, episode steps: 506, steps per second:  24, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011769, mae: 2.080691, mean_q: 2.538697, mean_eps: 0.917100
 231445/250000: episode: 346, duration: 38.162s, episode steps: 914, steps per second:  24, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.011959, mae: 2.100197, mean_q: 2.561568, mean_eps: 0.916844
 231818/250000: episode: 347, duration: 14.973s, episode steps: 373, steps per second:  25, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012389, mae: 2.129788, mean_q: 2.597706, mean_eps: 0.916612
 232488/250000: episode: 348, duration: 27.553s, episode steps: 670, steps per second:  24, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012174, mae: 2.110135, mean_q: 2.575655, mean_eps: 0.916425
 233304/250000: episode: 349, duration: 33.938s, episode steps: 816, steps per second:  24, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.013283, mae: 2.089309, mean_q: 2.549701, mean_eps: 0.916158
 234003/250000: episode: 350, duration: 27.177s, episode steps: 699, steps per second:  26, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.011705, mae: 2.080341, mean_q: 2.537119, mean_eps: 0.915885
 234690/250000: episode: 351, duration: 28.874s, episode steps: 687, steps per second:  24, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.010571, mae: 2.099366, mean_q: 2.560267, mean_eps: 0.915635
 235375/250000: episode: 352, duration: 27.875s, episode steps: 685, steps per second:  25, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.013166, mae: 2.090941, mean_q: 2.546878, mean_eps: 0.915388
 236433/250000: episode: 353, duration: 44.314s, episode steps: 1058, steps per second:  24, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.012934, mae: 2.079383, mean_q: 2.534936, mean_eps: 0.915075
 236924/250000: episode: 354, duration: 19.337s, episode steps: 491, steps per second:  25, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.011362, mae: 2.077075, mean_q: 2.533332, mean_eps: 0.914796
 237358/250000: episode: 355, duration: 18.301s, episode steps: 434, steps per second:  24, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.010210, mae: 2.103184, mean_q: 2.565068, mean_eps: 0.914630
 237972/250000: episode: 356, duration: 26.073s, episode steps: 614, steps per second:  24, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.011571, mae: 2.094565, mean_q: 2.553011, mean_eps: 0.914441
 238376/250000: episode: 357, duration: 16.454s, episode steps: 404, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.011601, mae: 2.113275, mean_q: 2.576628, mean_eps: 0.914258
 239732/250000: episode: 358, duration: 55.498s, episode steps: 1356, steps per second:  24, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012521, mae: 2.103346, mean_q: 2.565840, mean_eps: 0.913941
 240357/250000: episode: 359, duration: 25.778s, episode steps: 625, steps per second:  24, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.010738, mae: 2.102076, mean_q: 2.563970, mean_eps: 0.913584
 240770/250000: episode: 360, duration: 17.903s, episode steps: 413, steps per second:  23, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.011773, mae: 2.107844, mean_q: 2.570784, mean_eps: 0.913397
 241254/250000: episode: 361, duration: 22.484s, episode steps: 484, steps per second:  22, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.014225, mae: 2.088708, mean_q: 2.548731, mean_eps: 0.913236
 242615/250000: episode: 362, duration: 56.582s, episode steps: 1361, steps per second:  24, episode reward: 16.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.011411, mae: 2.085111, mean_q: 2.545059, mean_eps: 0.912904
 243241/250000: episode: 363, duration: 25.311s, episode steps: 626, steps per second:  25, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.011341, mae: 2.110563, mean_q: 2.574490, mean_eps: 0.912546
 244310/250000: episode: 364, duration: 44.144s, episode steps: 1069, steps per second:  24, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011329, mae: 2.083136, mean_q: 2.540302, mean_eps: 0.912241
 244731/250000: episode: 365, duration: 16.878s, episode steps: 421, steps per second:  25, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012077, mae: 2.114648, mean_q: 2.580429, mean_eps: 0.911973
 245608/250000: episode: 366, duration: 37.553s, episode steps: 877, steps per second:  23, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.014001, mae: 2.089583, mean_q: 2.547915, mean_eps: 0.911740
 246411/250000: episode: 367, duration: 32.566s, episode steps: 803, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.012071, mae: 2.096080, mean_q: 2.553819, mean_eps: 0.911437
 247029/250000: episode: 368, duration: 25.537s, episode steps: 618, steps per second:  24, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.011981, mae: 2.081041, mean_q: 2.537664, mean_eps: 0.911181
 247418/250000: episode: 369, duration: 15.934s, episode steps: 389, steps per second:  24, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.012211, mae: 2.121929, mean_q: 2.587719, mean_eps: 0.910999
 248214/250000: episode: 370, duration: 31.763s, episode steps: 796, steps per second:  25, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.013174, mae: 2.096731, mean_q: 2.555465, mean_eps: 0.910786
 248856/250000: episode: 371, duration: 27.749s, episode steps: 642, steps per second:  23, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.011495, mae: 2.097617, mean_q: 2.557323, mean_eps: 0.910528
 249469/250000: episode: 372, duration: 25.351s, episode steps: 613, steps per second:  24, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.012296, mae: 2.118110, mean_q: 2.579685, mean_eps: 0.910302
done, took 9873.939 seconds
########################################################
PROCESO TERMINADO
########################################################